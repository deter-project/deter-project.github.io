{
    "docs": [
        {
            "location": "/", 
            "text": "These pages constitute the end-user documentation for DETERLab documentation.\n\n\nWhat is DETERLab?\n\uf0c1\n\n\nDETERLab is a state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. To date, DETERLab-based projects have included behavior analysis and defensive technologies including DDoS attacks, worm and botnet attacks, encryption, pattern detection, and intrusion-tolerant storage protocols.\n\n\nImportant Links\n\uf0c1\n\n\n\n\nTestbed\n - this is the web interface to DETERLab (requires \nregistration\n)\n\n\nSupport\n - this website provides support for DETERLab using a ticketing system.\n\n\n\n\nOrganization\n\uf0c1\n\n\nDETERLab docs are organized by the dominant systems (Core, Orchestrator and Containers) and then each system includes:\n\n\n\n\nQuickstart\n - Gives a summary of the system and an overview of the steps involved to use it.\n\n\nGuide\n - Provides details on the most common, basic ways to use the system. It serves as a basic tutorial and reference.\n\n\nReference\n - Includes references to configuration, commands and other reference materials\n\n\nAd hoc topics\n - More advanced topics for more complicated features of the system.", 
            "title": "Welcome to the DETERLab Documentation"
        }, 
        {
            "location": "/#what-is-deterlab", 
            "text": "DETERLab is a state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. To date, DETERLab-based projects have included behavior analysis and defensive technologies including DDoS attacks, worm and botnet attacks, encryption, pattern detection, and intrusion-tolerant storage protocols.", 
            "title": "What is DETERLab?"
        }, 
        {
            "location": "/#important-links", 
            "text": "Testbed  - this is the web interface to DETERLab (requires  registration )  Support  - this website provides support for DETERLab using a ticketing system.", 
            "title": "Important Links"
        }, 
        {
            "location": "/#organization", 
            "text": "DETERLab docs are organized by the dominant systems (Core, Orchestrator and Containers) and then each system includes:   Quickstart  - Gives a summary of the system and an overview of the steps involved to use it.  Guide  - Provides details on the most common, basic ways to use the system. It serves as a basic tutorial and reference.  Reference  - Includes references to configuration, commands and other reference materials  Ad hoc topics  - More advanced topics for more complicated features of the system.", 
            "title": "Organization"
        }, 
        {
            "location": "/glossary/", 
            "text": "Agent Abstraction Language (AAL)\n\n  \nA YAML based descriptive language that the \nMAGI Orchestrator\n uses to describe an experiment\u2019s procedural workflow. The entire experiment procedure needs to be expressed as part of an AAL file. Find more information in the \nOrchestrator Guide\n.\n\n  \nagent\n\n  \nIn \nMAGI Orchestrator\n, a piece of code that instruments a given functional behavior. Find more information in the \nOrchestrator Guide\n.\n\n  \nBoss network (\nmyboss.isi.deterlab.net\n)\n\n  \nThe main testbed server that runs DETERLab. Users are not allowed to log directly into it.\n\n  \ncollectors/collector nodes\n\n  \nIn \nMAGI Orchestrator\n, a set of nodes that capture and store experiment status and data.\n\n  \ncontainer_image.py\n\n  \nIn the \nContainers (Virtualization) system\n, a command that draws a picture of the topology of an experiment. This is helpful in keeping track of how virtual nodes are connected. For more information, see the \nContainers Reference\n.\n\n  \ncontainerize.py\n\n  \nIn \nContainers (Virtualization) system\n, the command that creates a DETER experiment made up of containers. For more information, see \ncontainerize.py\n.\n\n  \ncontainer\n\n  \nIn the \nContainers (Virtualization) system\n, any one of various virtualization technologies, from an Openvz container to a physical machine to a simulation. The Containers system allows you to create interconnections of containers (in our sense) holding different experiment elements.\n\n  \ndata management layer\n\n  \nIn the \nMAGI Orchestrator\n, helps agents collect data. Users can query for the collected data by connecting to the data management layer.\n\n  \nDBDL\n\n  \nIn the \nMAGI Orchestrator\n, the section of the experiment config file that configures the data layer.\n\n  \nDETER web interface (isi.deterlab.net)\n\n  \nThe browser-based web portal for starting and defining experiments in DETERLab. \n\n  \nDETER\n\n  \nStands for cyber DEfense Technology Experimental Research and is an organization out of ISI/USC conducting research (the DETER Project) as well as the operator of a leading cyber security experimentation lab, DETERLab. DETER's mission is to readily enable the research community to conduct advanced research in cyber security through use of DETERLab's innovative methods and advanced tools -- that allow for repeatable, scalable and scientifically verifiable experimentation -- for homeland security and critical infrastructure protection.\n\n  \nDETER Project\n\n  \nA research project run by DETER that focuses on answering key questions about how best to conduct cyber-security research, what are the best methods and tools to carry out this kind of research, and how to conduct cyber research in a repeatable, archivable, hypothesis-based way. For more information, see \nhttp://www.deter-project.org/about_deter_project\n.\n\n  \nDETERLab\n\n  \nA state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. For more information, see \nhttp://www.deter-project.org/about_deterlab\n.\n\n  \nEmulab\n\n  \nA network testbed designed to provide researchers a wide range of environments in which to develop, debug, and evaluate their systems. The DETERLab system is based on Emulab.\n\n  \nendnode tracing/monitoring\n\n  \nRefers to putting the trace hooks on the end nodes of a link, instead of on delay nodes. This happens when there are no delay nodes in use or if you have explicitly requested end node shaping to reduce the number of nodes you need for an experiment.\n\n  \nevent groups\n\n  \nIn \nMAGI Orchestrator\n, a group of events that enable a behavior in the experiment, for example web server events.\n\n  \nevent streams\n\n  \nIn \nMAGI Orchestrator\n, the order of events in a particular MAGI (orchestrated) experiment. Events are one of two things, events or triggers. Events are similar to remote procedure calls on agents. Triggers are synchronization and/or branching points in your event stream.  A simple example of an event stream could be: configure the agent, tell it to begin monitoring the local node, wait for 60 seconds, then tell the agent to stop monitoring. When the event stream has no more events or triggers, the orchestrator will exit.\n\n  \nevent system\n\n  \nIn \nMAGI Orchestrator\n, a messaging system to send and receive orchestration messages.\n\n  \nevents\n\n  \nIn \nMAGI Orchestrator\n, events are one of two things, events or triggers. Events are similar to remote procedure calls on agents. Triggers are synchronization and/or branching points in your event stream. \n\n  \nExpDL\n\n  \nIn the \nMAGI Orchestrator\n, the section of the experiment config file that configures common experiment wide configuration that is used by the various MAGI tools. Most of the configuration in this section is automatically generated.\n\n  \nexperiment\n\n  \nWork in DETERLab is organized by experiments within project.\n\n  \nexperiment.conf\n\n  \nIn the \nMAGI Orchestrator\n, the experiment-wide, YAML-based configuration file.\n\n  \nfederation\n\n  \nIn general, refers to the ability to join disparate resources as if they were the same resource. DETERLab offers federated  architecture for creating experiments that span multiple testbeds through dynamically acquiring resources from other testbeds. \n\n  \nfidelity\n\n  \nIn general, means the degree to which something is faithfully reproduced. In DETERLab, refers to varying degrees different elements of a large-scale experiment (in the \nContainers (Virtualization) system\n, for example) need to be fully reproduced. Some processes require high degree of fidelity while others do not. \n\n  \ngatekeeper\n\n  \nProtects the internet facing side of the testbed and serves as a NAT machine for the Private Internet Network using a bridging firewall. \n\n  \nimage IDs\n\n  \nA descriptor for a disk image. This can be an image of an entire disk, a partition of a disk, or a set of partitions of a disk.\n\n  \nimage\n\n  \nRefers to a disk image.\n\n  \ninterface description file (IDL)\n\n  \nIn the \nMAGI Orchestrator\n, refers to a YAML-based file that describes an agent's interface. This is required when writing your own agents and allows an agent to be integrated with the experiment GUIs.\n\n  \nlink tracing/monitoring\n\n  \nThe ability to follow the path of a link or LAN in a DETERLab experiment.\n\n  \nMAGI\n\n  \nA DETERLab system that allows you to orchestrate very complex experiments. Stands for Montage AGent Infrastructure (Montage was originally an experiment lifecycle manager in DETERLab). For more information, see \nMAGI Orchestrator docs\n.\n\n  \nMAGI Graph\n\n  \nIn the \nMAGI Orchestrator\n, a graph generator for experiments executed on DETER Lab using MAGI. The tool fetches the required data using MAGI\u2019s data management layer and generates a graph in PNG format.\n\n  \nMAGI Status\n\n  \nIn the \nMAGI Orchestrator\n, a status tool that allows you to check MAGI\u2019s status on experiment nodes, reboot MAGI daemon process, and download logs from experiment nodes.\n\n  \nmagi_bootstrap.py\n\n  \nIn the \nMAGI Orchestrator\n, installs the MAGI distribution along with the supporting tools when setting up a MAGI-enabled experiment in DETERLab.\n\n  \nmagi_orchestrator.py\n\n  \nIn the \nMAGI Orchestrator\n, a tool that parses an AAL file and orchestrates an experiment based on the specified procedures.\n\n  \nMessaging Description Language (MESDL)\n\n  \nIn the \nMAGI Orchestrator\n, the section of the experiment config file that defines all the overlays and bridges for the experiment. The MAGI \u201coverlay\u201d is built on top of the control network on the DETERLab testbed. Nodes in the overlay can be thought of as being connected by virtual point-to-point links. The overlay provides a way to communicate with all the MAGI-enabled nodes across the testbed boundaries and even over the internet. \u201cBridges\u201d act as meeting places for all the members of the overlay it serves.\n\n  \nnode.conf\n\n  \nIn the \nMAGI Orchestrator\n, a node-specific configuration file used by the MAGI daemon process. As part of the bootstrap process, the experiment-wide config file (experiment.conf) is already broken down into node-level configuration. But you may customize node configuration via this file and it may be provided as an input to the MAGI bootstrap script, or to the MAGI daemon startup script, directly.\n\n  \nnodes\n\n  \nA node simply means any computer allocated to an experiment.\n\n  \nNS (Network Simulators) files syntax\n\n  \nUsed to describe topologies in network experiments. DETERLab-specific information may be found in the \nCore Guide\n and further documentation is available at \nhttp://www.isi.edu/nsnam/ns/\n.\n\n  \noperating system images (OSIDs)\n\n  \nDescribes an operating system which resides on a partition of a disk image. Every ImageID will have at least one OSID associated with it.\n\n  \norchestrator\n\n  \nsee [orchestrator.py], in the \nMAGI Orchestrator\n.\n\n  \nproject\n\n  \nEach group of users (or team) using DETERLab is grouped into 'projects', identified by a project ID (PID).\n\n  \nswapin\n\n  \nThe process where DETERLab allocates resources for your experiment and runs it.\n\n  \nswapout\n\n  \nThe process where DETERLab frees up the resources that were being used for your experiment. It is very important to do so when you are no longer using your experiment so that these resources are available for other experiments.\n\n  \ntopology\n\n  \nA description of the various elements of a computer network. In DETERLab, your experiment requires a topology in NS syntax that describes the links, nodes, etc of your experiment.\n\n  \ntriggers\n\n  \nIn the \nMAGI Orchestrator\n, these are synchronization and/or branching points in a stream of events.\n\n  \nusers network (\nusers.isi.deterlab.net\n)\n\n  \nDETERLab's file server that serves as a shell host for testbed users. \n\n  \nYAML\n\n  \nA simple format for describing data, used as the syntax for many configuration files throughout DETERLab systems. In general, just follow configuration file documentation, but if you are curious about specifications, you may find the latest here: \nhttp://www.yaml.org/spec/1.2/spec.html", 
            "title": "Glossary"
        }, 
        {
            "location": "/core/core-quickstart/", 
            "text": "This page describes basic information about DETERLab and its core functionality.\n\n\nWhat is DETERLab?\n\uf0c1\n\n\nThe DETERLab testbed is a security and education-enhanced version of Emulab. Funded by the National Science Foundation and the Department of Homeland Security, DETERLab is hosted by USC/ISI and UC Berkeley.\n\n\nDETERLab (like Emulab) offers user accounts with assorted permissions associated with different experiment groups. Each group can have its own pre-configured experimental environments running on Linux, BSD, Windows, or other operating systems. Users running DETERLab experiments have full control of real hardware and networks running preconfigured software packages.\n\n\nHow does it work?\n\uf0c1\n\n\nThe software running DETERLab loads operating system images (low level disk copies) onto free nodes in the testbed, and then reconfigures programmable switches to create VLANs with the newly-imaged nodes connected according to the topology specified by the experiment creator.\n\n\nAfter the system is fully imaged and configured, DETERLab executes specified scripts, unpacks tarballs, and/or installs RPM files according to the experiment's configuration. The end result is a live network of real machines, accessible via the Internet.\n\n\nWork in DETERLab is based on projects that include individual experiments and is accomplished either via the browser-based web interface (isi.deterlab.net) or via commandline on the DETERLab nodes.\n\n\nTo access DETERLab, you need to create an account, which provides credentials for accessing both the web interface and nodes.\n\n\nHow do I get a DETERLab account?\n\uf0c1\n\n\nYou may obtain a DETERLab account by either starting a new project (if you are a PI or instructor) or joining an existing project (if you are a project member or a student).\n\n\nIf you are the project investigator or instructor,\n you must create a project and invite your team members or students to join.\n\n\nIf you are the member of a team using DETERLab,\n your project leader will invite you to join the appropriate DETERLab project.\n\n\nIf you are a student,\n you may not create a project. Your instructor must create the project and, once approved, will give you information for joining the project.\n\n\nSee Getting Started for more information.\n\n\nHow do I use DETERLab?\n\uf0c1\n\n\nIn general, once you have a DETERLab account, you follow these steps. The DETERLab Core Guide will walk you through a basic tutorial of these steps.\n\n\n1. Design the topology\n\uf0c1\n\n\nEvery experiment in DETERLab is based on a network topology file written in NS format and saved on the users node. The following is a very basic example:\n\n\n# This is a simple ns script. Comments start with #.\nset ns [new Simulator]                 \nsource tb_compat.tcl\n\nset nodeA [$ns node]\nset nodeB [$ns node]\nset nodeC [$ns node]\nset nodeD [$ns node]\n\nset link0 [$ns duplex-link $nodeB $nodeA 30Mb 50ms DropTail]\ntb-set-link-loss $link0 0.01\n\nset lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms]\n\n# Set the OS on a couple.\ntb-set-node-os $nodeA FBSD7-STD\ntb-set-node-os $nodeC Ubuntu1004-STD\n\n$ns rtproto Static\n\n# Go!\n$ns run\n\n\n\n2. Create, start and swap in (allocate resources for) an experiment\n\uf0c1\n\n\nUsing your topology file, you start a new experiment via menu options in the DETERLab web interface.\n\n\n\n\n3. Generate traffic for your nodes\n\uf0c1\n\n\nNow you can experiment and start generating traffic for your nodes. We provide a flexible framework to pull together the software you'll need.\n\n\n4. View results by accessing nodes, modify the experiment as needed.\n\uf0c1\n\n\nOnce your experiment has started, you now can access nodes via SSH and conduct your desired experiments in your new environment.\n\n\nYou may modify aspects of a running experiment through the \"Modify experiment\" page in the web interface or by making changes to the NS file.\n\n\n5. Save your work and swap out your experiment (release the resources)\n\uf0c1\n\n\nWhen you are ready to stop working on an experiment but know you will want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users.\n\n\nThis is just a high-level overview. Go to the Core Guide for a basic hands-on example of using DETERLab Core.", 
            "title": "Core Quickstart"
        }, 
        {
            "location": "/core/core-quickstart/#what-is-deterlab", 
            "text": "The DETERLab testbed is a security and education-enhanced version of Emulab. Funded by the National Science Foundation and the Department of Homeland Security, DETERLab is hosted by USC/ISI and UC Berkeley.  DETERLab (like Emulab) offers user accounts with assorted permissions associated with different experiment groups. Each group can have its own pre-configured experimental environments running on Linux, BSD, Windows, or other operating systems. Users running DETERLab experiments have full control of real hardware and networks running preconfigured software packages.", 
            "title": "What is DETERLab?"
        }, 
        {
            "location": "/core/core-quickstart/#how-does-it-work", 
            "text": "The software running DETERLab loads operating system images (low level disk copies) onto free nodes in the testbed, and then reconfigures programmable switches to create VLANs with the newly-imaged nodes connected according to the topology specified by the experiment creator.  After the system is fully imaged and configured, DETERLab executes specified scripts, unpacks tarballs, and/or installs RPM files according to the experiment's configuration. The end result is a live network of real machines, accessible via the Internet.  Work in DETERLab is based on projects that include individual experiments and is accomplished either via the browser-based web interface (isi.deterlab.net) or via commandline on the DETERLab nodes.  To access DETERLab, you need to create an account, which provides credentials for accessing both the web interface and nodes.", 
            "title": "How does it work?"
        }, 
        {
            "location": "/core/core-quickstart/#how-do-i-get-a-deterlab-account", 
            "text": "You may obtain a DETERLab account by either starting a new project (if you are a PI or instructor) or joining an existing project (if you are a project member or a student).  If you are the project investigator or instructor,  you must create a project and invite your team members or students to join.  If you are the member of a team using DETERLab,  your project leader will invite you to join the appropriate DETERLab project.  If you are a student,  you may not create a project. Your instructor must create the project and, once approved, will give you information for joining the project.  See Getting Started for more information.", 
            "title": "How do I get a DETERLab account?"
        }, 
        {
            "location": "/core/core-quickstart/#how-do-i-use-deterlab", 
            "text": "In general, once you have a DETERLab account, you follow these steps. The DETERLab Core Guide will walk you through a basic tutorial of these steps.  1. Design the topology \uf0c1  Every experiment in DETERLab is based on a network topology file written in NS format and saved on the users node. The following is a very basic example:  # This is a simple ns script. Comments start with #.\nset ns [new Simulator]                 \nsource tb_compat.tcl\n\nset nodeA [$ns node]\nset nodeB [$ns node]\nset nodeC [$ns node]\nset nodeD [$ns node]\n\nset link0 [$ns duplex-link $nodeB $nodeA 30Mb 50ms DropTail]\ntb-set-link-loss $link0 0.01\n\nset lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms]\n\n# Set the OS on a couple.\ntb-set-node-os $nodeA FBSD7-STD\ntb-set-node-os $nodeC Ubuntu1004-STD\n\n$ns rtproto Static\n\n# Go!\n$ns run  2. Create, start and swap in (allocate resources for) an experiment \uf0c1  Using your topology file, you start a new experiment via menu options in the DETERLab web interface.   3. Generate traffic for your nodes \uf0c1  Now you can experiment and start generating traffic for your nodes. We provide a flexible framework to pull together the software you'll need.  4. View results by accessing nodes, modify the experiment as needed. \uf0c1  Once your experiment has started, you now can access nodes via SSH and conduct your desired experiments in your new environment.  You may modify aspects of a running experiment through the \"Modify experiment\" page in the web interface or by making changes to the NS file.  5. Save your work and swap out your experiment (release the resources) \uf0c1  When you are ready to stop working on an experiment but know you will want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users.  This is just a high-level overview. Go to the Core Guide for a basic hands-on example of using DETERLab Core.", 
            "title": "How do I use DETERLab?"
        }, 
        {
            "location": "/core/core-guide/", 
            "text": "In this tutorial we begin with a small 3-5 node experiment, so that you will become familiar with NS syntax and the practical aspects of DETERLab operation. Usually, you will want to incorporate another system such as the\n\nMAGI Orchestrator\n for more fully fleshed out experiments. But this is a good starting point for those new to DETERLab.\n\n\n\n\nNote\n\n\nIf you are a student, go to the \nhttp://education.deterlab.net\n site for classroom-specific instructions.\n\n\n\n\nNode Use Policy\n\uf0c1\n\n\nPlease make sure to read our \nguidelines for using nodes in DETERLab\n. These guidelines help keep DETERLab an effective environment for all users.\n\n\nDETERLab Environment\n\uf0c1\n\n\nYour experiment is made up of one or more machines on the internal DETERLab network, which is behind a firewall. \n\n\nusers.deterlab.net\n (or \nusers\n for short) is the \"control server\" for DETERLab. From \nusers\n, you can contact all your nodes, reboot them, connect to their serial ports, etc. Each user has a home directory on this server and you may SSH into it with your username and password for your DETERLab account.\n\n\nmyboss.isi.deterlab.net\n ( or \nboss\n for short) is the main testbed server that runs DETERLab. Users are not allowed to log directly into it.\n\n\nBasic Tutorial\n\uf0c1\n\n\nGetting Started\n\uf0c1\n\n\nWork in DETERLab is organized by \nexperiments\n within \nprojects\n. Each project is created and managed by a leader - usually the Principal Investigator (PI) of a research project or the instructor of a class on cybersecurity. The project leader then invites members to join by providing them with the project name and sending them the link to the 'Join a Project' page.\n\n\nBefore you can take the following tutorial, you need an active account in a project in DETERLab. See \nHow to Register\n to make sure if you're qualified, and then follow the directions to create a project or ask to join an existing project - if you go through either process for the first time, your account is created as a result. \n\n\nIf you already have an account, proceed to the next step.\n\n\nStep 1: Design the topology\n\uf0c1\n\n\nPart of DETERLab's power lies in its ability to assume many different topologies; the description of a such a topology is a necessary part\nof an experiment.  Before you can start your experiment, you must model the elements of the experiment's network with a topology. \n\n\nFor this basic tutorial, use this \nNS file\n which includes a simple topology and save it to a directory called \nbasicExp\n in your local directory on \nusers.deterlab.net\n.\n \n\n\nThe rest of this section describes NS format and walks you through the different parts of the sample file.\n\n\nNS Format\n\uf0c1\n\n\nDETERLab uses the \"NS\" (\"Network Simulator\") format to describe network topologies. This is substantially the same \nTcl-based format\n used by \nns-2\n. Since DETERLab offers emulation, rather than simulation, these files are interpreted in a somewhat different manner than ns-2. Therefore, some ns-2 functionality may work differently than you expect, or may not be implemented at all. Please look for warnings of the form: \n\n\n*** WARNING: Unsupported NS Statement!\n    Link type BAZ, using DropTail!\n\n\n\nIf you feel there is useful functionality missing, please \nlet us know\n. Also, some \ntestbed-specific syntax\n has been added, which, with the inclusion of the compatibility module \ntb_compat.tcl\n, will be ignored by the NS simulator. This allows the same NS file to work on both DETERLab and ns-2, most of the time.\n\n\nNS Example\n\uf0c1\n\n\nIn our example, we are creating a test network which looks like the following:\n\n\n\n\nFigure 1: A is connected to B, and B to C and D with a LAN.\n\n\nHere's how to describe this topology:\n\n\nDeclare a simulator and include a file that allows you to use the special \ntb-\n commands.\n\nFirst off, all NS files start with a simple prologue, declaring a simulator and including a file that allows you to use the special \ntb-\n commands:\n\n\n# This is a simple ns script. Comments start with #.\n    set ns [new Simulator]\n        source tb_compat.tcl\n\n\nDefine the 4 nodes in the topology.\n\n\nset nodeA [$ns node]\n    set nodeB [$ns node]\n    set nodeC [$ns node]\n    set nodeD [$ns node]\n\n\nnodeA\n and so on are the virtual names (\nvnames\n) of the nodes in your topology. When your experiment is swapped in (has allocated resources), they will be assigned to physical node names like \npc45\n, probably different ones each time. \n\n\nNOTE:\n Avoid vnames that clash with the physical node names in the testbed.**\n\n\nDefine the link and the LAN that connect the nodes.\n\n\nNS syntax permits you to specify the bandwidth, latency, and queue type. Note that since NS can't impose artificial losses like DETERLab can, we use a separate \ntb-\n command to add loss on a link. For our example, we will define a full speed LAN between B, C, and D, and a shaped link from node A to B.\n\nset link0 [$ns duplex-link $nodeB $nodeA 30Mb 50ms DropTail]\n    tb-set-link-loss $link0 0.01\n    set lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms]\n\nIn addition to the standard NS syntax above, a number of \nextensions\n are available in DETERLab that allow you to better control your experiment.\n\n\nFor example, you may specify what Operating System is booted on your nodes. For the versions of FreeBSD, Linux, and Windows we currently support, please refer to the \nOperating System Images\n page.\n\n\nClick \nList ImageIDs\n in the DETERLab web interface \nInteraction\n pane to see the current list of DETERLab-supplied operating systems. By default, our most recent Linux image is selected.\n\ntb-set-node-os $nodeA FBSD7-STD\n    tb-set-node-os $nodeC Ubuntu1004-STD\n    tb-set-node-os $nodeC WINXP-UPDATE\n\n\nEnable routing.\n\n\nIn a topology like this, you will likely want to communicate between all the nodes, including nodes that aren't directly connected, like \nA\n and \nC\n. In order for that to happen, we must enable routing in our experiment, so \nB\n can route packets for the other nodes. \n\n\nThe typical way to do this is with Static routing. (Other options are detailed in the \nRouting\n section below\n).\n\n$ns rtproto Static\n\n\nEnd with an epilogue that instructs the simulator to start.\n\n\n# Go!\n    $ns run\n\n\nStep 2: Create a new experiment\n\uf0c1\n\n\nFor this tutorial, we will use the web interface to create a new experiment. You could also use the \nDETERLab Shell Commands\n.\n\n\n\n\nLog into \nDETERLab\n with your account credentials (see \nHow to Register\n).\n\n\nClick the \nExperimentation\n menu item, then click \nBegin an Experiment\n.\n\n\nClick \nSelect Project\n and choose your project. This is also know as your \nproject name\n or Project ID (PID). This is used as an argument in many commands. Most people will be a member of just one project, and will not have a choice. If you are a member of multiple projects, be sure to select the correct project from the menu. In this example, we will refer to the project as \nDeterTest\n.\n\n\nLeave the \nGroup\n field set to \nDefault Group\n unless otherwise instructed.\n\n\nEnter the \nName\n field with an easily identifiable name for the experiment. The Name should be a single word (no spaces) identifier. For this tutorial, use \nbasic-experiment\n. This is also known as your \nexperiment name\n or Experiment ID (EID) and is used as an argument in many commands.\n\n\nEnter the \nDescription\n field with a brief description of the experiment.\n\n\nIn the \nYour NS File\n field, enter the local path to the basic.ns file you downloaded. This file will be uploaded through your browser when you choose \"Submit.\"\n     The rest of the settings depend on the goals of your experiment. In this case, you may simply set the \nIdle Swap\n field to \n1 h\n and leave the rest of the settings for \nSwapping\n, \nLinktest Option\n, and \nBatchMode\n at their default for now.\n\n\nCheck the \nSwap In Immediately\n box to start your lab now. If you did not check this box, you would follow the directions for [starting an experiment] to allocate resources later.\n\n\nClick \nSubmit\n.  \n\n\n\n\nAfter submission, DETERLab will begin processing your request. This process can take several minutes, depending on how large your topology is, and what other features (such as delay nodes and bandwidth limits) you are using. While you are waiting, you may watch the swap in process displayed in your web browser.\n\n\nAssuming all goes well, you will receive an email message indicating success or failure, and if successful, a listing of the nodes and IP address that were allocated to your experiment.\n\n\nFor the NS file in this example, you should receive a listing that looks similar to this:\n\n\n`\nExperiment: DeterTest/basic-experiment\nState: swapped\n\n\nVirtual Node Info:\nID              Type         OS              Qualified Name\n\n\n\n\nnodeA           pc           FBSD7-STD       nodeA.basic-experiment.DeterTest.isi.deterlab.net\nnodeB           pc                           nodeB.basic-experiment.DeterTest.isi.deterlab.net\nnodeC           pc           Ubuntu1004-STD  nodeC.basic-experiment.DeterTest.isi.deterlab.net\nnodeD           pc                           nodeD.basic-experiment.DeterTest.isi.deterlab.net\n\n\nVirtual Lan/Link Info:\nID              Member/Proto    IP/Mask         Delay     BW (Kbs)  Loss Rate\n\n\n\n\nlan0            nodeB:1         10.1.2.4        0.00      100000    0.00000000\n                ethernet        255.255.255.0   0.00      100000    0.00000000\nlan0            nodeC:0         10.1.2.3        0.00      100000    0.00000000\n                ethernet        255.255.255.0   0.00      100000    0.00000000\nlan0            nodeD:0         10.1.2.2        0.00      100000    0.00000000\n                ethernet        255.255.255.0   0.00      100000    0.00000000\nlink0           nodeA:0         10.1.1.3        25.00     30000     0.00501256\n                ethernet        255.255.255.0   25.00     30000     0.00501256\nlink0           nodeB:0         10.1.1.2        25.00     30000     0.00501256\n                ethernet        255.255.255.0   25.00     30000     0.00501256\n\n\nVirtual Queue Info:\nID              Member          Q Limit    Type    weight/min_th/max_th/linterm\n\n\n\n\nlan0            nodeB:1         100 slots  Tail    0/0/0/0\nlan0            nodeC:0         100 slots  Tail    0/0/0/0\nlan0            nodeD:0         100 slots  Tail    0/0/0/0\nlink0           nodeA:0         100 slots  Tail    0/0/0/0\nlink0           nodeB:0         100 slots  Tail    0/0/0/0\n\n\nEvent Groups:\nGroup Name      Members\n\n\n\n\nlink0-tracemon  link0-nodeB-tracemon,link0-nodeA-tracemon\n\nall_lans      lan0,link0\n\nall_tracemon  link0-nodeB-tracemon,link0-nodeA-tracemon,lan0-nodeD-tracemon,lan0-nodeC-tracemon,lan0-nodeB-tracemon\nlan0-tracemon   lan0-nodeB-tracemon,lan0-nodeC-tracemon,lan0-nodeD-tracemon\n`\n\n\nHere is a breakdown of the results:\n    * A single delay node was allocated and inserted into the link between \nnodeA\n and \nnodeB\n. This link is invisible from your perspective, except for the fact that it adds latency, error, or reduced bandwidth. However, the information for the delay links are included so that you can modify the delay parameters after the experiment has been created (Note that you cannot convert a non-shaped link into a shaped link; you can only modify the traffic shaping parameters of a link that is already being shaped). [[BR]]\n    * Delays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. A delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other. [[BR]]\n    * Each link in the \nVirtual Lan/Link\n section has its delay, etc., split between two entries. One is for traffic coming into the link from the node, and the other is for traffic leaving the link to the node. In the case of links, the four entries often get optimized to two entries in a \nPhysical Lan/Link\n section. [[BR]]\n    * The names in the \nQualified Name\n column refer to the control network interfaces for each of your allocated nodes. These names are added to the DETERLab nameserver map on the fly, and are immediately available for you to use so that you do not have to worry about the actual physical node names that were chosen. In the names listed above, \nDeterTest\n is the name of the project that you chose to work in, and \nbasic-experiment\n is the name of the experiment that you provided on the \nBegin an Experiment\n page. [[BR]]\n    * Please don't use the \nQualified Name\n from within nodes in your experiment, since it will contact them over the control network, bypassing the link shaping we configured.\n\n\nStarting an experiment (Swap-in)\n\uf0c1\n\n\nIf you want to go back to an existing experiment to start it and swap-in (allocate resources):\n\n\n\n\nGo to your dashboard by clicking the \nMy DETERLab\n link in the top menu.\n\n\nIn the \nCurrent Experiments\n table, click on the EID (Experiment ID) of the experiment you want to start.\n\n\nIn the left sidebar, click \nSwap Experiment In\n, then click \nConfirm\n. \n\n\n\n\nThe swap in process will take 5 to 10 minutes; you will receive an email notification when it is complete. While you are waiting, you can watch the swap in process displayed in your web browser.\n\n\nStep 3: Access nodes in your lab environment\n\uf0c1\n\n\nTo access your experimental nodes, you'll need to first \nSSH\n into \nusers.deterlab.net\n using your DETERLab username and password.\n\n\nOnce you log in to \nusers\n, you'll need to SSH again to your actual experimental nodes. Since your nodes addresses may change every time you swap them in, it's best to SSH to the permanent network names of the nodes. \n\n\nAs we mentioned in the previous step, the Qualified Names are included in the output after the experiment is swapped in. Here is another way to find them after swap-in:\n\n\na. \nNavigate to the experiment you just created in the web interface\n.  This location is usually called the \nexperiment page\n.\n     * If you just swapped in your experiment, the quickest way to find your node names is to click on the experiment name in the table under \nSwap Control\n.\n     * You can also get there by clicking \nMy DETERLab\n in the top navigation. Your experiment is listed as \"active\" in the \nState\n column. Click on the experiment's name in the \nEID\n column to display the experiment page.. \nb. \nClick on the \nDetails\n tab\n. \n     * Your nodes' network names are listed under the heading \nQualified Name\n. For example, \nnode1.basic-experiment.DeterTest.isi.deterlab.net\n. \n     * You should familiarize yourself with the information available on this page, but for now we just need to know the long DNS qualified name(s) node(s) you just swapped in. \n     * If you are curious, you should also look at the \nSettings\n (generic info), \nVisualization\n, and \nNS File\n tabs. (The topology mapplet may be disabled for some labs, so these last two may not be visible). \nc. \nSSH from \nusers\n to your experimental nodes by running a command with the following syntax\n: \n\nssh node1.ExperimentName.ProjectName.isi.deterlab.net\n\n     * You will not need to re-authenticate.\n     * You may need to wait a few more minutes. Once DETERLab is finished setting up the experiment, the nodes still need a minute or two to boot and complete their configuration. If you get a message about \"server configuration\" when you try to log in, wait a few minutes and try again.\nd. If you need to create new users on your experimental nodes, you may log in as them by running the following from the experimental node:\n  \nssh newuser@node1.basicExp.ProjectName.isi.deterlab.net\n\n  or\n  \nssh newuser@localhost\n \n\n\nStep 4: View results and modify the experiment\n\uf0c1\n\n\nYou can visualize the experiment by going to your experiment page (from My DETERLab, click the EID link for your experiment) and clicking the \nVisualization\n tab. From this page you can also change the NS file by clicking on the \nNS File\n tab or modify parameters by clicking \nModify Traffic Shaping\n in the left sidebar.\n\n\nAn alternative method is to log into \nusers.isi.deterlab.net\n and use the \ndelay_config\n program. This program requires that you know the symbolic names of the individual links. This information is available on the experiment page. \n\n\nStep 5: Configure and run your experiment.\n\uf0c1\n\n\nOnce you have all link modifications to your liking, you now need to install any additional tools you need (tools not included in the OS images you chose in Step 1), configure your tools and coordinate these tools to create events in your experiment.\n\n\nFor simple experiments, installation, configuration and triggering events can be done by hand or through small scripts. To accomplish this, log into your machines (see Step 3), perform the OS-specific steps needed to install and configure your tools, and run these tools by hand or through scripts, such as shell scripts or remote scripts such as Fabric-based scripts \nhttp://www.fabfile.org\n.\n\n\nFor more complicated experiments, you may need more automated ways to install and configure tools as well as coordinate events within your experiment. For fine-grained control over events and event triggers, see the \nMAGI Orchestrator\n.\n\n\nA large part of many experiments is traffic generation: the generation and modulation of packets on experiment links. Tools for such generation include the MAGI Orchestrator and \nLegoTG\n, as well as \nmany other possibilities\n.\n\n\nStep 6: Save your work and swap-out (release resources)\n\uf0c1\n\n\nWhen you are done working with your nodes, it is best practice to save your work and swap out the experiment so that other users have access to the physical machines.\n\n\nSaving and securing your files on DETERLab\n\uf0c1\n\n\nEvery user on DETERLab has a home directory on \nusers.deterlab.net\n which is mounted via NFS to experimental nodes. This means that anything you place in your home directory on one experimental node (or the \nusers\n machine) is visible in your home directory on your other experimental nodes. Your home directory is private and will not be overwritten, so you may save your work in that directory. \nHowever, everything else on experimental nodes is permanently lost when an experiment is swapped out.\n\n\nRemember: Make sure you save your work in your home directory before swapping out your experiment!\n\n\nAnother place you may save your files would be \n/proj/YourProject\n. This directory is also NFS-mounted to all experimental nodes, so the same rules apply about writing to it a lot, as for your home directory. It is shared by all members of your project/class.\n\n\nAgain, on DETERLab, files ARE NOT SAVED between swap-ins. Additionally, experiments may be forcibly swapped out after a certain number of idle hours (or some maximum amount of time).\n\n\nYou must manually save copies of any files you want to keep in your home directory. Any files left elsewhere on the experimental nodes will be erased and lost forever. This means that if you want to store progress for a lab and come back to it later, you will need to put it in your home directory before swapping out the experiment.\n\n\nSwap Out vs Terminate\n\uf0c1\n\n\nWhen to Swap Out\n\nWhen you are done with your experiment for the time being, make sure you save your work into an appropriate location and then swap out your experiment. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you are taking a break from the work, but coming back later. \n\n\nTo do this, click \nSwap Experiment Out\n link on the experiment page. This allows the resources to be de-allocated so that someone else can use them.\n\n\nWhen to Terminate\n\nWhen you are completely finished with your experiment and have no intention of running it again, use the \nTerminate Experiment\n link in the sidebar of the experiment page. Be careful: \ntermination will erase the experiment\n and you won't be able to swap it back in without recreating it. DETERLab will then tear down your experiment, and send you an email message when the process is complete. At this point you are allowed to reuse the experiment name (say, if you wanted to create a similar experiment with different parameters).\n\n\nTerminating says \"I won't need this experiment ever again.\" Just remember to Swap In/Out, and never \"Terminate\" unless you're sure you're completely done with the experiment. If you do end up terminating an experiment, you can always go back and recreate it.\n\n\nScheduling experiment swapout/termination\n\uf0c1\n\n\nIf you expect that your experiment should run for a set period of time, but you will not be around to terminate or swap the experiment out, then you should use the scheduled swapout/termination feature. This allows you to specify a maximum running time in your NS file so that you will not hold scarce resources when you are offline. To schedule a swapout or termination in your NS file:\n\n$ns at 2000.0 \"$ns terminate\"\n   or\n     $ns at 2000.0 \"$ns swapout\"\n\n\nThis will cause your experiment to either be terminated or swapped out after 2000 seconds of wallclock time.\n\n\nWhy can't I log in to DETERLab?\n\uf0c1\n\n\nDETERLab has an automatic blacklist mechanism. If you enter the wrong username and password combination too many times, your account will no longer be accessible from your current IP address. \n\n\nIf you think that this has happened to you, try logging in from another address (if you know how), or create an issue (see \nGetting Help\n), which will relay the request to the \ntestbed-ops\n group that this specific blacklist entry should be erased.\n\n\nInstalling RPMs automatically\n\uf0c1\n\n\nThe DETERLab NS extension \ntb-set-node-rpms\n allows you to specify a (space-separated) list of RPMs to install on each of your nodes when it boots:\n\ntb-set-node-rpms $nodeA /proj/myproj/rpms/silly-freebsd.rpm\n  tb-set-node-rpms $nodeB /proj/myproj/rpms/silly-linux.rpm\n  tb-set-node-rpms $nodeC /proj/myproj/rpms/silly-windows.rpm\n\nThe above NS code says to install the \nsilly-freebsd.rpm\n file on \nnodeA\n, the \nsilly-linux.rpm\n on \nnodeB\n, and the \nsilly-windows.rpm\n on \nnodeC\n. RPMs are installed as root, and must reside in either the project's \n/proj\n directory, or if the experiment has been created in a subgroup, in the \n/groups\n directory. You may not place your RPMs in your home directory.\n\n\nInstalling TAR files automatically\n\uf0c1\n\n\nThe DETERLab NS extension \ntb-set-node-tarfiles\n allows you to specify a set of tarfiles to install on each of your nodes when it boots. \n\n\nWhile similar to the \ntb-set-node-rpms\n command, the format of this command is slightly different in that you must specify a directory in which to unpack the tar file. This avoids problems with having to specify absolute pathnames in your tarfile, which many modern tar programs balk at.\n\ntb-set-node-tarfiles $nodeA /usr/site /proj/projectName/tarfiles/silly.tar.gz\n\nThe above NS code says to install the \nsilly.tar.gz\n tar file on \nnodeA\n from the working directory \n/usr/site\n when the node first boots. The tarfile must reside in either the project's \n/proj\n directory, or if the experiment has been created in a subgroup, in the \n/groups\n directory. You may not place your tarfiles in your home directory. You may specify as many tarfiles as you wish, as long as each one is preceded by the directory it should be unpacked in, all separated by spaces.\n\n\nStarting your application automatically\n\uf0c1\n\n\nYou may start your application automatically when your nodes boot for the first time (when an experiment is started or swapped in) by using the \ntb-set-node-startcmd\n NS extension. The argument is a command string (pathname of a script or program, plus arguments) that is run as the \nUID\n of the experiment creator, after the node has reached multiuser mode. \n\n\nThe command is invoked using \n/bin/csh\n, and the working directory is undefined (your script should \ncd\n to the directory you need). You can specify the same program for each node, or a different program. For example:\n\ntb-set-node-startcmd $nodeA \"/proj/projectName/runme.nodeA\"\n  tb-set-node-startcmd $nodeB \"/proj/projectName/runme.nodeB\"\n\nwill run \n/proj/projectName/runme.nodeA\n on nodeA and \n/proj/projectName/runme.nodeB\n on nodeB. The programs must reside on the node's local filesystem, or in a directory that can be reached via NFS. This is either the project's \n/proj\n directory, in the \n/groups\n directory if the experiment has been created in a subgroup, or a project member's home directory in \n/users\n. \n\n\nIf you need to see the output of your command, be sure to redirect the output into a file. You may place the file on the local node, or in one of the NFS mounted directories mentioned above. For example:\n\ntb-set-node-startcmd $nodeB \"/proj/myproj/runme \n /tmp/foo.log\"\n\nNote that the syntax and function of \n/bin/csh\n differs from other shells (including bash), specifically in redirection syntax.  Be sure to use \ncsh\n syntax or your start command will fail silently.\n\n\nThe exit value of the start command is reported back to the Web Interface, and is made available to you via the experiment page. There is a listing for all of the nodes in the experiment, and the exit value is recorded in this listing. The special symbol \nnone\n indicates that the node is still running the start command.\n\n\nNotifying the start program when all other nodes have started\n\uf0c1\n\n\nIt is often necessary for your start program to determine when all of the other nodes in the experiment have started, and are ready to proceed. Sometimes called a \nbarrier\n, this allows programs to wait at a specific point, and then all proceed at once. DETERLab provides a simple form of this mechanism using a synchronization server that runs on a node of your choice. \n\n\nSpecify the node in your NS file:\n\n\ntb-set-sync-server $nodeB\n\n\nWhen nodeB boots, the synchronization server will automatically start. Your software can then synchronize using the \nemulab-sync\n program that is installed on your nodes. For example, your node start command might look like this:\n\n\n#!/bin/sh\n   if [ \"$1\" = \"master\" ]; then\n       /usr/testbed/bin/emulab-sync -i 4\n   else\n       /usr/testbed/bin/emulab-sync fi /usr/site/bin/dosilly\n\n\nIn this example, there are five nodes in the experiment, one of which must be configured to operate as the master, initializing the barrier to the number of clients (four in the above example) that are expected to rendezvous at the barrier. The master will by default wait for all of the clients to reach the barrier. Each client of the barrier also waits until all of the clients have reached the barrier (and of course, until the master initializes the barrier to the proper count). Any number of clients may be specified (any subset of nodes in your experiment can wait). If the master does not need to wait for the clients, you may use the \nasync\n option which releases the master immediately:\n\n/usr/testbed/bin/emulab-sync -a -i 4\n\nYou may also specify the \nname\n of the barrier.\n\n/usr/testbed/bin/emulab-sync -a -i 4 -n mybarrier\n\n\nThis allows multiple barriers to be in use at the same time. Scripts on nodeA and nodeB can be waiting on a barrier named \"foo\" while (other) scripts on nodeA and nodeC can be waiting on a barrier named \"bar.\" You may reuse an existing barrier (including the default barrier) once it has been released (all clients arrived and woken up).\n\n\n Setting up IP routing between nodes\n\uf0c1\n\n\nAs DETER strives to make all aspects of the network controllable by the user, we do not attempt to impose any IP routing architecture or protocol by default. However, many users are more interested in end-to-end aspects and don't want to be bothered with setting up routes. For those users we provide an option to automatically set up routes on nodes which run one of our provided FreeBSD, Linux or \nWindows XP\n disk images.\n\n\nYou can use the NS \nrtproto\n syntax in your NS file to enable routing:\n\n\n$ns rtproto protocolOption\n\n\nwhere the \nprotocolOption\n is limited to one of \nSession\n, \nStatic\n, \nStatic-old\n, or \nManual\n.\n\n\n\n\nSession\n routing provides fully automated routing support, and is implemented by enabling \ngated\n running of the OSPF protocol on all nodes in the experiment. This is not supported on \nWindows XP\n nodes.\n\n\nStatic\n routing also provides automatic routing support, but rather than computing the routes dynamically, the routes are precomputed by a distributed route computation algorithm running in parallel on the experiment nodes.\n\n\nStatic-old\n specifies use of the older centralized route computation algorithm, precomputing the nodes when the experiment is created, and then loading them onto each node when it boots.\n\n\nManual\n routing allows you to explicitly specify per-node routing information in the NS file. To do this, use the \nManual\n routing option to \nrtproto\n, followed by a list of routes using the \nadd-route\n command:\n\n\n\n\n$node add-route $dst $nexthop\n\n\nwhere the \ndst\n can be either a node, a link, or a LAN. For example:\n\n\n$client add-route $server $router\n    $client add-route [$ns link $server $router] $router\n    $client add-route $serverlan $router\n\n\nNote that you would need a separate \nadd-route\n command to establish a route for the reverse direction; thus allowing you to specify differing forward and reverse routes if so desired. These statements are converted into appropriate \nroute(8)\n commands on your experimental nodes when they boot.\n\n\nIn the above examples, the first form says to set up a manual route between \n$client\n and \n$server\n, using \n$router\n as the nexthop; \n$client\n and \n$router\n should be directly connected, and the interface on \n$server\n should be unambiguous; either directly connected to the router, or an edge node that has just a single interface.\n\n\n\n\nIf the destination has multiple interfaces configured, and it is not connected directly to the nexthop, the interface that you are intending to route to is ambiguous. In the topology shown to the right, \n$nodeD\n has two interfaces configured. If you attempted to set up a route like this:\n\n$nodeA add-route $nodeD $nodeB\n\n\nyou would receive an error since DETERLab staff would not easily be able to determine which of the two links on \n$nodeD\n you are referring to. Fortunately, there is an easy solution. Instead of a node, specify the link directly:\n\n\n$nodeA add-route [$ns link $nodeD $nodeC] $nodeB\n\n\nThis tells us exactly which link you mean, enabling us to convert that information into a proper \nroute\n command on \n$nodeA\n.\n\n\nThe last form of the \nadd-route\n command is used when adding a route to an entire LAN. It would be tedious and error prone to specify a route to each node in a LAN by hand. Instead, just route to the entire network:\n\n\nset clientlan [$ns make-lan \"$nodeE $nodeF $nodeG\" 100Mb 0ms]\n    $nodeA add-route $clientlan $nodeB\n\n\nIn general, it is still best practice to use either \nSession\n or \nStatic\n routing for all but small, simple topologies. Explicitly setting up all the routes in even a moderately-sized experiment is extremely error prone. Consider this: a recently created experiment with 17 nodes and 10 subnets \nrequired 140 hand-created routes in the NS file\n.\n\n\nTwo final, cautionary notes on routing:\n    * The default route \nmust\n be set to use the control network interface. You might be tempted to set the default route on your nodes to reduce the number of explicit routes used. \nPlease avoid this.\n That would prevent nodes from contacting the outside world, i.e., you. \n    * If you use your own routing daemon, you must avoid using the control network interface in the configuration. Since every node in the testbed is directly connected to the control network LAN, a naive routing daemon configuration will discover that any node is just one hop away, via the control network, from any other node and \nall\n inter-node traffic will be routed via that interface.", 
            "title": "Core Guide"
        }, 
        {
            "location": "/core/core-guide/#node-use-policy", 
            "text": "Please make sure to read our  guidelines for using nodes in DETERLab . These guidelines help keep DETERLab an effective environment for all users.", 
            "title": "Node Use Policy"
        }, 
        {
            "location": "/core/core-guide/#deterlab-environment", 
            "text": "Your experiment is made up of one or more machines on the internal DETERLab network, which is behind a firewall.   users.deterlab.net  (or  users  for short) is the \"control server\" for DETERLab. From  users , you can contact all your nodes, reboot them, connect to their serial ports, etc. Each user has a home directory on this server and you may SSH into it with your username and password for your DETERLab account.  myboss.isi.deterlab.net  ( or  boss  for short) is the main testbed server that runs DETERLab. Users are not allowed to log directly into it.", 
            "title": "DETERLab Environment"
        }, 
        {
            "location": "/core/core-guide/#basic-tutorial", 
            "text": "Getting Started \uf0c1  Work in DETERLab is organized by  experiments  within  projects . Each project is created and managed by a leader - usually the Principal Investigator (PI) of a research project or the instructor of a class on cybersecurity. The project leader then invites members to join by providing them with the project name and sending them the link to the 'Join a Project' page.  Before you can take the following tutorial, you need an active account in a project in DETERLab. See  How to Register  to make sure if you're qualified, and then follow the directions to create a project or ask to join an existing project - if you go through either process for the first time, your account is created as a result.   If you already have an account, proceed to the next step.  Step 1: Design the topology \uf0c1  Part of DETERLab's power lies in its ability to assume many different topologies; the description of a such a topology is a necessary part\nof an experiment.  Before you can start your experiment, you must model the elements of the experiment's network with a topology.   For this basic tutorial, use this  NS file  which includes a simple topology and save it to a directory called  basicExp  in your local directory on  users.deterlab.net .    The rest of this section describes NS format and walks you through the different parts of the sample file.  NS Format \uf0c1  DETERLab uses the \"NS\" (\"Network Simulator\") format to describe network topologies. This is substantially the same  Tcl-based format  used by  ns-2 . Since DETERLab offers emulation, rather than simulation, these files are interpreted in a somewhat different manner than ns-2. Therefore, some ns-2 functionality may work differently than you expect, or may not be implemented at all. Please look for warnings of the form:   *** WARNING: Unsupported NS Statement!\n    Link type BAZ, using DropTail!  If you feel there is useful functionality missing, please  let us know . Also, some  testbed-specific syntax  has been added, which, with the inclusion of the compatibility module  tb_compat.tcl , will be ignored by the NS simulator. This allows the same NS file to work on both DETERLab and ns-2, most of the time.  NS Example \uf0c1  In our example, we are creating a test network which looks like the following:   Figure 1: A is connected to B, and B to C and D with a LAN.  Here's how to describe this topology:  Declare a simulator and include a file that allows you to use the special  tb-  commands. \nFirst off, all NS files start with a simple prologue, declaring a simulator and including a file that allows you to use the special  tb-  commands:  # This is a simple ns script. Comments start with #.\n    set ns [new Simulator]\n        source tb_compat.tcl  Define the 4 nodes in the topology.  set nodeA [$ns node]\n    set nodeB [$ns node]\n    set nodeC [$ns node]\n    set nodeD [$ns node]  nodeA  and so on are the virtual names ( vnames ) of the nodes in your topology. When your experiment is swapped in (has allocated resources), they will be assigned to physical node names like  pc45 , probably different ones each time.   NOTE:  Avoid vnames that clash with the physical node names in the testbed.**  Define the link and the LAN that connect the nodes.  NS syntax permits you to specify the bandwidth, latency, and queue type. Note that since NS can't impose artificial losses like DETERLab can, we use a separate  tb-  command to add loss on a link. For our example, we will define a full speed LAN between B, C, and D, and a shaped link from node A to B. set link0 [$ns duplex-link $nodeB $nodeA 30Mb 50ms DropTail]\n    tb-set-link-loss $link0 0.01\n    set lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms] \nIn addition to the standard NS syntax above, a number of  extensions  are available in DETERLab that allow you to better control your experiment.  For example, you may specify what Operating System is booted on your nodes. For the versions of FreeBSD, Linux, and Windows we currently support, please refer to the  Operating System Images  page.  Click  List ImageIDs  in the DETERLab web interface  Interaction  pane to see the current list of DETERLab-supplied operating systems. By default, our most recent Linux image is selected. tb-set-node-os $nodeA FBSD7-STD\n    tb-set-node-os $nodeC Ubuntu1004-STD\n    tb-set-node-os $nodeC WINXP-UPDATE  Enable routing.  In a topology like this, you will likely want to communicate between all the nodes, including nodes that aren't directly connected, like  A  and  C . In order for that to happen, we must enable routing in our experiment, so  B  can route packets for the other nodes.   The typical way to do this is with Static routing. (Other options are detailed in the  Routing  section below ). $ns rtproto Static  End with an epilogue that instructs the simulator to start.  # Go!\n    $ns run  Step 2: Create a new experiment \uf0c1  For this tutorial, we will use the web interface to create a new experiment. You could also use the  DETERLab Shell Commands .   Log into  DETERLab  with your account credentials (see  How to Register ).  Click the  Experimentation  menu item, then click  Begin an Experiment .  Click  Select Project  and choose your project. This is also know as your  project name  or Project ID (PID). This is used as an argument in many commands. Most people will be a member of just one project, and will not have a choice. If you are a member of multiple projects, be sure to select the correct project from the menu. In this example, we will refer to the project as  DeterTest .  Leave the  Group  field set to  Default Group  unless otherwise instructed.  Enter the  Name  field with an easily identifiable name for the experiment. The Name should be a single word (no spaces) identifier. For this tutorial, use  basic-experiment . This is also known as your  experiment name  or Experiment ID (EID) and is used as an argument in many commands.  Enter the  Description  field with a brief description of the experiment.  In the  Your NS File  field, enter the local path to the basic.ns file you downloaded. This file will be uploaded through your browser when you choose \"Submit.\"\n     The rest of the settings depend on the goals of your experiment. In this case, you may simply set the  Idle Swap  field to  1 h  and leave the rest of the settings for  Swapping ,  Linktest Option , and  BatchMode  at their default for now.  Check the  Swap In Immediately  box to start your lab now. If you did not check this box, you would follow the directions for [starting an experiment] to allocate resources later.  Click  Submit .     After submission, DETERLab will begin processing your request. This process can take several minutes, depending on how large your topology is, and what other features (such as delay nodes and bandwidth limits) you are using. While you are waiting, you may watch the swap in process displayed in your web browser.  Assuming all goes well, you will receive an email message indicating success or failure, and if successful, a listing of the nodes and IP address that were allocated to your experiment.  For the NS file in this example, you should receive a listing that looks similar to this:  `\nExperiment: DeterTest/basic-experiment\nState: swapped  Virtual Node Info:\nID              Type         OS              Qualified Name   nodeA           pc           FBSD7-STD       nodeA.basic-experiment.DeterTest.isi.deterlab.net\nnodeB           pc                           nodeB.basic-experiment.DeterTest.isi.deterlab.net\nnodeC           pc           Ubuntu1004-STD  nodeC.basic-experiment.DeterTest.isi.deterlab.net\nnodeD           pc                           nodeD.basic-experiment.DeterTest.isi.deterlab.net  Virtual Lan/Link Info:\nID              Member/Proto    IP/Mask         Delay     BW (Kbs)  Loss Rate   lan0            nodeB:1         10.1.2.4        0.00      100000    0.00000000\n                ethernet        255.255.255.0   0.00      100000    0.00000000\nlan0            nodeC:0         10.1.2.3        0.00      100000    0.00000000\n                ethernet        255.255.255.0   0.00      100000    0.00000000\nlan0            nodeD:0         10.1.2.2        0.00      100000    0.00000000\n                ethernet        255.255.255.0   0.00      100000    0.00000000\nlink0           nodeA:0         10.1.1.3        25.00     30000     0.00501256\n                ethernet        255.255.255.0   25.00     30000     0.00501256\nlink0           nodeB:0         10.1.1.2        25.00     30000     0.00501256\n                ethernet        255.255.255.0   25.00     30000     0.00501256  Virtual Queue Info:\nID              Member          Q Limit    Type    weight/min_th/max_th/linterm   lan0            nodeB:1         100 slots  Tail    0/0/0/0\nlan0            nodeC:0         100 slots  Tail    0/0/0/0\nlan0            nodeD:0         100 slots  Tail    0/0/0/0\nlink0           nodeA:0         100 slots  Tail    0/0/0/0\nlink0           nodeB:0         100 slots  Tail    0/0/0/0  Event Groups:\nGroup Name      Members   link0-tracemon  link0-nodeB-tracemon,link0-nodeA-tracemon all_lans      lan0,link0 all_tracemon  link0-nodeB-tracemon,link0-nodeA-tracemon,lan0-nodeD-tracemon,lan0-nodeC-tracemon,lan0-nodeB-tracemon\nlan0-tracemon   lan0-nodeB-tracemon,lan0-nodeC-tracemon,lan0-nodeD-tracemon\n`  Here is a breakdown of the results:\n    * A single delay node was allocated and inserted into the link between  nodeA  and  nodeB . This link is invisible from your perspective, except for the fact that it adds latency, error, or reduced bandwidth. However, the information for the delay links are included so that you can modify the delay parameters after the experiment has been created (Note that you cannot convert a non-shaped link into a shaped link; you can only modify the traffic shaping parameters of a link that is already being shaped). [[BR]]\n    * Delays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. A delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other. [[BR]]\n    * Each link in the  Virtual Lan/Link  section has its delay, etc., split between two entries. One is for traffic coming into the link from the node, and the other is for traffic leaving the link to the node. In the case of links, the four entries often get optimized to two entries in a  Physical Lan/Link  section. [[BR]]\n    * The names in the  Qualified Name  column refer to the control network interfaces for each of your allocated nodes. These names are added to the DETERLab nameserver map on the fly, and are immediately available for you to use so that you do not have to worry about the actual physical node names that were chosen. In the names listed above,  DeterTest  is the name of the project that you chose to work in, and  basic-experiment  is the name of the experiment that you provided on the  Begin an Experiment  page. [[BR]]\n    * Please don't use the  Qualified Name  from within nodes in your experiment, since it will contact them over the control network, bypassing the link shaping we configured.  Starting an experiment (Swap-in) \uf0c1  If you want to go back to an existing experiment to start it and swap-in (allocate resources):   Go to your dashboard by clicking the  My DETERLab  link in the top menu.  In the  Current Experiments  table, click on the EID (Experiment ID) of the experiment you want to start.  In the left sidebar, click  Swap Experiment In , then click  Confirm .    The swap in process will take 5 to 10 minutes; you will receive an email notification when it is complete. While you are waiting, you can watch the swap in process displayed in your web browser.  Step 3: Access nodes in your lab environment \uf0c1  To access your experimental nodes, you'll need to first  SSH  into  users.deterlab.net  using your DETERLab username and password.  Once you log in to  users , you'll need to SSH again to your actual experimental nodes. Since your nodes addresses may change every time you swap them in, it's best to SSH to the permanent network names of the nodes.   As we mentioned in the previous step, the Qualified Names are included in the output after the experiment is swapped in. Here is another way to find them after swap-in:  a.  Navigate to the experiment you just created in the web interface .  This location is usually called the  experiment page .\n     * If you just swapped in your experiment, the quickest way to find your node names is to click on the experiment name in the table under  Swap Control .\n     * You can also get there by clicking  My DETERLab  in the top navigation. Your experiment is listed as \"active\" in the  State  column. Click on the experiment's name in the  EID  column to display the experiment page.. \nb.  Click on the  Details  tab . \n     * Your nodes' network names are listed under the heading  Qualified Name . For example,  node1.basic-experiment.DeterTest.isi.deterlab.net . \n     * You should familiarize yourself with the information available on this page, but for now we just need to know the long DNS qualified name(s) node(s) you just swapped in. \n     * If you are curious, you should also look at the  Settings  (generic info),  Visualization , and  NS File  tabs. (The topology mapplet may be disabled for some labs, so these last two may not be visible). \nc.  SSH from  users  to your experimental nodes by running a command with the following syntax :  ssh node1.ExperimentName.ProjectName.isi.deterlab.net \n     * You will not need to re-authenticate.\n     * You may need to wait a few more minutes. Once DETERLab is finished setting up the experiment, the nodes still need a minute or two to boot and complete their configuration. If you get a message about \"server configuration\" when you try to log in, wait a few minutes and try again.\nd. If you need to create new users on your experimental nodes, you may log in as them by running the following from the experimental node:\n   ssh newuser@node1.basicExp.ProjectName.isi.deterlab.net \n  or\n   ssh newuser@localhost    Step 4: View results and modify the experiment \uf0c1  You can visualize the experiment by going to your experiment page (from My DETERLab, click the EID link for your experiment) and clicking the  Visualization  tab. From this page you can also change the NS file by clicking on the  NS File  tab or modify parameters by clicking  Modify Traffic Shaping  in the left sidebar.  An alternative method is to log into  users.isi.deterlab.net  and use the  delay_config  program. This program requires that you know the symbolic names of the individual links. This information is available on the experiment page.   Step 5: Configure and run your experiment. \uf0c1  Once you have all link modifications to your liking, you now need to install any additional tools you need (tools not included in the OS images you chose in Step 1), configure your tools and coordinate these tools to create events in your experiment.  For simple experiments, installation, configuration and triggering events can be done by hand or through small scripts. To accomplish this, log into your machines (see Step 3), perform the OS-specific steps needed to install and configure your tools, and run these tools by hand or through scripts, such as shell scripts or remote scripts such as Fabric-based scripts  http://www.fabfile.org .  For more complicated experiments, you may need more automated ways to install and configure tools as well as coordinate events within your experiment. For fine-grained control over events and event triggers, see the  MAGI Orchestrator .  A large part of many experiments is traffic generation: the generation and modulation of packets on experiment links. Tools for such generation include the MAGI Orchestrator and  LegoTG , as well as  many other possibilities .  Step 6: Save your work and swap-out (release resources) \uf0c1  When you are done working with your nodes, it is best practice to save your work and swap out the experiment so that other users have access to the physical machines.  Saving and securing your files on DETERLab \uf0c1  Every user on DETERLab has a home directory on  users.deterlab.net  which is mounted via NFS to experimental nodes. This means that anything you place in your home directory on one experimental node (or the  users  machine) is visible in your home directory on your other experimental nodes. Your home directory is private and will not be overwritten, so you may save your work in that directory.  However, everything else on experimental nodes is permanently lost when an experiment is swapped out.  Remember: Make sure you save your work in your home directory before swapping out your experiment!  Another place you may save your files would be  /proj/YourProject . This directory is also NFS-mounted to all experimental nodes, so the same rules apply about writing to it a lot, as for your home directory. It is shared by all members of your project/class.  Again, on DETERLab, files ARE NOT SAVED between swap-ins. Additionally, experiments may be forcibly swapped out after a certain number of idle hours (or some maximum amount of time).  You must manually save copies of any files you want to keep in your home directory. Any files left elsewhere on the experimental nodes will be erased and lost forever. This means that if you want to store progress for a lab and come back to it later, you will need to put it in your home directory before swapping out the experiment.  Swap Out vs Terminate \uf0c1  When to Swap Out \nWhen you are done with your experiment for the time being, make sure you save your work into an appropriate location and then swap out your experiment. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you are taking a break from the work, but coming back later.   To do this, click  Swap Experiment Out  link on the experiment page. This allows the resources to be de-allocated so that someone else can use them.  When to Terminate \nWhen you are completely finished with your experiment and have no intention of running it again, use the  Terminate Experiment  link in the sidebar of the experiment page. Be careful:  termination will erase the experiment  and you won't be able to swap it back in without recreating it. DETERLab will then tear down your experiment, and send you an email message when the process is complete. At this point you are allowed to reuse the experiment name (say, if you wanted to create a similar experiment with different parameters).  Terminating says \"I won't need this experiment ever again.\" Just remember to Swap In/Out, and never \"Terminate\" unless you're sure you're completely done with the experiment. If you do end up terminating an experiment, you can always go back and recreate it.  Scheduling experiment swapout/termination \uf0c1  If you expect that your experiment should run for a set period of time, but you will not be around to terminate or swap the experiment out, then you should use the scheduled swapout/termination feature. This allows you to specify a maximum running time in your NS file so that you will not hold scarce resources when you are offline. To schedule a swapout or termination in your NS file: $ns at 2000.0 \"$ns terminate\"\n   or\n     $ns at 2000.0 \"$ns swapout\"  This will cause your experiment to either be terminated or swapped out after 2000 seconds of wallclock time.", 
            "title": "Basic Tutorial"
        }, 
        {
            "location": "/core/core-guide/#why-cant-i-log-in-to-deterlab", 
            "text": "DETERLab has an automatic blacklist mechanism. If you enter the wrong username and password combination too many times, your account will no longer be accessible from your current IP address.   If you think that this has happened to you, try logging in from another address (if you know how), or create an issue (see  Getting Help ), which will relay the request to the  testbed-ops  group that this specific blacklist entry should be erased.", 
            "title": "Why can't I log in to DETERLab?"
        }, 
        {
            "location": "/core/core-guide/#installing-rpms-automatically", 
            "text": "The DETERLab NS extension  tb-set-node-rpms  allows you to specify a (space-separated) list of RPMs to install on each of your nodes when it boots: tb-set-node-rpms $nodeA /proj/myproj/rpms/silly-freebsd.rpm\n  tb-set-node-rpms $nodeB /proj/myproj/rpms/silly-linux.rpm\n  tb-set-node-rpms $nodeC /proj/myproj/rpms/silly-windows.rpm \nThe above NS code says to install the  silly-freebsd.rpm  file on  nodeA , the  silly-linux.rpm  on  nodeB , and the  silly-windows.rpm  on  nodeC . RPMs are installed as root, and must reside in either the project's  /proj  directory, or if the experiment has been created in a subgroup, in the  /groups  directory. You may not place your RPMs in your home directory.", 
            "title": "Installing RPMs automatically"
        }, 
        {
            "location": "/core/core-guide/#installing-tar-files-automatically", 
            "text": "The DETERLab NS extension  tb-set-node-tarfiles  allows you to specify a set of tarfiles to install on each of your nodes when it boots.   While similar to the  tb-set-node-rpms  command, the format of this command is slightly different in that you must specify a directory in which to unpack the tar file. This avoids problems with having to specify absolute pathnames in your tarfile, which many modern tar programs balk at. tb-set-node-tarfiles $nodeA /usr/site /proj/projectName/tarfiles/silly.tar.gz \nThe above NS code says to install the  silly.tar.gz  tar file on  nodeA  from the working directory  /usr/site  when the node first boots. The tarfile must reside in either the project's  /proj  directory, or if the experiment has been created in a subgroup, in the  /groups  directory. You may not place your tarfiles in your home directory. You may specify as many tarfiles as you wish, as long as each one is preceded by the directory it should be unpacked in, all separated by spaces.", 
            "title": "Installing TAR files automatically"
        }, 
        {
            "location": "/core/core-guide/#starting-your-application-automatically", 
            "text": "You may start your application automatically when your nodes boot for the first time (when an experiment is started or swapped in) by using the  tb-set-node-startcmd  NS extension. The argument is a command string (pathname of a script or program, plus arguments) that is run as the  UID  of the experiment creator, after the node has reached multiuser mode.   The command is invoked using  /bin/csh , and the working directory is undefined (your script should  cd  to the directory you need). You can specify the same program for each node, or a different program. For example: tb-set-node-startcmd $nodeA \"/proj/projectName/runme.nodeA\"\n  tb-set-node-startcmd $nodeB \"/proj/projectName/runme.nodeB\" \nwill run  /proj/projectName/runme.nodeA  on nodeA and  /proj/projectName/runme.nodeB  on nodeB. The programs must reside on the node's local filesystem, or in a directory that can be reached via NFS. This is either the project's  /proj  directory, in the  /groups  directory if the experiment has been created in a subgroup, or a project member's home directory in  /users .   If you need to see the output of your command, be sure to redirect the output into a file. You may place the file on the local node, or in one of the NFS mounted directories mentioned above. For example: tb-set-node-startcmd $nodeB \"/proj/myproj/runme   /tmp/foo.log\" \nNote that the syntax and function of  /bin/csh  differs from other shells (including bash), specifically in redirection syntax.  Be sure to use  csh  syntax or your start command will fail silently.  The exit value of the start command is reported back to the Web Interface, and is made available to you via the experiment page. There is a listing for all of the nodes in the experiment, and the exit value is recorded in this listing. The special symbol  none  indicates that the node is still running the start command.", 
            "title": "Starting your application automatically"
        }, 
        {
            "location": "/core/core-guide/#notifying-the-start-program-when-all-other-nodes-have-started", 
            "text": "It is often necessary for your start program to determine when all of the other nodes in the experiment have started, and are ready to proceed. Sometimes called a  barrier , this allows programs to wait at a specific point, and then all proceed at once. DETERLab provides a simple form of this mechanism using a synchronization server that runs on a node of your choice.   Specify the node in your NS file:  tb-set-sync-server $nodeB  When nodeB boots, the synchronization server will automatically start. Your software can then synchronize using the  emulab-sync  program that is installed on your nodes. For example, your node start command might look like this:  #!/bin/sh\n   if [ \"$1\" = \"master\" ]; then\n       /usr/testbed/bin/emulab-sync -i 4\n   else\n       /usr/testbed/bin/emulab-sync fi /usr/site/bin/dosilly  In this example, there are five nodes in the experiment, one of which must be configured to operate as the master, initializing the barrier to the number of clients (four in the above example) that are expected to rendezvous at the barrier. The master will by default wait for all of the clients to reach the barrier. Each client of the barrier also waits until all of the clients have reached the barrier (and of course, until the master initializes the barrier to the proper count). Any number of clients may be specified (any subset of nodes in your experiment can wait). If the master does not need to wait for the clients, you may use the  async  option which releases the master immediately: /usr/testbed/bin/emulab-sync -a -i 4 \nYou may also specify the  name  of the barrier. /usr/testbed/bin/emulab-sync -a -i 4 -n mybarrier  This allows multiple barriers to be in use at the same time. Scripts on nodeA and nodeB can be waiting on a barrier named \"foo\" while (other) scripts on nodeA and nodeC can be waiting on a barrier named \"bar.\" You may reuse an existing barrier (including the default barrier) once it has been released (all clients arrived and woken up).", 
            "title": "Notifying the start program when all other nodes have started"
        }, 
        {
            "location": "/core/sample-topologies/", 
            "text": "The following are various topologies you can use to experiment with DETERLab Core.\n\n\nToy topologies\n\uf0c1\n\n\nLAN\n\uf0c1\n\n\n![LAN Topology](/img/lan.png \nLAN Topology\n)\n\n    set ns [new Simulator]\n    source tb_compat.tcl\n\n    # Change this to a number of nodes you want\n    set NODES 5\n\n    set lanstr \n\n    for {set i 0} {$i \n $NODES} {incr i} {\n        set node($i) [$ns node]\n        append lanstr \n$node($i) \n\n    }\n\n    # Change the BW and delay if you want\n    set lan0 [$ns make-lan \n$lanstr\n 100Mb 0ms]\n\n    $ns rtproto Static\n    $ns run\n\n\n\n\nRing\n\uf0c1\n\n\n\n\n    set ns [new Simulator]\n    source tb_compat.tcl\n\n    # Change this to a number of nodes you want\n    set NODES 5\n\n    set node(0) [$ns node]\n    for {set i 1} {$i \n $NODES} {incr i} {\n        set node($i) [$ns node]\n        set lastindex [expr $i-1]\n\n        # Change BW and delay if you want\n        set Link$i [$ns duplex-link $node($i) $node($lastindex) 100Mb 0ms DropTail]\n    }\n\n    set lastindex [expr $i-1]\n\n    # Change BW and delay if you want\n    set Link$i [$ns duplex-link $node(0) $node($lastindex) 100Mb 0ms DropTail]\n\n    $ns rtproto Static\n    $ns run\n\n\n\n\nDumbbell\n\uf0c1\n\n\n\n\n    set ns [new Simulator]\n    source tb_compat.tcl\n\n    # Change this to a number of nodes you want\n    set NODES 10\n\n    set rem 0\n    set l 0\n    for {set i 0} {$i \n 2} {incr i} {\n       set node($rem) [$ns node]\n       for {set j 1} {$j \n $NODES/2} {incr j} {\n        set index [expr $rem+$j]\n        set node($index) [$ns node]\n\n        # Change BW and delay if you want\n        set Link$l [$ns duplex-link $node($rem) $node($index) 100Mb 0ms DropTail]\n\n        set l [expr $l+1]\n       }\n      set rem [expr $rem+$NODES/2]\n    }\n    set rem [expr $NODES/2]\n\n    # Change BW and delay if you want\n    set Link$l [$ns duplex-link $node($rem) $node(0) 100Mb 0ms DropTail]\n\n    $ns rtproto Static\n    $ns run\n\n\n\n\nTree\n\uf0c1\n\n\n\n\n    set ns [new Simulator]\n    source tb_compat.tcl\n\n    # Change fanout if you want but bear in mind that some of\n    # our nodes have only 5 interfaces, so max # of experimental\n    # interfaces (and fan out) is 4\n    set FANOUT 3\n\n    # Change depth if you want\n    set DEPTH 3\n\n    set node(0) [$ns node]\n    set lastj 0\n    set f $FANOUT\n    set lastl 0\n    for {set i 0} {$i \n $DEPTH} {incr i} {\n        for {set j 1} {$j \n# $f} {incr j} {\n        set index [expr $lastj+$j]\n        set node($index) [$ns node]\n        set lastindex [expr ($index-1)/$FANOUT]\n\n        # Change BW and delay if you want\n        set Link$lastl [$ns duplex-link $node($index) $node($lastindex) 100Mb 0ms DropTail]\n        set lastl [expr $lastl+1]\n       }\n    set f [expr $f*$FANOUT]\n    set lastj [expr $lastj+$j-1]\n    }\n\n    $ns rtproto Static\n    $ns run\n\n\n\n\nReal AS topologies\n\uf0c1\n\n\nBecause most of our PCs have up to 4 experimental interfaces, note that some of these topologies may have to be modified to have a fan out of at most 4.\n\n\nhttp://www.cs.purdue.edu/homes/fahmy/software/rf2ns/topo/", 
            "title": "Sample Topologies"
        }, 
        {
            "location": "/core/sample-topologies/#toy-topologies", 
            "text": "LAN \uf0c1  ![LAN Topology](/img/lan.png  LAN Topology )\n\n    set ns [new Simulator]\n    source tb_compat.tcl\n\n    # Change this to a number of nodes you want\n    set NODES 5\n\n    set lanstr  \n    for {set i 0} {$i   $NODES} {incr i} {\n        set node($i) [$ns node]\n        append lanstr  $node($i)  \n    }\n\n    # Change the BW and delay if you want\n    set lan0 [$ns make-lan  $lanstr  100Mb 0ms]\n\n    $ns rtproto Static\n    $ns run  Ring \uf0c1       set ns [new Simulator]\n    source tb_compat.tcl\n\n    # Change this to a number of nodes you want\n    set NODES 5\n\n    set node(0) [$ns node]\n    for {set i 1} {$i   $NODES} {incr i} {\n        set node($i) [$ns node]\n        set lastindex [expr $i-1]\n\n        # Change BW and delay if you want\n        set Link$i [$ns duplex-link $node($i) $node($lastindex) 100Mb 0ms DropTail]\n    }\n\n    set lastindex [expr $i-1]\n\n    # Change BW and delay if you want\n    set Link$i [$ns duplex-link $node(0) $node($lastindex) 100Mb 0ms DropTail]\n\n    $ns rtproto Static\n    $ns run  Dumbbell \uf0c1       set ns [new Simulator]\n    source tb_compat.tcl\n\n    # Change this to a number of nodes you want\n    set NODES 10\n\n    set rem 0\n    set l 0\n    for {set i 0} {$i   2} {incr i} {\n       set node($rem) [$ns node]\n       for {set j 1} {$j   $NODES/2} {incr j} {\n        set index [expr $rem+$j]\n        set node($index) [$ns node]\n\n        # Change BW and delay if you want\n        set Link$l [$ns duplex-link $node($rem) $node($index) 100Mb 0ms DropTail]\n\n        set l [expr $l+1]\n       }\n      set rem [expr $rem+$NODES/2]\n    }\n    set rem [expr $NODES/2]\n\n    # Change BW and delay if you want\n    set Link$l [$ns duplex-link $node($rem) $node(0) 100Mb 0ms DropTail]\n\n    $ns rtproto Static\n    $ns run  Tree \uf0c1       set ns [new Simulator]\n    source tb_compat.tcl\n\n    # Change fanout if you want but bear in mind that some of\n    # our nodes have only 5 interfaces, so max # of experimental\n    # interfaces (and fan out) is 4\n    set FANOUT 3\n\n    # Change depth if you want\n    set DEPTH 3\n\n    set node(0) [$ns node]\n    set lastj 0\n    set f $FANOUT\n    set lastl 0\n    for {set i 0} {$i   $DEPTH} {incr i} {\n        for {set j 1} {$j  # $f} {incr j} {\n        set index [expr $lastj+$j]\n        set node($index) [$ns node]\n        set lastindex [expr ($index-1)/$FANOUT]\n\n        # Change BW and delay if you want\n        set Link$lastl [$ns duplex-link $node($index) $node($lastindex) 100Mb 0ms DropTail]\n        set lastl [expr $lastl+1]\n       }\n    set f [expr $f*$FANOUT]\n    set lastj [expr $lastj+$j-1]\n    }\n\n    $ns rtproto Static\n    $ns run", 
            "title": "Toy topologies"
        }, 
        {
            "location": "/core/sample-topologies/#real-as-topologies", 
            "text": "Because most of our PCs have up to 4 experimental interfaces, note that some of these topologies may have to be modified to have a fan out of at most 4.  http://www.cs.purdue.edu/homes/fahmy/software/rf2ns/topo/", 
            "title": "Real AS topologies"
        }, 
        {
            "location": "/core/using-nodes/", 
            "text": "Know your DETER servers\n\uf0c1\n\n\nHere are the most important things to know.\n\n\n\n\nwww.isi.deterlab.net\n is the primary web interface for the testbed.\n\n\nusers.isi.deterlab.net\n is the host through which the testbed nodes are accessed and it is primary file server.\n\n\nscratch\n is the local package mirror for CentOS, Ubuntu, and FreeBSD.\n\n\n\n\nHostnames for your nodes\n\uf0c1\n\n\nWe set up names for your nodes in DNS and \n/etc/hosts\n files for use on the nodes in the experiment. Since our nodes have multiple interfaces (the control network, and, depending on the experiment, possibly several experimental interfaces) determining which name refers to which interface can be somewhat confusing. The rules below should help you figure this out.\n\n\n\n\nFrom users.isi.deterlab.net\n - We set up names in the form of \nnode.expt.proj.isi.deterlab.net\n in DNS, so that they are visible anywhere on the Internet. This name always refers to the node's control network interface, which is the only one reachable from \nusers.isi.deterlab.net\n. You can use \nnode.expt.proj\n as a shorthand.\n\n\nOn the nodes themselves\n - There are three basic ways to refer to the interfaces of a node. The first is stored in DNS, and the second two are stored on the node in the \n/etc/hosts\n file.\n\n\nFully-qualified hostnames\n - These names are the same ones visible from the outside world, and referred to by attaching the full domain name: ie. \nnode.expt.proj.isi.deterlab.net\n. (Note that, since we put \n.isi.deterlab.net\n in the nodes' domain search paths, you can use \nnode.expt.proj\n as a shorthand.) This name always refers to the control network.\n\n\nnode-link form\n - You can refer to an individual experimental interface by suffixing it with the name of the link or LAN (as defined in your NS file) that it is a member of. For example, \nnodeA-link0\n or \nserver-serverLAN\n. This is the preferred way to refer to experimental interfaces, since it uniquely and unambiguously identifies an interface.\n\n\nShort form\n - If a node is directly connected to the node you're on, you can refer to that node simply with its name (eg. \nnodeA\n.) Note that this differs from the fully- qualified name in that no domain is given. We also create short names for nodes you are not directly connected to. However, if two nodes are connected with more than one interface, or there is more than one route between them, there is no guarantee that the short name has been associated with the one is on the best (ie. shortest or highest bandwidth) path - so, if there is ambiguity, we strongly suggest that you use the \nnode-link\n form.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIt is a bad idea to pick virtual node names in your topology that clash with the physical node names in the testbed, such as \"pc45\".\n\n\n\n\nLogging into your Node\n\uf0c1\n\n\nBy the time you receive the email message listing your nodes, the DETER configuration system will have ensured that your nodes are fully configured and ready to use. If you have selected one of the DETER-supported operating system images \nsee supported images\n), this configuration process includes:\n    * loading fresh disk images so that each node is in a known clean state;\n    * rebooting each node so that it is running the OS specified in the NS script;\n    * configuring each of the network interfaces so that each one is \"up\" and talking to its virtual LAN (VLAN);\n    * creating user accounts for each of the project members;\n    * mounting the projects NFS directory in /proj so that project files are easily shared amongst all the nodes in the experiment;\n    * creating a /etc/hosts file on each node so that you may refer to the experimental interfaces of other nodes by name instead of IP number;\n    * configuring all of the delay parameters;\n    * configuring the serial console lines so that project members may access the console ports from users.isi.deterlab.net.\n\n\nAs this point you may log into any of the nodes in your experiment. You will need to use \nSecure Shell (ssh)\n to log into \nusers.isi.deterlab.net\n\nYour login name and password will be the same as your Web Interface login and password.  \n\n\n\n\nNote\n\n\nAlthough you can log into the web interface using your email address instead of your login name, you must use your login name when logging into \nusers.isi.deterlab.net\n.\n\n\n\n\nOnce logged into users you can then SSH to your nodes.  You should use the `qualified name' from the nodes mapping table so that you do not form dependencies on any particular physical node.  For more information on using SSH with DETER, please take a look at the \nDETER SSH\n wiki page.\n\n\nHow do I install software onto my node?\n\uf0c1\n\n\nEach \nsupported operating system\n has packages mirrored on a host called scratch and each operating system is configured to use this system to fetch packages from.  Information for specific operating systems is documented there.\n\n\nHow do I copy files onto my node?\n\uf0c1\n\n\nYour home directory on users is automatically mounted via NFS on every node in your experiment.  As are your project directory in \n/proj\n and a special filesystem called \n/share\n.\n\n\nI need \nroot\n access!\n\uf0c1\n\n\nIf you need to customize the configuration, or perhaps reboot nodes, you can use the \"sudo\" command, located in \n/usr/local/bin\n on FreeBSD and \n/usr/bin\n Linux. All users are added to the Administrators group on \nWindows XP\n nodes. Our policy is very liberal; you can customize the configuration in any way you like, provided it does not interfere with the operation of the testbed. As as example, to reboot a node that is running FreeBSD:\n\n\n/usr/local/bin/sudo reboot\n\n\n\nAlso, every testbed node has an automatically generated root password.  Simply click on a node in the \"Reserved Nodes\" for your experiment and look at the \nroot_password\n attribute.\n\n\nCan I access the nodes console?\n\uf0c1\n\n\nYes. Each of the PCs has its own serial console line connected to a \nserial server\n.  You can connect to a nodes serial console through the \nusers\n machine, using our \nconsole\n program located in '/usr/testbed/bin'. For example, to connect over serial line to \"pc001\" in your experiment, SSH into \nusers.isi.deterlab.net\n, and then type \nconsole pc001\n at the Unix prompt. You may then interact with the serial console (hit \"enter\" to elicit output from the target machine).\n\n\nTo exit the console program, type \nCtrl-]\n; it's just a telnet session.\n\n\nIn any case, all console output from each node is saved so that you may look at it it later. For each node, the console log is stored as \n/var/log/tiplogs/pcXXX.run\n. This \nrun\n file is created when nodes are first allocated to an experiment, and the Unix permissions of the run files permit only members of the project to view them. When the nodes are deallocated, the run files are cleared, so if you want to save them, you must do so before terminating the experiment.\n\n\nIn addition, you can view the console logs from the web interface, on the Show Experiment page. Of course, you may not interact with the console, but you can at least view the current log.\n\n\nEscape codes for Dell serial consoles are documented \nhere\n.\n\n\nMy node is wedged!\n\uf0c1\n\n\nPower cycling a node is easy since every node on the testbed is connected to a power controller.  If you need to power cycle a node, log on to users.isi.deterlab.net and use the \"node_reboot\" command:\n\n\nnode_reboot \nnode\n [node ... ]\n\n\n\nwhere \nnode\n is the physical name, as listed in the node mapping table. You may provide more than one node on the command line. Be aware that you may power cycle only nodes in projects that you are member of. Also, \nnode_reboot\n does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason, \nnode_reboot\n may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above).\nYou may also reboot all the nodes in an experiment by using the \n-e\n option to specify the project and experiment names. For example:\n\n\nnode_reboot -e testbed,multicast\n\n\n\nwill reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project. This option is provided as a shorthand method for rebooting large groups of nodes.\n\n\nI want to load a fresh operating system on my node\n\uf0c1\n\n\nScrogging your disk is certainly not as common, but it does happen. You can either swap your experiment out and then back in (which will allocate another group of nodes), or if you prefer you can reload the disk image yourself. You will of course lose anything you have stored on that disk; it is a good idea to store only data that can be easily recreated, or else store it in your project directory in \n/proj\n. \n\n\nReloading your disk with a fresh copy of an image is easy, and requires no intervention by DETER staff:\n\n\nos_load [-i ImageName] [-p Project] \nnode\n [node ... ]\n\n\n\nIf you do not specify an image name, the default image for that node type will be loaded (typically Ubuntu1004-STD).  For testbed wide images, you do not have to specify a project.  The os_load command will wait (not exit) until the nodes have been reloaded, so that you do not need to check the console lines of each node to determine when the load is done.\n\n\nFor example, to load the image 'testpc167' which is in the project 'DeterTest' onto pc167, we type:\n\n\nusers \n os_load -i testpc167 -p DeterTest pc167\nosload (pc167): Changing default OS to [OS 998: DeterTest,testpc167]\nosload: Updating image signature.\nSetting up reload for pc167 (mode: Frisbee)\nosload: Issuing reboot for pc167 and then waiting ...\nreboot (pc167): Attempting to reboot ...\nreboot (pc167): Successful!\nreboot: Done. There were 0 failures.\nreboot (pc167): child returned 0 status.\nosload (pc167): still waiting; it has been 1 minute(s)\nosload (pc167): still waiting; it has been 2 minute(s)\nosload (pc167): still waiting; it has been 3 minute(s)\nosload (pc167): still waiting; it has been 4 minute(s)\nosload: Done! There were 0 failures.\nusers \n\n\n\n\nI only want certain types of nodes for my experiment\n\uf0c1\n\n\nThe NS command \ntb-set-hardware\n only lets you pick one type of hardware.  If you are fine with a couple of different kinds of hardware, say you just want nodes that are at ISI part of the testbed, you can define a virtual node type in your NS file.  For more information on virtual types, please refer to the \nVirtual Type Commands\n section of the NS command reference.  Here is a quick example:\n\n\ntb-make-soft-vtype ISI {pc2133 pc3000 pc3060 pc3100} \ntb-make-soft-vtype UCB {bpc2133 bpc3000 bpc3060}\n\ntb-set-hardware $atISI ISI\ntb-set-hardware $atUCB UCB", 
            "title": "Using Your Nodes"
        }, 
        {
            "location": "/core/using-nodes/#know-your-deter-servers", 
            "text": "Here are the most important things to know.   www.isi.deterlab.net  is the primary web interface for the testbed.  users.isi.deterlab.net  is the host through which the testbed nodes are accessed and it is primary file server.  scratch  is the local package mirror for CentOS, Ubuntu, and FreeBSD.", 
            "title": "Know your DETER servers"
        }, 
        {
            "location": "/core/using-nodes/#hostnames-for-your-nodes", 
            "text": "We set up names for your nodes in DNS and  /etc/hosts  files for use on the nodes in the experiment. Since our nodes have multiple interfaces (the control network, and, depending on the experiment, possibly several experimental interfaces) determining which name refers to which interface can be somewhat confusing. The rules below should help you figure this out.   From users.isi.deterlab.net  - We set up names in the form of  node.expt.proj.isi.deterlab.net  in DNS, so that they are visible anywhere on the Internet. This name always refers to the node's control network interface, which is the only one reachable from  users.isi.deterlab.net . You can use  node.expt.proj  as a shorthand.  On the nodes themselves  - There are three basic ways to refer to the interfaces of a node. The first is stored in DNS, and the second two are stored on the node in the  /etc/hosts  file.  Fully-qualified hostnames  - These names are the same ones visible from the outside world, and referred to by attaching the full domain name: ie.  node.expt.proj.isi.deterlab.net . (Note that, since we put  .isi.deterlab.net  in the nodes' domain search paths, you can use  node.expt.proj  as a shorthand.) This name always refers to the control network.  node-link form  - You can refer to an individual experimental interface by suffixing it with the name of the link or LAN (as defined in your NS file) that it is a member of. For example,  nodeA-link0  or  server-serverLAN . This is the preferred way to refer to experimental interfaces, since it uniquely and unambiguously identifies an interface.  Short form  - If a node is directly connected to the node you're on, you can refer to that node simply with its name (eg.  nodeA .) Note that this differs from the fully- qualified name in that no domain is given. We also create short names for nodes you are not directly connected to. However, if two nodes are connected with more than one interface, or there is more than one route between them, there is no guarantee that the short name has been associated with the one is on the best (ie. shortest or highest bandwidth) path - so, if there is ambiguity, we strongly suggest that you use the  node-link  form.      Note  It is a bad idea to pick virtual node names in your topology that clash with the physical node names in the testbed, such as \"pc45\".", 
            "title": "Hostnames for your nodes"
        }, 
        {
            "location": "/core/using-nodes/#logging-into-your-node", 
            "text": "By the time you receive the email message listing your nodes, the DETER configuration system will have ensured that your nodes are fully configured and ready to use. If you have selected one of the DETER-supported operating system images  see supported images ), this configuration process includes:\n    * loading fresh disk images so that each node is in a known clean state;\n    * rebooting each node so that it is running the OS specified in the NS script;\n    * configuring each of the network interfaces so that each one is \"up\" and talking to its virtual LAN (VLAN);\n    * creating user accounts for each of the project members;\n    * mounting the projects NFS directory in /proj so that project files are easily shared amongst all the nodes in the experiment;\n    * creating a /etc/hosts file on each node so that you may refer to the experimental interfaces of other nodes by name instead of IP number;\n    * configuring all of the delay parameters;\n    * configuring the serial console lines so that project members may access the console ports from users.isi.deterlab.net.  As this point you may log into any of the nodes in your experiment. You will need to use  Secure Shell (ssh)  to log into  users.isi.deterlab.net \nYour login name and password will be the same as your Web Interface login and password.     Note  Although you can log into the web interface using your email address instead of your login name, you must use your login name when logging into  users.isi.deterlab.net .   Once logged into users you can then SSH to your nodes.  You should use the `qualified name' from the nodes mapping table so that you do not form dependencies on any particular physical node.  For more information on using SSH with DETER, please take a look at the  DETER SSH  wiki page.", 
            "title": "Logging into your Node"
        }, 
        {
            "location": "/core/using-nodes/#how-do-i-install-software-onto-my-node", 
            "text": "Each  supported operating system  has packages mirrored on a host called scratch and each operating system is configured to use this system to fetch packages from.  Information for specific operating systems is documented there.", 
            "title": "How do I install software onto my node?"
        }, 
        {
            "location": "/core/using-nodes/#how-do-i-copy-files-onto-my-node", 
            "text": "Your home directory on users is automatically mounted via NFS on every node in your experiment.  As are your project directory in  /proj  and a special filesystem called  /share .", 
            "title": "How do I copy files onto my node?"
        }, 
        {
            "location": "/core/using-nodes/#i-need-root-access", 
            "text": "If you need to customize the configuration, or perhaps reboot nodes, you can use the \"sudo\" command, located in  /usr/local/bin  on FreeBSD and  /usr/bin  Linux. All users are added to the Administrators group on  Windows XP  nodes. Our policy is very liberal; you can customize the configuration in any way you like, provided it does not interfere with the operation of the testbed. As as example, to reboot a node that is running FreeBSD:  /usr/local/bin/sudo reboot  Also, every testbed node has an automatically generated root password.  Simply click on a node in the \"Reserved Nodes\" for your experiment and look at the  root_password  attribute.", 
            "title": "I need root access!"
        }, 
        {
            "location": "/core/using-nodes/#my-node-is-wedged", 
            "text": "Power cycling a node is easy since every node on the testbed is connected to a power controller.  If you need to power cycle a node, log on to users.isi.deterlab.net and use the \"node_reboot\" command:  node_reboot  node  [node ... ]  where  node  is the physical name, as listed in the node mapping table. You may provide more than one node on the command line. Be aware that you may power cycle only nodes in projects that you are member of. Also,  node_reboot  does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason,  node_reboot  may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above).\nYou may also reboot all the nodes in an experiment by using the  -e  option to specify the project and experiment names. For example:  node_reboot -e testbed,multicast  will reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project. This option is provided as a shorthand method for rebooting large groups of nodes.", 
            "title": "My node is wedged!"
        }, 
        {
            "location": "/core/using-nodes/#i-want-to-load-a-fresh-operating-system-on-my-node", 
            "text": "Scrogging your disk is certainly not as common, but it does happen. You can either swap your experiment out and then back in (which will allocate another group of nodes), or if you prefer you can reload the disk image yourself. You will of course lose anything you have stored on that disk; it is a good idea to store only data that can be easily recreated, or else store it in your project directory in  /proj .   Reloading your disk with a fresh copy of an image is easy, and requires no intervention by DETER staff:  os_load [-i ImageName] [-p Project]  node  [node ... ]  If you do not specify an image name, the default image for that node type will be loaded (typically Ubuntu1004-STD).  For testbed wide images, you do not have to specify a project.  The os_load command will wait (not exit) until the nodes have been reloaded, so that you do not need to check the console lines of each node to determine when the load is done.  For example, to load the image 'testpc167' which is in the project 'DeterTest' onto pc167, we type:  users   os_load -i testpc167 -p DeterTest pc167\nosload (pc167): Changing default OS to [OS 998: DeterTest,testpc167]\nosload: Updating image signature.\nSetting up reload for pc167 (mode: Frisbee)\nosload: Issuing reboot for pc167 and then waiting ...\nreboot (pc167): Attempting to reboot ...\nreboot (pc167): Successful!\nreboot: Done. There were 0 failures.\nreboot (pc167): child returned 0 status.\nosload (pc167): still waiting; it has been 1 minute(s)\nosload (pc167): still waiting; it has been 2 minute(s)\nosload (pc167): still waiting; it has been 3 minute(s)\nosload (pc167): still waiting; it has been 4 minute(s)\nosload: Done! There were 0 failures.\nusers", 
            "title": "I want to load a fresh operating system on my node"
        }, 
        {
            "location": "/core/using-nodes/#i-only-want-certain-types-of-nodes-for-my-experiment", 
            "text": "The NS command  tb-set-hardware  only lets you pick one type of hardware.  If you are fine with a couple of different kinds of hardware, say you just want nodes that are at ISI part of the testbed, you can define a virtual node type in your NS file.  For more information on virtual types, please refer to the  Virtual Type Commands  section of the NS command reference.  Here is a quick example:  tb-make-soft-vtype ISI {pc2133 pc3000 pc3060 pc3100} \ntb-make-soft-vtype UCB {bpc2133 bpc3000 bpc3060}\n\ntb-set-hardware $atISI ISI\ntb-set-hardware $atUCB UCB", 
            "title": "I only want certain types of nodes for my experiment"
        }, 
        {
            "location": "/core/generating-traffic/", 
            "text": "Generating Traffic with LegoTG\n\uf0c1\n\n\nLegoTG is a flexible framework for pulling together the appropriate software for the traffic generation process. The key insight of LegoTG is that the definition of \"realism\" in traffic generation is entirely dependent on the experiment/scenario. LegoTG itself does not determine what \"realistic\" traffic means for a particular experiment or scenario. Rather, the definition of realism must come from the LegoTG user.\n\n\nLegoTG enables a modular and composable approach to the traffic generation process. LegoTG's Orchestrator handles tying together various modules which handle the different aspects of generation and allows a user to create a plug-and-play traffic generator. We are developing software to perform data extraction as well as working on a generator which will be generic enough to handle a variety of modeled dimensions.\n\n\nIn the LegoTG framework, each traffic generation functionality is realized through a separate piece of code, called a TGblock. The framework works like a child\u2019s building block set: TGblocks combine in different ways to achieve customizable and composable traffic generation. This combination and customization is achieved through LegoTG\u2019s Orchestrator, which sets up, configures, deploys, runs, synchronizes and stops TGblocks in distributed experiments. The entire specification of the traffic generation process for an experiment is in an experiment configuration file\u2014called an ExFile, which is an input for the Orchestrator. The ExFile offers a convenient capture of all the details of an experiment\u2019s background traffic set up, which promotes sharing and reproducibility of experiments. \n\n\nFor more information and to download software, please see the \nLegoTG\n project page.", 
            "title": "Generating Traffic"
        }, 
        {
            "location": "/core/generating-traffic/#generating-traffic-with-legotg", 
            "text": "LegoTG is a flexible framework for pulling together the appropriate software for the traffic generation process. The key insight of LegoTG is that the definition of \"realism\" in traffic generation is entirely dependent on the experiment/scenario. LegoTG itself does not determine what \"realistic\" traffic means for a particular experiment or scenario. Rather, the definition of realism must come from the LegoTG user.  LegoTG enables a modular and composable approach to the traffic generation process. LegoTG's Orchestrator handles tying together various modules which handle the different aspects of generation and allows a user to create a plug-and-play traffic generator. We are developing software to perform data extraction as well as working on a generator which will be generic enough to handle a variety of modeled dimensions.  In the LegoTG framework, each traffic generation functionality is realized through a separate piece of code, called a TGblock. The framework works like a child\u2019s building block set: TGblocks combine in different ways to achieve customizable and composable traffic generation. This combination and customization is achieved through LegoTG\u2019s Orchestrator, which sets up, configures, deploys, runs, synchronizes and stops TGblocks in distributed experiments. The entire specification of the traffic generation process for an experiment is in an experiment configuration file\u2014called an ExFile, which is an input for the Orchestrator. The ExFile offers a convenient capture of all the details of an experiment\u2019s background traffic set up, which promotes sharing and reproducibility of experiments.   For more information and to download software, please see the  LegoTG  project page.", 
            "title": "Generating Traffic with LegoTG"
        }, 
        {
            "location": "/core/link-delays/", 
            "text": "Per-Link Traffic Shaping (\"linkdelays\")\n\uf0c1\n\n\nIn order to conserve nodes, it is possible to specify that instead of doing traffic shaping on separate delay nodes (which eats up a node for every two shaped links), it be done on the nodes that are actually generating the traffic.\n\n\nUnder FreeBSD, just like normal delay nodes, end node (sometimes called \"per-link\") traffic shaping uses IPFW to direct traffic into the proper Dummynet pipe. On each node in a duplex link or LAN, a set of IPFW rules and Dummynet pipes is set up. As traffic enters or leaves your node, IPFW looks at the packet and stuffs it into the proper Dummynet pipe. At the proper time, Dummynet takes the packet and sends it on its way.\n\n\nUnder Linux, end node traffic shaping is performed by the packet scheduler modules, part of the kernel NET3 implementation. Each packet is added to the appropriate scheduler queue tree and shaped as specified in your NS file. Note that Linux traffic shaping currently only supports the drop-tail queueing discipline; gred and red are not available yet.\n\n\nTo specify end node shaping in your NS file, simply set up a normal link or LAN, and then mark it as wanting to use end node traffic shaping. For example: \n\n\nset link0 [$ns duplex-link $nodeA $nodeD 50Mb 0ms DropTail]\nset lan0  [$ns make-lan \"nodeA nodeB nodeC\" 1Mb 100ms]\n\ntb-set-endnodeshaping $link0 1\ntb-set-endnodeshaping $lan0 1\n\n\n\nPlease be aware though, that the kernels are different than the standard ones in a couple of ways:\n\n\n\n\nThe kernel runs at a 1000HZ (1024HZ in Linux) clockrate instead of 100HZ. That is, the timer interrupts 1000 (1024) times per second instead of 100. This finer granularity allows the traffic shapers to do a better job of scheduling packets.\n\n\nUnder FreeBSD, IPFW and Dummynet are compiled into the kernel, which affects the network stack; all incoming and outgoing packets are sent into IPFW to be matched on. Under Linux, packet scheduling exists implicitly, but uses lightweight modules by default.\n\n\nThe packet timing mechanism in the linkdelay Linux kernel uses a slightly heavier (but more precise) method.\n\n\nFlow-based IP forwarding is turned off. This is also known as IP ''fast forwarding'' in the FreeBSD kernel. Note that regular IP packet forwarding is still enabled.\n\n\n\n\nTo use end node traffic shaping globally, without having to specify per link or LAN, use the following in your NS file:\n\n\ntb-use-endnodeshaping   1\n\n\n\nTo specify non-shaped links, but perhaps control the shaping parameters later (increase delay, decrease bandwidth, etc.) after the experiment is swapped in, use the following in your NS file:\n\n\ntb-force-endnodeshaping   1\n\n\n\nMultiplexed Links\n\uf0c1\n\n\nAnother feature we have added (FreeBSD only) is ''multiplexed'' (sometimes called ''emulated'') links. An emulated link is one that can be multiplexed over a physical link along with other links. Say your experimental nodes have just one physical interface (call it \"fxp0\"), but you want to create two duplex links on it:\n\n\nset link0 [$ns duplex-link $nodeA $nodeB 50Mb 0ms DropTail]\nset link1 [$ns duplex-link $nodeA $nodeC 50Mb 0ms DropTail]\n\ntb-set-multiplexed $link0 1\ntb-set-multiplexed $link1 1\n\n\n\nWithout multiplexed links, your experiment would not be mappable since there are no nodes that can support the two duplex links that NodeA requires; there is only one physical interface. \n\n\nUsing multiplexed links however, the testbed software will assign both links on NodeA to one physical interface. That is because each duplex link is only 50Mbs, while the physical link (fxp0) is 100Mbs. \n\n\nOf course, if your application actually tried to use more than 50Mbs on each multiplexed link, there would be a problem; a flow using more than its share on link0 would cause packets on link1 to be dropped when they otherwise would not be. ('''At this time, you cannot specify that a LAN use multiplexed links''') \n\n\nTo prevent this problem, a multiplexed link is automatically setup to use [#LINKDELAYS per-link traffic shaping]. Each of the links in the above example would get a set of DummyNet pipes restricting their bandwidth to 50Mbs. Each link is forced to behave just as it would if the actual link bandwidth were 50Mbs. This allows the underlying physical link to support the aggregate bandwidth. Of course, the same caveats listed for per-link delays apply when using multiplexed links. \n\n\nAs a concrete example, consider the following NS file which creates a router and attaches it to 12 other nodes:\n\n\nset maxnodes 12\n\nset router [$ns node]\n\nfor {set i 1} {$i \n= $maxnodes} {incr i} {\n    set node($i) [$ns node]\n    set link($i) [$ns duplex-link $node($i) $router 30Mb 10ms DropTail]\n    tb-set-multiplexed $link($i) 1\n}\ntb-set-vlink-emulation vlan\n\n# Turn on routing.\n$ns rtproto Static\n\n\n\nSince each node has four 100Mbs interfaces, the above mapping would not be possible without the use of multiplexed links. However, since each link is defined to use 30Mbs, by using multiplexed links, the 12 links can be shared over the four physical interfaces, without oversubscribing the 400Mbs aggregate bandwidth available to the node that is assigned to the router. '' Note: while it may sound a little like channel bonding, it is not!''\n\n\nFreeBSD Technical Discussion\n\uf0c1\n\n\nFirst, let's just look at what happens with per-link delays on a duplex link. In this case, an IPFW pipe is set up on each node. The rule for the pipe looks like:\n\n\nipfw add pipe 10 ip from any to any out xmit fxp0\n\n\n\nwhich says that any packet going out on fxp0 should be stuffed into pipe 10. \n\n\nConsider the case of a ping packet that traverses a duplex link from NodeA to NodeB: \n\n Once the proper interface is chosen (based on routing or the fact that the destination is directly connected), the packet is handed off to IPFW, which determines that the interface (fxp0) matches the rule specified above. \n\n The packet is then stuffed into the corresponding Dummynet pipe, to emerge sometime later (based on the traffic shaping parameters) and be placed on the wire. \n\n The packet then arrives at NodeB. \n\n A ping reply packet is created and addressed to NodeA, placed into the proper Dummynet pipe, and arrives at NodeA. \n\n\nAs you can see, each packet traversed exactly one Dummynet pipe (or put another way, the entire ping/reply sequence traversed two pipes). \n\n\nConstructing delayed LANs is more complicated than duplex links because of the desire to allow each node in a LAN to see different delays when talking to any other node in the LAN. That is, the delay when traversing from NodeA to NodeB is different than when traversing from NodeA to NodeC. Further, the return delays might be specified completely differently so that the return trips take a different amount of time.  \n\n\nTo support this, it is necessary to insert two delay pipes for each node. One pipe is for traffic leaving the node for the LAN, and the other pipe is for traffic entering the node from the LAN. Why not create ''N'' pipes on each node for each possible destination address in the LAN, so that each packet traverses only one pipe? The reason is that a node on a LAN has only one connection to it, and multiple pipes would not respect the aggregate bandwidth cap specified. \n\n\nThe rule for the second pipe looks like:\n\n\nipfw add pipe 15 ip from any to any in recv fxp0\n\n\n\nwhich says that any packet received on fxp0 should be stuffed into pipe 15. The packet is later handed up to the application, or forwarded on to the next hop, if appropriate. \n\n\nThe addition of multiplexed links complicates things further. To multiplex several different links on a physical interface, one must use either encapsulation (ipinip, VLAN, etc) or IP interface aliases. We chose IP aliases because it does not affect the MTU size. The downside of IP aliases is that it is difficult (if not impossible) to determine what flow a packet is part of, and thus which IPFW pipe to stuff the packet into. In other words, the rules used above:\n\n\nipfw add ... out xmit fxp0\nipfw add ... in recv fxp0\n\n\n\ndo not work because there are now multiple flows multiplexed onto the interface (multiple IPs) and so there is no way to distinguish which flow. \n\n\nConsider a duplex link in which we use the first rule above. If the packet is not addressed to a direct neighbor, the routing code lookup will return a nexthop, which '''does''' indicate the flow, but because the rule is based simply on the interface (fxp0), all flows match! \n\n\nUnfortunately, IPFW does not provide an interface for matching on the nexthop address, but seeing as we are kernel hackers, this is easy to deal with by adding new syntax to IPFW to allow matching on nexthop:\n\n\nipfw add ... out xmit fxp0 nexthop 192.168.2.3:255.255.255.0\n\n\n\nNow, no matter how the user alters the routing table, packets will be stuffed into the proper pipe since the nexthop indicates which directly connected virtual link the packet was sent over. The use of a mask allows for matching when directly connected to a LAN (a simplification). \n\n\nMultiplexed LANs present even worse problems because of the need to figure out which flow an incoming packet is part of. When a packet arrives at an interface, there is nothing in the packet to indicate which IP alias the packet was intended for (or which it came from) when the packet is not destined for the local node (is being forwarded).\n\n\nLinux Technical Discussion\n\uf0c1\n\n\nTraffic shaping under Linux uses the NET3 packet scheduling modules, a hierarchically composable set of disciplines providing facilities such as bandwidth limiting, packet loss, and packet delay. As in the FreeBSD case, simplex (outgoing) link shaping is used on point-to-point links, while duplex shaping (going out, and coming in an interface) is used with LANs. See the previous section to understand why this is done. \n\n\nUnlike FreeBSD, Linux traffic shaping modules must be connected directly to a network device, and hence don't require a firewall directive to place packets into them. This means that all packets must pass through the shaping tree connected to a particular interface. Note that filters may be used on the shapers themselves to discriminate traffic flows, so it's not strictly the case that all traffic must be shaped if modules are attached. However, all traffic to an interface, at the least, is queued and de-queued through the root module of the shaping hierarchy. And all interfaces have at least a root module, but it is normally just a fast FIFO. \n\n\nAlso of note is the fact that Linux traffic shaping normally only happens on the outgoing side of an interface, and requires a special virtual network device (known as an intermediate queuing device or IMQ) to capture incoming packets for shaping. This also requires the aid of the Linux firewalling facility, iptables, to divert the packets to the IMQs prior to routing. \n\n\nHere is an example duplex-link configuration with 50Mbps of bandwidth, a 0.05 PLR, and 20ms of delay in both directions: \n\n\nOutgoing side setup commands:\n\n\n# implicitly sets up class 1:1\ntc qdisc add dev eth0 root handle 1 plr 0.05\n\n# attach to class 1:1 and tell the module the default place to send\n# traffic is to class 2:1 (could attach filters to discriminate)\ntc qdisc add dev eth0 parent 1:1 handle 2 htb default 1\n\n# class 2:1 does the actual limiting\ntc class add dev eth0 parent 2 classid 2:1 htb rate 50Mbit ceil 50Mbit\n\n# attach to class 2:1, also implicitly creates class 3:1, and attaches\n# a FIFO queue to it.\ntc qdisc add dev eth0 parent 2:1 handle 3 delay usecs 20000\n\n\n\nThe incoming side setup commands will look the same, but with eth0 replaced by imq0. Also, we have to tell the kernel to send packets coming into eth0 to imq0 (where they will be shaped):\n\n\niptables -t mangle -A PREROUTING -i eth0 -j IMQ --todev 0\n\n\n\nA flood ping sequence utilizing eth0 (echo-\necho-reply) would experience a round trip delay of 40 ms, be restricted to 50Mbit, and have a 10% chance of losing packets. The doubling of numbers is due to shaping as packets go out, and come back in the interface. \n\n\nAt the time of writing, we don't support multiplexed links under Linux, so no explicit matching against nexthop is necessary.", 
            "title": "End Node Traffic Shaping"
        }, 
        {
            "location": "/core/link-delays/#per-link-traffic-shaping-linkdelays", 
            "text": "In order to conserve nodes, it is possible to specify that instead of doing traffic shaping on separate delay nodes (which eats up a node for every two shaped links), it be done on the nodes that are actually generating the traffic.  Under FreeBSD, just like normal delay nodes, end node (sometimes called \"per-link\") traffic shaping uses IPFW to direct traffic into the proper Dummynet pipe. On each node in a duplex link or LAN, a set of IPFW rules and Dummynet pipes is set up. As traffic enters or leaves your node, IPFW looks at the packet and stuffs it into the proper Dummynet pipe. At the proper time, Dummynet takes the packet and sends it on its way.  Under Linux, end node traffic shaping is performed by the packet scheduler modules, part of the kernel NET3 implementation. Each packet is added to the appropriate scheduler queue tree and shaped as specified in your NS file. Note that Linux traffic shaping currently only supports the drop-tail queueing discipline; gred and red are not available yet.  To specify end node shaping in your NS file, simply set up a normal link or LAN, and then mark it as wanting to use end node traffic shaping. For example:   set link0 [$ns duplex-link $nodeA $nodeD 50Mb 0ms DropTail]\nset lan0  [$ns make-lan \"nodeA nodeB nodeC\" 1Mb 100ms]\n\ntb-set-endnodeshaping $link0 1\ntb-set-endnodeshaping $lan0 1  Please be aware though, that the kernels are different than the standard ones in a couple of ways:   The kernel runs at a 1000HZ (1024HZ in Linux) clockrate instead of 100HZ. That is, the timer interrupts 1000 (1024) times per second instead of 100. This finer granularity allows the traffic shapers to do a better job of scheduling packets.  Under FreeBSD, IPFW and Dummynet are compiled into the kernel, which affects the network stack; all incoming and outgoing packets are sent into IPFW to be matched on. Under Linux, packet scheduling exists implicitly, but uses lightweight modules by default.  The packet timing mechanism in the linkdelay Linux kernel uses a slightly heavier (but more precise) method.  Flow-based IP forwarding is turned off. This is also known as IP ''fast forwarding'' in the FreeBSD kernel. Note that regular IP packet forwarding is still enabled.   To use end node traffic shaping globally, without having to specify per link or LAN, use the following in your NS file:  tb-use-endnodeshaping   1  To specify non-shaped links, but perhaps control the shaping parameters later (increase delay, decrease bandwidth, etc.) after the experiment is swapped in, use the following in your NS file:  tb-force-endnodeshaping   1", 
            "title": "Per-Link Traffic Shaping (\"linkdelays\")"
        }, 
        {
            "location": "/core/link-delays/#multiplexed-links", 
            "text": "Another feature we have added (FreeBSD only) is ''multiplexed'' (sometimes called ''emulated'') links. An emulated link is one that can be multiplexed over a physical link along with other links. Say your experimental nodes have just one physical interface (call it \"fxp0\"), but you want to create two duplex links on it:  set link0 [$ns duplex-link $nodeA $nodeB 50Mb 0ms DropTail]\nset link1 [$ns duplex-link $nodeA $nodeC 50Mb 0ms DropTail]\n\ntb-set-multiplexed $link0 1\ntb-set-multiplexed $link1 1  Without multiplexed links, your experiment would not be mappable since there are no nodes that can support the two duplex links that NodeA requires; there is only one physical interface.   Using multiplexed links however, the testbed software will assign both links on NodeA to one physical interface. That is because each duplex link is only 50Mbs, while the physical link (fxp0) is 100Mbs.   Of course, if your application actually tried to use more than 50Mbs on each multiplexed link, there would be a problem; a flow using more than its share on link0 would cause packets on link1 to be dropped when they otherwise would not be. ('''At this time, you cannot specify that a LAN use multiplexed links''')   To prevent this problem, a multiplexed link is automatically setup to use [#LINKDELAYS per-link traffic shaping]. Each of the links in the above example would get a set of DummyNet pipes restricting their bandwidth to 50Mbs. Each link is forced to behave just as it would if the actual link bandwidth were 50Mbs. This allows the underlying physical link to support the aggregate bandwidth. Of course, the same caveats listed for per-link delays apply when using multiplexed links.   As a concrete example, consider the following NS file which creates a router and attaches it to 12 other nodes:  set maxnodes 12\n\nset router [$ns node]\n\nfor {set i 1} {$i  = $maxnodes} {incr i} {\n    set node($i) [$ns node]\n    set link($i) [$ns duplex-link $node($i) $router 30Mb 10ms DropTail]\n    tb-set-multiplexed $link($i) 1\n}\ntb-set-vlink-emulation vlan\n\n# Turn on routing.\n$ns rtproto Static  Since each node has four 100Mbs interfaces, the above mapping would not be possible without the use of multiplexed links. However, since each link is defined to use 30Mbs, by using multiplexed links, the 12 links can be shared over the four physical interfaces, without oversubscribing the 400Mbs aggregate bandwidth available to the node that is assigned to the router. '' Note: while it may sound a little like channel bonding, it is not!''", 
            "title": "Multiplexed Links"
        }, 
        {
            "location": "/core/link-delays/#freebsd-technical-discussion", 
            "text": "First, let's just look at what happens with per-link delays on a duplex link. In this case, an IPFW pipe is set up on each node. The rule for the pipe looks like:  ipfw add pipe 10 ip from any to any out xmit fxp0  which says that any packet going out on fxp0 should be stuffed into pipe 10.   Consider the case of a ping packet that traverses a duplex link from NodeA to NodeB:   Once the proper interface is chosen (based on routing or the fact that the destination is directly connected), the packet is handed off to IPFW, which determines that the interface (fxp0) matches the rule specified above.   The packet is then stuffed into the corresponding Dummynet pipe, to emerge sometime later (based on the traffic shaping parameters) and be placed on the wire.   The packet then arrives at NodeB.   A ping reply packet is created and addressed to NodeA, placed into the proper Dummynet pipe, and arrives at NodeA.   As you can see, each packet traversed exactly one Dummynet pipe (or put another way, the entire ping/reply sequence traversed two pipes).   Constructing delayed LANs is more complicated than duplex links because of the desire to allow each node in a LAN to see different delays when talking to any other node in the LAN. That is, the delay when traversing from NodeA to NodeB is different than when traversing from NodeA to NodeC. Further, the return delays might be specified completely differently so that the return trips take a different amount of time.    To support this, it is necessary to insert two delay pipes for each node. One pipe is for traffic leaving the node for the LAN, and the other pipe is for traffic entering the node from the LAN. Why not create ''N'' pipes on each node for each possible destination address in the LAN, so that each packet traverses only one pipe? The reason is that a node on a LAN has only one connection to it, and multiple pipes would not respect the aggregate bandwidth cap specified.   The rule for the second pipe looks like:  ipfw add pipe 15 ip from any to any in recv fxp0  which says that any packet received on fxp0 should be stuffed into pipe 15. The packet is later handed up to the application, or forwarded on to the next hop, if appropriate.   The addition of multiplexed links complicates things further. To multiplex several different links on a physical interface, one must use either encapsulation (ipinip, VLAN, etc) or IP interface aliases. We chose IP aliases because it does not affect the MTU size. The downside of IP aliases is that it is difficult (if not impossible) to determine what flow a packet is part of, and thus which IPFW pipe to stuff the packet into. In other words, the rules used above:  ipfw add ... out xmit fxp0\nipfw add ... in recv fxp0  do not work because there are now multiple flows multiplexed onto the interface (multiple IPs) and so there is no way to distinguish which flow.   Consider a duplex link in which we use the first rule above. If the packet is not addressed to a direct neighbor, the routing code lookup will return a nexthop, which '''does''' indicate the flow, but because the rule is based simply on the interface (fxp0), all flows match!   Unfortunately, IPFW does not provide an interface for matching on the nexthop address, but seeing as we are kernel hackers, this is easy to deal with by adding new syntax to IPFW to allow matching on nexthop:  ipfw add ... out xmit fxp0 nexthop 192.168.2.3:255.255.255.0  Now, no matter how the user alters the routing table, packets will be stuffed into the proper pipe since the nexthop indicates which directly connected virtual link the packet was sent over. The use of a mask allows for matching when directly connected to a LAN (a simplification).   Multiplexed LANs present even worse problems because of the need to figure out which flow an incoming packet is part of. When a packet arrives at an interface, there is nothing in the packet to indicate which IP alias the packet was intended for (or which it came from) when the packet is not destined for the local node (is being forwarded).", 
            "title": "FreeBSD Technical Discussion"
        }, 
        {
            "location": "/core/link-delays/#linux-technical-discussion", 
            "text": "Traffic shaping under Linux uses the NET3 packet scheduling modules, a hierarchically composable set of disciplines providing facilities such as bandwidth limiting, packet loss, and packet delay. As in the FreeBSD case, simplex (outgoing) link shaping is used on point-to-point links, while duplex shaping (going out, and coming in an interface) is used with LANs. See the previous section to understand why this is done.   Unlike FreeBSD, Linux traffic shaping modules must be connected directly to a network device, and hence don't require a firewall directive to place packets into them. This means that all packets must pass through the shaping tree connected to a particular interface. Note that filters may be used on the shapers themselves to discriminate traffic flows, so it's not strictly the case that all traffic must be shaped if modules are attached. However, all traffic to an interface, at the least, is queued and de-queued through the root module of the shaping hierarchy. And all interfaces have at least a root module, but it is normally just a fast FIFO.   Also of note is the fact that Linux traffic shaping normally only happens on the outgoing side of an interface, and requires a special virtual network device (known as an intermediate queuing device or IMQ) to capture incoming packets for shaping. This also requires the aid of the Linux firewalling facility, iptables, to divert the packets to the IMQs prior to routing.   Here is an example duplex-link configuration with 50Mbps of bandwidth, a 0.05 PLR, and 20ms of delay in both directions:   Outgoing side setup commands:  # implicitly sets up class 1:1\ntc qdisc add dev eth0 root handle 1 plr 0.05\n\n# attach to class 1:1 and tell the module the default place to send\n# traffic is to class 2:1 (could attach filters to discriminate)\ntc qdisc add dev eth0 parent 1:1 handle 2 htb default 1\n\n# class 2:1 does the actual limiting\ntc class add dev eth0 parent 2 classid 2:1 htb rate 50Mbit ceil 50Mbit\n\n# attach to class 2:1, also implicitly creates class 3:1, and attaches\n# a FIFO queue to it.\ntc qdisc add dev eth0 parent 2:1 handle 3 delay usecs 20000  The incoming side setup commands will look the same, but with eth0 replaced by imq0. Also, we have to tell the kernel to send packets coming into eth0 to imq0 (where they will be shaped):  iptables -t mangle -A PREROUTING -i eth0 -j IMQ --todev 0  A flood ping sequence utilizing eth0 (echo- echo-reply) would experience a round trip delay of 40 ms, be restricted to 50Mbit, and have a 10% chance of losing packets. The doubling of numbers is due to shaping as packets go out, and come back in the interface.   At the time of writing, we don't support multiplexed links under Linux, so no explicit matching against nexthop is necessary.", 
            "title": "Linux Technical Discussion"
        }, 
        {
            "location": "/core/sharing/", 
            "text": "Sharing and Using Shared Materials\n\uf0c1\n\n\nThese functionalities are available by choosing \nMy Deterlab\n and then the \nSharing\n tab.\n\n\nAny user can share materials they think would be helpful to others, and any user can find shared materials. \n\n\nWhat can be shared \n\uf0c1\n\n\nCurrently, you can share teaching materials (lectures, homeworks, teacher manuals or class capture-the-flag exercises) and research materials (experiments, tools, datasets and HOWTOs).\n\n\nAny material can be shared either by uploading a ZIP file or by specifying a URL. ZIP files will be unzipped prior to moving them to our shared space. \n\n\nRequirements for Sharing\n\uf0c1\n\n\nA material is uniquely identified by its title, type and username of the user who shared it. Multiple users can share versions of a material with the same title and type, or a single user can share multiple materials with the same title but different types.\n\n\nWe assume that ZIP files are created by having a folder with materials you want to share and zipping it. This folder \nmust\n have an index.html file inside, which contains at least 50 characters.\n\n\nWhen sharing something you need to specify its type (see \nhere\n), a few keywords that people can use to search for your material and an E-mail address of the person responsible for maintenance.\n\n\nMaterial Types \n\uf0c1\n\n\nHere are some semi-formal definitions of the types of materials that can be shared.\n\n\nLectures\n\uf0c1\n\n\nA lecture can be a Word document, a set of slides, or a URL to an online content. You could share a whole lecture or a set of smaller modules covering some topic.\n\n\nHomework\n\uf0c1\n\n\nA homework is a specification of an assignment that a student will see. Ideally it should follow the format specified \nhere\n but we will accept other formats.\n\n\nTeacher Manuals\n\uf0c1\n\n\nA teacher manual accompanies a homework or a CCTF. Ideally it should follow the format specified \nhere\n but we will accept other formats. \n\n\n\n\nNote\n\n\nTeacher manuals are only visible to heads of class projects on DETERLab and they can only be downloaded as ZIP files.\n\n\n\n\nCCTFs\n\uf0c1\n\n\nCCTFs or Class Capture-the-Flag exercises are targeted exercises that pit two student teams against each other in attack/defense scenarios. These are ideal to assign to classes after they have completed a few homeworks with DETERLab. For more information about CCTFs see \nthis paper\n.\n\n\nExperiments\n\uf0c1\n\n\nAn experiment is a set of files (e.g., NS file, input data, output data, setup scripts, etc.) that enables someone else to recreate an experiment done by a user. This definition is intentionally open-ended. Share any files you believe are useful to others that seek to repeat or build upon your work.\n\n\nDatasets\n\uf0c1\n\n\nA data set is a collection of data you want to share with others. Such data should be either related to your DETERLab use (e.g., it was used in an experiment by you that later produced results for a publication) or should be generated by your DETERLab experiment (e.g., traces of traffic collected in your experiment as you performed some attack).\n\n\n\n\nWarning\n\n\nOnly share data that is not private! If in doubt, ask us.\n\n\n\n\nTools\n\uf0c1\n\n\nA tool is some application that is useful for experimentation, such as a traffic generator, an attack generator, etc. Ideally you would share both the source and the binary of your tool, and some test data.\n\n\nHOWTOs\n\uf0c1\n\n\nA HOWTO is a small building block for an experiment. For example, it could be a recipe how to set up a DNS server, or how to perform a SYN flood attack. \n\n\nFinding Materials \n\uf0c1\n\n\nYou can find materials by searching for them under the Sharing tab. You can search by keyword (or leave empty to search for all materials), by type or both.\n\n\nAdopting Materials \n\uf0c1\n\n\nOnce you [#find find] materials you are interested in you can either download them as a ZIP file, or if you teach a class, you can adopt them directly into your class (visibility of adopted materials is set to all students). If you want to modify a material before adopting it to your class, download it as ZIP, apply changes locally and then upload it to your class. \n\n\nModifying Materials Shared by You \n\uf0c1\n\n\nIf you want to modify a material you have shared previously simply re-share it using the same title and type. This will overwrite the previously-shared content.", 
            "title": "Using Shared Materials"
        }, 
        {
            "location": "/core/sharing/#sharing-and-using-shared-materials", 
            "text": "These functionalities are available by choosing  My Deterlab  and then the  Sharing  tab.  Any user can share materials they think would be helpful to others, and any user can find shared materials.", 
            "title": "Sharing and Using Shared Materials"
        }, 
        {
            "location": "/core/sharing/#what-can-be-shared", 
            "text": "Currently, you can share teaching materials (lectures, homeworks, teacher manuals or class capture-the-flag exercises) and research materials (experiments, tools, datasets and HOWTOs).  Any material can be shared either by uploading a ZIP file or by specifying a URL. ZIP files will be unzipped prior to moving them to our shared space.   Requirements for Sharing \uf0c1  A material is uniquely identified by its title, type and username of the user who shared it. Multiple users can share versions of a material with the same title and type, or a single user can share multiple materials with the same title but different types.  We assume that ZIP files are created by having a folder with materials you want to share and zipping it. This folder  must  have an index.html file inside, which contains at least 50 characters.  When sharing something you need to specify its type (see  here ), a few keywords that people can use to search for your material and an E-mail address of the person responsible for maintenance.  Material Types  \uf0c1  Here are some semi-formal definitions of the types of materials that can be shared.  Lectures \uf0c1  A lecture can be a Word document, a set of slides, or a URL to an online content. You could share a whole lecture or a set of smaller modules covering some topic.  Homework \uf0c1  A homework is a specification of an assignment that a student will see. Ideally it should follow the format specified  here  but we will accept other formats.  Teacher Manuals \uf0c1  A teacher manual accompanies a homework or a CCTF. Ideally it should follow the format specified  here  but we will accept other formats.    Note  Teacher manuals are only visible to heads of class projects on DETERLab and they can only be downloaded as ZIP files.   CCTFs \uf0c1  CCTFs or Class Capture-the-Flag exercises are targeted exercises that pit two student teams against each other in attack/defense scenarios. These are ideal to assign to classes after they have completed a few homeworks with DETERLab. For more information about CCTFs see  this paper .  Experiments \uf0c1  An experiment is a set of files (e.g., NS file, input data, output data, setup scripts, etc.) that enables someone else to recreate an experiment done by a user. This definition is intentionally open-ended. Share any files you believe are useful to others that seek to repeat or build upon your work.  Datasets \uf0c1  A data set is a collection of data you want to share with others. Such data should be either related to your DETERLab use (e.g., it was used in an experiment by you that later produced results for a publication) or should be generated by your DETERLab experiment (e.g., traces of traffic collected in your experiment as you performed some attack).   Warning  Only share data that is not private! If in doubt, ask us.   Tools \uf0c1  A tool is some application that is useful for experimentation, such as a traffic generator, an attack generator, etc. Ideally you would share both the source and the binary of your tool, and some test data.  HOWTOs \uf0c1  A HOWTO is a small building block for an experiment. For example, it could be a recipe how to set up a DNS server, or how to perform a SYN flood attack.", 
            "title": "What can be shared "
        }, 
        {
            "location": "/core/sharing/#finding-materials", 
            "text": "You can find materials by searching for them under the Sharing tab. You can search by keyword (or leave empty to search for all materials), by type or both.", 
            "title": "Finding Materials "
        }, 
        {
            "location": "/core/sharing/#adopting-materials", 
            "text": "Once you [#find find] materials you are interested in you can either download them as a ZIP file, or if you teach a class, you can adopt them directly into your class (visibility of adopted materials is set to all students). If you want to modify a material before adopting it to your class, download it as ZIP, apply changes locally and then upload it to your class.", 
            "title": "Adopting Materials "
        }, 
        {
            "location": "/core/sharing/#modifying-materials-shared-by-you", 
            "text": "If you want to modify a material you have shared previously simply re-share it using the same title and type. This will overwrite the previously-shared content.", 
            "title": "Modifying Materials Shared by You "
        }, 
        {
            "location": "/core/deterlab-commands/", 
            "text": "The following commands are available from the commandline on \nusers.isi.deterlab.net\n. \n\n\n\n\nNote\n\n\nCommands should be pre-pended with the path: \n/usr/testbed/bin\n.\n\n\n\n\nFor example, to start an experiment, you would use:\n\n/usr/testbed/bin/startexp [options]\n\n\nTransport Layers\n\uf0c1\n\n\nThe DETERLab XMLRPC server can be accessed via two different transport layers: SSH and SSL.\n\n\nHow to use SSH keys\n\nFollow \nthese directions\n if you are unfamiliar with using SSH.\n\n\nHow to use SSL\n\nYou need to request a certificate from the \nDETERLab website\n in order to use the SSL based server. \n\n\n\n\nClick the ''My DETERLab'' menu item in the navbar, click the ''Profile'' tab on the page and then click on the ''Generate SSL Certificate'' link. \n\n\nEnter a passphrase to use to encrypt the private key. \n\n\nOnce the key has been created, you will be given a link to download a text version (in PEM format). Simply provide this certificate as an input to your SSL client. \n\n\n\n\nOperational Commands\n\uf0c1\n\n\nstartexp\n: Start an DETERLab experiment \n\uf0c1\n\n\nstartexp [-q] [-i [-w]] [-f] [-N] [-E description] [-g gid]\n         [-S reason] [-L reason] [-a \ntime\n] [-l \ntime\n]\n         -p \npid\n -e \neid\n \nnsfile\n\n\n\n\n\nOptions:\n\n\n-i\n Swapin immediately; by default, the experiment is batched.\n\n\n-w\n Wait for non-batchmode experiment to preload or swapin.\n\n\n-f\n Preload experiment (do not swapin or queue yet).\n\n\n-q\n Be less verbose.\n\n\n-S\n Experiment cannot be swapped; must provide reason.\n\n\n-L\n Experiment cannot be IDLE swapped; must provide reason.\n\n\n-a\n Auto swapout NN minutes after experiment is swapped in.\n\n\n-l\n Auto swapout NN minutes after experiment goes idle.\n\n\n-E\n A concise sentence describing your experiment.\n\n\n-g\n The subgroup in which to create the experiment.\n\n\n-p\n The project in which to create the experiment.\n\n\n-e\n The experiment name (unique, alphanumeric, no blanks).\n\n\n-N\n Suppress most email to the user and testbed-ops.\n\n\nnsfile\n NS file to parse for experiment\n\n\n\n\nbatchexp\n: Synonym for startexp \n\uf0c1\n\n\nThis is a legacy command. See command description for startexp.\n\n\n\n\nendexp\n: Terminate an experiment \n\uf0c1\n\n\nendexp [-w] [-N] -e pid,eid\nendexp [-w] [-N] pid eid\n\n\n\n\nOptions:\n\n\n-w\n  Wait for experiment to finish terminating.\n\n\n-e\n  Project and Experiment ID.\n\n\n-N\n  Suppress most email to the user and testbed-ops.\n\n\n\n\nNote\n\n\n\n\nUse with caution!\n This will tear down your experiment and you will not be able to swap it back in.\n\n\nBy default, \nendexp\n runs in the background, sending you email when the transition has completed. Use the \n-w\n option to wait in the foreground, returning exit status. Email is still sent.\n\n\nThe experiment may be terminated when it is currently swapped in ''or'' swapped out.\n\n\n\n\n\n\n\n\ndelay_config\n: Change the link shaping characteristics for a link or LAN \n\uf0c1\n\n\ndelay_config [options] -e pid,eid link PARAM#value ...\ndelay_config [options] pid eid link PARAM#value ...\n\n\n\n\nOptions:\n\n\n-m\n Modify virtual experiment as well as current state.\n\n\n-s\n Select the source of the link to change.\n\n\n-e\n Project and Experiment ID to operate on.\n\n\nlink\n Name of link from your NS file (ie: \nlink1\n).\n\n\nParameters:\n\n\nBANDWIDTH#NNN\n N#bandwidth (10-100000 Kbits per second)\n\n\nPLR#NNN\n N#lossrate (0 \n# plr \n 1)\n\n\nDELAY#NNN\n N#delay (one-way delay in milliseconds \n 0)\n\n\nLIMIT#NNN\n The queue size in bytes or packets\n\n\nQUEUE-IN-BYTES#N\n 0 means in packets, 1 means in bytes\n\n\nRED/GRED Options:\n (only if link was specified as RED/GRED)\n\n\nMAXTHRESH#NNN\n Maximum threshold for the average Q size\n\n\nTHRESH#NNN\n Minimum threshold for the average Q size\n\n\nLINTERM#NNN\n Packet dropping probability\n\n\nQ_WEIGHT#NNN\n For calculating the average queue size\n\n\n\n\nmodexp\n: Modify experiment \n\uf0c1\n\n\nmodexp [-r] [-s] [-w] [-N] -e pid,eid nsfile\nmodexp [-r] [-s] [-w] [-N] pid eid nsfile\n\n\n\n\nOptions:\n\n\n-w\n Wait for experiment to finish swapping.\n\n\n-e\n Project and Experiment ID.\n\n\n-r\n Reboot nodes (when experiment is active).\n\n\n-s\n Restart event scheduler (when experiment is active).\n\n\n-N\n Suppress most email to the user and testbed-ops\n\n\n\n\nNote\n\n\n\n\nBy default, \nmodexp\n runs in the background, sending you email when the transition has completed. Use the \n-w\n option to wait in the foreground, returning exit status. Email is still sent.\n\n\nThe experiment can be either swapped in ''or'' swapped out.\n\n\nIf the experiment is swapped out, the new NS file replaces the existing NS file (the virtual topology is updated). \n\n\nIf the experiment is swapped in (active), the physical topology is also updated, subject to the \n-r\n and \n-s\n options above.\n\n\n\n\n\n\n\n\n\n\n\n\nswapexp\n: Swap experiment in or out \n\uf0c1\n\n\nswapexp -e pid,eid in|out\nswapexp pid eid in|out\n\n\n\n\nOptions:\n\n\n-w\n Wait for experiment to finish swapping.\n\n\n-e\n Project and Experiment ID.\n\n\n-N\n Suppress most email to the user and testbed-ops.\n\n\nin\n Swap experiment in  (must currently be swapped out).\n\n\nout\n Swap experiment out (must currently be swapped in)\n\n\n\n\nNote\n\n\n\n\nBy default, \nswapexp\n runs in the background, sending you email when the transition has completed. Use the \n-w\n option to wait in the foreground, returning exit status. Email is still sent.\n\n\n\n\n\n\n\n\ncreate_image\n: Create a disk image from a node \n\uf0c1\n\n\ncreate_image [options] imageid node\n\n\n\n\nOptions:\n\n\n-w\n Wait for image to be created.\n\n\n-p\n Project ID of imageid.\n\n\nimageid\n Name of the image.\n\n\nnode\n Node to create image from (pcXXX).\n\n\nExample:\n\n\nThe following command creates or re-creates an image for a particular project:\n\n\ncreate_image -p \nproj\n \nimageid\n \nnode\n\n\n\n\n\n\n\neventsys_control\n: Start/Stop/Restart the event system \n\uf0c1\n\n\neventsys_control -e pid,eid start|stop|replay\neventsys_control pid eid start|stop|replay\n\n\n\n\nOptions:\n\n\n-e\n Project and Experiment ID.\n\n\nstop\n Stop the event scheduler.\n\n\nstart\n Start the event stream from time index 0.\n\n\nreplay\n Replay the event stream from time index 0\n\n\n\n\nloghole\n: Downloads and manages an experiment's log files \n\uf0c1\n\n\nThis utility downloads log files from certain directories on the experimental nodes (e.g. \n/local/logs\n) to the DETERLab \nusers\n machine. After downloading, it can also be used to produce and manage archives of the log files.\n\n\nUsing this utility to manage an experiment's log files is encouraged because it will transfer the logs in a network-friendly manner and is already integrated with the rest of DETERLab. For example, any programs executed using the DETERLab event-system will have their standard output/error automatically placed in the \n/local/logs\n directory. The tool can also be used to preserve multiple trials of an experiment by producing and managing zip archives of the logs. \n\n\nloghole [-hVdqva] [-e pid/eid] [-s server] [-P port] action [...]\n\nloghole sync [-nPs] [-r remotedir] [-l localdir] [node1 node2 ...]\n\nloghole validate\n\nloghole  archive [-k (i-delete|space-is-needed)] [-a days] [-c comment]\n    [-d] [archive-name]\n\nloghole change [-k (i-delete|space-is-needed)] [-a days]  [-c  comment]\n    archive-name1 [archive-name2 ...]\n\nloghole list [-O1!Xo] [-m atmost] [-s megabytes]\n\nloghole show [archive-name]\n\nloghole clean [-fne] [node1 node2 ...]\n\nloghole gc [-n] [-m atmost] [-s megabytes]\n\n\n\n\nOptions:\n\n\n-h, --help\n Print  the  usage message for the loghole utility as a whole or, if an action is given, the usage message for that action.\n\n\n-V, --version\n Print out version information and exit.\n\n\n-d, --debug\n Output debugging messages.\n\n\n-q, --quiet\n Decrease the level of verbosity, this is subtractive, so multiple uses of this option will make the utility quieter and quieter. The default level of verbosity is human-readable, below that is machine-readable, and below that is silent. For example, the default output from the \"list\" action looks like:\n\n\n  [ ] foobar.1.zip   10/15\n  [!] foobar.0.zip   10/13\n\n\n\n\nUsing a single \n-q\n option changes the output to look like: \n\n\n          foobar.1.zip\n          foobar.0.zip\n\n\n\n\n-e, --experiment#PID/EID\n Specify the experiment(s) to operate on using the Project ID (or project name) and Experiment ID (or experiment name).  If multiple \n-e\n options are  given,  the  action will apply to all of them.  This option overrides the default behavior of inferring the experiment  from (Note: this sentence was cut off in the Emulab documentation).\n\n\nExamples:\n\n\nTo synchronize the log directory for experiment \nneptune/myexp\n with the log holes on the experimental nodes:\n\n\n[vmars@users ~] loghole -e neptune/myexp sync\n\n\n\n\nTo archive the newly recovered logs and print out just the name of the new log file:\n\n\n[vmars@users ~] loghole -e neptune/myexp -q archive\n\n\n\n\nMore information:\n\n\nTo see the detailed documentation of \nloghole\n, view the man page on \nusers\n:\n\n\nloghole man\n\n\n\n\n\n\nos_load\n: Reload disks on selected nodes or all nodes in an experiment \n\uf0c1\n\n\nos_load [options] node [node ...]\nos_load [options] -e pid,eid\n\n\n\n\nOptions:\n\n\n-i\n Specify image name; otherwise load default image.\n\n\n-p\n Specify project for finding image name (\n-i\n).\n\n\n-s\n Do \nnot\n wait for nodes to finish reloading.\n\n\n-m\n Specify internal image id (instead of \n-i\n and \n-p\n).\n\n\n-r\n Do \nnot\n reboot nodes; do that yourself.\n\n\n-e\n Reboot all nodes in an experiment.\n\n\nnode\n Node to reboot (pcXXX).\n\n\n\n\nportstats\n: Get portstats from the switches \n\uf0c1\n\n\nportstats \n-p | pid eid\n [vname ...] [vname:port ...]\n\n\n\n\nOptions:\n\n\n-e\n Show only error counters.\n\n\n-a\n Show all stats.\n\n\n-z\n Zero out counts for selected counters after printing.\n\n\n-q\n Quiet: don't actually print counts - useful with \n-z\n.\n\n\n-c\n Print absolute, rather than relative, counts.\n\n\n-p\n The machines given are physical, not virtual, node IDs. \nNo pid and eid should be given with this option.\n\n\n\n\nWarning\n\n\nIf only the pid and eid are given, this command prints out information about all ports in the experiment. Otherwise, output is limited to the nodes and/or ports given.\n\n\n\n\n\n\nNote\n\n\n\n\nStatistics are reported from the switch's perspective. This means that ''In'' packets are those sent FROM the node, and ''Out'' packets are those sent TO the node.\n\n\nIn the output, packets described as 'NUnicast' or 'NUcast' are non-unicast (broadcast or multicast) packets.\n\n\n\n\n\n\n\n\nnode_reboot\n: Reboot selected nodes or all nodes in an experiment \n\uf0c1\n\n\nUse this if you need to powercycle a node.\n\n\nnode_reboot [options] node [node ...]\nnode_reboot [options] -e pid,eid\n\n\n\n\nOptions:\n\n\n-w\n Wait for nodes is come back up.\n\n\n-c\n Reconfigure nodes instead of rebooting.\n\n\n-f\n Power cycle nodes (skip reboot!)\n\n\n-e\n Reboot all nodes in an experiment.\n\n\nnode\n Node to reboot (pcXXX).\n\n\n\n\nNote\n\n\n\n\nYou may provide more than one node on the command line. \n\n\nBe aware that you may power cycle only nodes in projects that you are member of. \n\n\nnode_reboot\n does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason, \nnode_reboot\n may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above). \n\n\nYou may reboot all the nodes in an experiment by using the \n-e\n option to specify the project and experiment names. This option is provided as a shorthand method for rebooting large groups of nodes. \n\n\n\n\n\n\nExample:\n\n\nThe following command will reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project. \n\n\n    node_reboot -e testbed,multicast\n\n\n\n\n\n\nexpwait\n: Wait for experiment to reach a state \n\uf0c1\n\n\nexpwait [-t timeout] -e pid,eid state\nexpwait [-t timeout] pid eid state\n\n\n\n\nOptions:\n\n\n-e\n Project and Experiment ID in format \nprojectID\n/\nexperimentID\n.\n\n\n-t\n Maximum time to wait (in seconds).\n\n\nInformational Commands\n\uf0c1\n\n\nnode_list\n: Print physical mapping of nodes in an experiment \n\uf0c1\n\n\nnode_list [options] -e pid,eid\n\n\n\n\nOptions:\n\n\n-e\n Project and Experiment ID to list.\n\n\n-p\n Print physical (DETER database) names (default).\n\n\n-P\n Like \n-p\n, but include node type.\n\n\n-v\n Print virtual (experiment assigned) names.\n\n\n-h\n Print physical name of host for virtual nodes.\n\n\n-c\n Print container VMs and physical nodes.\n\n\n\n\nNote\n\n\n\n\nThis command now queries the XMLRPC interface as it used to do. Users who had been using \nscript_wrapper.py node_list\n to access this function should use \n/usr/testbed/bin/node_list\n instead.\n\n\nThe \n-c\n flag that outputs containerized node names has been modified in two ways. \n\n\nNames are produced without the DNS qualifiers as node names provided by other options of this command are. A node in a VM container named \na\n will be reported as \na\n not \na.exp.proj\n as earlier versions of this feature did.\n\n\nThis option now reports embedded_pnode containers as well (physical machines). If no containers VMs are present in an experiment, \nnode_list -c\n and \nnode_list -v\n produce identical output.\n\n\n\n\n\n\nThe \nnode_list\n command is now available as \nnode_summary\n.\n\n\n\n\n\n\n\n\nexpinfo\n: Get information about an experiment \n\uf0c1\n\n\nexpinfo [-n] [-m] [-l] [-d] [-a] -e pid,eid\nexpinfo [-n] [-m] [-l] [-d] [-a] pid eid\n\n\n\n\nOptions:\n\n\n-e\n Project and Experiment ID.\n\n\n-n\n Show node information.\n\n\n-m\n Show node mapping.\n\n\n-l\n Show link information.\n\n\n-a\n Show all of the above.\n\n\n\n\nnode_avail\n: Print free node counts \n\uf0c1\n\n\nnode_avail [-p project] [-c class] [-t type]\n\n\n\n\nOptions:\n\n\n-p project\n Specify project credentials for node types that are restricted.\n\n\n-c class\n The node class (Default: pc).\n\n\n-t type\n The node type.\n\n\nExample:\n\n\nThe following command will print free nodes on pc850 nodes:\n\n\n    $ node_avail -t pc850\n\n\n\n\n\n\nnode_avail_list\n: Print physical node_ids of available nodes \n\uf0c1\n\n\nnode_avail_list [-p project] [-c class] [-t type] [-n nodes]\n\n\n\n\nOptions:\n\n\n-p project\n Specify project credentials for node types that are restricted.\n\n\n-c class\n The node class (Default: pc).\n\n\n-t type\n The node type.\n\n\n-n pcX,pcY,...,pcZ\n A list of physical node_ids.\n\n\nExample:\n\n\nThe following command will print the physical node_ids for available pc850 nodes:\n\n\n    $ node_avail_list -t pc850\n\n\n\n\n\n\nnscheck\n: Check and NS file for parser errors \n\uf0c1\n\n\nnscheck nsfile\n\n\n\n\nOption:\n\n\nnsfile\n Path to NS file you to wish check for parse errors.", 
            "title": "DETERLab Commands"
        }, 
        {
            "location": "/core/deterlab-commands/#transport-layers", 
            "text": "The DETERLab XMLRPC server can be accessed via two different transport layers: SSH and SSL.  How to use SSH keys \nFollow  these directions  if you are unfamiliar with using SSH.  How to use SSL \nYou need to request a certificate from the  DETERLab website  in order to use the SSL based server.    Click the ''My DETERLab'' menu item in the navbar, click the ''Profile'' tab on the page and then click on the ''Generate SSL Certificate'' link.   Enter a passphrase to use to encrypt the private key.   Once the key has been created, you will be given a link to download a text version (in PEM format). Simply provide this certificate as an input to your SSL client.", 
            "title": "Transport Layers"
        }, 
        {
            "location": "/core/deterlab-commands/#operational-commands", 
            "text": "startexp : Start an DETERLab experiment  \uf0c1  startexp [-q] [-i [-w]] [-f] [-N] [-E description] [-g gid]\n         [-S reason] [-L reason] [-a  time ] [-l  time ]\n         -p  pid  -e  eid   nsfile   Options:  -i  Swapin immediately; by default, the experiment is batched.  -w  Wait for non-batchmode experiment to preload or swapin.  -f  Preload experiment (do not swapin or queue yet).  -q  Be less verbose.  -S  Experiment cannot be swapped; must provide reason.  -L  Experiment cannot be IDLE swapped; must provide reason.  -a  Auto swapout NN minutes after experiment is swapped in.  -l  Auto swapout NN minutes after experiment goes idle.  -E  A concise sentence describing your experiment.  -g  The subgroup in which to create the experiment.  -p  The project in which to create the experiment.  -e  The experiment name (unique, alphanumeric, no blanks).  -N  Suppress most email to the user and testbed-ops.  nsfile  NS file to parse for experiment   batchexp : Synonym for startexp  \uf0c1  This is a legacy command. See command description for startexp.   endexp : Terminate an experiment  \uf0c1  endexp [-w] [-N] -e pid,eid\nendexp [-w] [-N] pid eid  Options:  -w   Wait for experiment to finish terminating.  -e   Project and Experiment ID.  -N   Suppress most email to the user and testbed-ops.   Note   Use with caution!  This will tear down your experiment and you will not be able to swap it back in.  By default,  endexp  runs in the background, sending you email when the transition has completed. Use the  -w  option to wait in the foreground, returning exit status. Email is still sent.  The experiment may be terminated when it is currently swapped in ''or'' swapped out.     delay_config : Change the link shaping characteristics for a link or LAN  \uf0c1  delay_config [options] -e pid,eid link PARAM#value ...\ndelay_config [options] pid eid link PARAM#value ...  Options:  -m  Modify virtual experiment as well as current state.  -s  Select the source of the link to change.  -e  Project and Experiment ID to operate on.  link  Name of link from your NS file (ie:  link1 ).  Parameters:  BANDWIDTH#NNN  N#bandwidth (10-100000 Kbits per second)  PLR#NNN  N#lossrate (0  # plr   1)  DELAY#NNN  N#delay (one-way delay in milliseconds   0)  LIMIT#NNN  The queue size in bytes or packets  QUEUE-IN-BYTES#N  0 means in packets, 1 means in bytes  RED/GRED Options:  (only if link was specified as RED/GRED)  MAXTHRESH#NNN  Maximum threshold for the average Q size  THRESH#NNN  Minimum threshold for the average Q size  LINTERM#NNN  Packet dropping probability  Q_WEIGHT#NNN  For calculating the average queue size   modexp : Modify experiment  \uf0c1  modexp [-r] [-s] [-w] [-N] -e pid,eid nsfile\nmodexp [-r] [-s] [-w] [-N] pid eid nsfile  Options:  -w  Wait for experiment to finish swapping.  -e  Project and Experiment ID.  -r  Reboot nodes (when experiment is active).  -s  Restart event scheduler (when experiment is active).  -N  Suppress most email to the user and testbed-ops   Note   By default,  modexp  runs in the background, sending you email when the transition has completed. Use the  -w  option to wait in the foreground, returning exit status. Email is still sent.  The experiment can be either swapped in ''or'' swapped out.  If the experiment is swapped out, the new NS file replaces the existing NS file (the virtual topology is updated).   If the experiment is swapped in (active), the physical topology is also updated, subject to the  -r  and  -s  options above.       swapexp : Swap experiment in or out  \uf0c1  swapexp -e pid,eid in|out\nswapexp pid eid in|out  Options:  -w  Wait for experiment to finish swapping.  -e  Project and Experiment ID.  -N  Suppress most email to the user and testbed-ops.  in  Swap experiment in  (must currently be swapped out).  out  Swap experiment out (must currently be swapped in)   Note   By default,  swapexp  runs in the background, sending you email when the transition has completed. Use the  -w  option to wait in the foreground, returning exit status. Email is still sent.     create_image : Create a disk image from a node  \uf0c1  create_image [options] imageid node  Options:  -w  Wait for image to be created.  -p  Project ID of imageid.  imageid  Name of the image.  node  Node to create image from (pcXXX).  Example:  The following command creates or re-creates an image for a particular project:  create_image -p  proj   imageid   node    eventsys_control : Start/Stop/Restart the event system  \uf0c1  eventsys_control -e pid,eid start|stop|replay\neventsys_control pid eid start|stop|replay  Options:  -e  Project and Experiment ID.  stop  Stop the event scheduler.  start  Start the event stream from time index 0.  replay  Replay the event stream from time index 0   loghole : Downloads and manages an experiment's log files  \uf0c1  This utility downloads log files from certain directories on the experimental nodes (e.g.  /local/logs ) to the DETERLab  users  machine. After downloading, it can also be used to produce and manage archives of the log files.  Using this utility to manage an experiment's log files is encouraged because it will transfer the logs in a network-friendly manner and is already integrated with the rest of DETERLab. For example, any programs executed using the DETERLab event-system will have their standard output/error automatically placed in the  /local/logs  directory. The tool can also be used to preserve multiple trials of an experiment by producing and managing zip archives of the logs.   loghole [-hVdqva] [-e pid/eid] [-s server] [-P port] action [...]\n\nloghole sync [-nPs] [-r remotedir] [-l localdir] [node1 node2 ...]\n\nloghole validate\n\nloghole  archive [-k (i-delete|space-is-needed)] [-a days] [-c comment]\n    [-d] [archive-name]\n\nloghole change [-k (i-delete|space-is-needed)] [-a days]  [-c  comment]\n    archive-name1 [archive-name2 ...]\n\nloghole list [-O1!Xo] [-m atmost] [-s megabytes]\n\nloghole show [archive-name]\n\nloghole clean [-fne] [node1 node2 ...]\n\nloghole gc [-n] [-m atmost] [-s megabytes]  Options:  -h, --help  Print  the  usage message for the loghole utility as a whole or, if an action is given, the usage message for that action.  -V, --version  Print out version information and exit.  -d, --debug  Output debugging messages.  -q, --quiet  Decrease the level of verbosity, this is subtractive, so multiple uses of this option will make the utility quieter and quieter. The default level of verbosity is human-readable, below that is machine-readable, and below that is silent. For example, the default output from the \"list\" action looks like:    [ ] foobar.1.zip   10/15\n  [!] foobar.0.zip   10/13  Using a single  -q  option changes the output to look like:             foobar.1.zip\n          foobar.0.zip  -e, --experiment#PID/EID  Specify the experiment(s) to operate on using the Project ID (or project name) and Experiment ID (or experiment name).  If multiple  -e  options are  given,  the  action will apply to all of them.  This option overrides the default behavior of inferring the experiment  from (Note: this sentence was cut off in the Emulab documentation).  Examples:  To synchronize the log directory for experiment  neptune/myexp  with the log holes on the experimental nodes:  [vmars@users ~] loghole -e neptune/myexp sync  To archive the newly recovered logs and print out just the name of the new log file:  [vmars@users ~] loghole -e neptune/myexp -q archive  More information:  To see the detailed documentation of  loghole , view the man page on  users :  loghole man   os_load : Reload disks on selected nodes or all nodes in an experiment  \uf0c1  os_load [options] node [node ...]\nos_load [options] -e pid,eid  Options:  -i  Specify image name; otherwise load default image.  -p  Specify project for finding image name ( -i ).  -s  Do  not  wait for nodes to finish reloading.  -m  Specify internal image id (instead of  -i  and  -p ).  -r  Do  not  reboot nodes; do that yourself.  -e  Reboot all nodes in an experiment.  node  Node to reboot (pcXXX).   portstats : Get portstats from the switches  \uf0c1  portstats  -p | pid eid  [vname ...] [vname:port ...]  Options:  -e  Show only error counters.  -a  Show all stats.  -z  Zero out counts for selected counters after printing.  -q  Quiet: don't actually print counts - useful with  -z .  -c  Print absolute, rather than relative, counts.  -p  The machines given are physical, not virtual, node IDs.  No pid and eid should be given with this option.   Warning  If only the pid and eid are given, this command prints out information about all ports in the experiment. Otherwise, output is limited to the nodes and/or ports given.    Note   Statistics are reported from the switch's perspective. This means that ''In'' packets are those sent FROM the node, and ''Out'' packets are those sent TO the node.  In the output, packets described as 'NUnicast' or 'NUcast' are non-unicast (broadcast or multicast) packets.     node_reboot : Reboot selected nodes or all nodes in an experiment  \uf0c1  Use this if you need to powercycle a node.  node_reboot [options] node [node ...]\nnode_reboot [options] -e pid,eid  Options:  -w  Wait for nodes is come back up.  -c  Reconfigure nodes instead of rebooting.  -f  Power cycle nodes (skip reboot!)  -e  Reboot all nodes in an experiment.  node  Node to reboot (pcXXX).   Note   You may provide more than one node on the command line.   Be aware that you may power cycle only nodes in projects that you are member of.   node_reboot  does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason,  node_reboot  may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above).   You may reboot all the nodes in an experiment by using the  -e  option to specify the project and experiment names. This option is provided as a shorthand method for rebooting large groups of nodes.     Example:  The following command will reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project.       node_reboot -e testbed,multicast   expwait : Wait for experiment to reach a state  \uf0c1  expwait [-t timeout] -e pid,eid state\nexpwait [-t timeout] pid eid state  Options:  -e  Project and Experiment ID in format  projectID / experimentID .  -t  Maximum time to wait (in seconds).", 
            "title": "Operational Commands"
        }, 
        {
            "location": "/core/deterlab-commands/#informational-commands", 
            "text": "node_list : Print physical mapping of nodes in an experiment  \uf0c1  node_list [options] -e pid,eid  Options:  -e  Project and Experiment ID to list.  -p  Print physical (DETER database) names (default).  -P  Like  -p , but include node type.  -v  Print virtual (experiment assigned) names.  -h  Print physical name of host for virtual nodes.  -c  Print container VMs and physical nodes.   Note   This command now queries the XMLRPC interface as it used to do. Users who had been using  script_wrapper.py node_list  to access this function should use  /usr/testbed/bin/node_list  instead.  The  -c  flag that outputs containerized node names has been modified in two ways.   Names are produced without the DNS qualifiers as node names provided by other options of this command are. A node in a VM container named  a  will be reported as  a  not  a.exp.proj  as earlier versions of this feature did.  This option now reports embedded_pnode containers as well (physical machines). If no containers VMs are present in an experiment,  node_list -c  and  node_list -v  produce identical output.    The  node_list  command is now available as  node_summary .     expinfo : Get information about an experiment  \uf0c1  expinfo [-n] [-m] [-l] [-d] [-a] -e pid,eid\nexpinfo [-n] [-m] [-l] [-d] [-a] pid eid  Options:  -e  Project and Experiment ID.  -n  Show node information.  -m  Show node mapping.  -l  Show link information.  -a  Show all of the above.   node_avail : Print free node counts  \uf0c1  node_avail [-p project] [-c class] [-t type]  Options:  -p project  Specify project credentials for node types that are restricted.  -c class  The node class (Default: pc).  -t type  The node type.  Example:  The following command will print free nodes on pc850 nodes:      $ node_avail -t pc850   node_avail_list : Print physical node_ids of available nodes  \uf0c1  node_avail_list [-p project] [-c class] [-t type] [-n nodes]  Options:  -p project  Specify project credentials for node types that are restricted.  -c class  The node class (Default: pc).  -t type  The node type.  -n pcX,pcY,...,pcZ  A list of physical node_ids.  Example:  The following command will print the physical node_ids for available pc850 nodes:      $ node_avail_list -t pc850   nscheck : Check and NS file for parser errors  \uf0c1  nscheck nsfile  Option:  nsfile  Path to NS file you to wish check for parse errors.", 
            "title": "Informational Commands"
        }, 
        {
            "location": "/core/ns-commands/", 
            "text": "In order to use the testbed specific commands, you must include the following line near the top of your NS topology file (before any testbed commands are used):\n\n\n    source tb_compat.tcl\n\n\n\n\nIf you wish to use your file under NS, download \ntb_compat.tcl\n and place it in the same directory as your NS file. When run in this way under NS, the testbed commands will have no effect, but NS will be able to parse your file.\n\n\n\n\nTCL, NS, and node names\n\uf0c1\n\n\nIn your file, you will be creating nodes with something like the following line:\n\n\n    set node1 [$ns node]\n\n\n\n\nWith this command, the simulator, represented by \n$ns\n is creating a new node involving many internal data changes and returning a reference to it which is stored in the variable \nnode1\n. \n\n\nIn almost all cases when you need to refer to a node, you will do it as \n$node1\n, the \n$\n indicating that you want the value of the variable \nnode1\n, i.e. the reference to the node. Thus you will be issuing commands like:\n\n\n    $ns duplex-link $node1 $node2 100Mb 150ms DropTail\n    tb-set-ip $node1 10.1.0.2\n\n\n\n\nNote the instances of \n$\n.\n\n\nYou will notice that when your experiment is set up, the node names and such will be \nnode1\n, \nnode2\n, \nnode3\n, etc. This happens because the parser detects what variable you are using to store the node reference and uses that as the node name. In the case that you do something like:\n\n\n    set node1 [$ns node2]\n    set A $node1\n\n\n\n\nThe node will still be called \nnode1\n as that was the first variable to contain the reference.\n\n\nIf you are dealing with many nodes you may store them in an array, using a command similar to the following:\n\n\n    for {set i 0} {$i \n 4} {incr i} {\n       set nodes($i) [$ns node]\n    }\n\n\n\n\nIn this case, the names of the node will be \nnodes-0\n, \nnodes-1\n, \nnodes-2\n, \nnodes-3\n. In other words, the \"(\" character is replaced with \"-\", and \")\" is removed. This slightly different syntax is used to avoid any problems that \"()\" may cause later in the process. For example, the \"()\" characters may not appear in DNS entries.\n\n\nAs a final note, everything said above for nodes applies equally to LANs, i.e.:\n\n\n    set lan0 [$ns make-lan \n$node0 $node1\n 100Mb 0ms]\n    tb-set-lan-loss $lan0 .02\n\n\n\n\nAgain, note the instances of \n$\n.\n\n\nLinks may also be named just like nodes and LANs. The names may then be used to set loss rates or IP addresses. This technique is the only way to set such attributes when there are multiple links between two nodes.\n\n\n    set link1 [$ns duplex-link $node0 $node1 100Mb 0ms DropTail]\n    tb-set-link-loss $link1 0.05\n    tb-set-ip-link $node0 $link1 10.1.0.128\n\n\n\n\n\n\nCaptured NS file parameters\n\uf0c1\n\n\nA common convention when writing NS files is to place any parameters in an array named \nopt\n at the beginning of the file. For example:\n\n\n    set opt(CLIENT_COUNT) 5\n    set opt(BW) 10mb;    Link bandwidth\n    set opt(LAT) 10ms;   Link latency\n\n   ...\n\n    $ns duplex-link $server $router $opt(BW) $opt(LAT) DropTail\n\n    for {set i 0} {$i \n $opt(CLIENT_COUNT)} {incr i} {\n        set nodes($i) [$ns node]\n    ...\n    }\n    set serverprog [$server program-agent -command \nstarter.sh\n]\n\n\n\n\nNormally, this convention is only used to help organize the parameters. In DETERLab, however, the contents of the \nopt\n array are captured and made available to the emulated environment. For instance, the parameters are added as environment variables to any commands run by program-agents. So in the above example of NS code, the \nstarter.sh\n script will be able to reference parameters by name, like so:\n\n\n    #! /bin/sh\n\n    echo \nTesting with $CLIENT_COUNT clients.\n\n    ...\n\n\n\n\nNote that the contents of the \nopt\n array are not ordered, so you should not reference other parameters and expect the shell to expand them appropriately:\n\n\n    set opt(prefix) \n/foo/bar\n\n    set opt(BINDIR) '$prefix/bin'; # BAD\n\n    set opt(prefix) \n/foo/bar\n\n    set opt(BINDIR) \n$opt(prefix)/bin\n; # Good\n\n\n\n\n\n\nOrdering Issues\n\uf0c1\n\n\ntb-\n commands have the same status as all other Tcl and NS commands. Thus \nthe order matters\n not only relative to each other but also relative to other commands. One common example of this is that IP commands must be issued \nafter\n the links or LANs are created.\n\n\n\n\nHardware Commands\n\uf0c1\n\n\ntb-set-hardware\n\uf0c1\n\n\ntb-set-hardware node type [args]\n\ntb-set-hardware $node3 pc\ntb-set-hardware $node4 shark\n\n\n\n\nwhere:\n\n\nnode\n = The name of the node.\n\n\ntype\n = The type of the node.\n\n\n\n\nNote\n\n\n\n\nPlease see the \nNode Status\n page for a list of available types. \npc\n is the default type.\n\n\nNo current types have any additional arguments.\n\n\n\n\n\n\n\n\nIP Address Commands\n\uf0c1\n\n\nEach node will be assigned an IP address for each interface that is in use. The following commands will allow you to explicitly set those IP addresses. IP addresses will be automatically generated for all nodes for which you do not explicitly set IP addresses.\n\n\nIn most cases, the IP addresses on either side of a link must be in the same subnet. Likewise, all IP addresses on a LAN should be in the same subnet. Generally the same subnet should not be used for more than one link or LAN in a given experiment, nor should one node have multiple interfaces in the same subnet. Automatically generated IP addresses will conform to these requirements. If part of a link or LAN is explicitly specified with the commands below then the remainder will be automatically generated under the same subnet.\n\n\nIP address assignment is deterministic and tries to fill lower IP's first, starting at 2. Except in the partial specification case (see above), all automatic IP addresses are in the network \n10\n.\n\n\ntb-set-ip\n\uf0c1\n\n\ntb-set-ip node ip\n\ntb-set-ip $node1 142.3.4.5\n\n\n\n\nwhere:\n\n\nnode\n = The node to assign the IP address to\n\n\nip\n = The IP address.\n\n\n\n\nNote\n\n\n\n\nThis command should only be used for nodes that have a single link. For nodes with multiple links the following commands should be used. Mixing \ntb-set-ip\n and any other IP command on the same node will result in an error.\n\n\n\n\n\n\ntb-set-ip-link\n\uf0c1\n\n\ntb-set-ip-link node link ip\n\ntb-set-ip-link $node0 $link0 142.3.4.6\n\n\n\n\nwhere:\n\n\nnode\n = The node to set the IP for.\n\n\nlink\n = The link to set the IP for.\n\n\nip\n = The IP address.\n\n\n\n\nNote\n\n\n\n\nOne way to think of the arguments is a link with the node specifying which side of the link to set the IP for.\n\n\nThis command cannot be mixed with \ntb-set-ip\n on the same node.\n\n\n\n\n\n\ntb-set-ip-lan\n\uf0c1\n\n\ntb-set-ip-lan node lan ip\ntb-set-ip-lan $node1 $lan0 142.3.4.6\n\n\n\n\nwhere:\n\n\nnode\n = The node to set the IP for.\n\n\nlan\n = The lan the IP is on.\n\n\nip\n = The IP address.\n\n\n\n\nNote\n\n\n\n\nOne way to think of the arguments is a node with the LAN specifying which port to set the IP address for.\n\n\nThis command cannot be mixed with \ntb-set-ip\n on the same node.\n\n\n\n\n\n\ntb-set-ip-interface\n\uf0c1\n\n\ntb-set-ip-interface node dst ip\ntb-set-ip-interface $node2 $node1 142.3.4.6\n\n\n\n\nwhere:\n\n\nnode\n = The node to set the IP for.\n\n\ndst\n = The destination of the link to set the IP for.\n\n\nIP\n = The IP address.\n\n\n\n\nNote\n\n\n\n\nThis command cannot be mixed on the same node with \ntb-set-ip\n. (See above)\n\n\nIn the case of multiple links between the same pair of nodes, there is no way to distinguish which link to the set the IP for. This should be fixed soon.\n\n\nThis command is converted internally to either \ntb-set-ip-link\n or \ntb-set-ip-lan\n. It is possible that error messages will report either of those commands instead of \ntb-set-ip-interface\n.\n\n\n\n\n\n\ntb-set-netmask\n\uf0c1\n\n\ntb-set-netmask lanlink netmask\n\ntb-set-netmask $link0 \n255.255.255.248\n\n\n\n\n\nwhere:\n\n\nlanlink\n = The lan or link to set the netmask for.\n\n\nnetmask\n = The netmask in dotted notation.\n\n\n\n\nNote\n\n\n\n\nThis command sets the netmask for a LAN or link. The mask must be big enough to support all of the nodes on the LAN or link!\n\n\nYou may play with the bottom three octets (0xFFFFFXXX) of the mask; attempts to change the upper octets will cause an error.\n\n\n\n\n\n\n\n\nOS Commands \n\uf0c1\n\n\ntb-set-node-os\n\uf0c1\n\n\ntb-set-node-os node os\n\ntb-set-node-os $node1 FBSD-STD\ntb-set-node-os $node1 MY_OS\n\n\n\n\nwhere:\n\n\nnode\n = The node to set the OS for.\n\n\nos\n = The id of the OS for that node.\n\n\n\n\nNote\n\n\n\n\nThe OSID may either by one of the standard OS's we provide or a custom OSID, created via the web interface.\n\n\nIf no OS is specified for a node, a default OS is chosen based on the nodes type. This is currently \n'Ubuntu1004-STD\n' for PCs.\n\n\nThe currently available standard OS types are listed in the \nOS Images\n page.\n\n\n\n\n\n\ntb-set-node-rpms\n\uf0c1\n\n\ntb-set-node-rpms node rpms...\n\ntb-set-node-rpms $node0 rpm1 rpm2 rpm3\n\n\n\n\n\n\nNote\n\n\n\n\nThis command sets which RPMs are to be installed on the node when it first boots after being assigned to an experiment.\n\n\nEach RPM can be either a path to a file or a URL. Paths must be to files that reside in the \n/proj\n or \n/groups\n directory. You are not allowed to place your RPMs in your home directory. \nhttp(s)://\n and \nftp://\n URLs will be fetched into the experiment's directory, and re-distributed from there.\n\n\nSee the \nCore Guide\n for more information.\n\n\n\n\n\n\ntb-set-node-startcmd\n\uf0c1\n\n\ntb-set-node-startcmd node startupcmd\n\ntb-set-node-startcmd $node0 \nmystart.sh -a \n /tmp/node0.log\n\n\n\n\n\n\n\nNote\n\n\n\n\nSpecify a script or program to be run when the node is booted.\n\n\nSee the \nCore Guide\n for more information.\n\n\n\n\n\n\ntb-set-node-cmdline\n\uf0c1\n\n\ntb-set-node-cmdline node cmdline\n\ntb-set-node-cmdline $node0 {???}\n\n\n\n\n\n\nNote\n\n\n\n\nSet the commandline to be passed to the \nkernel\n when it is booted.\n\n\nCurrently, this is supported on OSKit kernels only.\n\n\n\n\n\n\ntb-set-node-tarfiles\n\uf0c1\n\n\ntb-set-node-tarfiles node install-dir1 tarfile1 ...\n\n\n\n\nThe \ntb-set-node-tarfiles\n command is used to install one or more tar files onto a node's local disk. This command is useful for installing files that are used frequently, but will change very little during the course of your experiments. For example, if your software depends on a third-party library not provided in the standard disk images, you can produce a tarball and have the library ready for use on all the experimental nodes. \n\n\nAnother example would be the data sets for your software. The benefit of installing files using this method is that they will reside on the node's local disk, so your experimental runs will not be disrupted by NFS traffic. \n\n\n\n\nNote\n\n\nAvoid using this command if the files are changing frequently because the tars are only (re)installed when the nodes boot.\n\n\n\n\nInstalling individual tar files or RPMs is a midpoint in the spectrum of getting software onto the experimental nodes. At one extreme, you can read everything over NFS, which works well if the files are changing constantly, but can generate a great deal of strain on the control network and disrupt your experiment. The tar files and RPMs are also read over NFS when the nodes initially boot; however, there won't be any extra NFS traffic while you are running your experiment. Finally, if you need a lot of software installed on a large number of nodes, say greater than 20, it might be best to create a \ncustom disk image\n. Using a disk image is easier on the control network since it is transferred using multicast, thus greatly reducing the amount of NFS traffic when the experiment is swapped in. \n\n\nRequired Parameters:\n\n\n\n\nnode\n - The node where the files should be installed. Each node has its own tar file list, which may or may not be different from the others.\n\n\nOne or more \ninstall-dir\n and \ntarfile\n pairs are then listed in the order you wish them to be installed:\n\n\ninstall-dir\n - An existing directory on the node where the tar file should be unarchived (e.g. \n/\n, \n/usr\n, \n/usr/local\n). The \ntar\n command will be run as \"root\" [#tb-set-node-tarfiles Note1], so all of the node's directories will be accessible to you. If the directory does not exist on the image or was not created by the unarchiving of a previous tar file, the installation will fail [#tb-set-node-tarfiles Note2].\n\n\ntarfile\n - An existing tar file located in a project directory (e.g. \n/proj\n or \n/groups\n) or an \nhttp\n, \nhttps\n, or \nftp\n URL. In the case of URLs, they are downloaded when the experiment is swapped in and cached in the experiment's directory for future use. In either case, the tar file name is \nrequired\n to have one of the following extensions: .tar, .tar.Z, .tar.gz, or .tgz. Note that the tar file could have been created anywhere; however, if you want the unarchived files to have valid DETERLab user and group id's, you should create the tar file on \nops\n or an experimental node.\n\n\n\n\n\n\n\n\nExample usage:\n\n\n    # Overwrite files in /bin and /sbin.\n    tb-set-node-tarfiles $node0 /bin /proj/foo/mybinmods.tar /sbin /proj/foo/mysbinmods.tar\n\n    # Programmatically generate the list of tarballs.\n    set tb [list]\n    # Add a tarball located on a web site.\n    lappend tb / http://foo.bar/bazzer.tgz\n    # Add a tarball located in the DETER NFS space.\n    lappend tb /usr/local /proj/foo/tarfiles/bar.tar.gz\n    # Use 'eval' to expand the 'tb' list into individual\n    # arguments to the tb-set-node-tarfiles command.\n    eval tb-set-node-tarfiles $node1 $tb\n\n\n\n\nSee also:\n\n\n\n\ntb-set-node-rpms\n\n\nCustom disk images\n\n\n\n\n\n\nNote\n\n\n\n\nBecause the files are installed as root, care must be taken to protect the tar file so it cannot be replaced with a trojan that allowed less privileged users to become root.\n\n\nCurrently, you can only tell how/why an installation failed by examining the node's console log on bootup.\n\n\n\n\n\n\n\n\nLink Loss Commands\n\uf0c1\n\n\nThis is the NS syntax for creating a link:\n\n\n$ns duplex-link $node1 $node2 100Mb 150ms DropTail\n\n\n\n\n\n\nNote\n\n\nThis does not allow for specifying link loss rates. DETERLab does, however, support link loss. The following commands can be used to specify link loss rates.\n\n\n\n\ntb-set-link-loss\n\uf0c1\n\n\ntb-set-link-loss src dst loss\ntb-set-link-loss link loss\n\ntb-set-link-loss $node1 $node2 0.05\ntb-set-link-loss $link1 0.02\n\n\n\n\nwhere:\n\n\nsrc\n, \ndst\n = Two nodes to describe the link.\n\n\nlink\n = The link to set the rate for.\n\n\nloss\n = The loss rate (between 0 and 1).\n\n\n\n\nNote\n\n\n\n\nThere are two syntaxes available. The first specifies a link by a source/destination pair. The second explicitly specifies the link.\n\n\nThe source/destination pair is incapable of describing an individual link in the case of multiple links between two nodes. Use the second syntax for this case.\n\n\n\n\n\n\ntb-set-lan-loss\n\uf0c1\n\n\ntb-set-lan-loss lan loss\n\ntb-set-lan-loss $lan1 0.3\n\n\n\n\nWhere:\n\n\nlan\n = The lan to set the loss rate for.\n\n\nloss\n = The loss rate (between 0 and 1).\n\n\n\n\nNote\n\n\nThis command sets the loss rate for the entire LAN.\n\n\n\n\ntb-set-node-lan-delay\n\uf0c1\n\n\ntb-set-node-lan-delay node lan delay\n\ntb-set-node-lan-delay $node0 $lan0 40ms\n\n\n\n\nWhere:\n\n\nnode\n = The node we are modifying the delay for.\n\n\nlan\n = Which LAN the node is in that we are affecting.\n\n\ndelay\n = The new node to switch delay (see below).\n\n\n\n\nNote\n\n\n\n\nThis command changes the delay between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node, possibly incurring additional latency from any delay parameters there.\n\n\nIf this command is not used to overwrite the delay, then the delay for a given node to switch link is taken as one half of the delay passed to \nmake-lan\n. Thus in a LAN where no \ntb-set-node-delay\n calls are made, the node-to-node latency will be the latency passed to \nmake-lan\n.\n\n\nThe behavior of this command is not defined when used with nodes that are in the same LAN multiple times.\n\n\nDelays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. As a convenience, a delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other.\n\n\n\n\n\n\ntb-set-node-lan-bandwidth\n\uf0c1\n\n\ntb-set-node-lan-bandwidth node lan bandwidth\n\ntb-set-node-lan-bandwidth $node0 $lan0 20Mb\n\n\n\n\nWhere:\n\n\nnode\n = The node we are modifying the bandwidth for.\n\n\nlan\n = Which LAN the node is in that we are affecting.\n\n\nbandwidth\n = The new node to switch bandwidth (see below).\n\n\n\n\nNote\n\n\n\n\nThis command changes the bandwidth between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may have a lower bandwidth.\n\n\nIf this command is not used to overwrite the bandwidth, then the bandwidth for a given node to switch link is taken directly from the bandwidth passed to \nmake-lan\n.\n\n\nThe behavior of this command is not defined when used with nodes that are in the same LAN multiple times.\n\n\n\n\n\n\ntb-set-node-lan-loss\n\uf0c1\n\n\ntb-set-node-lan-loss node lan loss\n\ntb-set-node-lan-loss $node0 $lan0 0.05\n\n\n\n\nWhere:\n\n\nnode\n = The node we are modifying the loss for.\n\n\nlan\n = Which LAN the node is in that we are affecting.\n\n\nloss\n = The new node to switch loss (see below).\n\n\n\n\nNote\n\n\n\n\nThis command changes the loss probability between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may also have a loss chance. Thus for packet going to switch with loss chance \nA\n and then going on the destination with loss chance \nB\n, the node-to-node loss chance is \n(1-(1-A)(1-B))\n.\n\n\nIf this command is not used to overwrite the loss, then the loss for a given node to switch link is taken from the loss rate passed to the \nmake-lan\n command. If a loss rate of \nL\n is passed to \nmake-lan\n then the node to switch loss rate for each node is set to \n(1-sqrt(1-L))\n. Because each packet will have two such chances to be lost, the node-to-loss rate comes out as the desired \nL\n.\n\n\nThe behavior of this command is not defined when used with nodes that are in the same LAN multiple times.\n\n\n\n\n\n\ntb-set-node-lan-params\n\uf0c1\n\n\ntb-set-node-lan-params node lan delay bandwidth loss\n\ntb-set-node-lan-params $node0 $lan0 40ms 20Mb 0.05\n\n\n\n\nWhere:\n\n\nnode\n = The node we are modifying the loss for.\n\n\nlan\n = Which LAN the node is in that we are affecting.\n\n\ndelay\n = The new node to switch delay.\n\n\nbandwidth\n = The new node to switch bandwidth.\n\n\nloss\n = The new node to switch loss.\n\n\n\n\nNote\n\n\nThis command is exactly equivalent to calling each of the above three commands appropriately. See above for more information.\n\n\n\n\ntb-set-link-simplex-params\n\uf0c1\n\n\ntb-set-link-simplex-params link src delay bw loss\n\ntb-set-link-simplex-params $link1 $srcnode 100ms 50Mb 0.2\n\n\n\n\nWhere:\n\n\nlink\n = The link we are modifying.\n\n\nsrc\n = The source, defining which direction we are modifying.\n\n\ndelay\n = The source to destination delay.\n\n\nbw\n = The source to destination bandwidth.\n\n\nloss\n = The source to destination loss.\n\n\n\n\nNote\n\n\n\n\nThis commands modifies the delay characteristics of a link in a single direction. The other direction is unchanged.\n\n\nThis command only applies to links. Use \ntb-set-lan-simplex-params\n below for LANs.\n\n\n\n\n\n\ntb-set-lan-simplex-params\n\uf0c1\n\n\ntb-set-lan-simplex-params lan node todelay tobw toloss fromdelay frombw fromloss\n\ntb-set-lan-simplex-params $lan1 $node1 100ms 10Mb 0.1 5ms 100Mb 0\n\n\n\n\nWhere:\n\n\nlan\n = The lan we are modifying.\n\n\nnode\n = The member of the lan we are modifying.\n\n\ntodelay\n = Node to lan delay.\n\n\ntobw\n = Node to lan bandwidth.\n\n\ntoloss\n = Node to lan loss.\n\n\nfromdelay\n = Lan to node delay.\n\n\nfrombw\n = Lan to node bandwidth.\n\n\nfromloss\n = Lan to node loss.\n\n\n\n\nNote\n\n\nThis command is exactly like \ntb-set-node-lan-params\n except that it allows the characteristics in each direction to be chosen separately. See all the notes for \ntb-set-node-lan-params\n.\n\n\n\n\ntb-set-endnodeshaping\n\uf0c1\n\n\ntb-set-endnodeshaping link-or-lan enable\n\ntb-set-endnodeshaping $link1 1\ntb-set-endnodeshaping $lan1 1\n\n\n\n\nWhere:\n\n\nlink-or-lan\n = The link or LAN we are modifying.\n\n\nenable\n = Set to 1 to enable, 0 to disable.\n\n\n\n\nNote\n\n\n\n\nThis command specifies whether \nend node shaping\n is used on the specified link or LAN (instead of a delay node).\n\n\nDisabled by default for all links and LANs.\n\n\nOnly available when running the standard DETERLab FreeBSD or Linux kernels.\n\n\nSee \nEnd Node Traffic Shaping and Multiplexed Links\n for more details.\n\n\n\n\n\n\ntb-set-noshaping\n\uf0c1\n\n\ntb-set-noshaping link-or-lan enable\n\ntb-set-noshaping $link1 1\ntb-set-noshaping $lan1 1\n\n\n\n\nWhere:\n\n\nlink-or-lan\n = The link or LAN we are modifying.\n\n\nenable\n = Set to 1 to disable bandwidth shaping, 0 to enable.\n\n\n\n\nNote\n\n\n\n\nThis command specifies whether link bandwidth shaping should be enforced on the specified link or LAN. When enabled, bandwidth limits indicated for a link or LAN will not be enforced.\n\n\nDisabled by default for all links and LANs. That is, link bandwidth shaping \nis\n enforced on all links and LANs by default.\n\n\nIf the delay and loss values for a \ntb-set-noshaping\n link are zero (the default), then no delay node or end-node delay pipe will be associated with the link or LAN.\n\n\nThis command is a hack.\n The primary purpose for this command is to subvert the topology mapper (\nassign\n). \nAssign\n always observes the physical bandwidth constraints of the testbed. By using \ntb-set-noshaping\n, you can convince \nassign\n that links are low-bandwidth and thus get your topology mapped, but then not actually have the links shaped.\n\n\n\n\n\n\ntb-use-endnodeshaping\n\uf0c1\n\n\ntb-use-endnodeshaping enable\n\ntb-use-endnodeshaping 1\n\n\n\n\nWhere:\n\n\nenable\n = Set to 1 to enable end-node traffic shaping on all links and LANs.\n\n\n\n\nNote\n\n\n\n\nThis command allows you to use end-node traffic shaping globally, without having to specify per link or LAN with \ntb-set-endnodeshaping\n.\n\n\nSee \nEnd Node Traffic Shaping and Multiplexed Links\n for more details.\n\n\n\n\n\n\ntb-force-endnodeshaping\n\uf0c1\n\n\ntb-force-endnodeshaping enable\n\ntb-force-endnodeshaping 1\n\n\n\n\nWhere:\n\n\nenable\n = Set to 1 to force end-node traffic shaping on all links and LANs.\n\n\n\n\nNote\n\n\n\n\nThis command allows you to specify non-shaped links and LANs at creation time, but still control the shaping parameters later (e.g., increase delay, decrease bandwidth) after the experiment is swapped in.\n\n\nThis command forces allocation of end-node shaping infrastructure for all links. There is no equivalent to force delay node allocation.\n\n\nSee \nEnd Node Traffic Shaping and Multiplexed Links\n for more details.\n\n\n\n\n\n\ntb-set-multiplexed\n\uf0c1\n\n\ntb-set-multiplexed link allow\n\ntb-set-multiplexed $link1 1\n\n\n\n\nWhere:\n\n\nlink\n = The link we are modifying.\n\n\n'allow` = Set to 1 to allow multiplexing of the link, 0 to disallow.\n\n\n\n\nNote\n\n\n\n\nThis command allows a link to be multiplexed over a physical link along with other links.\n\n\nDisabled by default for all links.\n\n\nOnly available when running the standard DETER FreeBSD (not Linux) and only for links (not LANs).\n\n\nSee \nEnd Node Traffic Shaping and Multiplexed Links\n for more details.\n\n\n\n\n\n\ntb-set-vlink-emulation\n\uf0c1\n\n\ntb-set-vlink-emulation style\n\ntb-set-vlink-emulation $link1 vlan\n\n\n\n\nWhere:\n\n\nstyle\n = One of \"vlan\" or \"veth-ne\"\n\n\n\n\nNote\n\n\nIt seems to be necessary to set the virtual link emulation style to vlan for multiplexed links to work under linux.\n\n\n\n\n\n\nVirtual Type Commands\n\uf0c1\n\n\nVirtual Types are a method of defining fuzzy types, i.e. types that can be fulfilled by multiple different physical types. The advantage of virtual types, also known as \n'vtypes\n', is that all nodes of the same vtype will usually be the same physical type of node. In this way, vtypes allows logical grouping of nodes.\n\n\nAs an example, imagine we have a network with internal routers connecting leaf nodes. We want the routers to all have the same hardware, and the leaf nodes to all have the same hardware, but the specifics do not matter. We have the following fragment in our NS file:\n\n\n...\ntb-make-soft-vtype router {pc600 pc850}\ntb-make-soft-vtype leaf {pc600 pc850}\n\ntb-set-hardware $router1 router\ntb-set-hardware $router2 router\ntb-set-hardware $leaf1 leaf\ntb-set-hardware $leaf2 leaf\n\n\n\n\nHere we have set up two soft (see below) vtypes: router and leaf. Our router nodes are then specified to be of type \nrouter\n, and the leaf nodes of type \nleaf\n. When the experiment is swapped in, the testbed will attempt to make router1 and router2 be of the same type, and similarly, leaf1 and leaf2 of the same type. However, the routers/leafs may be pc600s or they may be pc850s, whichever is easier to fit in to the available resources.\n\n\nAs a basic use, vtypes can be used to request nodes that are all the same type, but can be of any available type:\n\n\n...\ntb-make-soft-vtype N {pc600 pc850}\n\ntb-set-hardware $node1 N\ntb-set-hardware $node2 N\n\n\n\n\nVtypes come in two varieties: hard and soft. \n\n \n'Soft\n' - With soft vtypes, the testbed will try to make all nodes of that vtype the same physical type, but may do otherwise if resources are tight. \n\n \n'Hard\n' - Hard vtypes behave just like soft vtypes except that the testbed will give higher priority to vtype consistency and swapping in will fail if the vtypes cannot be satisfied. \n\n\nTherefore, if you use soft vtypes you are more likely to swap in but there is a chance your node of a specific vtype will not all be the same. If you use hard vtypes, all nodes of a given vtype will be the same, but swapping in may fail.\n\n\nFurther, you can have weighted soft vtypes. Here you assign a weight from 0 to 1 exclusive to your vtype. The testbed will give higher priority to consistency in the higher weighted vtypes. The primary use of this is to rank multiple vtypes by importance of consistency. Soft vtypes have a weight of 0.5 by default.\n\n\nAs a final note, when specifying the types of a vtype, use the most specific type possible. For example, the following command is not very useful: \n\n\ntb-make-soft-vtype router {pc pc600}\n\n\n\n\nThis is because pc600 is a sub type of pc. You may very well end up with two routers as type pc with different hardware, as pc covers multiple types of hardware.\n\n\ntb-make-soft-vtype\n\uf0c1\n\n\ntb-make-soft-vtype vtype {types}\ntb-make-hard-vtype vtype {types}\ntb-make-weighted-vtype vtype weight {types}\n\ntb-make-soft-vtype router {pc600 pc850}\ntb-make-hard-vtype leaf {pc600 pc850}\ntb-make-weighted-vtype A 0.1 {pc600 pc850}\n\n\n\n\nWhere:\n\n\nvtype\n = The name of the vtype to create.\n\n\ntypes\n = One or more physical types.\n\n\nweight\n = The weight of the vtype, 0 \n \nweight\n \n 1.\n\n\n\n\nNote\n\n\n\n\nThese commands create vtypes. See notes above for a description of vtypes and the difference between soft and hard.\n\n\ntb-make-soft-vtype\n creates vtypes with weight 0.5.\n\n\nvtype commands must appear before \ntb-set-hardware\n commands that use them.\n\n\nDo not use \ntb-fix-node\n with nodes that have a vtype.\n\n\n\n\n\n\nMisc. Commands\n\uf0c1\n\n\ntb-fix-node\n\uf0c1\n\n\ntb-fix-node vnode pnode\n\ntb-fix-node $node0 pc42\n\n\n\n\nWhere:\n\n\nvnode\n = The node we are fixing.\n\n\npnode\n = The physical node we want used.\n\n\n\n\nNote\n\n\n\n\nThis command forces the virtual node to be mapped to the specified physical node. Swap in will fail if this cannot be done.\n\n\nDo not use this command on nodes that are a virtual type.\n\n\n\n\n\n\ntb-fix-interface\n\uf0c1\n\n\ntb-fix-interface vnode vlink iface \n\ntb-fix-interface $node0 $link0 \neth0\n\n\n\n\n\nWhere:\n\n\nvnode\n = The node we are fixing.\n\n\nvlink\n = The link connecting to that node that we want to set.\n\n\niface\n = The DETERLab name for the interface that is to be used.\n\n\n\n\nNote\n\n\n\n\nThe interface names used are the ones in the DETERLab database - we can make no guarantee that the OS image that boots on the node assigns the same name.\n\n\nDifferent types of nodes have different sets of interfaces, so this command is most useful if you are also using \ntb-fix-node\n and/or \ntb-set-hardware\n on the \nvnode\n.\n\n\n\n\n\n\ntb-set-uselatestwadata\n\uf0c1\n\n\ntb-set-uselatestwadata 0\ntb-set-uselatestwadata 1\n\n\n\n\n\n\nNote\n\n\n\n\nThis command indicates which widearea data to use when mapping widearea nodes to links. The default is 0, which says to use the aged data. Setting it to 1 says to use the most recent data.\n\n\n\n\n\n\ntb-set-wasolver-weights\n\uf0c1\n\n\ntb-set-wasolver-weights delay bw plr\n\ntb-set-wasolver-weights 1 10 500\n\n\n\n\nWhere:\n\n\ndelay\n = The weight to give delay when solving.\n\n\nbw\n = The weight to give bandwidth when solving.\n\n\nplr\n = The weight to give lossrate when solving.\n\n\n\n\nNote\n\n\n\n\nThis command sets the relative weights to use when assigning widearea nodes to links. Specifying a zero says to ignore that particular metric when doing the assignment. Setting all three to zero results in an essentially random selection.\n\n\n\n\n\n\ntb-set-node-failure-action\n\uf0c1\n\n\ntb-set-node-failure-action node action\n\ntb-set-node-failure-action $nodeA \nfatal\n\ntb-set-node-failure-action $nodeB \nnonfatal\n\n\n\n\n\nWhere:\n\n\nnode\n = The node name.\n\n\naction\n = One of \"fatal\" or \"nonfatal\".\n\n\n\n\nNote\n\n\nThis command sets the failure mode for a node. When an experiment is swapped in, the default action is to abort the swapin if any nodes fail to come up normally. This is the \"fatal\" mode. You may also set a node to \"nonfatal\" which will cause node bootup failures to be reported, but otherwise ignored during swapin. Note that this can result in your experiment not working properly if a dependent node fails, but typically you can arrange your software to deal with this.", 
            "title": "NS Commands Extensions"
        }, 
        {
            "location": "/core/ns-commands/#tcl-ns-and-node-names", 
            "text": "In your file, you will be creating nodes with something like the following line:      set node1 [$ns node]  With this command, the simulator, represented by  $ns  is creating a new node involving many internal data changes and returning a reference to it which is stored in the variable  node1 .   In almost all cases when you need to refer to a node, you will do it as  $node1 , the  $  indicating that you want the value of the variable  node1 , i.e. the reference to the node. Thus you will be issuing commands like:      $ns duplex-link $node1 $node2 100Mb 150ms DropTail\n    tb-set-ip $node1 10.1.0.2  Note the instances of  $ .  You will notice that when your experiment is set up, the node names and such will be  node1 ,  node2 ,  node3 , etc. This happens because the parser detects what variable you are using to store the node reference and uses that as the node name. In the case that you do something like:      set node1 [$ns node2]\n    set A $node1  The node will still be called  node1  as that was the first variable to contain the reference.  If you are dealing with many nodes you may store them in an array, using a command similar to the following:      for {set i 0} {$i   4} {incr i} {\n       set nodes($i) [$ns node]\n    }  In this case, the names of the node will be  nodes-0 ,  nodes-1 ,  nodes-2 ,  nodes-3 . In other words, the \"(\" character is replaced with \"-\", and \")\" is removed. This slightly different syntax is used to avoid any problems that \"()\" may cause later in the process. For example, the \"()\" characters may not appear in DNS entries.  As a final note, everything said above for nodes applies equally to LANs, i.e.:      set lan0 [$ns make-lan  $node0 $node1  100Mb 0ms]\n    tb-set-lan-loss $lan0 .02  Again, note the instances of  $ .  Links may also be named just like nodes and LANs. The names may then be used to set loss rates or IP addresses. This technique is the only way to set such attributes when there are multiple links between two nodes.      set link1 [$ns duplex-link $node0 $node1 100Mb 0ms DropTail]\n    tb-set-link-loss $link1 0.05\n    tb-set-ip-link $node0 $link1 10.1.0.128", 
            "title": "TCL, NS, and node names"
        }, 
        {
            "location": "/core/ns-commands/#captured-ns-file-parameters", 
            "text": "A common convention when writing NS files is to place any parameters in an array named  opt  at the beginning of the file. For example:      set opt(CLIENT_COUNT) 5\n    set opt(BW) 10mb;    Link bandwidth\n    set opt(LAT) 10ms;   Link latency\n\n   ...\n\n    $ns duplex-link $server $router $opt(BW) $opt(LAT) DropTail\n\n    for {set i 0} {$i   $opt(CLIENT_COUNT)} {incr i} {\n        set nodes($i) [$ns node]\n    ...\n    }\n    set serverprog [$server program-agent -command  starter.sh ]  Normally, this convention is only used to help organize the parameters. In DETERLab, however, the contents of the  opt  array are captured and made available to the emulated environment. For instance, the parameters are added as environment variables to any commands run by program-agents. So in the above example of NS code, the  starter.sh  script will be able to reference parameters by name, like so:      #! /bin/sh\n\n    echo  Testing with $CLIENT_COUNT clients. \n    ...  Note that the contents of the  opt  array are not ordered, so you should not reference other parameters and expect the shell to expand them appropriately:      set opt(prefix)  /foo/bar \n    set opt(BINDIR) '$prefix/bin'; # BAD\n\n    set opt(prefix)  /foo/bar \n    set opt(BINDIR)  $opt(prefix)/bin ; # Good", 
            "title": "Captured NS file parameters"
        }, 
        {
            "location": "/core/ns-commands/#ordering-issues", 
            "text": "tb-  commands have the same status as all other Tcl and NS commands. Thus  the order matters  not only relative to each other but also relative to other commands. One common example of this is that IP commands must be issued  after  the links or LANs are created.", 
            "title": "Ordering Issues"
        }, 
        {
            "location": "/core/ns-commands/#hardware-commands", 
            "text": "tb-set-hardware \uf0c1  tb-set-hardware node type [args]\n\ntb-set-hardware $node3 pc\ntb-set-hardware $node4 shark  where:  node  = The name of the node.  type  = The type of the node.   Note   Please see the  Node Status  page for a list of available types.  pc  is the default type.  No current types have any additional arguments.", 
            "title": "Hardware Commands"
        }, 
        {
            "location": "/core/ns-commands/#ip-address-commands", 
            "text": "Each node will be assigned an IP address for each interface that is in use. The following commands will allow you to explicitly set those IP addresses. IP addresses will be automatically generated for all nodes for which you do not explicitly set IP addresses.  In most cases, the IP addresses on either side of a link must be in the same subnet. Likewise, all IP addresses on a LAN should be in the same subnet. Generally the same subnet should not be used for more than one link or LAN in a given experiment, nor should one node have multiple interfaces in the same subnet. Automatically generated IP addresses will conform to these requirements. If part of a link or LAN is explicitly specified with the commands below then the remainder will be automatically generated under the same subnet.  IP address assignment is deterministic and tries to fill lower IP's first, starting at 2. Except in the partial specification case (see above), all automatic IP addresses are in the network  10 .  tb-set-ip \uf0c1  tb-set-ip node ip\n\ntb-set-ip $node1 142.3.4.5  where:  node  = The node to assign the IP address to  ip  = The IP address.   Note   This command should only be used for nodes that have a single link. For nodes with multiple links the following commands should be used. Mixing  tb-set-ip  and any other IP command on the same node will result in an error.    tb-set-ip-link \uf0c1  tb-set-ip-link node link ip\n\ntb-set-ip-link $node0 $link0 142.3.4.6  where:  node  = The node to set the IP for.  link  = The link to set the IP for.  ip  = The IP address.   Note   One way to think of the arguments is a link with the node specifying which side of the link to set the IP for.  This command cannot be mixed with  tb-set-ip  on the same node.    tb-set-ip-lan \uf0c1  tb-set-ip-lan node lan ip\ntb-set-ip-lan $node1 $lan0 142.3.4.6  where:  node  = The node to set the IP for.  lan  = The lan the IP is on.  ip  = The IP address.   Note   One way to think of the arguments is a node with the LAN specifying which port to set the IP address for.  This command cannot be mixed with  tb-set-ip  on the same node.    tb-set-ip-interface \uf0c1  tb-set-ip-interface node dst ip\ntb-set-ip-interface $node2 $node1 142.3.4.6  where:  node  = The node to set the IP for.  dst  = The destination of the link to set the IP for.  IP  = The IP address.   Note   This command cannot be mixed on the same node with  tb-set-ip . (See above)  In the case of multiple links between the same pair of nodes, there is no way to distinguish which link to the set the IP for. This should be fixed soon.  This command is converted internally to either  tb-set-ip-link  or  tb-set-ip-lan . It is possible that error messages will report either of those commands instead of  tb-set-ip-interface .    tb-set-netmask \uf0c1  tb-set-netmask lanlink netmask\n\ntb-set-netmask $link0  255.255.255.248   where:  lanlink  = The lan or link to set the netmask for.  netmask  = The netmask in dotted notation.   Note   This command sets the netmask for a LAN or link. The mask must be big enough to support all of the nodes on the LAN or link!  You may play with the bottom three octets (0xFFFFFXXX) of the mask; attempts to change the upper octets will cause an error.", 
            "title": "IP Address Commands"
        }, 
        {
            "location": "/core/ns-commands/#os-commands", 
            "text": "tb-set-node-os \uf0c1  tb-set-node-os node os\n\ntb-set-node-os $node1 FBSD-STD\ntb-set-node-os $node1 MY_OS  where:  node  = The node to set the OS for.  os  = The id of the OS for that node.   Note   The OSID may either by one of the standard OS's we provide or a custom OSID, created via the web interface.  If no OS is specified for a node, a default OS is chosen based on the nodes type. This is currently  'Ubuntu1004-STD ' for PCs.  The currently available standard OS types are listed in the  OS Images  page.    tb-set-node-rpms \uf0c1  tb-set-node-rpms node rpms...\n\ntb-set-node-rpms $node0 rpm1 rpm2 rpm3   Note   This command sets which RPMs are to be installed on the node when it first boots after being assigned to an experiment.  Each RPM can be either a path to a file or a URL. Paths must be to files that reside in the  /proj  or  /groups  directory. You are not allowed to place your RPMs in your home directory.  http(s)://  and  ftp://  URLs will be fetched into the experiment's directory, and re-distributed from there.  See the  Core Guide  for more information.    tb-set-node-startcmd \uf0c1  tb-set-node-startcmd node startupcmd\n\ntb-set-node-startcmd $node0  mystart.sh -a   /tmp/node0.log    Note   Specify a script or program to be run when the node is booted.  See the  Core Guide  for more information.    tb-set-node-cmdline \uf0c1  tb-set-node-cmdline node cmdline\n\ntb-set-node-cmdline $node0 {???}   Note   Set the commandline to be passed to the  kernel  when it is booted.  Currently, this is supported on OSKit kernels only.    tb-set-node-tarfiles \uf0c1  tb-set-node-tarfiles node install-dir1 tarfile1 ...  The  tb-set-node-tarfiles  command is used to install one or more tar files onto a node's local disk. This command is useful for installing files that are used frequently, but will change very little during the course of your experiments. For example, if your software depends on a third-party library not provided in the standard disk images, you can produce a tarball and have the library ready for use on all the experimental nodes.   Another example would be the data sets for your software. The benefit of installing files using this method is that they will reside on the node's local disk, so your experimental runs will not be disrupted by NFS traffic.    Note  Avoid using this command if the files are changing frequently because the tars are only (re)installed when the nodes boot.   Installing individual tar files or RPMs is a midpoint in the spectrum of getting software onto the experimental nodes. At one extreme, you can read everything over NFS, which works well if the files are changing constantly, but can generate a great deal of strain on the control network and disrupt your experiment. The tar files and RPMs are also read over NFS when the nodes initially boot; however, there won't be any extra NFS traffic while you are running your experiment. Finally, if you need a lot of software installed on a large number of nodes, say greater than 20, it might be best to create a  custom disk image . Using a disk image is easier on the control network since it is transferred using multicast, thus greatly reducing the amount of NFS traffic when the experiment is swapped in.   Required Parameters:   node  - The node where the files should be installed. Each node has its own tar file list, which may or may not be different from the others.  One or more  install-dir  and  tarfile  pairs are then listed in the order you wish them to be installed:  install-dir  - An existing directory on the node where the tar file should be unarchived (e.g.  / ,  /usr ,  /usr/local ). The  tar  command will be run as \"root\" [#tb-set-node-tarfiles Note1], so all of the node's directories will be accessible to you. If the directory does not exist on the image or was not created by the unarchiving of a previous tar file, the installation will fail [#tb-set-node-tarfiles Note2].  tarfile  - An existing tar file located in a project directory (e.g.  /proj  or  /groups ) or an  http ,  https , or  ftp  URL. In the case of URLs, they are downloaded when the experiment is swapped in and cached in the experiment's directory for future use. In either case, the tar file name is  required  to have one of the following extensions: .tar, .tar.Z, .tar.gz, or .tgz. Note that the tar file could have been created anywhere; however, if you want the unarchived files to have valid DETERLab user and group id's, you should create the tar file on  ops  or an experimental node.     Example usage:      # Overwrite files in /bin and /sbin.\n    tb-set-node-tarfiles $node0 /bin /proj/foo/mybinmods.tar /sbin /proj/foo/mysbinmods.tar\n\n    # Programmatically generate the list of tarballs.\n    set tb [list]\n    # Add a tarball located on a web site.\n    lappend tb / http://foo.bar/bazzer.tgz\n    # Add a tarball located in the DETER NFS space.\n    lappend tb /usr/local /proj/foo/tarfiles/bar.tar.gz\n    # Use 'eval' to expand the 'tb' list into individual\n    # arguments to the tb-set-node-tarfiles command.\n    eval tb-set-node-tarfiles $node1 $tb  See also:   tb-set-node-rpms  Custom disk images    Note   Because the files are installed as root, care must be taken to protect the tar file so it cannot be replaced with a trojan that allowed less privileged users to become root.  Currently, you can only tell how/why an installation failed by examining the node's console log on bootup.", 
            "title": "OS Commands "
        }, 
        {
            "location": "/core/ns-commands/#link-loss-commands", 
            "text": "This is the NS syntax for creating a link:  $ns duplex-link $node1 $node2 100Mb 150ms DropTail   Note  This does not allow for specifying link loss rates. DETERLab does, however, support link loss. The following commands can be used to specify link loss rates.   tb-set-link-loss \uf0c1  tb-set-link-loss src dst loss\ntb-set-link-loss link loss\n\ntb-set-link-loss $node1 $node2 0.05\ntb-set-link-loss $link1 0.02  where:  src ,  dst  = Two nodes to describe the link.  link  = The link to set the rate for.  loss  = The loss rate (between 0 and 1).   Note   There are two syntaxes available. The first specifies a link by a source/destination pair. The second explicitly specifies the link.  The source/destination pair is incapable of describing an individual link in the case of multiple links between two nodes. Use the second syntax for this case.    tb-set-lan-loss \uf0c1  tb-set-lan-loss lan loss\n\ntb-set-lan-loss $lan1 0.3  Where:  lan  = The lan to set the loss rate for.  loss  = The loss rate (between 0 and 1).   Note  This command sets the loss rate for the entire LAN.   tb-set-node-lan-delay \uf0c1  tb-set-node-lan-delay node lan delay\n\ntb-set-node-lan-delay $node0 $lan0 40ms  Where:  node  = The node we are modifying the delay for.  lan  = Which LAN the node is in that we are affecting.  delay  = The new node to switch delay (see below).   Note   This command changes the delay between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node, possibly incurring additional latency from any delay parameters there.  If this command is not used to overwrite the delay, then the delay for a given node to switch link is taken as one half of the delay passed to  make-lan . Thus in a LAN where no  tb-set-node-delay  calls are made, the node-to-node latency will be the latency passed to  make-lan .  The behavior of this command is not defined when used with nodes that are in the same LAN multiple times.  Delays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. As a convenience, a delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other.    tb-set-node-lan-bandwidth \uf0c1  tb-set-node-lan-bandwidth node lan bandwidth\n\ntb-set-node-lan-bandwidth $node0 $lan0 20Mb  Where:  node  = The node we are modifying the bandwidth for.  lan  = Which LAN the node is in that we are affecting.  bandwidth  = The new node to switch bandwidth (see below).   Note   This command changes the bandwidth between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may have a lower bandwidth.  If this command is not used to overwrite the bandwidth, then the bandwidth for a given node to switch link is taken directly from the bandwidth passed to  make-lan .  The behavior of this command is not defined when used with nodes that are in the same LAN multiple times.    tb-set-node-lan-loss \uf0c1  tb-set-node-lan-loss node lan loss\n\ntb-set-node-lan-loss $node0 $lan0 0.05  Where:  node  = The node we are modifying the loss for.  lan  = Which LAN the node is in that we are affecting.  loss  = The new node to switch loss (see below).   Note   This command changes the loss probability between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may also have a loss chance. Thus for packet going to switch with loss chance  A  and then going on the destination with loss chance  B , the node-to-node loss chance is  (1-(1-A)(1-B)) .  If this command is not used to overwrite the loss, then the loss for a given node to switch link is taken from the loss rate passed to the  make-lan  command. If a loss rate of  L  is passed to  make-lan  then the node to switch loss rate for each node is set to  (1-sqrt(1-L)) . Because each packet will have two such chances to be lost, the node-to-loss rate comes out as the desired  L .  The behavior of this command is not defined when used with nodes that are in the same LAN multiple times.    tb-set-node-lan-params \uf0c1  tb-set-node-lan-params node lan delay bandwidth loss\n\ntb-set-node-lan-params $node0 $lan0 40ms 20Mb 0.05  Where:  node  = The node we are modifying the loss for.  lan  = Which LAN the node is in that we are affecting.  delay  = The new node to switch delay.  bandwidth  = The new node to switch bandwidth.  loss  = The new node to switch loss.   Note  This command is exactly equivalent to calling each of the above three commands appropriately. See above for more information.   tb-set-link-simplex-params \uf0c1  tb-set-link-simplex-params link src delay bw loss\n\ntb-set-link-simplex-params $link1 $srcnode 100ms 50Mb 0.2  Where:  link  = The link we are modifying.  src  = The source, defining which direction we are modifying.  delay  = The source to destination delay.  bw  = The source to destination bandwidth.  loss  = The source to destination loss.   Note   This commands modifies the delay characteristics of a link in a single direction. The other direction is unchanged.  This command only applies to links. Use  tb-set-lan-simplex-params  below for LANs.    tb-set-lan-simplex-params \uf0c1  tb-set-lan-simplex-params lan node todelay tobw toloss fromdelay frombw fromloss\n\ntb-set-lan-simplex-params $lan1 $node1 100ms 10Mb 0.1 5ms 100Mb 0  Where:  lan  = The lan we are modifying.  node  = The member of the lan we are modifying.  todelay  = Node to lan delay.  tobw  = Node to lan bandwidth.  toloss  = Node to lan loss.  fromdelay  = Lan to node delay.  frombw  = Lan to node bandwidth.  fromloss  = Lan to node loss.   Note  This command is exactly like  tb-set-node-lan-params  except that it allows the characteristics in each direction to be chosen separately. See all the notes for  tb-set-node-lan-params .   tb-set-endnodeshaping \uf0c1  tb-set-endnodeshaping link-or-lan enable\n\ntb-set-endnodeshaping $link1 1\ntb-set-endnodeshaping $lan1 1  Where:  link-or-lan  = The link or LAN we are modifying.  enable  = Set to 1 to enable, 0 to disable.   Note   This command specifies whether  end node shaping  is used on the specified link or LAN (instead of a delay node).  Disabled by default for all links and LANs.  Only available when running the standard DETERLab FreeBSD or Linux kernels.  See  End Node Traffic Shaping and Multiplexed Links  for more details.    tb-set-noshaping \uf0c1  tb-set-noshaping link-or-lan enable\n\ntb-set-noshaping $link1 1\ntb-set-noshaping $lan1 1  Where:  link-or-lan  = The link or LAN we are modifying.  enable  = Set to 1 to disable bandwidth shaping, 0 to enable.   Note   This command specifies whether link bandwidth shaping should be enforced on the specified link or LAN. When enabled, bandwidth limits indicated for a link or LAN will not be enforced.  Disabled by default for all links and LANs. That is, link bandwidth shaping  is  enforced on all links and LANs by default.  If the delay and loss values for a  tb-set-noshaping  link are zero (the default), then no delay node or end-node delay pipe will be associated with the link or LAN.  This command is a hack.  The primary purpose for this command is to subvert the topology mapper ( assign ).  Assign  always observes the physical bandwidth constraints of the testbed. By using  tb-set-noshaping , you can convince  assign  that links are low-bandwidth and thus get your topology mapped, but then not actually have the links shaped.    tb-use-endnodeshaping \uf0c1  tb-use-endnodeshaping enable\n\ntb-use-endnodeshaping 1  Where:  enable  = Set to 1 to enable end-node traffic shaping on all links and LANs.   Note   This command allows you to use end-node traffic shaping globally, without having to specify per link or LAN with  tb-set-endnodeshaping .  See  End Node Traffic Shaping and Multiplexed Links  for more details.    tb-force-endnodeshaping \uf0c1  tb-force-endnodeshaping enable\n\ntb-force-endnodeshaping 1  Where:  enable  = Set to 1 to force end-node traffic shaping on all links and LANs.   Note   This command allows you to specify non-shaped links and LANs at creation time, but still control the shaping parameters later (e.g., increase delay, decrease bandwidth) after the experiment is swapped in.  This command forces allocation of end-node shaping infrastructure for all links. There is no equivalent to force delay node allocation.  See  End Node Traffic Shaping and Multiplexed Links  for more details.    tb-set-multiplexed \uf0c1  tb-set-multiplexed link allow\n\ntb-set-multiplexed $link1 1  Where:  link  = The link we are modifying.  'allow` = Set to 1 to allow multiplexing of the link, 0 to disallow.   Note   This command allows a link to be multiplexed over a physical link along with other links.  Disabled by default for all links.  Only available when running the standard DETER FreeBSD (not Linux) and only for links (not LANs).  See  End Node Traffic Shaping and Multiplexed Links  for more details.    tb-set-vlink-emulation \uf0c1  tb-set-vlink-emulation style\n\ntb-set-vlink-emulation $link1 vlan  Where:  style  = One of \"vlan\" or \"veth-ne\"   Note  It seems to be necessary to set the virtual link emulation style to vlan for multiplexed links to work under linux.", 
            "title": "Link Loss Commands"
        }, 
        {
            "location": "/core/ns-commands/#virtual-type-commands", 
            "text": "Virtual Types are a method of defining fuzzy types, i.e. types that can be fulfilled by multiple different physical types. The advantage of virtual types, also known as  'vtypes ', is that all nodes of the same vtype will usually be the same physical type of node. In this way, vtypes allows logical grouping of nodes.  As an example, imagine we have a network with internal routers connecting leaf nodes. We want the routers to all have the same hardware, and the leaf nodes to all have the same hardware, but the specifics do not matter. We have the following fragment in our NS file:  ...\ntb-make-soft-vtype router {pc600 pc850}\ntb-make-soft-vtype leaf {pc600 pc850}\n\ntb-set-hardware $router1 router\ntb-set-hardware $router2 router\ntb-set-hardware $leaf1 leaf\ntb-set-hardware $leaf2 leaf  Here we have set up two soft (see below) vtypes: router and leaf. Our router nodes are then specified to be of type  router , and the leaf nodes of type  leaf . When the experiment is swapped in, the testbed will attempt to make router1 and router2 be of the same type, and similarly, leaf1 and leaf2 of the same type. However, the routers/leafs may be pc600s or they may be pc850s, whichever is easier to fit in to the available resources.  As a basic use, vtypes can be used to request nodes that are all the same type, but can be of any available type:  ...\ntb-make-soft-vtype N {pc600 pc850}\n\ntb-set-hardware $node1 N\ntb-set-hardware $node2 N  Vtypes come in two varieties: hard and soft.    'Soft ' - With soft vtypes, the testbed will try to make all nodes of that vtype the same physical type, but may do otherwise if resources are tight.    'Hard ' - Hard vtypes behave just like soft vtypes except that the testbed will give higher priority to vtype consistency and swapping in will fail if the vtypes cannot be satisfied.   Therefore, if you use soft vtypes you are more likely to swap in but there is a chance your node of a specific vtype will not all be the same. If you use hard vtypes, all nodes of a given vtype will be the same, but swapping in may fail.  Further, you can have weighted soft vtypes. Here you assign a weight from 0 to 1 exclusive to your vtype. The testbed will give higher priority to consistency in the higher weighted vtypes. The primary use of this is to rank multiple vtypes by importance of consistency. Soft vtypes have a weight of 0.5 by default.  As a final note, when specifying the types of a vtype, use the most specific type possible. For example, the following command is not very useful:   tb-make-soft-vtype router {pc pc600}  This is because pc600 is a sub type of pc. You may very well end up with two routers as type pc with different hardware, as pc covers multiple types of hardware.  tb-make-soft-vtype \uf0c1  tb-make-soft-vtype vtype {types}\ntb-make-hard-vtype vtype {types}\ntb-make-weighted-vtype vtype weight {types}\n\ntb-make-soft-vtype router {pc600 pc850}\ntb-make-hard-vtype leaf {pc600 pc850}\ntb-make-weighted-vtype A 0.1 {pc600 pc850}  Where:  vtype  = The name of the vtype to create.  types  = One or more physical types.  weight  = The weight of the vtype, 0    weight    1.   Note   These commands create vtypes. See notes above for a description of vtypes and the difference between soft and hard.  tb-make-soft-vtype  creates vtypes with weight 0.5.  vtype commands must appear before  tb-set-hardware  commands that use them.  Do not use  tb-fix-node  with nodes that have a vtype.", 
            "title": "Virtual Type Commands"
        }, 
        {
            "location": "/core/ns-commands/#misc-commands", 
            "text": "tb-fix-node \uf0c1  tb-fix-node vnode pnode\n\ntb-fix-node $node0 pc42  Where:  vnode  = The node we are fixing.  pnode  = The physical node we want used.   Note   This command forces the virtual node to be mapped to the specified physical node. Swap in will fail if this cannot be done.  Do not use this command on nodes that are a virtual type.    tb-fix-interface \uf0c1  tb-fix-interface vnode vlink iface \n\ntb-fix-interface $node0 $link0  eth0   Where:  vnode  = The node we are fixing.  vlink  = The link connecting to that node that we want to set.  iface  = The DETERLab name for the interface that is to be used.   Note   The interface names used are the ones in the DETERLab database - we can make no guarantee that the OS image that boots on the node assigns the same name.  Different types of nodes have different sets of interfaces, so this command is most useful if you are also using  tb-fix-node  and/or  tb-set-hardware  on the  vnode .    tb-set-uselatestwadata \uf0c1  tb-set-uselatestwadata 0\ntb-set-uselatestwadata 1   Note   This command indicates which widearea data to use when mapping widearea nodes to links. The default is 0, which says to use the aged data. Setting it to 1 says to use the most recent data.    tb-set-wasolver-weights \uf0c1  tb-set-wasolver-weights delay bw plr\n\ntb-set-wasolver-weights 1 10 500  Where:  delay  = The weight to give delay when solving.  bw  = The weight to give bandwidth when solving.  plr  = The weight to give lossrate when solving.   Note   This command sets the relative weights to use when assigning widearea nodes to links. Specifying a zero says to ignore that particular metric when doing the assignment. Setting all three to zero results in an essentially random selection.    tb-set-node-failure-action \uf0c1  tb-set-node-failure-action node action\n\ntb-set-node-failure-action $nodeA  fatal \ntb-set-node-failure-action $nodeB  nonfatal   Where:  node  = The node name.  action  = One of \"fatal\" or \"nonfatal\".   Note  This command sets the failure mode for a node. When an experiment is swapped in, the default action is to abort the swapin if any nodes fail to come up normally. This is the \"fatal\" mode. You may also set a node to \"nonfatal\" which will cause node bootup failures to be reported, but otherwise ignored during swapin. Note that this can result in your experiment not working properly if a dependent node fails, but typically you can arrange your software to deal with this.", 
            "title": "Misc. Commands"
        }, 
        {
            "location": "/core/swapping/", 
            "text": "What are DETERLab's use policies?\n\uf0c1\n\n\nAs a courtesy to other experimenters, we ask that experiments be swapped out or terminated when they are no longer in active use. There are a limited number of nodes available, and node reservations are exclusive, so it is important to free nodes that will be idle so that others may use them. In summary, our policy is that experiments should be swapped out when they are not in use. We encourage you to do that yourself. In general, if experiments are idle for several hours, the system will automatically swap them out, or send you mail about it, and/or an operator may manually swap them out. The actual grace period will differ depending on the size of the experiment, the current demand for resources, and other factors (such as whether you've been a good DETERLab citizen in the past!). If you mark your experiment \"non-idle-swappable\" at creation time or before swapin, and testbed-ops approves your justification, we will make every effort to contact you before swapping it, since local node state could be lost on swapout. Please see full details below.\n\n\nWhat is \"active use\"?\n\uf0c1\n\n\nA node or experiment that is being actively used will be doing something related to your experiment. In almost all cases, someone will either be logged into it using it interactively, or some program will be running, sending and receiving packets, and performing the operations necessary to carry out the experiment.\n\n\nWhen is an experiment considered idle?\n\uf0c1\n\n\nYour experiment will be considered idle if it has no measurable activity for a significant period of time (a few hours; the exact time is typically set at swapin time). We detect the following types of activity:\n\n\n\n\nAny network activity on the experimental network\n\n\nSubstantial activity on the control network\n\n\nTTY/console activity on nodes\n\n\nHigh CPU activity on nodes\n\n\nCertain external events, such as rebooting a node with \nnode_reboot\n \n\n\n\n\nIf your experiment's activity falls outside these measured types of activity, or it seems that DETERLab is not assessing your idle time correctly, please be sure to let us know when you create your experiment, or you may be swapped out unexpectedly.\n\n\n''It is considered abuse to generate artificial activity in order to prevent your experiment from being marked idle. Abusers' access to DETERLab will be revoked, and their actions will be reported to their project leader. Please do not do this. If you think you need special assistance for a deadline, demo or other reason, please \ncontact us\n.''\n\n\nWhat is \"swapping\"? \n\uf0c1\n\n\nSwapping is the process of instantiating your experiment, i.e., allocating nodes, configuring links, etc. It also refers to the reverse process, in which nodes are released. These processes are called \"swapping in\" and \"swapping out\" respectively.\n\n\nWhat is an \"Idle-Swap\"? \n\uf0c1\n\n\nAn \"Idle-Swap\" is when DETERLab or its operators swap out your experiment because it was idle for too long. There are two ways that your experiment may be idle-swapped: automatic and manual. \n\n\nThe most common is automatic, which happens when Idle-Swap is enabled for your experiment and the experiment has been continuously idle for the idle-swap time that was set at creation/swapin time (usually a few hours). DETERLab will then automatically swap it out. \n\n\nThe other way to get idle-swapped is manually, by a DETERLab operator. This typically happens when there is very high resource demand and the experiment has been idle a substantial time, usually a few hours. In this case we will typically make every effort to contact you, since it may cause you to lose data stored on the nodes. \n\n\n''Note that operators (and you) may swap your excessively idle experiment whether or not it is marked idle-swappable.''\n\n\nWhen you create your experiment, you may uncheck the \"Idle-Swap\" box, disabling the automatic idle-swapping of your experiment. If you do so, you must specify the reason, which will be reviewed by testbed-ops. If your reason is judged unacceptable or insufficient, we will explain why, and your experiment will be marked idle-swappable. Valid reasons might be things such as:\n\n\n\n\n''Your idle-detection system fails to detect my experimental activity.''\n\n\n''I have node-local state that is impractical to copy off in a timely or reliable manner, because .....''\n\n\n''My experiment takes a huge number of nodes, I have several runs to make with intervening think time, and if someone grabs some of these nodes if I'm swapped while thinking, I'll miss my deadline 2 days from now.''\n\n\n\n\nIf an experiment is non-idle-swappable, our system will not automatically swap it out, and testbed administrators will attempt to contact you in the event a swapout becomes necessary. However, we expect you to be responsible for managing your experiment in a responsible way, a way that uses DETERLab's hardware resources efficiently.\n\n\nWhen you create your experiment, you may decrease the idle-swap time from the displayed default, but you may not raise it. If lowering it is compatible with your planned use, doing so helps you be a good DETERLab citizen. If you want it raised, for example for reasons similar to those given above, send mail to testbed-ops AT deterlab.net.\n\n\nYou may edit the swap settings (Idle-Swap, Max-Duration, and corresponding reasons and timeouts) using the \"Modify Settings\" menu item on the ''Experiment'' page for your experiment.\n\n\nHow long is too long for a node to be idle?\n\uf0c1\n\n\nIdeally, an experiment should be used nearly continuously from start to finish of the experiment, then swapped out or terminated. However, this isn't always possible. In general, if your experiment is idle for 2 hours or more, it should be swapped out. This is especially true at night (in U.S. timezones) and on weekends. Many experimenters take advantage of lower demand during evenings and weekends to run their large-scale (50-150 node) tests. If your experiment uses 10 nodes or more, it is even more important to release your nodes as soon as possible. Swapin and swapout only take a few minutes (typically 3-5 for swapin, and less than 1 for swapout), so you won't lose much time by doing it.\n\n\nSometimes an experiment will run long enough that you cannot be online to terminate it, for example, if the experiment completes in the middle of the night. We provide three mechanisms to assist you in terminating your experiment and releasing nodes in a timely manner. The first is the Idle Swap, explained above, the second is \nScheduled Termination\n, and the third is the \"Max Duration\" option, \nexplained below\n.\n\n\nWhat is \"node state\"? \n\uf0c1\n\n\nSome experiments have state that is stored exclusively on the nodes themselves, on their local hard drives. This is state that is not in your NS file or files or disk images that it references, and therefore is not preserved in our database across swapin/swapout. This is state you add to your machines \"by hand\" after DETERLab sets up your experiment, like files you add or modify on filesystems local to test nodes. Local node state does not include any data you store in \n/users\n, \n/proj\n, or \n/groups\n, since those are saved on a fileserver, and not on the local nodes.\n\n\nMost experiments don't have any local node state, and can be swapped out and in without losing any information. This is highly recommended, since it is more courteous to other experimenters. It allows you or DETERLab to easily free up your nodes at any time without losing any of your work. '''Please make your experiments adhere to this guideline whenever possible.'''\n\n\nAn experiment that needs local state that inherently cannot be saved (for some reason) or that you will not be able to copy off before your experiment hits the \"idle-swap time,\" should not be marked \"idle-swap\" when you create it. In the ''Begin Experiment'' form you must explain the reason. If you must have node state, you can save it before you swap out by copying it by hand (e.g., into a tar or RPM file), or creating a disk image of the node in question, and later reloading it to a new node after you swap in again. Disk images in effect create a \"custom OS\" that may be loaded automatically based on your NS file. More information about disk images can be found on our \nDisk Image page\n (you must be logged in to use it). We will be developing a system that will allow the swapping system automatically to save and restore the local node state of an entire experiment.\n\n\nI just received an email asking me to swap or terminate my experiment. \n\uf0c1\n\n\nDETERLab has a system for detecting node use, to help achieve more efficient and fair use of DETERLab's limited resources. This system sends email messages to experiment leaders whose experiments have been idle for several hours. If you get a message like this, your experiment has been inactive for too long and you should free up its nodes. If the experiment continues to be idle, more reminders may be sent, and soon your project leader will be one of the recipients. After you have been notified, your experiment may be swapped at any time, depending on current demand for nodes, and other factors.\n\n\nIf you feel you received the message in error, please respond to Testbed Operations (testbed-ops@isi.deterlab.net) as soon as possible, describing how you have used your node in the last few hours. There are some types of activity that are difficult to accurately detect, so we'd like to know how we can improve our activity detection system. '''Above all, do not ignore these messages.''' If you get several reminders and don't respond, your experiment will be swapped out, potentially causing loss of some of your work (see \"node state\" above). If there is a reason you need to keep your experiment running, tell us so we don't inadvertently cause problems for you.\n\n\nSomeone swapped my experiment!\n\uf0c1\n\n\nAs described above, the system automatically swaps out your experiment after it reaches its idle time limit, or sometimes an DETERLab operator does it earlier when resources are in especially high demand. In the latter case, we will typically try to contact you by email before we swap it out. However, especially if the experiment has been idle for several hours, we may swap it out for you without waiting very long to hear from you. Because of this, it is critical that you keep in close contact with us about an experiment that we may perceive as idle if you want to avoid any loss of your work.\n\n\nWhat is \"Max duration\"?\n\uf0c1\n\n\nEach experiment may have a Maximum Duration, where an experimenter specifies the maximum amount of time that the experiment should stay swapped in. When that time is exceeded, the experiment is unconditionally swapped out. The timer is reset every time the experiment swaps in. A reminder message is sent about an hour before the experiment is swapped. This swapout happens regardless of any activity on the nodes, and can be averted by using the \"Edit Metadata\" menu item on the experiment's page to turn off the Maximum Duration feature or to lengthen the duration.\n\n\nThis feature allows users to schedule experiment swapouts, helping them to release nodes in a timely manner. For instance, if you plan to use your experiment throughout an 8 hour work day, you can schedule a swapout for 8 hours after it is swapped in. That way, if you forget to swap out before leaving for the day, it will automatically free up the nodes for other users, without leaving the nodes idle for several hours before being idle-swapped, and will work even if you leave your test programs running, making the experiment look non-idle. For automated experiments, it lets you schedule a swapout for slightly after the maximum amount of time your experiment should last. It can also help catch \"runaway\" experiments (typically batch).\n\n\n\"Max duration\" has a similar effect as \nscheduled termination/swapout\n, which is specified in the NS file. The differences are that the former lets you adjust the duration while the experiment is running, you get a warning email, and you're always swapped, never terminated. (It's also implemented differently, with a 5 minute scheduling granularity.)", 
            "title": "Understanding Swapping (Node Use Policies)"
        }, 
        {
            "location": "/core/swapping/#what-are-deterlabs-use-policies", 
            "text": "As a courtesy to other experimenters, we ask that experiments be swapped out or terminated when they are no longer in active use. There are a limited number of nodes available, and node reservations are exclusive, so it is important to free nodes that will be idle so that others may use them. In summary, our policy is that experiments should be swapped out when they are not in use. We encourage you to do that yourself. In general, if experiments are idle for several hours, the system will automatically swap them out, or send you mail about it, and/or an operator may manually swap them out. The actual grace period will differ depending on the size of the experiment, the current demand for resources, and other factors (such as whether you've been a good DETERLab citizen in the past!). If you mark your experiment \"non-idle-swappable\" at creation time or before swapin, and testbed-ops approves your justification, we will make every effort to contact you before swapping it, since local node state could be lost on swapout. Please see full details below.", 
            "title": "What are DETERLab's use policies?"
        }, 
        {
            "location": "/core/swapping/#what-is-active-use", 
            "text": "A node or experiment that is being actively used will be doing something related to your experiment. In almost all cases, someone will either be logged into it using it interactively, or some program will be running, sending and receiving packets, and performing the operations necessary to carry out the experiment.", 
            "title": "What is \"active use\"?"
        }, 
        {
            "location": "/core/swapping/#when-is-an-experiment-considered-idle", 
            "text": "Your experiment will be considered idle if it has no measurable activity for a significant period of time (a few hours; the exact time is typically set at swapin time). We detect the following types of activity:   Any network activity on the experimental network  Substantial activity on the control network  TTY/console activity on nodes  High CPU activity on nodes  Certain external events, such as rebooting a node with  node_reboot     If your experiment's activity falls outside these measured types of activity, or it seems that DETERLab is not assessing your idle time correctly, please be sure to let us know when you create your experiment, or you may be swapped out unexpectedly.  ''It is considered abuse to generate artificial activity in order to prevent your experiment from being marked idle. Abusers' access to DETERLab will be revoked, and their actions will be reported to their project leader. Please do not do this. If you think you need special assistance for a deadline, demo or other reason, please  contact us .''", 
            "title": "When is an experiment considered idle?"
        }, 
        {
            "location": "/core/swapping/#what-is-swapping", 
            "text": "Swapping is the process of instantiating your experiment, i.e., allocating nodes, configuring links, etc. It also refers to the reverse process, in which nodes are released. These processes are called \"swapping in\" and \"swapping out\" respectively.", 
            "title": "What is \"swapping\"? "
        }, 
        {
            "location": "/core/swapping/#what-is-an-idle-swap", 
            "text": "An \"Idle-Swap\" is when DETERLab or its operators swap out your experiment because it was idle for too long. There are two ways that your experiment may be idle-swapped: automatic and manual.   The most common is automatic, which happens when Idle-Swap is enabled for your experiment and the experiment has been continuously idle for the idle-swap time that was set at creation/swapin time (usually a few hours). DETERLab will then automatically swap it out.   The other way to get idle-swapped is manually, by a DETERLab operator. This typically happens when there is very high resource demand and the experiment has been idle a substantial time, usually a few hours. In this case we will typically make every effort to contact you, since it may cause you to lose data stored on the nodes.   ''Note that operators (and you) may swap your excessively idle experiment whether or not it is marked idle-swappable.''  When you create your experiment, you may uncheck the \"Idle-Swap\" box, disabling the automatic idle-swapping of your experiment. If you do so, you must specify the reason, which will be reviewed by testbed-ops. If your reason is judged unacceptable or insufficient, we will explain why, and your experiment will be marked idle-swappable. Valid reasons might be things such as:   ''Your idle-detection system fails to detect my experimental activity.''  ''I have node-local state that is impractical to copy off in a timely or reliable manner, because .....''  ''My experiment takes a huge number of nodes, I have several runs to make with intervening think time, and if someone grabs some of these nodes if I'm swapped while thinking, I'll miss my deadline 2 days from now.''   If an experiment is non-idle-swappable, our system will not automatically swap it out, and testbed administrators will attempt to contact you in the event a swapout becomes necessary. However, we expect you to be responsible for managing your experiment in a responsible way, a way that uses DETERLab's hardware resources efficiently.  When you create your experiment, you may decrease the idle-swap time from the displayed default, but you may not raise it. If lowering it is compatible with your planned use, doing so helps you be a good DETERLab citizen. If you want it raised, for example for reasons similar to those given above, send mail to testbed-ops AT deterlab.net.  You may edit the swap settings (Idle-Swap, Max-Duration, and corresponding reasons and timeouts) using the \"Modify Settings\" menu item on the ''Experiment'' page for your experiment.", 
            "title": "What is an \"Idle-Swap\"? "
        }, 
        {
            "location": "/core/swapping/#how-long-is-too-long-for-a-node-to-be-idle", 
            "text": "Ideally, an experiment should be used nearly continuously from start to finish of the experiment, then swapped out or terminated. However, this isn't always possible. In general, if your experiment is idle for 2 hours or more, it should be swapped out. This is especially true at night (in U.S. timezones) and on weekends. Many experimenters take advantage of lower demand during evenings and weekends to run their large-scale (50-150 node) tests. If your experiment uses 10 nodes or more, it is even more important to release your nodes as soon as possible. Swapin and swapout only take a few minutes (typically 3-5 for swapin, and less than 1 for swapout), so you won't lose much time by doing it.  Sometimes an experiment will run long enough that you cannot be online to terminate it, for example, if the experiment completes in the middle of the night. We provide three mechanisms to assist you in terminating your experiment and releasing nodes in a timely manner. The first is the Idle Swap, explained above, the second is  Scheduled Termination , and the third is the \"Max Duration\" option,  explained below .", 
            "title": "How long is too long for a node to be idle?"
        }, 
        {
            "location": "/core/swapping/#what-is-node-state", 
            "text": "Some experiments have state that is stored exclusively on the nodes themselves, on their local hard drives. This is state that is not in your NS file or files or disk images that it references, and therefore is not preserved in our database across swapin/swapout. This is state you add to your machines \"by hand\" after DETERLab sets up your experiment, like files you add or modify on filesystems local to test nodes. Local node state does not include any data you store in  /users ,  /proj , or  /groups , since those are saved on a fileserver, and not on the local nodes.  Most experiments don't have any local node state, and can be swapped out and in without losing any information. This is highly recommended, since it is more courteous to other experimenters. It allows you or DETERLab to easily free up your nodes at any time without losing any of your work. '''Please make your experiments adhere to this guideline whenever possible.'''  An experiment that needs local state that inherently cannot be saved (for some reason) or that you will not be able to copy off before your experiment hits the \"idle-swap time,\" should not be marked \"idle-swap\" when you create it. In the ''Begin Experiment'' form you must explain the reason. If you must have node state, you can save it before you swap out by copying it by hand (e.g., into a tar or RPM file), or creating a disk image of the node in question, and later reloading it to a new node after you swap in again. Disk images in effect create a \"custom OS\" that may be loaded automatically based on your NS file. More information about disk images can be found on our  Disk Image page  (you must be logged in to use it). We will be developing a system that will allow the swapping system automatically to save and restore the local node state of an entire experiment.", 
            "title": "What is \"node state\"? "
        }, 
        {
            "location": "/core/swapping/#i-just-received-an-email-asking-me-to-swap-or-terminate-my-experiment", 
            "text": "DETERLab has a system for detecting node use, to help achieve more efficient and fair use of DETERLab's limited resources. This system sends email messages to experiment leaders whose experiments have been idle for several hours. If you get a message like this, your experiment has been inactive for too long and you should free up its nodes. If the experiment continues to be idle, more reminders may be sent, and soon your project leader will be one of the recipients. After you have been notified, your experiment may be swapped at any time, depending on current demand for nodes, and other factors.  If you feel you received the message in error, please respond to Testbed Operations (testbed-ops@isi.deterlab.net) as soon as possible, describing how you have used your node in the last few hours. There are some types of activity that are difficult to accurately detect, so we'd like to know how we can improve our activity detection system. '''Above all, do not ignore these messages.''' If you get several reminders and don't respond, your experiment will be swapped out, potentially causing loss of some of your work (see \"node state\" above). If there is a reason you need to keep your experiment running, tell us so we don't inadvertently cause problems for you.", 
            "title": "I just received an email asking me to swap or terminate my experiment. "
        }, 
        {
            "location": "/core/swapping/#someone-swapped-my-experiment", 
            "text": "As described above, the system automatically swaps out your experiment after it reaches its idle time limit, or sometimes an DETERLab operator does it earlier when resources are in especially high demand. In the latter case, we will typically try to contact you by email before we swap it out. However, especially if the experiment has been idle for several hours, we may swap it out for you without waiting very long to hear from you. Because of this, it is critical that you keep in close contact with us about an experiment that we may perceive as idle if you want to avoid any loss of your work.", 
            "title": "Someone swapped my experiment!"
        }, 
        {
            "location": "/core/swapping/#what-is-max-duration", 
            "text": "Each experiment may have a Maximum Duration, where an experimenter specifies the maximum amount of time that the experiment should stay swapped in. When that time is exceeded, the experiment is unconditionally swapped out. The timer is reset every time the experiment swaps in. A reminder message is sent about an hour before the experiment is swapped. This swapout happens regardless of any activity on the nodes, and can be averted by using the \"Edit Metadata\" menu item on the experiment's page to turn off the Maximum Duration feature or to lengthen the duration.  This feature allows users to schedule experiment swapouts, helping them to release nodes in a timely manner. For instance, if you plan to use your experiment throughout an 8 hour work day, you can schedule a swapout for 8 hours after it is swapped in. That way, if you forget to swap out before leaving for the day, it will automatically free up the nodes for other users, without leaving the nodes idle for several hours before being idle-swapped, and will work even if you leave your test programs running, making the experiment look non-idle. For automated experiments, it lets you schedule a swapout for slightly after the maximum amount of time your experiment should last. It can also help catch \"runaway\" experiments (typically batch).  \"Max duration\" has a similar effect as  scheduled termination/swapout , which is specified in the NS file. The differences are that the former lets you adjust the duration while the experiment is running, you get a warning email, and you're always swapped, never terminated. (It's also implemented differently, with a 5 minute scheduling granularity.)", 
            "title": "What is \"Max duration\"?"
        }, 
        {
            "location": "/core/DETERSSH/", 
            "text": "Each node on the testbed is reachable via \nSSH\n.  One main difference between DETER and Emulab is that DETER nodes are not accessible directly from the internet.  In order to log into your nodes, you must first log into '''users.isi.deterlab.net''' using your DETER username (not your email address) and password (or your SSH public key).  From users you can log into your nodes.  To save on connections, you might want to look into using \nGNU screen\n on \nusers\n.  Also refer to the \nTips and Tricks\n section below for ways to make accessing DETER easier.\n\n\nUploading files to DETER\n\uf0c1\n\n\nYou can upload files to users.isi.deterlab.net via \nSCP\n  or \nSFTP\n.  Files in your home directory and in your project directory will be made available to you on all of your testbed nodes via \nNFS\n.\n\n\nAn Example Session with Windows and Putty\n\uf0c1\n\n\nPutty\n is a free, lightweight SSH client for Windows.  Here is an example session in which I connect to my experimental node \"node0\" in my experiment \"jjh-ubuntu1004\" in the project \"DeterTest\".\n\n\nFirst we connect to '''users.isi.deterlab.net''':\n\n\n\n\nWe then enter in our just our username and password (the same password as the DETERLab web interface).  Trying to use your email address or something like \njjh@users.isi.deterlab.net\n will '''not''' work:\n\n\n\n\nNow we have successfully logged into users.isi.deterlab.net:\n\n\n\n\nFrom users, we now ssh into our experimental node, \"node0.jjh-ubuntu.detertest\":\n\n\n\n\nTips and Tricks\n\uf0c1\n\n\nListing your nodes from the command line\n\uf0c1\n\n\nWhen logged into \nusers.isi.deterlab.net\n, the \nnode_list\n command will list the names of all your nodes.  You can log into your nodes using either the pcXXX name or the full experimental name.\n\n\n  [jhickey@users ~]$ node_list\n  DeterTest/jjh-ubuntu\n      node1.jjh-ubuntu.DeterTest / pc118\n      node0.jjh-ubuntu.DeterTest / pc054\n      node2.jjh-ubuntu.DeterTest / pc167\n  DeterTest/jjh-ubuntu1004\n      node0.jjh-ubuntu1004.DeterTest / pc026\n  [jhickey@users ~]$ \n\n\n\n\nSSH port forwarding\n\uf0c1\n\n\nIf you are, for example, running an internal web server on one of your DETER nodes, you can access it via SSH through users.  For example to redirect port 80 on pcXXX to your local machine on port 8080 you would do:\n\n\n    ssh -L 8080:pcXXX:80 username@users.isi.deterlab.net\n\n\n\n\nOnce logged in, you should be able to access the web server on your DETER node by going to http://localhost:8080.  For more information on port forwarding with SSH, please refer to the SSH man page.\n\n\nSSH port forwarding with Putty\n\uf0c1\n\n\nTo use putty for port forwarding, configure putty to open a connection to \nusers.isi.deterlab.net\n\n\n\n\nbefore you make that connection, set up the tunneling parameters.  This example forwards local port 12345 to a remode desktop protocol server (port 3389) on a testbed node.  select the \nTunnels\n menu from under the \nSSH\n choice in the \nConnection\n menu on the left hand side.  Add a forwarded port using the \nLocal\n type, a local port number (12345 in the image) and the DETER hostname and port in the \nDestination\n field.  In the example we are forwarding the connection to port 3389 (the remote desktop protocol) on \npc102\n. \n\n\n\n\nBe sure to press \nAdd\n to add the port.  The putty window will look like this:\n\n\n\n\nNow open that connection.  You will see a login prompt, and you should log in to users.\n\n\n\n\nNow you should be able to point your local remote desktop viewer to localhost port 12345 and see the login screen of pc102.  If the node is a Windows node, you will see something like this:\n\n\n\n\nBe sure that your local machine does not firewall the local port 12345.  Replace \npc102\n with a node in your experiment and the forwarded port with the port your service uses.\n\n\nUploading your SSH key from OS X \n\uf0c1\n\n\nThe Upload File dialog in Macintosh OS X does not show hidden directories by default.  This creates and extra hassle when uploading SSH public keys from an OS X machine.  \n\n\nIn the \"Upload File\" dialog, use the shortcut '''Shift-Command-G''' and type in \"~/.ssh\" to navigate to the contents of your .ssh directory.\n\n\n\n\nThen you will be presented with the contents of your .ssh directory and will be able to upload your id_rsa.pub file to DETER:\n\n\n\n\nOpenSSH Configuration for Directly Logging into testbed nodes\n\uf0c1\n\n\nThese configuration tweaks should work for any operating system that runs OpenSSH (Linux, BSD, and OS X typically use OpenSSH as the default SSH client).\n\n\nIt is possible to log directly into testbed nodes with a little SSH configuration tweaking.  Adding the following statement to '''~/.ssh/config''' will allow you to skip logging into users in order to access a particular testbed node.  Change MyProject to the name of your project.\n\n\n    Host pc*.isi.deterlab.net\n        ProxyCommand ssh users.isi.deterlab.net nc %h %p\n        StrictHostKeyChecking no\n\n    Host *.MyProject.isi.deterlab.net\n        ProxyCommand ssh users.isi.deterlab.net nc %h %p\n        StrictHostKeyChecking no\n\n\n\n\nWith this configuration change and a proper SSH key setup, you will be able to directly log into nodes in your experiment.\n\n\nYou will now be able to log into nodes in your experiment using either the actual node name, e.g. pc025.isi.deterlab.net, or [host].[experiment].[project].isi.deterlab.net.\n\n\nFor example:\n\n\n    jjhs-mac-mini:~ jjh$ ssh node0.jjh-ubuntu1004.DeterTest.isi.deterlab.net\n    Warning: Permanently added 'node0.jjh-ubuntu1004.detertest.isi.deterlab.net' (RSA) to the list of known hosts.\n    Linux node0.jjh-ubuntu1004.detertest.isi.deterlab.net 2.6.32-25-generic-pae #45-Ubuntu SMP Sat Oct 16 21:01:33 UTC 2010 i686 GNU/Linux\n    Ubuntu 10.04.1 LTS\n\n    Welcome to Ubuntu!\n     * Documentation:  https://help.ubuntu.com/\n\n      System information as of Wed Nov 10 20:41:19 PST 2010\n\n      System load:  0.0                Processes:           116\n      Usage of /:   12.3% of 14.67GB   Users logged in:     0\n      Memory usage: 1%                 IP address for eth1: 192.168.1.26\n      Swap usage:   0%\n\n      Graph this data and manage this system at https://landscape.canonical.com/\n\n    4 packages can be updated.\n    2 updates are security updates.\n\n    Last login: Wed Nov 10 20:13:15 2010 from users.isi.deterlab.net\n    node0:~\n \n\n\n\n\nAccelerating Multiple Connections using OpenSSH Connection Multplexing\n\uf0c1\n\n\nYou can log in multiple times using the same SSH connection.  This dramatically speeds up creating new connections.  To enable SSH connection multiplexing, add the following lines to ~/.ssh/config.  If you are on a multiuser machine, you may want to store the control socket someplace other than /tmp.\n\n\n    Host users.isi.deterlab.net\n        ControlMaster auto\n        ControlPath /tmp/%r@%h:%p\n\n\n\n\nTo verify that it is working, you can use the '''-v''' option:\n\n\n    jjhs-mac-mini:~ jjh$ ssh -v pc026.isi.deterlab.net\n    OpenSSH_5.2p1, OpenSSL 0.9.8l 5 Nov 2009\n    debug1: Reading configuration data /Users/jjh/.ssh/config\n    debug1: Applying options for *isi.deterlab.net\n    debug1: Applying options for pc*.isi.deterlab.net\n    debug1: Applying options for *\n    debug1: Reading configuration data /etc/ssh_config\n    debug1: auto-mux: Trying existing master\n    Last login: Wed Nov 10 20:51:43 2010 from users.isi.deterlab.net\n    node0:~\n \n\n\n\n\nIf you try to close your master connection while other connections are active, the connection will stay running until the other sessions end.", 
            "title": "Accessing testbeds using SSH"
        }, 
        {
            "location": "/core/DETERSSH/#uploading-files-to-deter", 
            "text": "You can upload files to users.isi.deterlab.net via  SCP   or  SFTP .  Files in your home directory and in your project directory will be made available to you on all of your testbed nodes via  NFS .", 
            "title": "Uploading files to DETER"
        }, 
        {
            "location": "/core/DETERSSH/#an-example-session-with-windows-and-putty", 
            "text": "Putty  is a free, lightweight SSH client for Windows.  Here is an example session in which I connect to my experimental node \"node0\" in my experiment \"jjh-ubuntu1004\" in the project \"DeterTest\".  First we connect to '''users.isi.deterlab.net''':   We then enter in our just our username and password (the same password as the DETERLab web interface).  Trying to use your email address or something like  jjh@users.isi.deterlab.net  will '''not''' work:   Now we have successfully logged into users.isi.deterlab.net:   From users, we now ssh into our experimental node, \"node0.jjh-ubuntu.detertest\":", 
            "title": "An Example Session with Windows and Putty"
        }, 
        {
            "location": "/core/DETERSSH/#tips-and-tricks", 
            "text": "Listing your nodes from the command line \uf0c1  When logged into  users.isi.deterlab.net , the  node_list  command will list the names of all your nodes.  You can log into your nodes using either the pcXXX name or the full experimental name.    [jhickey@users ~]$ node_list\n  DeterTest/jjh-ubuntu\n      node1.jjh-ubuntu.DeterTest / pc118\n      node0.jjh-ubuntu.DeterTest / pc054\n      node2.jjh-ubuntu.DeterTest / pc167\n  DeterTest/jjh-ubuntu1004\n      node0.jjh-ubuntu1004.DeterTest / pc026\n  [jhickey@users ~]$   SSH port forwarding \uf0c1  If you are, for example, running an internal web server on one of your DETER nodes, you can access it via SSH through users.  For example to redirect port 80 on pcXXX to your local machine on port 8080 you would do:      ssh -L 8080:pcXXX:80 username@users.isi.deterlab.net  Once logged in, you should be able to access the web server on your DETER node by going to http://localhost:8080.  For more information on port forwarding with SSH, please refer to the SSH man page.  SSH port forwarding with Putty \uf0c1  To use putty for port forwarding, configure putty to open a connection to  users.isi.deterlab.net   before you make that connection, set up the tunneling parameters.  This example forwards local port 12345 to a remode desktop protocol server (port 3389) on a testbed node.  select the  Tunnels  menu from under the  SSH  choice in the  Connection  menu on the left hand side.  Add a forwarded port using the  Local  type, a local port number (12345 in the image) and the DETER hostname and port in the  Destination  field.  In the example we are forwarding the connection to port 3389 (the remote desktop protocol) on  pc102 .    Be sure to press  Add  to add the port.  The putty window will look like this:   Now open that connection.  You will see a login prompt, and you should log in to users.   Now you should be able to point your local remote desktop viewer to localhost port 12345 and see the login screen of pc102.  If the node is a Windows node, you will see something like this:   Be sure that your local machine does not firewall the local port 12345.  Replace  pc102  with a node in your experiment and the forwarded port with the port your service uses.  Uploading your SSH key from OS X  \uf0c1  The Upload File dialog in Macintosh OS X does not show hidden directories by default.  This creates and extra hassle when uploading SSH public keys from an OS X machine.    In the \"Upload File\" dialog, use the shortcut '''Shift-Command-G''' and type in \"~/.ssh\" to navigate to the contents of your .ssh directory.   Then you will be presented with the contents of your .ssh directory and will be able to upload your id_rsa.pub file to DETER:   OpenSSH Configuration for Directly Logging into testbed nodes \uf0c1  These configuration tweaks should work for any operating system that runs OpenSSH (Linux, BSD, and OS X typically use OpenSSH as the default SSH client).  It is possible to log directly into testbed nodes with a little SSH configuration tweaking.  Adding the following statement to '''~/.ssh/config''' will allow you to skip logging into users in order to access a particular testbed node.  Change MyProject to the name of your project.      Host pc*.isi.deterlab.net\n        ProxyCommand ssh users.isi.deterlab.net nc %h %p\n        StrictHostKeyChecking no\n\n    Host *.MyProject.isi.deterlab.net\n        ProxyCommand ssh users.isi.deterlab.net nc %h %p\n        StrictHostKeyChecking no  With this configuration change and a proper SSH key setup, you will be able to directly log into nodes in your experiment.  You will now be able to log into nodes in your experiment using either the actual node name, e.g. pc025.isi.deterlab.net, or [host].[experiment].[project].isi.deterlab.net.  For example:      jjhs-mac-mini:~ jjh$ ssh node0.jjh-ubuntu1004.DeterTest.isi.deterlab.net\n    Warning: Permanently added 'node0.jjh-ubuntu1004.detertest.isi.deterlab.net' (RSA) to the list of known hosts.\n    Linux node0.jjh-ubuntu1004.detertest.isi.deterlab.net 2.6.32-25-generic-pae #45-Ubuntu SMP Sat Oct 16 21:01:33 UTC 2010 i686 GNU/Linux\n    Ubuntu 10.04.1 LTS\n\n    Welcome to Ubuntu!\n     * Documentation:  https://help.ubuntu.com/\n\n      System information as of Wed Nov 10 20:41:19 PST 2010\n\n      System load:  0.0                Processes:           116\n      Usage of /:   12.3% of 14.67GB   Users logged in:     0\n      Memory usage: 1%                 IP address for eth1: 192.168.1.26\n      Swap usage:   0%\n\n      Graph this data and manage this system at https://landscape.canonical.com/\n\n    4 packages can be updated.\n    2 updates are security updates.\n\n    Last login: Wed Nov 10 20:13:15 2010 from users.isi.deterlab.net\n    node0:~    Accelerating Multiple Connections using OpenSSH Connection Multplexing \uf0c1  You can log in multiple times using the same SSH connection.  This dramatically speeds up creating new connections.  To enable SSH connection multiplexing, add the following lines to ~/.ssh/config.  If you are on a multiuser machine, you may want to store the control socket someplace other than /tmp.      Host users.isi.deterlab.net\n        ControlMaster auto\n        ControlPath /tmp/%r@%h:%p  To verify that it is working, you can use the '''-v''' option:      jjhs-mac-mini:~ jjh$ ssh -v pc026.isi.deterlab.net\n    OpenSSH_5.2p1, OpenSSL 0.9.8l 5 Nov 2009\n    debug1: Reading configuration data /Users/jjh/.ssh/config\n    debug1: Applying options for *isi.deterlab.net\n    debug1: Applying options for pc*.isi.deterlab.net\n    debug1: Applying options for *\n    debug1: Reading configuration data /etc/ssh_config\n    debug1: auto-mux: Trying existing master\n    Last login: Wed Nov 10 20:51:43 2010 from users.isi.deterlab.net\n    node0:~    If you try to close your master connection while other connections are active, the connection will stay running until the other sessions end.", 
            "title": "Tips and Tricks"
        }, 
        {
            "location": "/core/node-types/", 
            "text": "This is not a complete list of all node types available at DETERLab, but below are the primary types.\n\n\n\n\ndl380g3\n\n\nMicroCloud\n\n\npc2133\n (including pc2133 and bpc2133)\n\n\npc3000\n (including pc3000, bpc3000, pc3060, bpc3060, and pc3100)\n\n\nbvx2200\n\n\nbpc2800\n\n\nnetfpga2\n\n\n\n\ndl380g3\n\uf0c1\n\n\nThere are 120 dl380g3 class nodes available at \nISI\n.\n\n\nMachine: \nHP Proliant DL360 G8 Server\n\n\nEach node has:\n\n\n\n\nDual \nIntel(R) Xeon(R)\n hexa-core processors running at 2.2 Ghz with 15MB cache\n\n\nIntel VT-x support\n\n\n\n\n\n\n24GB of RAM\n\n\nOne 1Tb SATA HP Proliant Disk Drive 7.2k rpm G8 (boot priority)\n\n\nOne 240Gb SATA HP Proliant Solid State Drive G8\n\n\nTwo experimental interfaces:\n\n\nOne Dual port PCIe Intel Ten Gigabit Ethernet card for experimental ports\n\n\nOne Quad port PCIe Intel Gigabit Ethernet card, presently with one port wired to the control network\n\n\n\n\n\n\n\n\nMicroCloud\n\uf0c1\n\n\nThere are 128 MicroCloud nodes at \nISI\n.\n\n\nMachine: High Density \nSuperMicro MicroCloud\n Chassis that fits 8 nodes in 3u of rack space.\n\n\nEach node has:\n\n\n\n\nOne \nIntel(R) Xeon(R) E3-1260L\n quad-core processor running at 2.4 Ghz\n\n\nIntel VT-x and VT-d support\n\n\n\n\n\n\n16GB of RAM\n\n\nOne 250Gb SATA Western Digital RE4 Disk Drive\n\n\n5 experimental interfaces\n\n\nOne Dual port PCIe Intel Gigabit Ethernet card for the control network and an experimental port\n\n\nOne Quad port PCIe Intel Gigabit Ethernet card for experimental network\n\n\n\n\n\n\n\n\npc2133\n\uf0c1\n\n\nThis node type includes pc2133 and bpc2133:\n\n\n\n\nThere are 63 pc2133 nodes at \nISI\n.\n\n\nThere are 64 bpc2133 nodes at \nUCB\n.\n\n\n\n\nThe pc2133 and bpc2133 machines have the following features:\n\n\n\n\nDell PowerEdge 860 Chasis\n\n\nOne Intel(R) Xeon(R) CPU X3210 quad core processor running at 2.13 Ghz\n\n\n4GB of RAM\n\n\nOne 250Gb SATA Disk Drive\n\n\nOne Dual port PCI-X Intel Gigabit Ethernet card for the control network (only one port is used).\n\n\nOne Quad port PCIe Intel Gigabit Ethernet card for experimental network.\n\n\n\n\nCPU flags:\n\uf0c1\n\n\nfpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx lm constant_tsc pni monitor ds_cpl '''vmx''' est tm2 ssse3 cx16 xtpr lahf_lm\n\n\npc3000\n\uf0c1\n\n\nThis node type includes pc3000, bpc3000, pc3060, bpc3060, and pc3100.\n\n\nThere are:\n\n\n\n\n0 pc3000 nodes at \nISI\n\n\n32 bpc3000 nodes at \nUCB\n\n\n17 pc3060 nodes at \nISI\n\n\n32 bpc3060 nodes at \nUCB\n\n\n4 pc3100 nodes at \nISI\n\n\n\n\npc3000 and bpc3000 have the following features:\n\n\n\n\nDell PowerEdge 1850 Chassis.\n\n\nDual 3Ghz Intel Xeon processors.\n\n\n2 GB of RAM\n\n\nOne 36Gb 15k RPM SCSI drive (bpc machines may be configured with two).\n\n\n4 Intel Gigabit experimental network ports.\n\n\n1 Intel Gigabit experimental network port.\n\n\n\n\npc3060 and bpc3060 machines are the same as the pc3000/bpc3000 machines except that they have one more experimental network interface.\n\n\npc3100 machines have a total of 9 experimental interfaces and 1 control network interface.  There are only 4 of these type of machine.\n\n\nbvx2200\n\uf0c1\n\n\nThere are 31 bvx2200 nodes at \nUCB\n.\n\n\nbvx2200 has the following features:\n\n\n\n\nSun Microsystems Sun Fire X2100 M2 Chassis.\n\n\nDual-Core 1.8 Ghz AMD Opteron(tm) Processor 1210.\n\n\nOne 250Gb 7200 RPM SATA drive.\n\n\n1 Broadcom NetXtreme Gigabit experimental network port.\n\n\n2 Nvidia nForce MCP55 experimental network ports.\n\n\n2 Intel Gigabit experimental network ports.\n\n\n\n\nbpc2800\n\uf0c1\n\n\nThere are 30 bpc2800 at \nUCB\n.\n\n\nThe bpc2800 machines have the following features:\n\n\n\n\nSun Microsystems Sun Fire V60 Chassis\n\n\nOne Intel(R) Xeon(R) CPU dual core processor running at 2.8 GHz\n\n\n2 GB of RAM\n\n\nOne 36 GB SCSI Disk Drive\n\n\nTwo Dual port PCI-X Intel Gigabit Ethernet cards, 1 port for control network and 3 ports for experimental network\n\n\nOne Single port PCI-X Intel Gigabit Ethernet card for experimental network\n\n\n\n\nCPU flags\n\uf0c1\n\n\nfpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe pebs bts cid xtpr\n\n\nnetfpga2\n\uf0c1\n\n\nThere are 10 netfpga2 class nodes in the testbed. These are pc2133 class machines. Each of these nodes has a single \nNetFPGA card\n installed.", 
            "title": "DETERLab Node Types"
        }, 
        {
            "location": "/core/node-types/#dl380g3", 
            "text": "There are 120 dl380g3 class nodes available at  ISI .  Machine:  HP Proliant DL360 G8 Server  Each node has:   Dual  Intel(R) Xeon(R)  hexa-core processors running at 2.2 Ghz with 15MB cache  Intel VT-x support    24GB of RAM  One 1Tb SATA HP Proliant Disk Drive 7.2k rpm G8 (boot priority)  One 240Gb SATA HP Proliant Solid State Drive G8  Two experimental interfaces:  One Dual port PCIe Intel Ten Gigabit Ethernet card for experimental ports  One Quad port PCIe Intel Gigabit Ethernet card, presently with one port wired to the control network", 
            "title": "dl380g3"
        }, 
        {
            "location": "/core/node-types/#microcloud", 
            "text": "There are 128 MicroCloud nodes at  ISI .  Machine: High Density  SuperMicro MicroCloud  Chassis that fits 8 nodes in 3u of rack space.  Each node has:   One  Intel(R) Xeon(R) E3-1260L  quad-core processor running at 2.4 Ghz  Intel VT-x and VT-d support    16GB of RAM  One 250Gb SATA Western Digital RE4 Disk Drive  5 experimental interfaces  One Dual port PCIe Intel Gigabit Ethernet card for the control network and an experimental port  One Quad port PCIe Intel Gigabit Ethernet card for experimental network", 
            "title": "MicroCloud"
        }, 
        {
            "location": "/core/node-types/#pc2133", 
            "text": "This node type includes pc2133 and bpc2133:   There are 63 pc2133 nodes at  ISI .  There are 64 bpc2133 nodes at  UCB .   The pc2133 and bpc2133 machines have the following features:   Dell PowerEdge 860 Chasis  One Intel(R) Xeon(R) CPU X3210 quad core processor running at 2.13 Ghz  4GB of RAM  One 250Gb SATA Disk Drive  One Dual port PCI-X Intel Gigabit Ethernet card for the control network (only one port is used).  One Quad port PCIe Intel Gigabit Ethernet card for experimental network.   CPU flags: \uf0c1  fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx lm constant_tsc pni monitor ds_cpl '''vmx''' est tm2 ssse3 cx16 xtpr lahf_lm", 
            "title": "pc2133"
        }, 
        {
            "location": "/core/node-types/#pc3000", 
            "text": "This node type includes pc3000, bpc3000, pc3060, bpc3060, and pc3100.  There are:   0 pc3000 nodes at  ISI  32 bpc3000 nodes at  UCB  17 pc3060 nodes at  ISI  32 bpc3060 nodes at  UCB  4 pc3100 nodes at  ISI   pc3000 and bpc3000 have the following features:   Dell PowerEdge 1850 Chassis.  Dual 3Ghz Intel Xeon processors.  2 GB of RAM  One 36Gb 15k RPM SCSI drive (bpc machines may be configured with two).  4 Intel Gigabit experimental network ports.  1 Intel Gigabit experimental network port.   pc3060 and bpc3060 machines are the same as the pc3000/bpc3000 machines except that they have one more experimental network interface.  pc3100 machines have a total of 9 experimental interfaces and 1 control network interface.  There are only 4 of these type of machine.", 
            "title": "pc3000"
        }, 
        {
            "location": "/core/node-types/#bvx2200", 
            "text": "There are 31 bvx2200 nodes at  UCB .  bvx2200 has the following features:   Sun Microsystems Sun Fire X2100 M2 Chassis.  Dual-Core 1.8 Ghz AMD Opteron(tm) Processor 1210.  One 250Gb 7200 RPM SATA drive.  1 Broadcom NetXtreme Gigabit experimental network port.  2 Nvidia nForce MCP55 experimental network ports.  2 Intel Gigabit experimental network ports.", 
            "title": "bvx2200"
        }, 
        {
            "location": "/core/node-types/#bpc2800", 
            "text": "There are 30 bpc2800 at  UCB .  The bpc2800 machines have the following features:   Sun Microsystems Sun Fire V60 Chassis  One Intel(R) Xeon(R) CPU dual core processor running at 2.8 GHz  2 GB of RAM  One 36 GB SCSI Disk Drive  Two Dual port PCI-X Intel Gigabit Ethernet cards, 1 port for control network and 3 ports for experimental network  One Single port PCI-X Intel Gigabit Ethernet card for experimental network", 
            "title": "bpc2800"
        }, 
        {
            "location": "/core/node-types/#cpu-flags_1", 
            "text": "fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe pebs bts cid xtpr", 
            "title": "CPU flags"
        }, 
        {
            "location": "/core/node-types/#netfpga2", 
            "text": "There are 10 netfpga2 class nodes in the testbed. These are pc2133 class machines. Each of these nodes has a single  NetFPGA card  installed.", 
            "title": "netfpga2"
        }, 
        {
            "location": "/core/serial-console/", 
            "text": "Determining which nodes to connect to\n\uf0c1\n\n\nYou can determine the nodes allocated to your experiment by looking at the \nReserved Nodes\n table on the \nShow Experiment\n page on the web interface.  DETERLab nodes are generally named 'pcXXX' for nodes at \nISI\n and 'bpcXXX' for node at \nUCB\n.\n\n\nConnecting to the Serial Console\n\uf0c1\n\n\nEvery node on the testbed has serial console access enabled.  \n\n\nTo connect to a node's serial console, you must first log into \nusers.isi.deterlab.net\n and use the \nconsole\n command located in \n/usr/testbed/bin\n (which should be in every user's PATH by default).  \n\n\nTo connect to a particular node, type \nconsole pcXXX\n where \npcXXX\n is a node allocated to your experiment.\n\n\nTo disconnect from the console session, type \nCtrl\n and then \nexit\n.  The console command is actually a wrapper for telnet.\n\n\nSerial Console Logs\n\uf0c1\n\n\nAll console output from each node is saved in \n/var/log/tiplogs/pcXXX.run\n, where \npcXXX\n is a node allocated to your experiment.  \n\n\nThis run file is created when nodes are first allocated to an experiment, and the Unix permissions of the run files permit only members of the project to view them.  \n\n\n\n\nWarning\n\n\nWhen the experiment is swapped out, the run logs are removed.  In order to preserve them, you must make a copy before swapping out your experiment.\n\n\n\n\nConsole logs may be viewed through the web interface on the \nShow Experiment\n page by clicking on the icon in the Console column of the \nReserved Nodes\n table.\n\n\nAdditional information\n\uf0c1\n\n\n\n\nDell Serial Console Information", 
            "title": "Using the Serial Console"
        }, 
        {
            "location": "/core/serial-console/#determining-which-nodes-to-connect-to", 
            "text": "You can determine the nodes allocated to your experiment by looking at the  Reserved Nodes  table on the  Show Experiment  page on the web interface.  DETERLab nodes are generally named 'pcXXX' for nodes at  ISI  and 'bpcXXX' for node at  UCB .", 
            "title": "Determining which nodes to connect to"
        }, 
        {
            "location": "/core/serial-console/#connecting-to-the-serial-console", 
            "text": "Every node on the testbed has serial console access enabled.    To connect to a node's serial console, you must first log into  users.isi.deterlab.net  and use the  console  command located in  /usr/testbed/bin  (which should be in every user's PATH by default).    To connect to a particular node, type  console pcXXX  where  pcXXX  is a node allocated to your experiment.  To disconnect from the console session, type  Ctrl  and then  exit .  The console command is actually a wrapper for telnet.", 
            "title": "Connecting to the Serial Console"
        }, 
        {
            "location": "/core/serial-console/#serial-console-logs", 
            "text": "All console output from each node is saved in  /var/log/tiplogs/pcXXX.run , where  pcXXX  is a node allocated to your experiment.    This run file is created when nodes are first allocated to an experiment, and the Unix permissions of the run files permit only members of the project to view them.     Warning  When the experiment is swapped out, the run logs are removed.  In order to preserve them, you must make a copy before swapping out your experiment.   Console logs may be viewed through the web interface on the  Show Experiment  page by clicking on the icon in the Console column of the  Reserved Nodes  table.", 
            "title": "Serial Console Logs"
        }, 
        {
            "location": "/core/serial-console/#additional-information", 
            "text": "Dell Serial Console Information", 
            "title": "Additional information"
        }, 
        {
            "location": "/core/dell-serial-console/", 
            "text": "Most of the machines at DETERLab are Dell servers.  When connected via the serial console, some special key sequences are available.  Please refer to the \nnode types\n page for more information on the types of machines available through DETERLab.\n\n\nKEY MAPPING FOR CONSOLE REDIRECTION:\n\n    Use the \nESC\n0\n key sequence for \nF10\n\n    Use the \nESC\n@\n key sequence for \nF12\n\n\n    Use the \nESC\nCtrl\nM\n key sequence for \nCtrl\nM\n\n    Use the \nESC\nCtrl\nH\n key sequence for \nCtrl\nH\n\n    Use the \nESC\nCtrl\nI\n key sequence for \nCtrl\nI\n\n    Use the \nESC\nCtrl\nJ\n key sequence for \nCtrl\nJ\n\n\n    Use the \nESC\nX\nX\n key sequence for \nAlt\nx\n, where x is any letter\n    key, and X is the upper case of that key\n\n    Use the \nESC\nR\nESC\nr\nESC\nR\n key sequence for \nCtrl\nAlt\nDel", 
            "title": "Dell Serial Console"
        }, 
        {
            "location": "/core/os-images/", 
            "text": "Here is the list of currently supported DETERLab operating system images. If you have a DETERLab account, you can view the most updated information as well as statistics on each machine on the \nOSID page\n on the testbed.\n\n\nSupported OS Images as of 07/15/2015\n\uf0c1\n\n\n\n\n\n\n\n\nName\n\n\nOS\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFBSD10-STD\n\n\nFreeBSD\n\n\nFreeBSD 10.x Standard\n\n\n\n\n\n\nFBSD8-STD\n\n\nFreeBSD\n\n\nFreeBSD 8.x Standard\n\n\n\n\n\n\nFBSD9-64-STD\n\n\nFreeBSD\n\n\nFreeBSD 9.x Standard\n\n\n\n\n\n\nCentOS5\n\n\nLinux\n\n\nMore or less current version of CentOS 5\n\n\n\n\n\n\nCentOS6-64-STD\n\n\nLinux\n\n\nCentOS6 64-Bit image\n\n\n\n\n\n\nKALI1\n\n\nLinux\n\n\nKali Penetration Testing\n\n\n\n\n\n\nMetasploitable2\n\n\nLinux\n\n\nAn intentionally vulnerable system\n\n\n\n\n\n\nUbuntu1004-STD\n\n\nLinux\n\n\nUbuntu 10.04 LTS Standard Image\n\n\n\n\n\n\nUbuntu1204-64-STD\n\n\nLinux\n\n\nUbuntu 12.04 LTS 64 bit Standard Image\n\n\n\n\n\n\nUbuntu1404-32-STD\n\n\nLinux\n\n\nUbuntu 14.04 LTS 32 bit Standard Image\n\n\n\n\n\n\nUbuntu1404-64-STD\n\n\nLinux\n\n\nUbuntu 14.04 LTS 64 bit Standard Image\n\n\n\n\n\n\nWINXP-UPDATE\n\n\nWindows\n\n\nWindows XP with SP3 and patches\n\n\n\n\n\n\n\n\nUpdates for Custom Images\n\uf0c1\n\n\nUpdating Linux images made before Jan 25, 2013\n\uf0c1\n\n\nWe made a change to make mounting NFS home directories more robust.  You may update your custom images by running:\n\n\nsudo curl --output /usr/local/etc/emulab/liblocsetup.pm boss.isi.deterlab.net/downloads/client-update/linux-liblocsetup.pm\nsudo chmod a+rx /usr/local/etc/emulab/liblocsetup.pm\n\n\n\n\nand taking a snapshot.  \n\n\nOn CentOS-6-64-STD you must to install Time::HiRes by running:\n\n\nsudo yum install perl-Time-HiRes", 
            "title": "OS Images"
        }, 
        {
            "location": "/core/os-images/#supported-os-images-as-of-07152015", 
            "text": "Name  OS  Description      FBSD10-STD  FreeBSD  FreeBSD 10.x Standard    FBSD8-STD  FreeBSD  FreeBSD 8.x Standard    FBSD9-64-STD  FreeBSD  FreeBSD 9.x Standard    CentOS5  Linux  More or less current version of CentOS 5    CentOS6-64-STD  Linux  CentOS6 64-Bit image    KALI1  Linux  Kali Penetration Testing    Metasploitable2  Linux  An intentionally vulnerable system    Ubuntu1004-STD  Linux  Ubuntu 10.04 LTS Standard Image    Ubuntu1204-64-STD  Linux  Ubuntu 12.04 LTS 64 bit Standard Image    Ubuntu1404-32-STD  Linux  Ubuntu 14.04 LTS 32 bit Standard Image    Ubuntu1404-64-STD  Linux  Ubuntu 14.04 LTS 64 bit Standard Image    WINXP-UPDATE  Windows  Windows XP with SP3 and patches", 
            "title": "Supported OS Images as of 07/15/2015"
        }, 
        {
            "location": "/core/os-images/#updates-for-custom-images", 
            "text": "Updating Linux images made before Jan 25, 2013 \uf0c1  We made a change to make mounting NFS home directories more robust.  You may update your custom images by running:  sudo curl --output /usr/local/etc/emulab/liblocsetup.pm boss.isi.deterlab.net/downloads/client-update/linux-liblocsetup.pm\nsudo chmod a+rx /usr/local/etc/emulab/liblocsetup.pm  and taking a snapshot.    On CentOS-6-64-STD you must to install Time::HiRes by running:  sudo yum install perl-Time-HiRes", 
            "title": "Updates for Custom Images"
        }, 
        {
            "location": "/core/custom-images/", 
            "text": "What is an Operating System ID (OSID) versus an Image ID?\n\uf0c1\n\n\nIn order to make the best use of custom operating system images, it is important to understand the difference between these two concepts.\n\n\n\n\n\n\nAn \nImage ID\n is a descriptor for a disk image.  This can be an image of an entire disk, a partition of a disk, or a set of partitions of a disk.  By supporting multiple partitions, we can technically support different operating systems within the same disk image.  These are referred to as \ncombo images\n.  The Image ID points to a real file that is stored in the directory \n/proj/YourProjectName/images\n.  Other critical information is associated with the Image ID, such as what node types are supported by the images and what operating systems are on each partition.  You can view the Image IDs on the \nList ImageIDs page\n, which is in the \nExperimentation\n drop down menu on the testbed web interface.\n\n\n\n\n\n\nAn \nOSID\n describes an operating system which resides on a partition of a disk image.  Every Image ID will have at least one OSID associated with it.  In typical testbed usage, the Image ID and OSID will be the same since we usually do not put multiple operating systems on a single image.  You can view the OSIDs on the \nList OSIDs page\n, which is in the \nExperimentation\n drop down menu on the testbed web interface.\n\n\n\n\n\n\nStandard Testbed Images\n\uf0c1\n\n\nWe provide a number of supported testbed images here at DETERLab.  These images can be viewed by looking at the \nOperating System ID list\n.  \n\n\nMost new operating system images that we support are whole disk images, which is different from the more traditional scheme of using partition 1 for FreeBSD and partition 2 for Linux.  To view what nodes a particular operating system image runs on and what sort of partition scheme it uses, please refer to the \nImage ID list page\n.\n\n\nThe supported testbed images are listed in the \nOperating System Images\n documentation page. \n\n\nCustom OS Images\n\uf0c1\n\n\nIf your set of operating system customizations cannot be easily contained within an RPM/TAR (or multiple RPM/TARs), then you can create your own custom OS image. DETERLab allows you to create your own disk images and load them on your experimental nodes automatically when your experiment is created or swapped in. \n\n\nOnce you have created a custom disk image (and the associated \n ImageID/OSID descriptor\n for it, you can use that OSID in your NS file. When your experiment is swapped in, the testbed system will arrange for your disks to be loaded in parallel using a locally written multicast disk loading protocol. \n\n\n\n\nNote\n\n\nExperience has shown that it is much faster to load a disk image on 10 nodes at once, then it is to load a bunch of RPMS or tarballs on each node as it boots. So while it may seem like overkill to create your own disk image, we can assure you it is not.\n\n\n\n\nThe most common approach is to use the \nNew Image Descriptor\n form to create a disk image that contains a customized version of a standard Linux or the FreeBSD image. All you need to do is enter the node name in the form, and the testbed system will create the image for you automatically, notifying you via email when it is finished. You can then use that image in subsequent experiments by specifying the descriptor name in your NS file with the \ntb-set-node-os\n command. When the experiment is configured, the proper image will be loaded on each node automatically by the system.\n\n\nCreating Your Custom Image\n\uf0c1\n\n\nA typical approach to creating your own disk image is using one of the default images as a base or template. To do this:\n\n\n\n\n\n\nCreate a single-node Linux or FreeBSD experiment. In your NS file, use the appropriate \ntb-set-node-os\n command, as in the following example:\n\n\ntb-set-node-os $nodeA FBSD7-STD\ntb-set-node-os $nodeA Ubuntu1004-STD\n\n\n\n\n\n\n\nAfter your experiment has swapped in (you have received the email saying it is running), log into the node and load all of the software packages that you wish to load. If you want to install the latest version of the Linux kernel on one of our standard disk images, or on your own custom Linux image, be sure to arrange for any programs that need to be started at boot time. It is a good idea to reboot the node and make sure that everything is running as expected when it comes up.\n\n\n\n\nIf you are creating a Windows-based image, you \nmust\n \"prepare\" the node. The final thing to do before grabbing the image is to login on the \nconsole\n, drop to single user mode, and run the \nprepare\n script. This is described in detail in the \ncustom Windows images\n section of the Windows page.\n\n\nNote the physical (\npcXXX\n) name of the machine used!\n\n\nCreate an image descriptor and image using the \nNew Image Descriptor\n form.\n\n\nWait for the email saying the image creation is done.\n\n\nNow you can create a second single-node experiment to test your new image. In your NS file, use \ntb-set-node-os\n to select the OSID that you just created. Be sure to remove any RPM or tarball directives. Submit that NS file and wait for the email notification. Then log into the new node and check to make sure everything is running normally.\n\n\n\n\nIf everything is going well, terminate both of these single-node experiments. If not, release the experiment created in the previous step, and then go back and fix the original node (\npcXXX\n above). Recreate the image as needed:\n\n\ncreate_image -p \nproj\n \nimageid\n \nnode\n\n\n\n\n\n\n\n\nOnce your image is working properly, you can use it in any NS file by using the \ntb-set-node-os\n command. If you ever want to reload a node in your experiment, either with one of your images or with one of the default images, you can use the \nos_load\n command. Log into \nusers\n and run:\n\n\nos_load -p \nproj\n -i \nimageid\n \nnode\n\n\n\n\nThis program will run in the foreground, waiting until the image has been loaded. At that point you should log in and make sure everything is working okay. You might want to watch the console line as well (see the \nNode Console section\n). If you want to load the default image, then simply run:\n\n\nos_load \nnode\n\n\n\n\n\n\n\n\nHints When Making New OS Images\n\uf0c1\n\n\n\n\nPlease, \nnever\n try to create an image from a node or type that begins with the letter \nb\n, e.g. \nbpc183\n or  \nbpc2133\n. These nodes are located in Berkeley which is physically located 400 miles away from the \nboss\n and \nusers\n servers.\n\n\n\n\nAfter you have created an image, load it back and watch what happens through the serial port.\n\n\nConsider creating a \ntwo\n node experiment, one to create the image and the other to load it back.\n\n\nThere is a command called \nos_load\n available on the \nusers\n server:\n\n\nusers% which os_load\n/usr/testbed/bin/os_load\nusers% os_load -h\noption -h not recognized\nos_load [options] node [node ...]\nos_load [options] -e pid,eid\nwhere:\n    -i    Specify image name; otherwise load default image\n    -p    Specify project for finding image name (-i)\n    -s    Do *not* wait for nodes to finish reloading\n    -m    Specify internal image id (instead of -i and -p)\n    -r    Do *not* reboot nodes; do that yourself\n    -e    Reboot all nodes in an experiment\n  node    Node to reboot (pcXXX)\n\n\n\nWhile the second node is reloading, watch its progress in real time using the console command from the \nusers\n server, ie:\n\n\nusers% console pc193\n\n\n\n\n\n\n\nIf you think you've got a good image, but it flounders while coming up, create another experiment with an NS directive that says \n\"Even if you think the node has not booted, let my experiment swap in anyway.\"\n This may allow you to log in through the console and figure out what went wrong. An example of such a directive is:\n\n\ntb-set-node-failure-action $nodeA \"nonfatal\"\n\n\n\n\n\n\n\nCreate whole disk images on a smaller machine rather than a single partition image.\n\n\n\n\n\n\nUpdating your Custom Image\n\uf0c1\n\n\nOnce you have your image, it is easy to update it later by taking a new snapshot from a node running your image. Assuming you have swapped in an experiment with a node running your image and you have made changes to that node, use the DETERLab web interface to navigate to the descriptor page for your image:\n\n\n\n\nUse the \nExperimentation\n drop down menu, and choose \nList ImageIDs\n to see the entire list of Images you may access.\n\n\nFind your custom image and click on it.\n\n\nIn the \nMore Options\n menu, click on \nSnapshot Node ...\n\n\nFill in the name of the node that is running your image, and click on \nGo\n.\n\n\nAs in the above instructions, wait for the email saying your image has been updated before you try and use the image.", 
            "title": "Creating Custom Images"
        }, 
        {
            "location": "/core/custom-images/#what-is-an-operating-system-id-osid-versus-an-image-id", 
            "text": "In order to make the best use of custom operating system images, it is important to understand the difference between these two concepts.    An  Image ID  is a descriptor for a disk image.  This can be an image of an entire disk, a partition of a disk, or a set of partitions of a disk.  By supporting multiple partitions, we can technically support different operating systems within the same disk image.  These are referred to as  combo images .  The Image ID points to a real file that is stored in the directory  /proj/YourProjectName/images .  Other critical information is associated with the Image ID, such as what node types are supported by the images and what operating systems are on each partition.  You can view the Image IDs on the  List ImageIDs page , which is in the  Experimentation  drop down menu on the testbed web interface.    An  OSID  describes an operating system which resides on a partition of a disk image.  Every Image ID will have at least one OSID associated with it.  In typical testbed usage, the Image ID and OSID will be the same since we usually do not put multiple operating systems on a single image.  You can view the OSIDs on the  List OSIDs page , which is in the  Experimentation  drop down menu on the testbed web interface.", 
            "title": "What is an Operating System ID (OSID) versus an Image ID?"
        }, 
        {
            "location": "/core/custom-images/#standard-testbed-images", 
            "text": "We provide a number of supported testbed images here at DETERLab.  These images can be viewed by looking at the  Operating System ID list .    Most new operating system images that we support are whole disk images, which is different from the more traditional scheme of using partition 1 for FreeBSD and partition 2 for Linux.  To view what nodes a particular operating system image runs on and what sort of partition scheme it uses, please refer to the  Image ID list page .  The supported testbed images are listed in the  Operating System Images  documentation page.", 
            "title": "Standard Testbed Images"
        }, 
        {
            "location": "/core/custom-images/#creating-your-custom-image", 
            "text": "A typical approach to creating your own disk image is using one of the default images as a base or template. To do this:    Create a single-node Linux or FreeBSD experiment. In your NS file, use the appropriate  tb-set-node-os  command, as in the following example:  tb-set-node-os $nodeA FBSD7-STD\ntb-set-node-os $nodeA Ubuntu1004-STD    After your experiment has swapped in (you have received the email saying it is running), log into the node and load all of the software packages that you wish to load. If you want to install the latest version of the Linux kernel on one of our standard disk images, or on your own custom Linux image, be sure to arrange for any programs that need to be started at boot time. It is a good idea to reboot the node and make sure that everything is running as expected when it comes up.   If you are creating a Windows-based image, you  must  \"prepare\" the node. The final thing to do before grabbing the image is to login on the  console , drop to single user mode, and run the  prepare  script. This is described in detail in the  custom Windows images  section of the Windows page.  Note the physical ( pcXXX ) name of the machine used!  Create an image descriptor and image using the  New Image Descriptor  form.  Wait for the email saying the image creation is done.  Now you can create a second single-node experiment to test your new image. In your NS file, use  tb-set-node-os  to select the OSID that you just created. Be sure to remove any RPM or tarball directives. Submit that NS file and wait for the email notification. Then log into the new node and check to make sure everything is running normally.   If everything is going well, terminate both of these single-node experiments. If not, release the experiment created in the previous step, and then go back and fix the original node ( pcXXX  above). Recreate the image as needed:  create_image -p  proj   imageid   node     Once your image is working properly, you can use it in any NS file by using the  tb-set-node-os  command. If you ever want to reload a node in your experiment, either with one of your images or with one of the default images, you can use the  os_load  command. Log into  users  and run:  os_load -p  proj  -i  imageid   node   This program will run in the foreground, waiting until the image has been loaded. At that point you should log in and make sure everything is working okay. You might want to watch the console line as well (see the  Node Console section ). If you want to load the default image, then simply run:  os_load  node", 
            "title": "Creating Your Custom Image"
        }, 
        {
            "location": "/core/custom-images/#hints-when-making-new-os-images", 
            "text": "Please,  never  try to create an image from a node or type that begins with the letter  b , e.g.  bpc183  or   bpc2133 . These nodes are located in Berkeley which is physically located 400 miles away from the  boss  and  users  servers.   After you have created an image, load it back and watch what happens through the serial port.  Consider creating a  two  node experiment, one to create the image and the other to load it back.  There is a command called  os_load  available on the  users  server:  users% which os_load\n/usr/testbed/bin/os_load\nusers% os_load -h\noption -h not recognized\nos_load [options] node [node ...]\nos_load [options] -e pid,eid\nwhere:\n    -i    Specify image name; otherwise load default image\n    -p    Specify project for finding image name (-i)\n    -s    Do *not* wait for nodes to finish reloading\n    -m    Specify internal image id (instead of -i and -p)\n    -r    Do *not* reboot nodes; do that yourself\n    -e    Reboot all nodes in an experiment\n  node    Node to reboot (pcXXX)  While the second node is reloading, watch its progress in real time using the console command from the  users  server, ie:  users% console pc193    If you think you've got a good image, but it flounders while coming up, create another experiment with an NS directive that says  \"Even if you think the node has not booted, let my experiment swap in anyway.\"  This may allow you to log in through the console and figure out what went wrong. An example of such a directive is:  tb-set-node-failure-action $nodeA \"nonfatal\"    Create whole disk images on a smaller machine rather than a single partition image.", 
            "title": "Hints When Making New OS Images"
        }, 
        {
            "location": "/core/custom-images/#updating-your-custom-image", 
            "text": "Once you have your image, it is easy to update it later by taking a new snapshot from a node running your image. Assuming you have swapped in an experiment with a node running your image and you have made changes to that node, use the DETERLab web interface to navigate to the descriptor page for your image:   Use the  Experimentation  drop down menu, and choose  List ImageIDs  to see the entire list of Images you may access.  Find your custom image and click on it.  In the  More Options  menu, click on  Snapshot Node ...  Fill in the name of the node that is running your image, and click on  Go .  As in the above instructions, wait for the email saying your image has been updated before you try and use the image.", 
            "title": "Updating your Custom Image"
        }, 
        {
            "location": "/core/windows/", 
            "text": "Microsoft \nWindows XP\n is supported as one of the operating system types for experiment nodes in DETER.\n\n\nAs much as possible, we have left Windows XP \"stock\". Some Windows services are shut down: Messenger, SSDP Discovery Service, Universal Plug and Play Device Host, and Remote Registry. Other setting changes are described under \nNetwork config\n and \nRouting\n below.\n\n\nBefore booting the node at swap-in time, DETER loads a \nfresh image of Windows XP\n onto the experiment nodes in parallel, using our \nfrisbee\n service. DETER software automatically configures each Windows XP node, providing the expected experiment user environment including: user accounts and DETER SSH keys; remote home, project, and shared directories; and network connections.\n\n\nThe \nCygwin GNU\n environment is provided, including Bash and TCSH shells, the C/C++, Perl and Python programming languages, and several editors including Emacs, vim, nano and ed. \nCygwin\n handles both Unix and Windows-style command paths, as described \nbelow\n.\n\n\nThe DETERLab web interface manages a separate \nWindows password\n in the user profile, as well as making \nlogin connections\n to the experiment nodes. Remote Desktop Protocol service supports Windows Desktop logins from the user's workstation screen to the experiment node. SSH and Serial Console command-line connections are also supported.\n\n\nWindows XP installations are more hardware dependent than Linux or FreeBSD. At present, this Windows XP image runs on only \npc3000 class\n machines.\n\n\nDifferences from FreeBSD and Linux\n\uf0c1\n\n\nThe biggest difference of course, is that this is \nWindows\n, with Cygwin layered on top, and DETERLab management services added. In particular, this is Windows XP (NT 5.1), with various levels of service packs and updates (see \nbelow\n.)\n\n\nFile Sharing\n\uf0c1\n\n\nThe second-biggest difference is that shared directories are provided not by the NFS (Network File System) protocol, but instead by the \nSMB\n (Server Message Block) protocol, otherwise known as Windows File Sharing.\n\n\nThe \"Client for Microsoft Networks\" software contacts the SMB server, in this case \nSamba\n running on the file server known as \nFs\n (an alias for \nUsers\n.) The SMB protocol authenticates using a plain-text user name and password, encrypted as they go across the network. (These Windows Shares are then accessed by UNC paths under Cygwin mounts, [#SMB_mounts described below].)\n\n\nIn Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with \nbackslashes\n, e.g. \n\\\\fs\\share\n or \n\\\\fs\\\nusername\n. User and project shares are marked \"not browsable\", so just \n\\\\fs\n shows only \nshare\n.\n\n\nIf you want to serve files from one of your experiment nodes to others, see the section on [#netbt_command  The netbt command ].\n\n\nWindows Passwords\n\uf0c1\n\n\nA separate \nWindows password\n is kept for use only with experiment nodes running Windows. It is presented behind-the-scenes to \nrdesktop\n for RDP logins by our Web interface under Unix, and for the Samba mount of shared directories like your home directory under an SSH login, so you don't have to type it in those cases. You \nwill\n have to type it each time if you use the \nMicrosoft RDC (Remote Desktop Connector) client program\n from a Windows machine.\n\n\nThe default Windows password is randomly generated. It's easy to change it to something easier to remember.\n\n\nTo see or edit your Windows password, log in to DETERLab, and click \nManage User Profile\n and then \nEdit Profile\n under \nUser Options\n. You will see \nWindows Password\n fields in addition to the regular DETERLab \nPassword\n fields.\n\n\nWhen you change your Windows password, you will also have to re-type it as a check. The new Windows password should propagate to the Samba server on Fs instantly, so you can swap in an experiment and log in to its Windows nodes with the new password.\n\n\nIf you have already swapped-in experiment nodes and changed your Windows password, the account information including passwords will be updated at the next DETERLab watchdog daemon \nisalive\n interval. This should be in 3 to 6 minutes.\n\n\nExperiment setup for Windows nodes\n\uf0c1\n\n\nAll you have to do is put a line specifying a WINXP OS image in your experiment NS file, like this:\n\n\n    tb-set-node-os $node WINXP-UPDATE\n\n\n\n\nThe Windows XP images are not specific to a particular hardware type. (See the \nChange Log\n for more information.) You may explicitly specify the hardware type to run on if you wish, for example:\n\n\n    tb-set-hardware $node pc3000\n\n\n\n\nSince the bandwidth of the connection between the ISI and Berkeley portions of the testbed is constrained, it is best to run Windows nodes only on the ISI side of the testbed. This can be accomplished by creating a custom hardware type with\n\n\n    tb-make-soft-vtype my_custom_node_type {pc3060 pc3000 pc2133 pc2133x}\n\n\n\n\nand then specifying that your Windows nodes must only swap-in on this custom type using \n\n\n    tb-set-hardware $node my_custom_node_type\n\n\n\n\nSee the \nnote below\n on using the \ntb-set-node-failure-action\n command for experiments with a large number of Windows nodes. This can save a swap-in with a large number of Windows nodes, or prevent a single node boot failure on a swapmod from swapping-out the whole experiment.\n\n\nIf you use these commands: \ntb-set-node-startcmd\n, \ntb-set-node-tarfiles\n, or \ntb-set-node-rpms\n you should read the sections on \nPermissions\n and \nWindows GUI programs\n below.\n\n\nThe only available Windows image currently is:\n\n\n\n\nWINXP-UPDATE\n - The most recent Windows XP-SP3+. It is updated periodically from Windows Update, typically after a Microsoft \"Patch Tuesday\", the second Tuesday of each month. All critical and security fixes are installed, up through the date we pull the image file. (See the date created field on the individual WINXP \nImage IDs\n).\n\n\n\n\n\n\nNote\n\n\nThe Windows Firewall is disabled by default (as it will inform you repeatedly!)\n\n\n\n\nNetwork config\n\uf0c1\n\n\nSome default Windows networking features are disabled. \nNetBT (!NetBios over TCP)\n (\nNetbiosOptions=2 \n) and \nDNS auto-registration\n (\nDisableDynamicUpdate=1\n) are disabled to allow network \nidle detection\n by the slothd service. \nTCP/IP address autoconfiguration\n is disabled (\nIPAutoconfigurationEnabled=0\n) so that un-switched interfaces like the sixth NICs on the pc3000's don't get bogus Microsoft class B network 169.254.0.0 addresses assigned.\n\n\nThe Windows \nipconfig /all\n command only shows the configuration information for the enabled network interfaces. There will always be one enabled control net interface on the \n192.168.0.0/22\n network. The others are disabled if not used in your experiment. (See file \n/var/emulab/boot/ipconfig-cache\n for a full listing from boot time, including the interfaces that were later disabled.)\n\n\nIf you specified links or LANs in your experiment network topology, other interfaces will be enabled, with an IP address, subnet mask, and gateway that you can specify in the NS file. Notice that the Windows names of the interfaces start with \nLocal Area Connection\n and have a number appended. You can't count on what this number is, since it depends on the order the NIC's are probed as Windows boots.\n\n\n\n\nNote\n\n\nOften, we have seen \nipconfig\n report an IP address and mask of \n0.0.0.0\n, while the TCP/IP properties dialog boxes and the \nnetsh\n command show the proper values. Our startup scripts disable and re-enable the network interface in an attempt to reset this. Sometimes it doesn't work, and another reboot is done in an attempt to get the network up.\n\n\n\n\nRouting\n\uf0c1\n\n\nFull-blown router nodes cannot run Windows, i.e. \nrtproto Session\n is not supported. However, basic routing between connected network components of your experiment topology works. The Windows command to see the routing tables is \nroute print\n. The \nIPEnableRouter=1\n registry key is set on multi-homed hosts in the experiment network, before they are rebooted to change the hostname.\n\n\nrtproto Static\n is supported in all recent WINXP images, but not in WINXP-02-16 (2005) or before.\n\n\nrtproto Static-old\n or \nrtproto Manual\n will work in any image.\n\n\nThere is more information on routing in the \nRouting section of the Core Guide\n.\n\n\nWindows nodes boot twice\n\uf0c1\n\n\nNotice that Windows reboots an extra time after being loaded onto a node during swap-in. It must reboot after changing the node name to set up the network stack properly. Be patient, Windows XP doesn't boot quickly.\n\n\nWith \nhardware-independent\n, (\nsysprep'ed\n) images, the first boot is actually running \nMini-Setup\n as well, setting up device drivers and so on.\n\n\nIt's best not to log in to the nodes until the experiment is fully swapped-in. (You may be able to log in briefly between the first two reboots; if you see the wrong \npcXXX\n name, you'll know that a reboot is imminent.) You can know that the swap-in process is finished by any of these methods:\n\n\n\n\nWaiting until you get the \"experiment swapped in\" email from DETERLab.\n\n\nChecking the node status on the experiment status page in DETERLab. (You must refresh the page to see node status change.)\n\n\nWatching the realtime swap-in log to monitor its progress.\n\n\n\n\n\n\nNote\n\n\nSometimes Windows XP fails to do the second reboot. One reason is transient race conditions in the Windows startup, for example in the network stack when there are multiple network interface devices being initialized at the same time. We make a strong effort to recover from this, but if the recovery code fails, by default it results in a swap-in or swapmod failure.\n\n\n\n\nAt boot time, the startup service on Windows XP runs the \n/usr/local/etc/emulab/rc/rc.bootsetup\n script, logging output to \n/var/log/bootsetup.log\n. If you're having swap-in problems and \nrc.bootsetup\n doesn't finish sending \nISUP\n to DETERLab within 10 minutes, the node will be rebooted. After a couple of reboot cycles without a \nISUP\n, DETERLab gives up on the node.\n\n\nYou can cause these boot-time problems to be nonfatal by adding this line to your \nns file\n \nfor each Windows node\n:\n\n\n tb-set-node-failure-action $node \nnonfatal\n\n\n\n\n\n(where \n$node\n is replaced with the node variable, of course.)\n\n\nDETERLab will still complain if it doesn't get the ISUP signal at the end of rc.bootsetup, but the swap-in or swapmod will proceed and allow you to figure out what's happening. Then you will probably have to manually reboot the failed Windows node to make it available to your experiment.\n\n\nIf you try to login to a node after swap-in to diagnose the problem and your Windows password isn't honored, use this command on Ops to remotely reboot the node:\n\n\n node_reboot pcxxx\n\n\n\n\nIf you are able to log in but your remote home directory isn't mounted, this is another symptom of a partial set-up. You have the additional option of executing this command on the node itself:\n\n\n /sbin/reboot\n\n\n\n\nThis gives Windows another chance to get it right.\n\n\nLogin connections to Windows\n\uf0c1\n\n\nYou can manually start up the SSH or RDP client programs to connect and log in to nodes in your experiment, or use the \nconsole\n command on Ops. You will have to type your \nWindows Password\n when logging in, except for SSH when you have ssh-agent keys loaded.\n\n\nOr you can set up your browser to automatically connect in one click from the DETERLab web interface and pop up a connection window. Once you start swapping in an experiment, the \nExperiment Information\n page contains a table of the physical node ID and logical node name, status, and connection buttons. The captions at the top of the button columns link to pages explaining how to set up up mime-types in your browser to make the buttons work, from FreeBSD, Linux, and Windows workstations:\n\n\n\n\nSSH\n \n(setup)\n - The \nSSH\n connection button gives a Bash or TCSH shell, as usual. Your DETERLab SSH keys are installed on the node in a \n/sshkeys\n subdirectory.\n\n\nConsole\n - The \nserial console\n is supported for Cygwin shell logins using the \nagetty\n and \nsysvinit\n packages. This is the only way in when network connections are closed down! You can also monitor the Frisbee loading and booting of the Windows image on the console.\n\n\nRDP\n - The \nRDP\n button starts up a Remote Desktop Protocol connection, giving a Windows Desktop login from the user's workstation screen to the experiment node.\n\n\nThe \nrdesktop\n client software is used from Linux and Unix client workstations.\n\n\nA Microsoft \nRDC\n (Remote Desktop Connector) client program is included in Windows XP, and may be installed onto other versions of Windows as well. It has the feature that you can make it full-screen without (too much) confusion, since it hangs a little tab at the top of the screen to switch back. Unfortunately, we have no way to present your DETERLab Windows password to RDC, so you'll have to type it on each login.\n\n\n\n\n\n\nNote\n\n\nIf you import dot-files into DETERLab that replace the system execution search path rather than add to it, you will have a problem running Windows system commands in shells. Fix this by adding \n/cygdrive/c/WINDOWS/system32\n and \n/cygdrive/c/WINDOWS\n to your \n$PATH\n in \n~/.cshrc\n and either \n~/.bash_profile\n or \n~/.profile\n. Don't worry about your home directory dot-files being shared among Windows, FreeBSD, and Linux nodes; non-existent directories in the \n$PATH\n are ignored by shells.\n\n\n\n\nWhen new DETERLab user accounts are created, the default CSH and Bash dotfiles are copied from the FreeBSD \n/usr/share/skel\n. They replace the whole $PATH rather than add to it. Then we append a DETERLab-specific part that takes care of the path, conditionally adding the Windows directories on Cygwin.\n\n\n\n\nNote\n\n\nThe Windows \nping\n program has completely different option arguments from the Linux and FreeBSD ones, and they differ widely from each other. There is a ping package in Cygwin that is a port of the 4.3bsd ping. Its options are close to a common subset of the Linux and FreeBSD options, so it will be included in future WINXP images:\n\n\n\n\n ping [ -dfqrv ] host [ packetsize [count [ preload]]]\n\n\n\n\nYou can load it yourself now using \nCygwin Setup\n.\n\n\n\n\nNote\n\n\nThere are no Cygwin ports of some other useful networking commands, such as \ntraceroute\n and \nifconfig -a\n. The Windows system equivalents are \ntracert\n and \nipconfig /all\n.\n\n\n\n\nRDP details\n\uf0c1\n\n\nHere are some fine points and hints for RDP logins to remote Windows desktops:\n\n\n\n\n\n\nMicrosoft allows only \none desktop login at a time\n to \nWindows XP\n, although this is the same Citrix Hydra technology that supports many concurrent logins to Terminal Server or Server 2003. \n\n\nThe \nFast User Switching\n option to XP is turned on, so a second RDP connection disconnects a previous one rather than killing it. Similarly, just closing your RDP client window disconnects your Windows Login session rather than killing it. You can reconnect later on without losing anything. SSH doesn't count as a desktop, so you can SSH in and use this command: \nqwinsta\n (Query WINdows STAtion) to show existing winstation sessions and their session ID's, and this one to reset (kill) a session by ID: \nrwinsta\n.\n\n\n\n\n\n\nWe rename \nMy Computer\n to show the PCxxx physical node name, but it doesn't appear on the \nWindows XP\n desktop by default. The XP user interface incorporates \"My Computer\" into the upper-right quadrant of the \"Start\" menu by default, and removes it from the desktop. \n\n\nYou can go back to the \"classic\" user interface of Windows 2000, including showing \"My Computer\". Right-click on the background of the Taskbar which contains the \"Start\" button at the left, and choose \"Properties\". Select the \"Start Menu\" tab, click the \"Classic Start menu\" radio-button, and click \"OK\".\n\n\nAlternatively, you can force \"My Computer\" to appear on your XP desktop by right-clicking on the desktop background and choosing \"Properties\". Select the \"Desktop\" tab and click \"Customize Desktop...\" to get the \"Desktop Items\" dialog. Turn on the \"My Computer\" checkbox, then click \"OK\" twice.\n\n\n\n\n\n\nThere are several \nDesktop icons\n (i.e. \"shortcuts\") installed by default in the XP images: Computer Management, Bash and TCSH shells, and \nNtEmacs\n. You will notice two flavors of Bash and TCSH icons on the desktop, labeled \nrxvt\n and \nCygwin\n.\n\n\n\n\n\n\nThe \nrxvt\n shells\n run in windows with \nX\n-like cut-and-paste mouse clicks:\n\n\n\n\nLeft-click\n starts a selection,\n\n\nRight-click\n extends it, and\n\n\nmiddle-click\n pastes.  These are the ones to use if you're connecting from an X workstation.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe default colors used in Bash and rxvt don't work well in 4-bit color mode under RDP. Make sure you update your rdp-mime.pl to get the rdesktop \n-a 16\n argument for 16-bit color. Or, you can over-ride the rxvt defaults by putting lines in your \n~/.Xdefaults\n file like this: \n\n\nrxvt*background: steelblue\n\n\n\n\n\n\n\n\n\nThe \nCygwin\n shells\n run in a Windows Terminal window, just as the Windows cmd.exe does. These are the ones to use if you're connecting from a Windows workstation.\n\n\nQuick-edit mode\n is on by default, so you can cut-and-paste freely between your local workstation desktop and your remote RDP desktops. In a Windows Terminal window on your RDP remote desktop, the quick-edit cut-and-paste mouse clicks are:\n\n\n\n\nLeft-drag\n the mouse to \nmark\n a rectangle of text, highlighting it.\n\n\nType \nEnter\n or \nright-click\n the mouse when text is highlighted\n, to \ncopy\n the selected text to the clipboard. (\nEscape\n \ncancels\n the selection without copying it.)\n\n\nRight-click the mouse with nothing selected\n to \npaste\n the contents of the clipboard.\n\n\n\n\n\n\n\n\nOn the \nfirst login by a user\n, Windows creates the user's \nWindows profile directory\n under \nC:\\Documents and Settings\n, and creates the \nregistry key\n (folder) for persistent settings for that user.\n\n\nWe arrange that early in the user's login process, a user \nHOME\n environment variable value is set in the user's registry. Otherwise Emacs wouldn't know how to find your \n.emacs\n setup file in your remotely mounted home directory.\n\n\nUser \"root\" is special, and has a local home directory under \n/home\n. \n/home\n is a Cygwin symbolic link to \nC:\\Documents and Settings\n.\n\n\n\n\n\n\nThe \nWindows XP\n Start menu has no \nShutdown\n button under RDP. Instead, it is labeled \nDisconnect\n and only closes the RDP client window, leaving the login session and the node running. If you simply close the window, or the RDP client network connection is lost, you are also disconnected rather than logged out. When you reconnect, it comes right back, just as it was.\n\n\nTo restart the computer, run \n/sbin/reboot\n, or use the \"Shut Down\" menu of \nTask Manager\n. One way to start Task Manager is to right-click on the background of the Taskbar at the bottom of the screen and select \"Task Manager\".\n\n\n\n\n\n\nThe \nnetbt\n command\n\uf0c1\n\n\nThe \nNetBT\n (Netbios over TCP) protocol is used to announce shared directories (folders) from one Windows machine to others. (See the Name and Session services in \nhttp://en.wikipedia.org/wiki/Netbios\n.)\n\n\nThe \nSMB\n (Server Message Block) protocol is used to actually serve files. (See \nhttp://en.wikipedia.org/wiki/Server_Message_Block\n.)\n\n\nIn DETERLab, we normally disable NetBT on experiment nodes, because it chatters and messes up slothd network idle detection, and is not needed for the usual SMB mounts of \n/users\n, \n/proj\n, and \n/share\n dirs, which are served from a Samba service on \nfs\n.\n\n\nHowever, NetBT \ndoes\n have to be enabled on the experiment nodes if you want to make Windows file shares between them. The \nnetbt\n script sets the registry keys on the Windows network interface objects. Run it on the server nodes (the ones containing directories which you want to share) and reboot them afterwards to activate. There is an optional \n-r\n argument to reboot the node.\n\n\n    Usage: netbt [-r] off|on\n\n\n\n\nIf you use \nnetbt\n to turn on NetBT, it persists across reboots.\n\n\nNo reboot is necessary if you use Network Connections in the Control Panel to turn on NetBT. It takes effect immediately, but is turned off at reboot unless you do \nnetbt on\n afterward as well.\n\n\n\n\nRight-click Local Area Connection (or the name of another connection, if appropriate), click Properties, click Internet Protocol (TCP/IP), and then click the Properties button.\n\n\nOn the Internet Protocol (TCP/IP) Properties page, click the Advanced button, and click the WINS tab.\n\n\nSelect Enable or Disable NetBIOS over TCP/IP.\n\n\n\n\nipconfig /all\n reports \"NetBIOS over Tcpip . . . : Disabled\" on interfaces where NetBT is disabled, and says nothing where NetBT is enabled.\n\n\nTo start sharing a directory, on the node, use the \nnet share\n command, or turn on network sharing on the Sharing tab of the Properties of a directory (folder.)\n\n\n\n\n\n\nOn XP-SP2 or above, when you first do this, the \"Network sharing and security\" subdialog says:\n\n\nAs a security measure, Windows has disabled remote access to this\ncomputer.  However, you can enable remote access and safely share files by\nrunning the _Network_Setup_Wizard_.\n_If_you_understand_the_security_risks_but_want_to_share_\n_files_without_running_the_wizard,_click_here._\"\n\n\n\n\n\n\n\nSkip the wizard and click the latter (\"I understand\") link. Then click \"Just enable file sharing\", and \"OK\".\n\n\n\n\nThen you finally get the click-box to \"Share this folder on the network\".\n\n\n\n\nThe machine names for UNC paths sharing are the same as in shell prompts: \npcXXX\n, where \nXXX\n is the machine number. These will show up in \nMy Network Places / Entire Network / Microsoft Windows Network / DETER\n once you have used them.\n\n\nIP numbers can also be used in UNC paths, giving you a way to share files across experiment network links rather than the control network.\n\n\nThere is an DETER-generated \nLMHOSTS\n file, to provide the usual node aliases within an experiment, but it is currently ignored even though \"Enable LMHOSTS lookup\" is turned on in the TCP/IP WINS settings. Try \nnbtstat -c\n and \nnbtstat -R\n to experiment with this. (See the \nMicrosoft doc for nbtstat\n.\n\n\nMaking Custom Windows OS Images\n\uf0c1\n\n\nMaking custom Windows images is similar to \ndoing it on the other DETER operating systems\n, except that you must do a little more work to run the \nprepare\n script as user \nroot\n since there are no \nsu\n or \nsudo\n commands on Windows. This is optional on the other OS types, but on Windows, proper TCP/IP network setup depends on \nprepare\n being run.\n\n\n\n\n\n\nLog in to the node where you want to save a custom image. Give the shell command to change the root password. Pick a password string you can remember, typing it twice as prompted:\n\n\n% passwd root\nEnter the new password (minimum of 5, maximum of 8 characters).\nPlease use a combination of upper and lower case letters and numbers.\nNew password:\nRe-enter new password:\n\n\n\nThis works because you are part of the Windows \nAdministrators group\n. Otherwise you would have to already know the root password to change it.\n\n\n\n\nNote\n\n\nIf you change the root password and reboot Windows \nbefore running \nprepare\n below, the root password will not match the definitions of the DETER Windows services (daemons) that run as root, so they will not start up. \n\n\n\n\n\n\n\n\nLog out all sessions by users other than \nroot\n, because \nprepare\n will be unable to remove their login profile directories if they are logged in. (See \nQWINSTA\n.)\n\n\n\n\n\n\nLog in to the node as user \nroot\n through the Console or SSH, using the password you set above, then run the \nprepare\n command. (It will print \"Must be root to run this script!\" and do nothing if not run as root.)\n\n\n/usr/local/etc/emulab/prepare\n\n\n\nIf run without option arguments, \nprepare\n will ask for the root password you want to use in your new image, prompting twice as the passwd command did above. It needs this to redefine the DETER Windows services (daemons) that run as root. It doesn't need to be the same as the root password you logged in with, since it sets the root password to be sure. The Administrator password is changed as well, since the Sysprep option needs that (below.)\n\n\n\n\n\n\nYou can give the \n-p\n option to specify the root password on the command line:\n\n\n/usr/local/etc/emulab/prepare -p myRootPwd\n\n\n\n\n\n\n\nThe \n-n\n option says not to change the passwords at all, and the DETER Windows services are not redefined.\n\n\n/usr/local/etc/emulab/prepare -n\n\n\n\n\n\n\n\nThe \n-s\n option is used to make \nhardware-independent\n images using the Windows \nSysprep\n deploy tool. If you use it with the \n-n\n option instead of giving a password, it assumes that you separately blank the Administrator password, or edit your Administrator password into the \n[GuiUnattended]AdminPassword\n entry of the sysprep.inf file.\n\n\n/usr/local/etc/emulab/prepare -s -p myRootPwd\n\n\n\n\n\nNote\n\n\nThis must be done from a login on the \nserial console\n, because Sysprep shuts down the network. \nprepare -s\n refuses to run from an SSH or RDP login. \n\n\n\n\n\n\nNote\n\n\nCurrently, hardware-independent images must be made on a pc850, and will then run on the pc600, pc3000, and pc3000w as well. There is an unresolved boot-time problem going the other direction, from the pc3000 to a pc850 or pc600. \n\n\nWindows normally casts some aspects of the NT image into concrete at the first boot after installation, including the specific boot disk driver to be used by the NT loader (IDE, SCSI, or SATA.) \nSysprep\n is used by PC hardware manufacturers as they make XP installation disks with their own drivers installed. The \nSysprep\n option to run an unattended \nMini-Setup\n at first boot instead of the normal \"Out Of the Box Experience\" is used in some large corporate roll-outs. We do both.\n\n\n\n\nThe DETER \n/share/windows/sysprep\n directory contains several versions of the XP deploy tools matched to the XP service pack level, appropriate device driver directories, and a draft \nsysprep.inf\n file to direct the automated install process.\n\n\nMini-setup\n needs to reboot after setting up device drivers. XP also needs to [#Boots_twice reboot] after changing the host name. We combine the two by using a \nCmdlines.txt\n script\n to run \nrc.firstboot -mini\n to set the host name at the end of \nMini-Setup\n.\n\n\nThus we only pay the extra time to set up device drivers and so on from scratch, about two minutes, rather than adding a third hardware and XP reboot cycle.\n\n\n\n\nNote\n\n\nAs you create your Image Descriptor, set the \nreboot wait-time\n to \n360\n rather than 240 so that swap-ins don't time out.\n\n\n\n\n\n\nThen log out and \ncreate your custom image\n.\n\n\n\n\n\n\nNote\n\n\nWindows XP is too big to fit in the partitioning scheme used by FreeBSD and Linux, so it's necessary when making a Windows custom image to specify \nPartition 1\n, and click \nWhole Disk Image.\n\n\n\n\n\n\n\n\nWhen you're testing your custom image, it's a good idea to set the \ntb-set-node-failure-action\n to \"nonfatal\" in the ns file so you get a chance to examine an image that hasn't completed the set-up process. See the \nnote below\n for other useful ideas.\n\n\n\n\n\n\nCygwin\n\uf0c1\n\n\nCygwin is \nGNU + Cygnus + Windows\n, providing Linux-like functionality at the API, command-line, and package installation levels.\n\n\nCygwin documentation\n\uf0c1\n\n\nCygwin is well documented. Here are some links to get you started:\n\n\n\n\nUsers guide\n\n\nCygwin highlights\n\n\nCygwin-added utilities\n\n\nFAQ\n\n\nAPI compatibility and Cygwin functions\n\n\n\n\nCygwin packages\n\uf0c1\n\n\nA number of optional Cygwin packages are installed in the image due to our building and running the DETER client software, plus some editors for convenience. These packages are currently agetty, bison, cvs, cygrunsrv, ed, file, flex, gcc, gdb, inetutils, make, minires-devel, more, nano, openssh, openssl-devel, patch, perl, perl-libwin32, psmisc, python, rpm, rsync, shutdown, sysvinit, tcsh, vim, wget, and zip.\n\n\nThe Cygwin command \ncygcheck -c\n lists the packages that are installed, and their current version number and status. Package-specific notes and/or documentation for installed packages are in \n/usr{,/share}/doc/Cygwin/*.README\n and \n/usr/share/doc/*/README\n files. The \nCygwin package site\n lists the available pre-compiled packages and provides a search engine.\n\n\nIf you want to install more Cygwin pre-compiled packages, run the graphical installer:\n\n\nC:/Software/Cygwin/setup.exe\n\n\n\nThe Cygwin command \ncygcheck -l package-name\n lists the contents of an installed package, which may help you to make a tarfile or rpm from a package you have installed. You can then cause it to be installed automatically by DETER into all of the nodes of your experiment. See the [Tutorial#TARBALLS Tutorial] for more information about installing \nRPM's\n and \ntarballs\n.\n\n\nWatch out for post-install scripts in:\n\n\n    /etc/postinstall/package-name.sh{,.done}\n\n\n\n\nMany packages not in the Cygwin package site have also been ported to Cygwin already. Download the sources to an experiment node and try\n\n\n\n    ./configure\n    make\n    make install\n\n\n\n\n\nas usual.\n\n\nSMB mounts and Samba\n\uf0c1\n\n\nUser home directories and other shared directories are served by \nfs\n, another alias for Ops/Users, via the SMB protocol (Server Message Block, also known as Windows File Sharing) with the Windows Client connecting to the Samba server.\n\n\nUNC paths with leading double-slashes and a server name, e.g. \n//fs\n, are used to access the SMB Shares under Cygwin. DETER then uses the \nCygwin mount command\n to make them appear on the usual Unix paths for the DETER shared directories: \n/users/\nusername\n, \n/proj/\npid\n, \n/group/\npid\n/\ngid\n, and \n/share\n.\n\n\nThe Cygwin \nmount\n command lists what you could access on the Samba server, with the UNC path in the first column. Unix file permissions may further limit your access on the Samba server. Log in to Ops to investigate.\n\n\n/share/windows\n contains Windows software. See \n/share/windows/README.bin\n for descriptions of binary packages available for installation.\n\n\nIn Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with \nbackslashes\n, e.g. \n\\\\fs\\share\n or \n\\\\fs\\\nusername\n. User and project shares are marked \"not browsable\", so just \n\\\\fs\n shows only \nshare\n.\n\n\nWindows limitation:\n There is only \none protection mask\n for everything in a whole mount/share under SMB. It's set in the \"share properties\" on the server (Samba config file in this case) so \nchmod\n will do you no good across SMB.\n\n\nCygwin limitation:\n There is a hard-coded \nlimit of 30 mount points\n in Cygwin. Cygwin uses 4 of them, and DETER uses another 3 or 4. So some of your \n/users\n mounts will fail on Windows startup if you have more than 23 or 24 members in your project, unless they are grouped into smaller subgroups.\n\n\nCygwin arcana\n\uf0c1\n\n\n\n\n\n\nFile paths\n\n\nCygwin accepts either flavor of slashes in paths, Unix/POSIX-style forward-slashes, or Windows-style back-slashes. In Unix shell commands, backslashes need to be quoted.\n\n\nSingle-quotes work best. Doubling each backslash also works. This must also be done inside double-quotes. Examples: \n'\\single\\quoted'\n, \n\"\\\\double\\\\quoted\"\n, \n\\\\un\\\\quoted\n. (The difference between double and single quotes is that $variable references and back-quoted command execution are expanded in double-quotes.)\n\n\nWhen you invoke Windows (as opposed to Cygwin) commands, for example \nnet use\n, they will know nothing about Unix-style paths in their arguments. The \ncygpath\n utility is an aid to converting paths between the Unix and Windows conventions.\n\n\ncygpath -w\n converts its arguments to Windows format, and \ncygpath -u\n converts its arguments to Unix format, e.g.\n\n\n$ cygpath -w /cygdrive/c/WINDOWS\nc:\\WINDOWS\n$ cygpath -u 'c:\\WINDOWS'\n/cygdrive/c/WINDOWS\n\n\n\n\n\n\n\nMount points\n\n\nCygwin mount points\n are shown by the \nmount\n and \ndf\n commands.\n\n\nNote that there is a hard-coded limit of 30 mount points in Cygwin. Attempts to use the Cygwin \nmount\n command after that will fail.\n\n\nSee the discussion of mount points and UNC \n//machine\n paths to SMB shares \nabove\n.\n\n\nAnother special case is the \nUnix root\n, \"\n/\n\". It's mounted to \nC:\\cygwin\n in the Windows filesystem.\n\n\n\n\n\n\nDrive letter mounts\n\n\nCygwin knows about drive letter prefixes like \nC:\n\u00c2 , which are equivalent to \n/cygdrive/\ndrive-letter\n\u00c2 . However, \n/cygdrive\n, like \n/dev\n, isn't a real directory, so you can't \nls\n it.\n\n\nSome Windows software requires drive-letter mounts to be created for its use.\n\n\nYou can use the Windows \nnet use\n command to associate drive letters with UNC paths to SMB shares, e.g.\n\n\nnet use W: '\\\\fs\\share\\windows'\n\n\n\nYou can use the Windows \nsubst\n command to associate drive letters with local paths, e.g.\n\n\nsubst T: 'C:\\Temp'\n\n\n\nFilename completion in Cygwin shells with \nTab\n doesn't work following a drive-letter prefix, but it works normally after a \n/cygdrive/\n prefix. Also, filename completion is case-sensitive, although the underlying Windows is case-insensitive, so a filename in the wrong case is still opened properly.\n\n\n\n\n\n\nNTSEC\n\n\nCygwin is running in \nNTSEC\n (NT Security) mode, so \n/etc/passwd\n and \n/etc/group\n contain Windows SID's as user and group ID's. Your Windows UID is the computer SID with a user number appended, something like \nS-1-5-21-2000478354-436374069-1060284298-1334\n.\n\n\nCygwin commands, such as \nid\n, \nls -ln\n, and \nchown/chgrp\n, use the numeric suffix as the uid, e.g. \n1334\n. This is different from your normal DETER Unix user ID number, and the Samba server takes care of the difference.\n\n\nThe \nid\n command reports your user id and group memberships.\n\n\nNote that all users are in group \nNone\n on XP. Contrary to the name, this is a group that contains \nall users\n. It was named \nEverybody\n on Windows 2000, which was a better name.\n\n\n\n\n\n\nsetuid\n\n\nThere is no direct equivalent of the Unix \nsetuid\n programs under Windows, and hence no \nsu\n or \nsudo\n commands.\n\n\nThe Windows equivalent to running a Unix command as \nroot\n is membership in the Windows \nAdministrators\n group. DETER project members who have either \nlocal_root\n or \ngroup_root\n privileges are put in group \nwheel\n, another alias for \nAdministrators\n. Project members with \nuser\n privileges are not members of the wheel group.\n\n\nYou can \nssh\n a command to the node as the target user, as long as you arrange for the proper authentication.\n\n\nFor C/C++ code, there is a \nsetuid()\n function in the Cygwin library, which \"impersonates\" the user if proper setup is done first.\n\n\n\n\n\n\nroot\n\n\nThere is not normally a Windows account named \nroot\n. \nroot\n on XP is just another user who is a member of the \nAdministrators\n group, see below.\n\n\nWe create a \nroot\n account as part of the DETER setup to own installed software, and to run services and Unix scripts that check that they're running with root privileges.\n\n\nYou can log in as \nroot\n via RDP, \nssh\n, or the serial console if you change the root password as described in the \ncustom Windows OS images\n section.\n\n\nThe \nroot\n user does not have any Samba privileges to access Samba shared mounts, including the \n/proj\n, \n/groups\n, and \n/users\n.\n\n\n\n\n\n\nAdministrators\n group\n\n\nAll users are members of the Windows \nAdministrators\n group. (The DETER non-local-root user property is not implemented on Windows.)\n\n\nMembership in the Windows \nAdministrators\n group is very different from being\nroot\n on Unix, and is also different from being logged in as Administrator.\n\n\nAdministrators\n group membership on Windows only means you can set the ownership, group, and permissions on any file using the Cygwin \nchown\n, \nchgrp\n, \nchmod\n, or their Windows equivalents. Until you have done that, you can be completely locked out by read, write, or execute/open permissions of the directory or files.\n\n\nAnother subtlety is that the group called \nNone\n on XP is what used to be named \nEverybody\n on Windows 2000. All users are automatically in group \nNone\n, so in practice setting group \nNone\n permissions is no different from setting public access permissions.\n\n\n\n\n\n\nPermissions\n\n\nCygwin does a pretty good job of mapping Unix user-group-other file permissions to Windows NT security ACLs.\n\n\nOn Windows, unlike Unix, file and directory permissions can lock out root, Administrator, or SYSTEM user access. Many Unix scripts don't bother with permissions if they're running as root, and hence need modification to run on Cygwin.\n\n\nThis creates a potential problem with the tb-set-node-tarfiles and tb-set-node-rpms commands. The tb-set-node-tarfiles page says \"Notes: 1. ... the files are installed as root\". So you can easily install files that \nyour\n login doesn't have permission to access.\n\n\nThe solution is to \nchmod\n the files before making the tarball or rpm file to grant appropriate access permissions.\n\n\n\n\n\n\nExecutables\n\n\nCygwin tries to treat \n.exe\n files the same as executable files without the \n.exe\n suffix, but with execute permissions turned on. (See the \nCygwin Users Guide\n.)\n\n\nThis breaks down in Makefile actions and scripts, where \nrm\n, \nls -l\n, and \ninstall\n commands may need an explicit \n.exe\n added.\n\n\n\n\n\n\nWindows GUI programs\n\n\nYou cannot run Windows GUI (Graphical User Interface) programs under ssh, on the serial console, or by tb-set-node-startcmd. There is no user login graphics context until you log in via RDP.\n\n\nHowever, you \ncan\n use a \nstartcmd\n to set a Windows registry key that causes a GUI program to be run automatically for all users \nwhen they log in to the node via RDP\n, if that's what you want. The program can be one that is installed by tb-set-node-tarfiles.\n\n\nYou can pick any regkey name you want and put it in the \nRun\n registry folder. It's good not to step on the ones already there, so choose a name specific to your program. Put the following in your \nstartcmd\n script:\n\n\nregtool -s set /HKLM/SOFTWARE/Microsoft/Windows/CurrentVersion/Run/mypgm 'C:\\mypgm\\mypgm.exe'\n\n\n\n(where \nmypgm\n is the name of your program, of course.)\n\n\nNotice that the value string is single-quoted with \nC:\n and backslashes. Windows interprets this regkey, and wants its flavor of file path.\n\n\n\n\n\n\nNtEmacs\n\uf0c1\n\n\nWe don't include the Cygwin X server in our XP images to keep the bulk and complexity down. So \nNtEmacs 21.3\n is provided instead of the Cygwin \nX\n Emacs. \nNtEmacs\n \"frames\" are windows on the Windows Desktop, e.g. \n^X-5-2\n makes another one.\n\n\nThe \n/usr/local/bin/emacs\n executable is a symlink to \n/cygdrive/c/emacs-21.3/bin/runemacs.exe\n, which starts up an Emacs on the desktop. This only works under RDP, since SSH logins have a null desktop.\n\n\nThere is also a \n/usr/local/bin/emacs-exe\n executable, a symlink to \n/cygdrive/c/emacs-21.3/bin/emacs.exe\n, which is only useful as an Emacs compiler. It could be used to run Emacs in an SSH or Serial Console login window with the \n-nw\n (no windows) flag, except that it exits with \nemacs: standard input is not a tty\n. Another thing not to try is running \nemacs-exe -nw\n in a Bash or TCSH shell on the RDP desktop. It crashed \nWindows XP\n when I tried it.\n\n\n\n\nCan drag-and-drop files from \nWindows Explorer\n to \n!NtEmacs\n windows.\n\n\ncygwin-mount.el\n in \nc:/emacs-21.3/site-lisp/\n makes Cygwin mounts visible within \n!NtEmacs\n. It doesn't do Cygwin symlinks yet.\n\n\nOptions - See \n~root/.emacs\n\n\nmouse-wheel-mode\n\n\nCUA-mode\n option ( \n^C\n copy / \n^X\n cut on selection, \n^V\n paste, \n^Z\n undo).\n\n\nCtrl\n and \nAlt\n key mappings, etc.", 
            "title": "Windows XP"
        }, 
        {
            "location": "/core/windows/#differences-from-freebsd-and-linux", 
            "text": "The biggest difference of course, is that this is  Windows , with Cygwin layered on top, and DETERLab management services added. In particular, this is Windows XP (NT 5.1), with various levels of service packs and updates (see  below .)  File Sharing \uf0c1  The second-biggest difference is that shared directories are provided not by the NFS (Network File System) protocol, but instead by the  SMB  (Server Message Block) protocol, otherwise known as Windows File Sharing.  The \"Client for Microsoft Networks\" software contacts the SMB server, in this case  Samba  running on the file server known as  Fs  (an alias for  Users .) The SMB protocol authenticates using a plain-text user name and password, encrypted as they go across the network. (These Windows Shares are then accessed by UNC paths under Cygwin mounts, [#SMB_mounts described below].)  In Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with  backslashes , e.g.  \\\\fs\\share  or  \\\\fs\\ username . User and project shares are marked \"not browsable\", so just  \\\\fs  shows only  share .  If you want to serve files from one of your experiment nodes to others, see the section on [#netbt_command  The netbt command ].  Windows Passwords \uf0c1  A separate  Windows password  is kept for use only with experiment nodes running Windows. It is presented behind-the-scenes to  rdesktop  for RDP logins by our Web interface under Unix, and for the Samba mount of shared directories like your home directory under an SSH login, so you don't have to type it in those cases. You  will  have to type it each time if you use the  Microsoft RDC (Remote Desktop Connector) client program  from a Windows machine.  The default Windows password is randomly generated. It's easy to change it to something easier to remember.  To see or edit your Windows password, log in to DETERLab, and click  Manage User Profile  and then  Edit Profile  under  User Options . You will see  Windows Password  fields in addition to the regular DETERLab  Password  fields.  When you change your Windows password, you will also have to re-type it as a check. The new Windows password should propagate to the Samba server on Fs instantly, so you can swap in an experiment and log in to its Windows nodes with the new password.  If you have already swapped-in experiment nodes and changed your Windows password, the account information including passwords will be updated at the next DETERLab watchdog daemon  isalive  interval. This should be in 3 to 6 minutes.  Experiment setup for Windows nodes \uf0c1  All you have to do is put a line specifying a WINXP OS image in your experiment NS file, like this:      tb-set-node-os $node WINXP-UPDATE  The Windows XP images are not specific to a particular hardware type. (See the  Change Log  for more information.) You may explicitly specify the hardware type to run on if you wish, for example:      tb-set-hardware $node pc3000  Since the bandwidth of the connection between the ISI and Berkeley portions of the testbed is constrained, it is best to run Windows nodes only on the ISI side of the testbed. This can be accomplished by creating a custom hardware type with      tb-make-soft-vtype my_custom_node_type {pc3060 pc3000 pc2133 pc2133x}  and then specifying that your Windows nodes must only swap-in on this custom type using       tb-set-hardware $node my_custom_node_type  See the  note below  on using the  tb-set-node-failure-action  command for experiments with a large number of Windows nodes. This can save a swap-in with a large number of Windows nodes, or prevent a single node boot failure on a swapmod from swapping-out the whole experiment.  If you use these commands:  tb-set-node-startcmd ,  tb-set-node-tarfiles , or  tb-set-node-rpms  you should read the sections on  Permissions  and  Windows GUI programs  below.  The only available Windows image currently is:   WINXP-UPDATE  - The most recent Windows XP-SP3+. It is updated periodically from Windows Update, typically after a Microsoft \"Patch Tuesday\", the second Tuesday of each month. All critical and security fixes are installed, up through the date we pull the image file. (See the date created field on the individual WINXP  Image IDs ).    Note  The Windows Firewall is disabled by default (as it will inform you repeatedly!)   Network config \uf0c1  Some default Windows networking features are disabled.  NetBT (!NetBios over TCP)  ( NetbiosOptions=2  ) and  DNS auto-registration  ( DisableDynamicUpdate=1 ) are disabled to allow network  idle detection  by the slothd service.  TCP/IP address autoconfiguration  is disabled ( IPAutoconfigurationEnabled=0 ) so that un-switched interfaces like the sixth NICs on the pc3000's don't get bogus Microsoft class B network 169.254.0.0 addresses assigned.  The Windows  ipconfig /all  command only shows the configuration information for the enabled network interfaces. There will always be one enabled control net interface on the  192.168.0.0/22  network. The others are disabled if not used in your experiment. (See file  /var/emulab/boot/ipconfig-cache  for a full listing from boot time, including the interfaces that were later disabled.)  If you specified links or LANs in your experiment network topology, other interfaces will be enabled, with an IP address, subnet mask, and gateway that you can specify in the NS file. Notice that the Windows names of the interfaces start with  Local Area Connection  and have a number appended. You can't count on what this number is, since it depends on the order the NIC's are probed as Windows boots.   Note  Often, we have seen  ipconfig  report an IP address and mask of  0.0.0.0 , while the TCP/IP properties dialog boxes and the  netsh  command show the proper values. Our startup scripts disable and re-enable the network interface in an attempt to reset this. Sometimes it doesn't work, and another reboot is done in an attempt to get the network up.   Routing \uf0c1  Full-blown router nodes cannot run Windows, i.e.  rtproto Session  is not supported. However, basic routing between connected network components of your experiment topology works. The Windows command to see the routing tables is  route print . The  IPEnableRouter=1  registry key is set on multi-homed hosts in the experiment network, before they are rebooted to change the hostname.  rtproto Static  is supported in all recent WINXP images, but not in WINXP-02-16 (2005) or before.  rtproto Static-old  or  rtproto Manual  will work in any image.  There is more information on routing in the  Routing section of the Core Guide .  Windows nodes boot twice \uf0c1  Notice that Windows reboots an extra time after being loaded onto a node during swap-in. It must reboot after changing the node name to set up the network stack properly. Be patient, Windows XP doesn't boot quickly.  With  hardware-independent , ( sysprep'ed ) images, the first boot is actually running  Mini-Setup  as well, setting up device drivers and so on.  It's best not to log in to the nodes until the experiment is fully swapped-in. (You may be able to log in briefly between the first two reboots; if you see the wrong  pcXXX  name, you'll know that a reboot is imminent.) You can know that the swap-in process is finished by any of these methods:   Waiting until you get the \"experiment swapped in\" email from DETERLab.  Checking the node status on the experiment status page in DETERLab. (You must refresh the page to see node status change.)  Watching the realtime swap-in log to monitor its progress.    Note  Sometimes Windows XP fails to do the second reboot. One reason is transient race conditions in the Windows startup, for example in the network stack when there are multiple network interface devices being initialized at the same time. We make a strong effort to recover from this, but if the recovery code fails, by default it results in a swap-in or swapmod failure.   At boot time, the startup service on Windows XP runs the  /usr/local/etc/emulab/rc/rc.bootsetup  script, logging output to  /var/log/bootsetup.log . If you're having swap-in problems and  rc.bootsetup  doesn't finish sending  ISUP  to DETERLab within 10 minutes, the node will be rebooted. After a couple of reboot cycles without a  ISUP , DETERLab gives up on the node.  You can cause these boot-time problems to be nonfatal by adding this line to your  ns file   for each Windows node :   tb-set-node-failure-action $node  nonfatal   (where  $node  is replaced with the node variable, of course.)  DETERLab will still complain if it doesn't get the ISUP signal at the end of rc.bootsetup, but the swap-in or swapmod will proceed and allow you to figure out what's happening. Then you will probably have to manually reboot the failed Windows node to make it available to your experiment.  If you try to login to a node after swap-in to diagnose the problem and your Windows password isn't honored, use this command on Ops to remotely reboot the node:   node_reboot pcxxx  If you are able to log in but your remote home directory isn't mounted, this is another symptom of a partial set-up. You have the additional option of executing this command on the node itself:   /sbin/reboot  This gives Windows another chance to get it right.  Login connections to Windows \uf0c1  You can manually start up the SSH or RDP client programs to connect and log in to nodes in your experiment, or use the  console  command on Ops. You will have to type your  Windows Password  when logging in, except for SSH when you have ssh-agent keys loaded.  Or you can set up your browser to automatically connect in one click from the DETERLab web interface and pop up a connection window. Once you start swapping in an experiment, the  Experiment Information  page contains a table of the physical node ID and logical node name, status, and connection buttons. The captions at the top of the button columns link to pages explaining how to set up up mime-types in your browser to make the buttons work, from FreeBSD, Linux, and Windows workstations:   SSH   (setup)  - The  SSH  connection button gives a Bash or TCSH shell, as usual. Your DETERLab SSH keys are installed on the node in a  /sshkeys  subdirectory.  Console  - The  serial console  is supported for Cygwin shell logins using the  agetty  and  sysvinit  packages. This is the only way in when network connections are closed down! You can also monitor the Frisbee loading and booting of the Windows image on the console.  RDP  - The  RDP  button starts up a Remote Desktop Protocol connection, giving a Windows Desktop login from the user's workstation screen to the experiment node.  The  rdesktop  client software is used from Linux and Unix client workstations.  A Microsoft  RDC  (Remote Desktop Connector) client program is included in Windows XP, and may be installed onto other versions of Windows as well. It has the feature that you can make it full-screen without (too much) confusion, since it hangs a little tab at the top of the screen to switch back. Unfortunately, we have no way to present your DETERLab Windows password to RDC, so you'll have to type it on each login.    Note  If you import dot-files into DETERLab that replace the system execution search path rather than add to it, you will have a problem running Windows system commands in shells. Fix this by adding  /cygdrive/c/WINDOWS/system32  and  /cygdrive/c/WINDOWS  to your  $PATH  in  ~/.cshrc  and either  ~/.bash_profile  or  ~/.profile . Don't worry about your home directory dot-files being shared among Windows, FreeBSD, and Linux nodes; non-existent directories in the  $PATH  are ignored by shells.   When new DETERLab user accounts are created, the default CSH and Bash dotfiles are copied from the FreeBSD  /usr/share/skel . They replace the whole $PATH rather than add to it. Then we append a DETERLab-specific part that takes care of the path, conditionally adding the Windows directories on Cygwin.   Note  The Windows  ping  program has completely different option arguments from the Linux and FreeBSD ones, and they differ widely from each other. There is a ping package in Cygwin that is a port of the 4.3bsd ping. Its options are close to a common subset of the Linux and FreeBSD options, so it will be included in future WINXP images:    ping [ -dfqrv ] host [ packetsize [count [ preload]]]  You can load it yourself now using  Cygwin Setup .   Note  There are no Cygwin ports of some other useful networking commands, such as  traceroute  and  ifconfig -a . The Windows system equivalents are  tracert  and  ipconfig /all .   RDP details \uf0c1  Here are some fine points and hints for RDP logins to remote Windows desktops:    Microsoft allows only  one desktop login at a time  to  Windows XP , although this is the same Citrix Hydra technology that supports many concurrent logins to Terminal Server or Server 2003.   The  Fast User Switching  option to XP is turned on, so a second RDP connection disconnects a previous one rather than killing it. Similarly, just closing your RDP client window disconnects your Windows Login session rather than killing it. You can reconnect later on without losing anything. SSH doesn't count as a desktop, so you can SSH in and use this command:  qwinsta  (Query WINdows STAtion) to show existing winstation sessions and their session ID's, and this one to reset (kill) a session by ID:  rwinsta .    We rename  My Computer  to show the PCxxx physical node name, but it doesn't appear on the  Windows XP  desktop by default. The XP user interface incorporates \"My Computer\" into the upper-right quadrant of the \"Start\" menu by default, and removes it from the desktop.   You can go back to the \"classic\" user interface of Windows 2000, including showing \"My Computer\". Right-click on the background of the Taskbar which contains the \"Start\" button at the left, and choose \"Properties\". Select the \"Start Menu\" tab, click the \"Classic Start menu\" radio-button, and click \"OK\".  Alternatively, you can force \"My Computer\" to appear on your XP desktop by right-clicking on the desktop background and choosing \"Properties\". Select the \"Desktop\" tab and click \"Customize Desktop...\" to get the \"Desktop Items\" dialog. Turn on the \"My Computer\" checkbox, then click \"OK\" twice.    There are several  Desktop icons  (i.e. \"shortcuts\") installed by default in the XP images: Computer Management, Bash and TCSH shells, and  NtEmacs . You will notice two flavors of Bash and TCSH icons on the desktop, labeled  rxvt  and  Cygwin .    The  rxvt  shells  run in windows with  X -like cut-and-paste mouse clicks:   Left-click  starts a selection,  Right-click  extends it, and  middle-click  pastes.  These are the ones to use if you're connecting from an X workstation.      Note  The default colors used in Bash and rxvt don't work well in 4-bit color mode under RDP. Make sure you update your rdp-mime.pl to get the rdesktop  -a 16  argument for 16-bit color. Or, you can over-ride the rxvt defaults by putting lines in your  ~/.Xdefaults  file like this:   rxvt*background: steelblue     The  Cygwin  shells  run in a Windows Terminal window, just as the Windows cmd.exe does. These are the ones to use if you're connecting from a Windows workstation.  Quick-edit mode  is on by default, so you can cut-and-paste freely between your local workstation desktop and your remote RDP desktops. In a Windows Terminal window on your RDP remote desktop, the quick-edit cut-and-paste mouse clicks are:   Left-drag  the mouse to  mark  a rectangle of text, highlighting it.  Type  Enter  or  right-click  the mouse when text is highlighted , to  copy  the selected text to the clipboard. ( Escape   cancels  the selection without copying it.)  Right-click the mouse with nothing selected  to  paste  the contents of the clipboard.     On the  first login by a user , Windows creates the user's  Windows profile directory  under  C:\\Documents and Settings , and creates the  registry key  (folder) for persistent settings for that user.  We arrange that early in the user's login process, a user  HOME  environment variable value is set in the user's registry. Otherwise Emacs wouldn't know how to find your  .emacs  setup file in your remotely mounted home directory.  User \"root\" is special, and has a local home directory under  /home .  /home  is a Cygwin symbolic link to  C:\\Documents and Settings .    The  Windows XP  Start menu has no  Shutdown  button under RDP. Instead, it is labeled  Disconnect  and only closes the RDP client window, leaving the login session and the node running. If you simply close the window, or the RDP client network connection is lost, you are also disconnected rather than logged out. When you reconnect, it comes right back, just as it was.  To restart the computer, run  /sbin/reboot , or use the \"Shut Down\" menu of  Task Manager . One way to start Task Manager is to right-click on the background of the Taskbar at the bottom of the screen and select \"Task Manager\".    The  netbt  command \uf0c1  The  NetBT  (Netbios over TCP) protocol is used to announce shared directories (folders) from one Windows machine to others. (See the Name and Session services in  http://en.wikipedia.org/wiki/Netbios .)  The  SMB  (Server Message Block) protocol is used to actually serve files. (See  http://en.wikipedia.org/wiki/Server_Message_Block .)  In DETERLab, we normally disable NetBT on experiment nodes, because it chatters and messes up slothd network idle detection, and is not needed for the usual SMB mounts of  /users ,  /proj , and  /share  dirs, which are served from a Samba service on  fs .  However, NetBT  does  have to be enabled on the experiment nodes if you want to make Windows file shares between them. The  netbt  script sets the registry keys on the Windows network interface objects. Run it on the server nodes (the ones containing directories which you want to share) and reboot them afterwards to activate. There is an optional  -r  argument to reboot the node.      Usage: netbt [-r] off|on  If you use  netbt  to turn on NetBT, it persists across reboots.  No reboot is necessary if you use Network Connections in the Control Panel to turn on NetBT. It takes effect immediately, but is turned off at reboot unless you do  netbt on  afterward as well.   Right-click Local Area Connection (or the name of another connection, if appropriate), click Properties, click Internet Protocol (TCP/IP), and then click the Properties button.  On the Internet Protocol (TCP/IP) Properties page, click the Advanced button, and click the WINS tab.  Select Enable or Disable NetBIOS over TCP/IP.   ipconfig /all  reports \"NetBIOS over Tcpip . . . : Disabled\" on interfaces where NetBT is disabled, and says nothing where NetBT is enabled.  To start sharing a directory, on the node, use the  net share  command, or turn on network sharing on the Sharing tab of the Properties of a directory (folder.)    On XP-SP2 or above, when you first do this, the \"Network sharing and security\" subdialog says:  As a security measure, Windows has disabled remote access to this\ncomputer.  However, you can enable remote access and safely share files by\nrunning the _Network_Setup_Wizard_.\n_If_you_understand_the_security_risks_but_want_to_share_\n_files_without_running_the_wizard,_click_here._\"    Skip the wizard and click the latter (\"I understand\") link. Then click \"Just enable file sharing\", and \"OK\".   Then you finally get the click-box to \"Share this folder on the network\".   The machine names for UNC paths sharing are the same as in shell prompts:  pcXXX , where  XXX  is the machine number. These will show up in  My Network Places / Entire Network / Microsoft Windows Network / DETER  once you have used them.  IP numbers can also be used in UNC paths, giving you a way to share files across experiment network links rather than the control network.  There is an DETER-generated  LMHOSTS  file, to provide the usual node aliases within an experiment, but it is currently ignored even though \"Enable LMHOSTS lookup\" is turned on in the TCP/IP WINS settings. Try  nbtstat -c  and  nbtstat -R  to experiment with this. (See the  Microsoft doc for nbtstat .  Making Custom Windows OS Images \uf0c1  Making custom Windows images is similar to  doing it on the other DETER operating systems , except that you must do a little more work to run the  prepare  script as user  root  since there are no  su  or  sudo  commands on Windows. This is optional on the other OS types, but on Windows, proper TCP/IP network setup depends on  prepare  being run.    Log in to the node where you want to save a custom image. Give the shell command to change the root password. Pick a password string you can remember, typing it twice as prompted:  % passwd root\nEnter the new password (minimum of 5, maximum of 8 characters).\nPlease use a combination of upper and lower case letters and numbers.\nNew password:\nRe-enter new password:  This works because you are part of the Windows  Administrators group . Otherwise you would have to already know the root password to change it.   Note  If you change the root password and reboot Windows  before running  prepare  below, the root password will not match the definitions of the DETER Windows services (daemons) that run as root, so they will not start up.      Log out all sessions by users other than  root , because  prepare  will be unable to remove their login profile directories if they are logged in. (See  QWINSTA .)    Log in to the node as user  root  through the Console or SSH, using the password you set above, then run the  prepare  command. (It will print \"Must be root to run this script!\" and do nothing if not run as root.)  /usr/local/etc/emulab/prepare  If run without option arguments,  prepare  will ask for the root password you want to use in your new image, prompting twice as the passwd command did above. It needs this to redefine the DETER Windows services (daemons) that run as root. It doesn't need to be the same as the root password you logged in with, since it sets the root password to be sure. The Administrator password is changed as well, since the Sysprep option needs that (below.)    You can give the  -p  option to specify the root password on the command line:  /usr/local/etc/emulab/prepare -p myRootPwd    The  -n  option says not to change the passwords at all, and the DETER Windows services are not redefined.  /usr/local/etc/emulab/prepare -n    The  -s  option is used to make  hardware-independent  images using the Windows  Sysprep  deploy tool. If you use it with the  -n  option instead of giving a password, it assumes that you separately blank the Administrator password, or edit your Administrator password into the  [GuiUnattended]AdminPassword  entry of the sysprep.inf file.  /usr/local/etc/emulab/prepare -s -p myRootPwd   Note  This must be done from a login on the  serial console , because Sysprep shuts down the network.  prepare -s  refuses to run from an SSH or RDP login.     Note  Currently, hardware-independent images must be made on a pc850, and will then run on the pc600, pc3000, and pc3000w as well. There is an unresolved boot-time problem going the other direction, from the pc3000 to a pc850 or pc600.   Windows normally casts some aspects of the NT image into concrete at the first boot after installation, including the specific boot disk driver to be used by the NT loader (IDE, SCSI, or SATA.)  Sysprep  is used by PC hardware manufacturers as they make XP installation disks with their own drivers installed. The  Sysprep  option to run an unattended  Mini-Setup  at first boot instead of the normal \"Out Of the Box Experience\" is used in some large corporate roll-outs. We do both.   The DETER  /share/windows/sysprep  directory contains several versions of the XP deploy tools matched to the XP service pack level, appropriate device driver directories, and a draft  sysprep.inf  file to direct the automated install process.  Mini-setup  needs to reboot after setting up device drivers. XP also needs to [#Boots_twice reboot] after changing the host name. We combine the two by using a  Cmdlines.txt  script  to run  rc.firstboot -mini  to set the host name at the end of  Mini-Setup .  Thus we only pay the extra time to set up device drivers and so on from scratch, about two minutes, rather than adding a third hardware and XP reboot cycle.   Note  As you create your Image Descriptor, set the  reboot wait-time  to  360  rather than 240 so that swap-ins don't time out.    Then log out and  create your custom image .    Note  Windows XP is too big to fit in the partitioning scheme used by FreeBSD and Linux, so it's necessary when making a Windows custom image to specify  Partition 1 , and click  Whole Disk Image.     When you're testing your custom image, it's a good idea to set the  tb-set-node-failure-action  to \"nonfatal\" in the ns file so you get a chance to examine an image that hasn't completed the set-up process. See the  note below  for other useful ideas.", 
            "title": "Differences from FreeBSD and Linux"
        }, 
        {
            "location": "/core/windows/#cygwin", 
            "text": "Cygwin is  GNU + Cygnus + Windows , providing Linux-like functionality at the API, command-line, and package installation levels.  Cygwin documentation \uf0c1  Cygwin is well documented. Here are some links to get you started:   Users guide  Cygwin highlights  Cygwin-added utilities  FAQ  API compatibility and Cygwin functions   Cygwin packages \uf0c1  A number of optional Cygwin packages are installed in the image due to our building and running the DETER client software, plus some editors for convenience. These packages are currently agetty, bison, cvs, cygrunsrv, ed, file, flex, gcc, gdb, inetutils, make, minires-devel, more, nano, openssh, openssl-devel, patch, perl, perl-libwin32, psmisc, python, rpm, rsync, shutdown, sysvinit, tcsh, vim, wget, and zip.  The Cygwin command  cygcheck -c  lists the packages that are installed, and their current version number and status. Package-specific notes and/or documentation for installed packages are in  /usr{,/share}/doc/Cygwin/*.README  and  /usr/share/doc/*/README  files. The  Cygwin package site  lists the available pre-compiled packages and provides a search engine.  If you want to install more Cygwin pre-compiled packages, run the graphical installer:  C:/Software/Cygwin/setup.exe  The Cygwin command  cygcheck -l package-name  lists the contents of an installed package, which may help you to make a tarfile or rpm from a package you have installed. You can then cause it to be installed automatically by DETER into all of the nodes of your experiment. See the [Tutorial#TARBALLS Tutorial] for more information about installing  RPM's  and  tarballs .  Watch out for post-install scripts in:      /etc/postinstall/package-name.sh{,.done}  Many packages not in the Cygwin package site have also been ported to Cygwin already. Download the sources to an experiment node and try  \n    ./configure\n    make\n    make install  as usual.  SMB mounts and Samba \uf0c1  User home directories and other shared directories are served by  fs , another alias for Ops/Users, via the SMB protocol (Server Message Block, also known as Windows File Sharing) with the Windows Client connecting to the Samba server.  UNC paths with leading double-slashes and a server name, e.g.  //fs , are used to access the SMB Shares under Cygwin. DETER then uses the  Cygwin mount command  to make them appear on the usual Unix paths for the DETER shared directories:  /users/ username ,  /proj/ pid ,  /group/ pid / gid , and  /share .  The Cygwin  mount  command lists what you could access on the Samba server, with the UNC path in the first column. Unix file permissions may further limit your access on the Samba server. Log in to Ops to investigate.  /share/windows  contains Windows software. See  /share/windows/README.bin  for descriptions of binary packages available for installation.  In Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with  backslashes , e.g.  \\\\fs\\share  or  \\\\fs\\ username . User and project shares are marked \"not browsable\", so just  \\\\fs  shows only  share .  Windows limitation:  There is only  one protection mask  for everything in a whole mount/share under SMB. It's set in the \"share properties\" on the server (Samba config file in this case) so  chmod  will do you no good across SMB.  Cygwin limitation:  There is a hard-coded  limit of 30 mount points  in Cygwin. Cygwin uses 4 of them, and DETER uses another 3 or 4. So some of your  /users  mounts will fail on Windows startup if you have more than 23 or 24 members in your project, unless they are grouped into smaller subgroups.  Cygwin arcana \uf0c1    File paths  Cygwin accepts either flavor of slashes in paths, Unix/POSIX-style forward-slashes, or Windows-style back-slashes. In Unix shell commands, backslashes need to be quoted.  Single-quotes work best. Doubling each backslash also works. This must also be done inside double-quotes. Examples:  '\\single\\quoted' ,  \"\\\\double\\\\quoted\" ,  \\\\un\\\\quoted . (The difference between double and single quotes is that $variable references and back-quoted command execution are expanded in double-quotes.)  When you invoke Windows (as opposed to Cygwin) commands, for example  net use , they will know nothing about Unix-style paths in their arguments. The  cygpath  utility is an aid to converting paths between the Unix and Windows conventions.  cygpath -w  converts its arguments to Windows format, and  cygpath -u  converts its arguments to Unix format, e.g.  $ cygpath -w /cygdrive/c/WINDOWS\nc:\\WINDOWS\n$ cygpath -u 'c:\\WINDOWS'\n/cygdrive/c/WINDOWS    Mount points  Cygwin mount points  are shown by the  mount  and  df  commands.  Note that there is a hard-coded limit of 30 mount points in Cygwin. Attempts to use the Cygwin  mount  command after that will fail.  See the discussion of mount points and UNC  //machine  paths to SMB shares  above .  Another special case is the  Unix root , \" / \". It's mounted to  C:\\cygwin  in the Windows filesystem.    Drive letter mounts  Cygwin knows about drive letter prefixes like  C: \u00c2 , which are equivalent to  /cygdrive/ drive-letter \u00c2 . However,  /cygdrive , like  /dev , isn't a real directory, so you can't  ls  it.  Some Windows software requires drive-letter mounts to be created for its use.  You can use the Windows  net use  command to associate drive letters with UNC paths to SMB shares, e.g.  net use W: '\\\\fs\\share\\windows'  You can use the Windows  subst  command to associate drive letters with local paths, e.g.  subst T: 'C:\\Temp'  Filename completion in Cygwin shells with  Tab  doesn't work following a drive-letter prefix, but it works normally after a  /cygdrive/  prefix. Also, filename completion is case-sensitive, although the underlying Windows is case-insensitive, so a filename in the wrong case is still opened properly.    NTSEC  Cygwin is running in  NTSEC  (NT Security) mode, so  /etc/passwd  and  /etc/group  contain Windows SID's as user and group ID's. Your Windows UID is the computer SID with a user number appended, something like  S-1-5-21-2000478354-436374069-1060284298-1334 .  Cygwin commands, such as  id ,  ls -ln , and  chown/chgrp , use the numeric suffix as the uid, e.g.  1334 . This is different from your normal DETER Unix user ID number, and the Samba server takes care of the difference.  The  id  command reports your user id and group memberships.  Note that all users are in group  None  on XP. Contrary to the name, this is a group that contains  all users . It was named  Everybody  on Windows 2000, which was a better name.    setuid  There is no direct equivalent of the Unix  setuid  programs under Windows, and hence no  su  or  sudo  commands.  The Windows equivalent to running a Unix command as  root  is membership in the Windows  Administrators  group. DETER project members who have either  local_root  or  group_root  privileges are put in group  wheel , another alias for  Administrators . Project members with  user  privileges are not members of the wheel group.  You can  ssh  a command to the node as the target user, as long as you arrange for the proper authentication.  For C/C++ code, there is a  setuid()  function in the Cygwin library, which \"impersonates\" the user if proper setup is done first.    root  There is not normally a Windows account named  root .  root  on XP is just another user who is a member of the  Administrators  group, see below.  We create a  root  account as part of the DETER setup to own installed software, and to run services and Unix scripts that check that they're running with root privileges.  You can log in as  root  via RDP,  ssh , or the serial console if you change the root password as described in the  custom Windows OS images  section.  The  root  user does not have any Samba privileges to access Samba shared mounts, including the  /proj ,  /groups , and  /users .    Administrators  group  All users are members of the Windows  Administrators  group. (The DETER non-local-root user property is not implemented on Windows.)  Membership in the Windows  Administrators  group is very different from being root  on Unix, and is also different from being logged in as Administrator.  Administrators  group membership on Windows only means you can set the ownership, group, and permissions on any file using the Cygwin  chown ,  chgrp ,  chmod , or their Windows equivalents. Until you have done that, you can be completely locked out by read, write, or execute/open permissions of the directory or files.  Another subtlety is that the group called  None  on XP is what used to be named  Everybody  on Windows 2000. All users are automatically in group  None , so in practice setting group  None  permissions is no different from setting public access permissions.    Permissions  Cygwin does a pretty good job of mapping Unix user-group-other file permissions to Windows NT security ACLs.  On Windows, unlike Unix, file and directory permissions can lock out root, Administrator, or SYSTEM user access. Many Unix scripts don't bother with permissions if they're running as root, and hence need modification to run on Cygwin.  This creates a potential problem with the tb-set-node-tarfiles and tb-set-node-rpms commands. The tb-set-node-tarfiles page says \"Notes: 1. ... the files are installed as root\". So you can easily install files that  your  login doesn't have permission to access.  The solution is to  chmod  the files before making the tarball or rpm file to grant appropriate access permissions.    Executables  Cygwin tries to treat  .exe  files the same as executable files without the  .exe  suffix, but with execute permissions turned on. (See the  Cygwin Users Guide .)  This breaks down in Makefile actions and scripts, where  rm ,  ls -l , and  install  commands may need an explicit  .exe  added.    Windows GUI programs  You cannot run Windows GUI (Graphical User Interface) programs under ssh, on the serial console, or by tb-set-node-startcmd. There is no user login graphics context until you log in via RDP.  However, you  can  use a  startcmd  to set a Windows registry key that causes a GUI program to be run automatically for all users  when they log in to the node via RDP , if that's what you want. The program can be one that is installed by tb-set-node-tarfiles.  You can pick any regkey name you want and put it in the  Run  registry folder. It's good not to step on the ones already there, so choose a name specific to your program. Put the following in your  startcmd  script:  regtool -s set /HKLM/SOFTWARE/Microsoft/Windows/CurrentVersion/Run/mypgm 'C:\\mypgm\\mypgm.exe'  (where  mypgm  is the name of your program, of course.)  Notice that the value string is single-quoted with  C:  and backslashes. Windows interprets this regkey, and wants its flavor of file path.", 
            "title": "Cygwin"
        }, 
        {
            "location": "/core/windows/#ntemacs", 
            "text": "We don't include the Cygwin X server in our XP images to keep the bulk and complexity down. So  NtEmacs 21.3  is provided instead of the Cygwin  X  Emacs.  NtEmacs  \"frames\" are windows on the Windows Desktop, e.g.  ^X-5-2  makes another one.  The  /usr/local/bin/emacs  executable is a symlink to  /cygdrive/c/emacs-21.3/bin/runemacs.exe , which starts up an Emacs on the desktop. This only works under RDP, since SSH logins have a null desktop.  There is also a  /usr/local/bin/emacs-exe  executable, a symlink to  /cygdrive/c/emacs-21.3/bin/emacs.exe , which is only useful as an Emacs compiler. It could be used to run Emacs in an SSH or Serial Console login window with the  -nw  (no windows) flag, except that it exits with  emacs: standard input is not a tty . Another thing not to try is running  emacs-exe -nw  in a Bash or TCSH shell on the RDP desktop. It crashed  Windows XP  when I tried it.   Can drag-and-drop files from  Windows Explorer  to  !NtEmacs  windows.  cygwin-mount.el  in  c:/emacs-21.3/site-lisp/  makes Cygwin mounts visible within  !NtEmacs . It doesn't do Cygwin symlinks yet.  Options - See  ~root/.emacs  mouse-wheel-mode  CUA-mode  option (  ^C  copy /  ^X  cut on selection,  ^V  paste,  ^Z  undo).  Ctrl  and  Alt  key mappings, etc.", 
            "title": "NtEmacs"
        }, 
        {
            "location": "/core/legacy-tools/", 
            "text": "This page includes links to tools that have been useful to DETER users in the past. There is no guarantee that they will perform with current DETER software and they are listed for legacy purposes.\n\n\nBenchmarks\n\uf0c1\n\n\nDDoS Defense Benchmarks\n\uf0c1\n\n\nDeveloped and maintained by University of Delaware, this tool contains:\n\n\n\n\nA benchmark suite with a set of scenarios to be used for defense evaluation, integrated with SEER,\n\n\nA set of performance metrics that characterize an attack's impact and a defense's performance, and\n\n\n\n\nA set of tools used for benchmark development, integration of benchmarks with the DETER testbed and calculation of performance metrics from tcpdump traces collected during DDoS experimentation.\n\n\n\n\n\n\nWebsite\n: \nhttp://www.isi.edu/~mirkovic/bench\n \n\n\n\n\nRuns on\n: Any platform \n\n\nBest for\n: Testing DDoS defenses \n\n\nFor questions, contact\n: \nJelena Mirkovic\n at ISI\n\n\n\n\nLegitimate Traffic Generators\n\uf0c1\n\n\nSEER\n\uf0c1\n\n\nThe Security Experimentation EnviRonment (SEER), developed by SPARTA, Inc., is a GUI-based user interface to DeterLab, helping an experimenter to set up, script, and perform experiments in the DETER environment. The SEER back-end includes tools to generate legitimate traffic using Harpoon or custom-made Web, DNS, Ping, IRC, FTP and VoIP agents. Note that this tool is no longer supported and is offered as-is.\n\n\n\n\nWebsite\n: \nhttp://seer.deterlab.net/trac\n \n\n\nRuns on\n: All platforms, written in Java \n\n\nBest for\n: Legitimate traffic generation, DoS traffic generation, visualization of traffic levels in topology\n\n\n\n\nTcpreplay\n\uf0c1\n\n\nTcpreplay is a suite of BSD licensed tools, which gives you the ability to inject previously captured traffic in libpcap format to test a variety of network devices. It allows you to classify traffic as client or server, rewrite Layer 2, 3 and 4 headers and finally replay the traffic back onto the network and through other devices such as switches, routers, firewalls, NIDS and IPS's. Tcpreplay supports both single and dual NIC modes for testing both sniffing and inline devices. \n\n\n\n\nWebsite\n: \nhttp://tcpreplay.synfin.net/trac/\n \n\n\nRuns on\n: UNIX-flavored OSes and Win32 with Cygwin \n\n\nBest for\n: Replaying traces to regenerate same or similar traffic \n\n\nFor questions, contact\n: \nTcpreplay support\n\n\n\n\nWebstone\n\uf0c1\n\n\nWebstone, a benchmark owned by Mindcraft Inc., measures performance of web server software and hardware products. Webstone consists of a program called the webmaster which can be installed on a client in the network or on a separate computer. The webmaster distributes web client software as well as configuration files for testing to the client computers, that contact the web server to retrieve web pages or files in order to test web server performance. Webstone also tests operating system software, CPU and network speeds. While it was developed with the idea of measuring the performance of web servers, it can be used to generate background traffic in a network as the multiple clients keep contacting the server over a period of time thereby simulating web traffic in the network.\n\n\n\n\nWebsite\n: \nhttp://www.mindcraft.com/webstone/\n \n\n\nRuns on\n: UNIX-flavored OSes and Windows NT \n\n\nBest for\n: Web traffic generation\n\n\n\n\nHarpoon\n\uf0c1\n\n\nHarpoon, developed at University of Wisconsin, is a flow-level traffic generator. It uses a set of distributional parameters that can be automatically extracted from Netflow traces to generate flows that exhibit the same statistical qualities present in measured Internet traces, including temporal and spatial characteristics. Harpoon can be used to generate representative background traffic for application or protocol testing, or for testing network switching hardware. Note, however, that while traffic dynamics will resemble the one found in traces, Harpoon traffic runs over HTTP and application behavior may be different from the real one.\n\n\n\n\nWebsite\n: \nhttps://github.com/jsommers/harpoon\n \n\n\nRuns on\n: UNIX-flavored OSes \n\n\nBest for\n: Generating traffic from traces or from high-level specifications.\n\n\n\n\nDoS and DDoS Attack Traffic Generators\n\uf0c1\n\n\nSEER\n\uf0c1\n\n\n(\nSee above\n) SEER generates attack traffic using the Flooder tool, developed by SPARTA, and the Cleo tool developed by UCLA. Look at SEER's Web page for a more detailed description of these tools.\n\n\nThe following collection of real DDoS tools has little new to offer with regard to attack traffic generation, when compared to SEER's capabilities. In general, SEER can generate same traffic variations as this tools, and is easier to control and customize. If, however, you are testing a defense that looks at control traffic of DoS networks these tools may be useful to you. They are all downloadable from third-party Web sites and are not maintained.\n\n\nStacheldraht\n\uf0c1\n\n\nStacheldraht combines features of Trinoo and TFN tools and adds encrypted communication between the attacker and the masters. Stacheldraht uses TCP for encrypted communication between the attacker and the masters, and TCP or ICMP for communication between master and agents. Another added feature is the ability to perform automatic updates of agent code. Available attacks are UDP flood, TCP SYN flood, ICMP ECHO flood and Smurf attacks.\n\n\n\n\nWebsite\n: \nhttp://packetstormsecurity.org/distributed/stachel.tgz\n\n\n\n\nMstream\n\uf0c1\n\n\nMstream generates a flood of TCP packets with the ACK bit set. Masters can be controlled remotely by one or more attackers using a password- protected interactive login. The communications between attacker and masters, and a master and agents, are configurable at compile time and have varied signif- icantly from incident to incident. Source addresses in attack packets are spoofed at random. The TCP ACK attack exhausts network resources and will likely cause a TCP RST to be sent to the spoofed source address (potentially also creating outgoing bandwidth consumption at the victim).\n\n\n\n\nWebsite\n: \nhttp://packetstormsecurity.org/distributed/mstream.txt\n\n\n\n\nTopology Generators and Convertors\n\uf0c1\n\n\nRocketfuel-to-ns\n\uf0c1\n\n\nRocketfuel-to-ns, developed by Purdue University, is a utility to convert RocketFuel-format data files into a set of configuration files runnable on am emulation testbed like the DETER testbed. Experiment configurations generated with this tool have the advantage of not being totally synthetic representations of the Internet; they provide a router-level topology based off real measurement data. This distribution also contains many sample NS files that represent real AS topologies.\n\n\n\n\nWebsite\n: \nhttp://www.cs.purdue.edu/homes/fahmy/software/rf2ns/index.html\n \n\n\nRuns on\n: UNIX \n\n\nBest for\n: Collecting real AS topologies and importing them into DETERLab.\n\n\n\n\nInet\n\uf0c1\n\n\nInet, developed by University of Michigan, is a generator of representative Autonomous System (AS) level Internet topologies.\n\n\n\n\nWebsite\n: \nhttp://topology.eecs.umich.edu/inet/\n \n\n\nRuns on\n: FreeBSD, Linux, Mac OS and Solaris \n\n\nBest for\n: Synthetic topology generation, following a power law.\n\n\n\n\nBrite\n\uf0c1\n\n\nBrite, developed by Boston University, is a generator of flat AS, flat Router and hierarchical topologies, interoperable with various topology generators and simulators.\n\n\n\n\nWebsite\n: \nhttp://www.cs.bu.edu/brite/\n \n\n\nBest for\n: Synthetic topology generation using different models and a GUI.\n\n\n\n\nGT-ITM\n\uf0c1\n\n\nGT-ITM: Georgia Tech Internetwork Topology Models, developed by Georgia Tech, generates graphs that model the topological structure of internetworks.\n\n\n\n\nWebsite\n: \nhttp://www.cc.gatech.edu/projects/gtitm/\n \n\n\nRuns on\n: SunOS and Linux\n\n\nBest for\n: Synthetic topology generation for small size topologies.", 
            "title": "Legacy Tools"
        }, 
        {
            "location": "/core/legacy-tools/#benchmarks", 
            "text": "DDoS Defense Benchmarks \uf0c1  Developed and maintained by University of Delaware, this tool contains:   A benchmark suite with a set of scenarios to be used for defense evaluation, integrated with SEER,  A set of performance metrics that characterize an attack's impact and a defense's performance, and   A set of tools used for benchmark development, integration of benchmarks with the DETER testbed and calculation of performance metrics from tcpdump traces collected during DDoS experimentation.    Website :  http://www.isi.edu/~mirkovic/bench     Runs on : Any platform   Best for : Testing DDoS defenses   For questions, contact :  Jelena Mirkovic  at ISI", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/core/legacy-tools/#legitimate-traffic-generators", 
            "text": "SEER \uf0c1  The Security Experimentation EnviRonment (SEER), developed by SPARTA, Inc., is a GUI-based user interface to DeterLab, helping an experimenter to set up, script, and perform experiments in the DETER environment. The SEER back-end includes tools to generate legitimate traffic using Harpoon or custom-made Web, DNS, Ping, IRC, FTP and VoIP agents. Note that this tool is no longer supported and is offered as-is.   Website :  http://seer.deterlab.net/trac    Runs on : All platforms, written in Java   Best for : Legitimate traffic generation, DoS traffic generation, visualization of traffic levels in topology   Tcpreplay \uf0c1  Tcpreplay is a suite of BSD licensed tools, which gives you the ability to inject previously captured traffic in libpcap format to test a variety of network devices. It allows you to classify traffic as client or server, rewrite Layer 2, 3 and 4 headers and finally replay the traffic back onto the network and through other devices such as switches, routers, firewalls, NIDS and IPS's. Tcpreplay supports both single and dual NIC modes for testing both sniffing and inline devices.    Website :  http://tcpreplay.synfin.net/trac/    Runs on : UNIX-flavored OSes and Win32 with Cygwin   Best for : Replaying traces to regenerate same or similar traffic   For questions, contact :  Tcpreplay support   Webstone \uf0c1  Webstone, a benchmark owned by Mindcraft Inc., measures performance of web server software and hardware products. Webstone consists of a program called the webmaster which can be installed on a client in the network or on a separate computer. The webmaster distributes web client software as well as configuration files for testing to the client computers, that contact the web server to retrieve web pages or files in order to test web server performance. Webstone also tests operating system software, CPU and network speeds. While it was developed with the idea of measuring the performance of web servers, it can be used to generate background traffic in a network as the multiple clients keep contacting the server over a period of time thereby simulating web traffic in the network.   Website :  http://www.mindcraft.com/webstone/    Runs on : UNIX-flavored OSes and Windows NT   Best for : Web traffic generation   Harpoon \uf0c1  Harpoon, developed at University of Wisconsin, is a flow-level traffic generator. It uses a set of distributional parameters that can be automatically extracted from Netflow traces to generate flows that exhibit the same statistical qualities present in measured Internet traces, including temporal and spatial characteristics. Harpoon can be used to generate representative background traffic for application or protocol testing, or for testing network switching hardware. Note, however, that while traffic dynamics will resemble the one found in traces, Harpoon traffic runs over HTTP and application behavior may be different from the real one.   Website :  https://github.com/jsommers/harpoon    Runs on : UNIX-flavored OSes   Best for : Generating traffic from traces or from high-level specifications.", 
            "title": "Legitimate Traffic Generators"
        }, 
        {
            "location": "/core/legacy-tools/#dos-and-ddos-attack-traffic-generators", 
            "text": "SEER \uf0c1  ( See above ) SEER generates attack traffic using the Flooder tool, developed by SPARTA, and the Cleo tool developed by UCLA. Look at SEER's Web page for a more detailed description of these tools.  The following collection of real DDoS tools has little new to offer with regard to attack traffic generation, when compared to SEER's capabilities. In general, SEER can generate same traffic variations as this tools, and is easier to control and customize. If, however, you are testing a defense that looks at control traffic of DoS networks these tools may be useful to you. They are all downloadable from third-party Web sites and are not maintained.  Stacheldraht \uf0c1  Stacheldraht combines features of Trinoo and TFN tools and adds encrypted communication between the attacker and the masters. Stacheldraht uses TCP for encrypted communication between the attacker and the masters, and TCP or ICMP for communication between master and agents. Another added feature is the ability to perform automatic updates of agent code. Available attacks are UDP flood, TCP SYN flood, ICMP ECHO flood and Smurf attacks.   Website :  http://packetstormsecurity.org/distributed/stachel.tgz   Mstream \uf0c1  Mstream generates a flood of TCP packets with the ACK bit set. Masters can be controlled remotely by one or more attackers using a password- protected interactive login. The communications between attacker and masters, and a master and agents, are configurable at compile time and have varied signif- icantly from incident to incident. Source addresses in attack packets are spoofed at random. The TCP ACK attack exhausts network resources and will likely cause a TCP RST to be sent to the spoofed source address (potentially also creating outgoing bandwidth consumption at the victim).   Website :  http://packetstormsecurity.org/distributed/mstream.txt", 
            "title": "DoS and DDoS Attack Traffic Generators"
        }, 
        {
            "location": "/core/legacy-tools/#topology-generators-and-convertors", 
            "text": "Rocketfuel-to-ns \uf0c1  Rocketfuel-to-ns, developed by Purdue University, is a utility to convert RocketFuel-format data files into a set of configuration files runnable on am emulation testbed like the DETER testbed. Experiment configurations generated with this tool have the advantage of not being totally synthetic representations of the Internet; they provide a router-level topology based off real measurement data. This distribution also contains many sample NS files that represent real AS topologies.   Website :  http://www.cs.purdue.edu/homes/fahmy/software/rf2ns/index.html    Runs on : UNIX   Best for : Collecting real AS topologies and importing them into DETERLab.   Inet \uf0c1  Inet, developed by University of Michigan, is a generator of representative Autonomous System (AS) level Internet topologies.   Website :  http://topology.eecs.umich.edu/inet/    Runs on : FreeBSD, Linux, Mac OS and Solaris   Best for : Synthetic topology generation, following a power law.   Brite \uf0c1  Brite, developed by Boston University, is a generator of flat AS, flat Router and hierarchical topologies, interoperable with various topology generators and simulators.   Website :  http://www.cs.bu.edu/brite/    Best for : Synthetic topology generation using different models and a GUI.   GT-ITM \uf0c1  GT-ITM: Georgia Tech Internetwork Topology Models, developed by Georgia Tech, generates graphs that model the topological structure of internetworks.   Website :  http://www.cc.gatech.edu/projects/gtitm/    Runs on : SunOS and Linux  Best for : Synthetic topology generation for small size topologies.", 
            "title": "Topology Generators and Convertors"
        }, 
        {
            "location": "/orchestrator/orchestrator-quickstart/", 
            "text": "This page describes basic information about the MAGI Orchestrator and provides a high-level overview of how to use it. More details are available in the \nOrchestrator Guide\n.\n\n\nWhat is the MAGI Orchestrator?\n\uf0c1\n\n\nMAGI allows you to automate and manage the procedures of a DETERLab experiment which is very useful for highly complex experiments. It's essentially a workflow management system for DETERLab that provides deterministic control and orchestration over event streams, repeatable enactment of procedures and control and data management for experiments.\n\n\nMAGI replaces the SEER experimentation framework and is part of the DETER experiment lifecycle management tools.\n\n\nHow does it work?\n\uf0c1\n\n\nThe procedure for a MAGI experiment is expressed in a YAML-based Agent Activation Language (AAL) file. The MAGI Orchestrator tool parses the procedure AAL file and maintains experiment-wide state to execute the procedure. The Orchestrator then sends and receives events from agents on the experiment nodes and enforces synchronization points -- called as triggers \u2014 to deterministically execute the procedure. \n\n\nHow do I use the Orchestrator?\n\uf0c1\n\n\n1. Include a special start command in your topology\n\uf0c1\n\n\nAdd a special start command in your topology to install MAGI and supporting tools on all nodes at startup. The command will be similar to the following:\n\n\ntb-set-node-startcmd $NodeName \nsudo python /share/magi/current/magi_bootstrap.py\n\n\n\n\n\n2. Write the AAL file that describes the experiment's workflows\n\uf0c1\n\n\nDescribe the experiment procedure (ie, workflow) in a YAML-based AAL (.aal) file that describes:\n\n\n\n\ngroups - one or more nodes of with similar behavior. \n\n\nagents - sets of behaviors \n\n\nevent streams - list of events and triggers that make up a procedure.\n\n\nevents - invoke a procedure implemented in an agent\n\n\ntriggers - synchronization mechanism based on events or time.\n\n\n\n\nThe following is an example of an event in the AAL file:\n\n\n- type: event\n  agent: server_agent\n  method: startServer\n  trigger: serverStartedSuccessfully\n  args: {}\n\n\n\n\nYou'll find more detailed information about writing the AAL file in the \nOrchestrator Guide\n.\n\n\n3. Run the \nmagi_orchestrator.py\n tool on a physical experiment in DETERLab\n\uf0c1\n\n\nSimilar to \nContainers\n, you run the Orchestrator tool in conjunction with a swapped-in physical experiment in DETERLab. \nmagi_orchestrator.py\n reads the procedure's AAL file and orchestrates an experiment based on the specified procedures.\n\n\nYou would run a command similar to the following on \nusers\n:\n\n\n/share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal\n\n\n\n\n4. View results by accessing nodes, modify the experiment as needed.\n\uf0c1\n\n\nIn an orchestrated experiment, you can access the virtual nodes with the same directories mounted as in a Core DETERLab experiment. You can load and run software and conduct experiments as you would in a Core experiment. \n\n\n5. Save your work and swap out your experiment (release the resources)\n\uf0c1\n\n\nAs with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users.\n\n\nMore Information\n\uf0c1\n\n\nFor more detailed information about the Orchestrator, read the following:\n\n\n\n\nOrchestrator Guide\n - This guide walks you through a basic example of using the Orchestrator and includes some advanced topics.\n\n\nOrchestrator Case Studies\n - Includes details of real-world examples of using Orchestrator.\n\n\nOrchestrator Reference\n - This reference includes commands, configuration details and logs.", 
            "title": "Orchestrator Quickstart"
        }, 
        {
            "location": "/orchestrator/orchestrator-quickstart/#what-is-the-magi-orchestrator", 
            "text": "MAGI allows you to automate and manage the procedures of a DETERLab experiment which is very useful for highly complex experiments. It's essentially a workflow management system for DETERLab that provides deterministic control and orchestration over event streams, repeatable enactment of procedures and control and data management for experiments.  MAGI replaces the SEER experimentation framework and is part of the DETER experiment lifecycle management tools.", 
            "title": "What is the MAGI Orchestrator?"
        }, 
        {
            "location": "/orchestrator/orchestrator-quickstart/#how-does-it-work", 
            "text": "The procedure for a MAGI experiment is expressed in a YAML-based Agent Activation Language (AAL) file. The MAGI Orchestrator tool parses the procedure AAL file and maintains experiment-wide state to execute the procedure. The Orchestrator then sends and receives events from agents on the experiment nodes and enforces synchronization points -- called as triggers \u2014 to deterministically execute the procedure.", 
            "title": "How does it work?"
        }, 
        {
            "location": "/orchestrator/orchestrator-quickstart/#how-do-i-use-the-orchestrator", 
            "text": "1. Include a special start command in your topology \uf0c1  Add a special start command in your topology to install MAGI and supporting tools on all nodes at startup. The command will be similar to the following:  tb-set-node-startcmd $NodeName  sudo python /share/magi/current/magi_bootstrap.py   2. Write the AAL file that describes the experiment's workflows \uf0c1  Describe the experiment procedure (ie, workflow) in a YAML-based AAL (.aal) file that describes:   groups - one or more nodes of with similar behavior.   agents - sets of behaviors   event streams - list of events and triggers that make up a procedure.  events - invoke a procedure implemented in an agent  triggers - synchronization mechanism based on events or time.   The following is an example of an event in the AAL file:  - type: event\n  agent: server_agent\n  method: startServer\n  trigger: serverStartedSuccessfully\n  args: {}  You'll find more detailed information about writing the AAL file in the  Orchestrator Guide .  3. Run the  magi_orchestrator.py  tool on a physical experiment in DETERLab \uf0c1  Similar to  Containers , you run the Orchestrator tool in conjunction with a swapped-in physical experiment in DETERLab.  magi_orchestrator.py  reads the procedure's AAL file and orchestrates an experiment based on the specified procedures.  You would run a command similar to the following on  users :  /share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal  4. View results by accessing nodes, modify the experiment as needed. \uf0c1  In an orchestrated experiment, you can access the virtual nodes with the same directories mounted as in a Core DETERLab experiment. You can load and run software and conduct experiments as you would in a Core experiment.   5. Save your work and swap out your experiment (release the resources) \uf0c1  As with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users.", 
            "title": "How do I use the Orchestrator?"
        }, 
        {
            "location": "/orchestrator/orchestrator-quickstart/#more-information", 
            "text": "For more detailed information about the Orchestrator, read the following:   Orchestrator Guide  - This guide walks you through a basic example of using the Orchestrator and includes some advanced topics.  Orchestrator Case Studies  - Includes details of real-world examples of using Orchestrator.  Orchestrator Reference  - This reference includes commands, configuration details and logs.", 
            "title": "More Information"
        }, 
        {
            "location": "/orchestrator/orchestrator-guide/", 
            "text": "In this tutorial we walk you through setting up a basic orchestrated experiment. This page also includes common advanced topics. Detailed descriptions of the commands and configuration files are available in the \nreference section\n. \n\n\n\n\nNote\n\n\nIf you are a student, go to the \neducation.deterlab.net\n site for classroom-specific instructions.\n\n\n\n\nBasic MAGI Tutorial\n\uf0c1\n\n\nIn this tutorial, we demonstrate how to set up client and server traffic generators with only one server and one client. (For more complex examples, see the \nCase Studies\n.)\n\n\nThe basic steps in creating an orchestrated experiment are:\n\n\n\n\nWrite the AAL file that describes the experiment's workflows.\n\n\nInclude a special start command in your topology.\n\n\nCreate or use a physical experiment in DETERLab.\n\n\nRun the Orchestrator tool on a physical experiment on \nusers.isi.deterlab.net\n.\n\n\n\n\nThe following sections describe each step in detail.\n\n\nStep 1. Write the AAL file\n\uf0c1\n\n\nDescribe the experiment procedure (ie, workflow) in an AAL (.aal) file. First we'll cover the parts of an AAL file and then we'll walk through writing the AAL file for this tutorial (we also provide the AAL file itself).\n\n\nAAL File Overview\n\uf0c1\n\n\nAgent Activation Language (AAL) is a YAML-based descriptive language that describes an experiment\u2019s workflow. It identifies groups (nodes with similar behaviors), agents (a set of behaviors that may be invoked as events) and events/triggers (the different things you want agents to do and the things that trigger them).  \n\n\nAn AAL specification has mainly three parts: groups, agents and event streams.\n\n\nGroups\n\uf0c1\n\n\nGroups\n define sets of one or more nodes with the same behavior and enable a coupling between the experiment procedure and the experiment nodes.\n\n\nExample:\n\n\ngroups:\n   clients: [ node1, node2, node7 ]\n   defender: [ router3 ]\n\n\n\n\n\nAgents\n\uf0c1\n\n\nAgents\n map a functional behavior onto a set of experiment nodes. An agent provides a set of behaviors that may be invoked with events. The agent directive requires the following keys:\n\n\n\n\n\n\nagent\n\n\nName for the set of nodes that represent the behavior\n\n\n\n\n\n\ngroup\n\n\nA set of experiment nodes the will functional as the agent\n\n\n\n\n\n\npath\n\n\nThe path to the agent implementation code\n\n\n\n\n\n\ncode\n\n\nDirectory name of the agent implementation. The code directive is used in the absence of a path directive. MAGI requires the agent implementation to be part of the python distribution if the path directive is not specified.\n\n\n\n\n\n\nexecargs\n\n\nZero or more arguments that will be passed to the agent during initialization.\n\n\n\n\n\n\nExample:\n\n\nagents:\n   smallwebclient:\n      group: smallclients\n      path: /share/magi/modules/http_client/http_client.tar.gz\n      execargs: { servers: [ servernode ], interval: '2', sizes: 'minmax(300,500)'}\n\n\n\n\nEvent Streams\n\uf0c1\n\n\nEvent Streams\n are lists of events and triggers that are parsed and executed by the Orchestrator tool. A procedure typically contains multiple event streams. Different event streams execute concurrently and are synchronized with each other using triggers.\n\n\nThe set of event streams listed using the \nstreamstarts\n directive are invoked at the start of procedure. However, note that the Orchestrator will perform several setup actions, such as create groups, load agents, get status, before the event streams start.\n\n\nEvents\n\uf0c1\n\n\nEvents\n invoke a procedure implemented in an agent. An event is sent to a group. An event directive requires the following keys:\n\n\nagent\n:: The agent to send the event.\n \nmethod\n:: The method to be invoked.\n \nargs\n:: Zero or more arguments required by the method.\n\n\nAdditionally, it may also contain the \ntrigger\n key to flag the return status from the method. The return status may be either \nTrue\n or \nFalse\n.\n\n\nExample:\n\n\n- type: event\n  agent: server_agent\n  method: startServer\n  trigger: serverStartedSuccessfully\n  args: {}\n\n\n\n\nTriggers\n\uf0c1\n\n\nTriggers\n are used as a synchronization mechanism, guard points, or rendezvous points in an experiment procedure. There are two types of triggers that may be combined in several different ways:\n\n\n\n\n\n\nEvent-based triggers\n are received from agents after a method. The \nserverStartedSuccessfully\n is an example of an event-based trigger. The Orchestrator keeps track of outstanding triggers to follow the experiment execution state space. When the server_agent returns \nTrue\n after the method \nstartServer\n, the Orchestrator tags it as a received trigger.\n\n\nExample:\n\n\n- type: trigger\n  triggers: [ {event: ClientStopped} ]\n\n\n\n\n\n\n\nTime-based triggers\n wait for a specified amount time to elapse at the Orchestrator before proceeding.\n\n\nExample:\n\n\n- type: trigger\n  triggers: { [ timeout: 60000 ] }  # wait for 60 seconds\n\n\n\n\n\n\n\nYou may find several specific examples of declaring groups, agents, events, and triggers in the \nCase Studies\n. \n\n\nFor this basic tutorial, save this code to a file named \nprocedure.aal\n and save it to the experiment folder.\n\n\nOur AAL Example\n\uf0c1\n\n\nNow we'll write an AAL that demonstrates three aspects of MAGI: specifying multiple event streams, synchronizing with triggers, and a special target called \nexit\n to unload agents.\n\n\nEvent Streams\n\uf0c1\n\n\nThis example has three events streams; the server stream, the client stream, and the cleanup stream.\n\n\nThe coordination between the events can be illustrated as follows:\n\n\n\n\nEvent streams can be synchronized using \nevent-based triggers\n (such as after the server has started) or \ntime-based triggers\n (such as wait for 30 seconds). The triggers are indicated as \nwait\n states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above.\n\n\nServer Stream\n\uf0c1\n\n\nThe server event stream consists of three states. \n\n\n\n\n\n\nThe \nstart\n state generates a trigger, called \nserverStarted\n, once the server agent is activated on the experiment nodes.\n\n\n\n\n\n\nIt then enters the \nwait\n state where it waits for a trigger from the client event stream.\n\n\n\n\n\n\nOnce the trigger is received, it enters the \nstop\n state, when the server is deactivated or terminated.\n\n\n\n\n\n\nHere is the relevant AAL description:\n\n\nserverstream:\n    - type: event\n      agent: server_agent\n      method: startServer\n        trigger: serverStarted\n      args: {}\n\n    - type: trigger\n      triggers: [ {event: ClientStopped} ]\n\n    - type: event\n      agent: server_agent\n      method: stopServer\n      trigger: ServerStopped\n      args: {}\n\n\n\n\nClient Stream\n\uf0c1\n\n\nThe client event stream consists of five states. \n\n\n\n\nFirst, the client agent implementation is \nparameterized\n by the configuration state. This occurs as part of the agent loading process.\n\n\nThe client stream then \nsynchronizes\n with the server stream by waiting for the \nserverStarted\n trigger from the server nodes. \n\n\nOnce it receives the trigger, the client agent is \nactivated\n in the start state.\n\n\nNext, the client stream \nwaits\n for a period of time and then \nterminates\n the client agents in the stop state.\n\n\nOn termination, the client agents send a \nclientStopped\n trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated.\n\n\n\n\nHere is the relevant AAL description:\n\n\nclientstream:\n    - type: trigger\n      triggers: [ {event: ServerStarted} ]\n\n    - type: event\n      agent: client_agent\n      method: startClient\n      args: {}\n\n    - type: trigger\n      triggers: [ {timeout: 60000} ]\n\n    - type: event\n      agent: client_agent\n      method: stopClient\n      trigger: clientStopped\n      args: {}\n\n\n\n\n\nCleanup Stream\n\uf0c1\n\n\nThe last event stream, the cleanup stream consists of two states. \n\n\n\n\nFirst, it \nwaits\n for all the servers to stop.\n\n\nThen it enters the \nexit\n state.\n\n\n\n\nThe exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator.\n\n\nIt causes the Orchestrator to send agent \nunload\n and \ndisband\n group messages to all of the experiment nodes and then it exits the Orchestrator.\n\n\nHere is the relevant AAL code:\n\n\n\ncleanup:\n    - type: trigger\n      triggers: [ {event: ServerStopped, target: exit} ]\n\n\n\n\n\nYou can see all of the code together in this file: \ncasestudy_clientserver.aal\n.\n\n\nStep 2: Swap in the the physical experiment using topology with MAGI start command\n\uf0c1\n\n\nSwap in the experiment using this network description file: \ncasestudy_clientserver.tcl\n.\n\n\nThis start command installs MAGI and supporting tools on all nodes at startup.\n\n\nThe normal syntax is as follows:\n\n\ntb-set-node-startcmd $NodeName \nsudo python /share/magi/current/magi_bootstrap.py\n\n\n\n\n\nwhere \n$NodeName\n is the control node.\n\n\nIf you look at this file, you'll see the MAGI start command is added as a variable and then used for two nodes: the clientnode and servernode. \n\n\nIn this example, we set the start command as a variable:\n\n\nset magi_start \nsudo python /share/magi/current/magi_bootstrap.py\n\n\n\n\n\nand then use it \n\n\nStep 2: Set up your environment\n\uf0c1\n\n\nSet up environment variables for your environment, replacing the value for \nmyExp\n with your experiment name and \nmyProj\n with your project name.\n\n\nPROJ=myExp\nEXP=myProj\nAAL=casestudy_clientserver.aal\n\n\n\n\nCreate/Use an experiment in DETERLab\n\uf0c1\n\n\nMAGI needs to be enabled on a new or existing swapped-in DETERLab experiment (via interface or using \nstartexp\n on the commandline). You will need its Experiment Name and Project Name when you run the Orchestrator in the next step. \n\n\nMake sure you\u2019ve swapped-in resources before the next step.\n\n\nRun the \nmagi_orchestrator.py\n tool\n\uf0c1\n\n\nThe MAGI Orchestrator tool, \nmagi_orchestrator.py\n, is a tool that reads the procedure's AAL file and orchestrates an experiment based on the specified procedures. The Orchestrator does the following in this order:\n\n\n\n\nJoins Groups\n - The Orchestrator iterates over the list of groups and for each group sends a request to all the mapped nodes to join the group. A corresponding reply adds the replier to the group. Messages addressed to a group are sent to all the nodes that are part of the group.\n\n\nLoads Agents\n - The Orchestrator iterates over the list of agents and for each agent sends an agent load request to the mapped groups. An agent load message tells the Orchestrator to start an instance of the agent implementation and to put it in a listening mode, where the instance waits for further messages.\n\n\nExecutes Event Streams\n - Next, the Orchestrator concurrently executes all the event streams listed as part of \nstreamstarts\n.\nThe Orchestrator has a predefined event stream called \nexit\n. The purpose of this event stream is to unload all the agents and disjoin groups. All workflows should end with executing this stream for a clean exit.\n\n\n\n\nFrom your home directory on \nusers.isi.deterlab.net\n, run the following command:\n\n\n/share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal\n\n\n\n\nwhere:\n\n\n\n\nclientnode\n equals the node you want to start with\n\n\nmyExp\n is the Experiment Name\n\n\nmyProj\n is the Project Name\n\n\nprocedural.aal\n is the name of the AAL file.\n\n\n\n\nThe various command line options are as follows\n\n\nUsage: magi_orchestrator.py [options]\n\nOptions:\n     -h, --help\n                     show this help message and exit\n     -c CONTROL, --control=CONTROL\n                     The control node to connect to (i.e. control.exp.proj)\n     -f EVENTS, --events=EVENTS\n                     The events.aal file(s) to use. Can be specified\n                     multiple times for multiple AAL files\n     -l LOGLEVEL, --loglevel=LOGLEVEL\n                     The level at which to log. Must be one of none, debug,\n                     info, warning, error, or critical. Default is info.\n     -o LOGFILE, --logfile=LOGFILE\n                     If given, log to the file instead of the console\n                     (stdout).\n    -e EXITONFAILURE, --exitOnFailure=EXITONFAILURE\n                     If any method call fails (returns False), then exit\n                     all streams, unload all agents, and exit the\n                     orchestrator. Default value is True\n    -g GROUPBUILDTIMEOUT, --groupBuildTimeout=GROUPBUILDTIMEOUT\n                     When building the initial groups for agents in the\n                     given AAL, use the timeout given (in milliseconds)\n                     when waiting for group formation to complete.\n    --nocolor\n                     If given, do not use color in output.\n    -v, --verbose\n                     Tell orchestrator to print info about what its doing\n   -n, --tunnel\n                    Tell orchestrator to tunnel data through Deter Ops\n                     (users.deterlab.net).", 
            "title": "Orchestrator Guide"
        }, 
        {
            "location": "/orchestrator/orchestrator-guide/#basic-magi-tutorial", 
            "text": "In this tutorial, we demonstrate how to set up client and server traffic generators with only one server and one client. (For more complex examples, see the  Case Studies .)  The basic steps in creating an orchestrated experiment are:   Write the AAL file that describes the experiment's workflows.  Include a special start command in your topology.  Create or use a physical experiment in DETERLab.  Run the Orchestrator tool on a physical experiment on  users.isi.deterlab.net .   The following sections describe each step in detail.", 
            "title": "Basic MAGI Tutorial"
        }, 
        {
            "location": "/orchestrator/orchestrator-guide/#step-1-write-the-aal-file", 
            "text": "Describe the experiment procedure (ie, workflow) in an AAL (.aal) file. First we'll cover the parts of an AAL file and then we'll walk through writing the AAL file for this tutorial (we also provide the AAL file itself).  AAL File Overview \uf0c1  Agent Activation Language (AAL) is a YAML-based descriptive language that describes an experiment\u2019s workflow. It identifies groups (nodes with similar behaviors), agents (a set of behaviors that may be invoked as events) and events/triggers (the different things you want agents to do and the things that trigger them).    An AAL specification has mainly three parts: groups, agents and event streams.  Groups \uf0c1  Groups  define sets of one or more nodes with the same behavior and enable a coupling between the experiment procedure and the experiment nodes.  Example:  groups:\n   clients: [ node1, node2, node7 ]\n   defender: [ router3 ]  Agents \uf0c1  Agents  map a functional behavior onto a set of experiment nodes. An agent provides a set of behaviors that may be invoked with events. The agent directive requires the following keys:    agent  Name for the set of nodes that represent the behavior    group  A set of experiment nodes the will functional as the agent    path  The path to the agent implementation code    code  Directory name of the agent implementation. The code directive is used in the absence of a path directive. MAGI requires the agent implementation to be part of the python distribution if the path directive is not specified.    execargs  Zero or more arguments that will be passed to the agent during initialization.    Example:  agents:\n   smallwebclient:\n      group: smallclients\n      path: /share/magi/modules/http_client/http_client.tar.gz\n      execargs: { servers: [ servernode ], interval: '2', sizes: 'minmax(300,500)'}  Event Streams \uf0c1  Event Streams  are lists of events and triggers that are parsed and executed by the Orchestrator tool. A procedure typically contains multiple event streams. Different event streams execute concurrently and are synchronized with each other using triggers.  The set of event streams listed using the  streamstarts  directive are invoked at the start of procedure. However, note that the Orchestrator will perform several setup actions, such as create groups, load agents, get status, before the event streams start.  Events \uf0c1  Events  invoke a procedure implemented in an agent. An event is sent to a group. An event directive requires the following keys:  agent :: The agent to send the event.\n  method :: The method to be invoked.\n  args :: Zero or more arguments required by the method.  Additionally, it may also contain the  trigger  key to flag the return status from the method. The return status may be either  True  or  False .  Example:  - type: event\n  agent: server_agent\n  method: startServer\n  trigger: serverStartedSuccessfully\n  args: {}  Triggers \uf0c1  Triggers  are used as a synchronization mechanism, guard points, or rendezvous points in an experiment procedure. There are two types of triggers that may be combined in several different ways:    Event-based triggers  are received from agents after a method. The  serverStartedSuccessfully  is an example of an event-based trigger. The Orchestrator keeps track of outstanding triggers to follow the experiment execution state space. When the server_agent returns  True  after the method  startServer , the Orchestrator tags it as a received trigger.  Example:  - type: trigger\n  triggers: [ {event: ClientStopped} ]    Time-based triggers  wait for a specified amount time to elapse at the Orchestrator before proceeding.  Example:  - type: trigger\n  triggers: { [ timeout: 60000 ] }  # wait for 60 seconds    You may find several specific examples of declaring groups, agents, events, and triggers in the  Case Studies .   For this basic tutorial, save this code to a file named  procedure.aal  and save it to the experiment folder.  Our AAL Example \uf0c1  Now we'll write an AAL that demonstrates three aspects of MAGI: specifying multiple event streams, synchronizing with triggers, and a special target called  exit  to unload agents.  Event Streams \uf0c1  This example has three events streams; the server stream, the client stream, and the cleanup stream.  The coordination between the events can be illustrated as follows:   Event streams can be synchronized using  event-based triggers  (such as after the server has started) or  time-based triggers  (such as wait for 30 seconds). The triggers are indicated as  wait  states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above.  Server Stream \uf0c1  The server event stream consists of three states.     The  start  state generates a trigger, called  serverStarted , once the server agent is activated on the experiment nodes.    It then enters the  wait  state where it waits for a trigger from the client event stream.    Once the trigger is received, it enters the  stop  state, when the server is deactivated or terminated.    Here is the relevant AAL description:  serverstream:\n    - type: event\n      agent: server_agent\n      method: startServer\n        trigger: serverStarted\n      args: {}\n\n    - type: trigger\n      triggers: [ {event: ClientStopped} ]\n\n    - type: event\n      agent: server_agent\n      method: stopServer\n      trigger: ServerStopped\n      args: {}  Client Stream \uf0c1  The client event stream consists of five states.    First, the client agent implementation is  parameterized  by the configuration state. This occurs as part of the agent loading process.  The client stream then  synchronizes  with the server stream by waiting for the  serverStarted  trigger from the server nodes.   Once it receives the trigger, the client agent is  activated  in the start state.  Next, the client stream  waits  for a period of time and then  terminates  the client agents in the stop state.  On termination, the client agents send a  clientStopped  trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated.   Here is the relevant AAL description:  clientstream:\n    - type: trigger\n      triggers: [ {event: ServerStarted} ]\n\n    - type: event\n      agent: client_agent\n      method: startClient\n      args: {}\n\n    - type: trigger\n      triggers: [ {timeout: 60000} ]\n\n    - type: event\n      agent: client_agent\n      method: stopClient\n      trigger: clientStopped\n      args: {}  Cleanup Stream \uf0c1  The last event stream, the cleanup stream consists of two states.    First, it  waits  for all the servers to stop.  Then it enters the  exit  state.   The exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator.  It causes the Orchestrator to send agent  unload  and  disband  group messages to all of the experiment nodes and then it exits the Orchestrator.  Here is the relevant AAL code:  \ncleanup:\n    - type: trigger\n      triggers: [ {event: ServerStopped, target: exit} ]  You can see all of the code together in this file:  casestudy_clientserver.aal .  Step 2: Swap in the the physical experiment using topology with MAGI start command \uf0c1  Swap in the experiment using this network description file:  casestudy_clientserver.tcl .  This start command installs MAGI and supporting tools on all nodes at startup.  The normal syntax is as follows:  tb-set-node-startcmd $NodeName  sudo python /share/magi/current/magi_bootstrap.py   where  $NodeName  is the control node.  If you look at this file, you'll see the MAGI start command is added as a variable and then used for two nodes: the clientnode and servernode.   In this example, we set the start command as a variable:  set magi_start  sudo python /share/magi/current/magi_bootstrap.py   and then use it   Step 2: Set up your environment \uf0c1  Set up environment variables for your environment, replacing the value for  myExp  with your experiment name and  myProj  with your project name.  PROJ=myExp\nEXP=myProj\nAAL=casestudy_clientserver.aal", 
            "title": "Step 1. Write the AAL file"
        }, 
        {
            "location": "/orchestrator/orchestrator-guide/#createuse-an-experiment-in-deterlab", 
            "text": "MAGI needs to be enabled on a new or existing swapped-in DETERLab experiment (via interface or using  startexp  on the commandline). You will need its Experiment Name and Project Name when you run the Orchestrator in the next step.   Make sure you\u2019ve swapped-in resources before the next step.", 
            "title": "Create/Use an experiment in DETERLab"
        }, 
        {
            "location": "/orchestrator/orchestrator-guide/#run-the-magi_orchestratorpy-tool", 
            "text": "The MAGI Orchestrator tool,  magi_orchestrator.py , is a tool that reads the procedure's AAL file and orchestrates an experiment based on the specified procedures. The Orchestrator does the following in this order:   Joins Groups  - The Orchestrator iterates over the list of groups and for each group sends a request to all the mapped nodes to join the group. A corresponding reply adds the replier to the group. Messages addressed to a group are sent to all the nodes that are part of the group.  Loads Agents  - The Orchestrator iterates over the list of agents and for each agent sends an agent load request to the mapped groups. An agent load message tells the Orchestrator to start an instance of the agent implementation and to put it in a listening mode, where the instance waits for further messages.  Executes Event Streams  - Next, the Orchestrator concurrently executes all the event streams listed as part of  streamstarts .\nThe Orchestrator has a predefined event stream called  exit . The purpose of this event stream is to unload all the agents and disjoin groups. All workflows should end with executing this stream for a clean exit.   From your home directory on  users.isi.deterlab.net , run the following command:  /share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal  where:   clientnode  equals the node you want to start with  myExp  is the Experiment Name  myProj  is the Project Name  procedural.aal  is the name of the AAL file.   The various command line options are as follows  Usage: magi_orchestrator.py [options]\n\nOptions:\n     -h, --help\n                     show this help message and exit\n     -c CONTROL, --control=CONTROL\n                     The control node to connect to (i.e. control.exp.proj)\n     -f EVENTS, --events=EVENTS\n                     The events.aal file(s) to use. Can be specified\n                     multiple times for multiple AAL files\n     -l LOGLEVEL, --loglevel=LOGLEVEL\n                     The level at which to log. Must be one of none, debug,\n                     info, warning, error, or critical. Default is info.\n     -o LOGFILE, --logfile=LOGFILE\n                     If given, log to the file instead of the console\n                     (stdout).\n    -e EXITONFAILURE, --exitOnFailure=EXITONFAILURE\n                     If any method call fails (returns False), then exit\n                     all streams, unload all agents, and exit the\n                     orchestrator. Default value is True\n    -g GROUPBUILDTIMEOUT, --groupBuildTimeout=GROUPBUILDTIMEOUT\n                     When building the initial groups for agents in the\n                     given AAL, use the timeout given (in milliseconds)\n                     when waiting for group formation to complete.\n    --nocolor\n                     If given, do not use color in output.\n    -v, --verbose\n                     Tell orchestrator to print info about what its doing\n   -n, --tunnel\n                    Tell orchestrator to tunnel data through Deter Ops\n                     (users.deterlab.net).", 
            "title": "Run the magi_orchestrator.py tool"
        }, 
        {
            "location": "/orchestrator/orchestrator-case-studies/", 
            "text": "This section includes more detailed descriptions of how to conduct an experiment in DETERLab using MAGI Orchestrator. Each case study also includes a complete archive with logs and data files. Before you try out the examples below, we recommend reading the \nMAGI Orchestrator Guide\n.\n\n\nSimple Client Server\n\uf0c1\n\n\n \n \n\n\nScaled Client Server\n\uf0c1\n\n\n \n \n\n\nFeedback\n\uf0c1", 
            "title": "Orchestrator Case Studies"
        }, 
        {
            "location": "/orchestrator/orchestrator-case-studies/#simple-client-server", 
            "text": "", 
            "title": "Simple Client Server"
        }, 
        {
            "location": "/orchestrator/orchestrator-case-studies/#scaled-client-server", 
            "text": "", 
            "title": "Scaled Client Server"
        }, 
        {
            "location": "/orchestrator/orchestrator-case-studies/#feedback", 
            "text": "", 
            "title": "Feedback"
        }, 
        {
            "location": "/orchestrator/simple-client-server/", 
            "text": "In this example, we demonstrate how to set up client and server traffic generators with only one server and one client. (In the \nScaled Server Case Study\n, we will show how the same procedure can be used for a significantly larger topology.)\n\n\nWe demonstrate three aspects of MAGI: specifying multiple event streams, synchronizing with triggers, and a special target called \nexit\n to unload agents.\n\n\nEvent Streams\n\uf0c1\n\n\nThis example has three events streams; the server stream, the client stream, and the cleanup stream.\n\n\nThe coordination between the events can be illustrated as follows:\n\n\n\n\nEvent streams can be synchronized using \nevent-based triggers\n (such as after the server has started) or \ntime-based triggers\n (such as wait for 30 seconds). The triggers are indicated as \nwait\n states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above.\n\n\nServer Stream\n\uf0c1\n\n\nThe server event stream consists of three states. \n\n\n\n\n\n\nThe \nstart\n state generates a trigger, called \nserverStarted\n, once the server agent is activated on the experiment nodes.\n\n\n\n\n\n\nIt then enters the \nwait\n state where it waits for a trigger from the client event stream.\n\n\n\n\n\n\nOnce the trigger is received, it enters the \nstop\n state, when the server is deactivated or terminated.\n\n\n\n\n\n\nHere is the relevant AAL description:\n\n\nserverstream:\n    - type: event\n      agent: server_agent\n      method: startServer\n        trigger: serverStarted\n      args: {}\n\n    - type: trigger\n      triggers: [ {event: ClientStopped} ]\n\n    - type: event\n      agent: server_agent\n      method: stopServer\n      trigger: ServerStopped\n      args: {}\n\n\n\n\nClient Stream\n\uf0c1\n\n\nThe client event stream consists of five states. \n\n\n\n\nFirst, the client agent implementation is \nparameterized\n by the configuration state. This occurs as part of the agent loading process.\n\n\nThe client stream then \nsynchronizes\n with the server stream by waiting for the \nserverStarted\n trigger from the server nodes. \n\n\nOnce it receives the trigger, the client agent is \nactivated\n in the start state.\n\n\nNext, the client stream \nwaits\n for a period of time and then \nterminates\n the client agents in the stop state.\n\n\nOn termination, the client agents send a \nclientStopped\n trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated.\n\n\n\n\nHere is the relevant AAL description:\n\n\nclientstream:\n    - type: trigger\n      triggers: [ {event: ServerStarted} ]\n\n    - type: event\n      agent: client_agent\n      method: startClient\n      args: {}\n\n    - type: trigger\n      triggers: [ {timeout: 60000} ]\n\n    - type: event\n      agent: client_agent\n      method: stopClient\n      trigger: clientStopped\n      args: {}\n\n\n\n\n\nCleanup Stream\n\uf0c1\n\n\nThe last event stream, the cleanup stream consists of two states. \n\n\n\n\nFirst, it \nwaits\n for all the servers to stop.\n\n\nThen it enters the \nexit\n state.\n\n\n\n\nThe exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator.\n\n\nIt causes the Orchestrator to send agent \nunload\n and \ndisband\n group messages to all of the experiment nodes and then it exits the Orchestrator.\n\n\nHere is the relevant AAL code:\n\n\n\ncleanup:\n    - type: trigger\n      triggers: [ {event: ServerStopped, target: exit} ]\n\n\n\n\n\nRunning the Experiment\n\uf0c1\n\n\nStep 1: Swap in the experiment\n\uf0c1\n\n\nSwap in the experiment using this network description file: \ncasestudy_clientserver.tcl\n.\n\n\nStep 2: Set up your environment\n\uf0c1\n\n\nAssuming your experiment is named \nmyExp\n, your project is named \nmyProj\n, and the AAL file is called \ncasestudy_clientserver.aal\n, include this in your environment:\n\n\nPROJ=myExp\nEXP=myProj\nAAL=casestudy_clientserver.aal\n\n\n\n\nStep 3: Run the Orchestrator\n\uf0c1\n\n\nOnce the experiment is swapped in, run the Orchestrator using this AAL file: \ncasestudy_clientserver.aal\n. \n\n\n /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL\n\n\n\n\nOnce run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \nmontage\n with experiment \ncaseClientServer\n. The output will be as follows:\n\n\n\n\n\n\nThe Orchestrator runs an internally defined stream called \ninitilization\n that is responsible for establishing the \nserver_group\n and the \nclient_group\n and loading the agents. Once the agents are loaded, as indicated by the received trigger \nAgentLoadDone\n, the initialization stream is complete.\n\n\nNow the \nserverstream\n, \nclientstream\n and the \ncleanup\n stream start concurrently. The \nserverstream\n sends the \nstartServer\n event to the \nserver_group\n. All members of the \nserver_group\n start the server and fire a trigger \nserverStarted\n.\n\n\nThe \nclientstream\n then sends the \nstartClient\n event to the client_group. One minute later, the \nclientstream\n sends the event \nstopClient\n to the client_group and terminates the \nclientstream\n. All members of the client_group terminate the client_agent and generate a \nclientStopped\n trigger which is sent back to the Orchestrator.\n\n\nOnce the \nserverstream\n receives the \nclientStopped\n trigger from the client_group, it sends out the \nstopServer\n event on the server_group. Once all the servers are stopped, the members of the server_group respond with a \nserverStopped\n trigger, which is forwarded to the \ncleanupstream\n.\n\n\nUpon receiving the \nserverStopped\n trigger, the \ncleanupstream\n enacts an internally define stream called \nexit\n that is responsible for unloading agents and tearing down the groups.\n\n\n\n\nThe experiment artifacts, the procedure and topology file that were used for the case study are attached below.\n\n\n\n\nProcedure: \ncasestudy_clientserver.aal\n\n\nTopology: \ncasestudy_clientserver.tcl\n\n\nArchived Logs: \ncasestudy_clientserver.tar.gz\n\n\n\n\nVisualizing Experiment Results\n\uf0c1\n\n\nIn order to visualize the traffic on the network, modify the above-mentioned procedure to add another stream called \nmonitorstream\n. This stream deploys a packet sensor agent on the server node to measure the traffic on the link in the experiment. The packet sensor agent records the traffic data using MAGI\u2019s data management layer.\n\n\nmonitor_group: [servernode]\n\nmonitor_agent:\n  group: monitor_group\n  path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz\n  execargs: {}\n\nmonitorstream:\n    - type: trigger\n      triggers: [ { event: serverStarted } ]\n\n    - type: event\n      agent: monitor_agent\n      method: startCollection\n      trigger: collectionServer\n      args: {}\n\n    - type: trigger\n      triggers: [ { event: clientStopped } ]\n\n    - type: event\n      agent: monitor_agent\n      method: stopCollection\n      args: {}\n\n\n\n\nThe recorded data is then pulled out by the [below mentioned] tools to create a traffic plot.\n\n\nIn order to populate the traffic data, re-run the experiment using the updated procedure file: \ncasestudy_clientserver_monitor.aal\n. The corresponding logs are also \navailable here\n.\n\n\nThe traffic may then be plotted in two ways:\n\n\nOffline Plotting\n\uf0c1\n\n\nA plot of the traffic on the link connecting the client and the server can be generated by the MAGI Graph Creation Tool.\n\n\n GRAPHCONF=cs_magi_graph.conf\n\n /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png\n\n\n\n\n\n\nReal-time Plotting\n\uf0c1\n\n\nA real time simulated traffic plot using canned data from a pre-run experiment may be visualized \nhere\n.\n\n\nA similar plot using live data may be plotted by visiting the same web page and additionally passing it the hostname of the database config node of your experiment.\n\n\nYou can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following.\n\n\n cat /proj/myProject/exp/myExperiment/experiment.conf\n\n\n\n\nThe database config node is as follows:\n\n\n\ndbdl:\n  configHost: node-1\n\nexpdl:\n  experimentName: myExperiment\n  projectName: myProject\n\n\n\n\nThen edit the simulated traffic plot URL, passing it the hostname.\n\n\nhost=node-1.myExperiment.myProject\n\nhttp://\nweb-host\n/traffic.html?host=node-1.myExperiment.myProject\n\n\n\n\nThe procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below.\n\n\n\n\nProcedure: \ncasestudy_clientserver_monitor.aal\n\n\nArchived Logs: \ncasestudy_clientserver_monitor.tar.gz\n\n\nGraph Config: \ncs_magi_graph.conf\n\n\n\n\nScaling the Experiment\n\uf0c1\n\n\nNow suppose you wanted to generate web traffic for a larger topology. We discuss how the above AAL can be applied to a topology of 55 nodes in the \nnext tutorial\n.", 
            "title": "Simple Client Server Case Study"
        }, 
        {
            "location": "/orchestrator/simple-client-server/#event-streams", 
            "text": "This example has three events streams; the server stream, the client stream, and the cleanup stream.  The coordination between the events can be illustrated as follows:   Event streams can be synchronized using  event-based triggers  (such as after the server has started) or  time-based triggers  (such as wait for 30 seconds). The triggers are indicated as  wait  states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above.  Server Stream \uf0c1  The server event stream consists of three states.     The  start  state generates a trigger, called  serverStarted , once the server agent is activated on the experiment nodes.    It then enters the  wait  state where it waits for a trigger from the client event stream.    Once the trigger is received, it enters the  stop  state, when the server is deactivated or terminated.    Here is the relevant AAL description:  serverstream:\n    - type: event\n      agent: server_agent\n      method: startServer\n        trigger: serverStarted\n      args: {}\n\n    - type: trigger\n      triggers: [ {event: ClientStopped} ]\n\n    - type: event\n      agent: server_agent\n      method: stopServer\n      trigger: ServerStopped\n      args: {}  Client Stream \uf0c1  The client event stream consists of five states.    First, the client agent implementation is  parameterized  by the configuration state. This occurs as part of the agent loading process.  The client stream then  synchronizes  with the server stream by waiting for the  serverStarted  trigger from the server nodes.   Once it receives the trigger, the client agent is  activated  in the start state.  Next, the client stream  waits  for a period of time and then  terminates  the client agents in the stop state.  On termination, the client agents send a  clientStopped  trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated.   Here is the relevant AAL description:  clientstream:\n    - type: trigger\n      triggers: [ {event: ServerStarted} ]\n\n    - type: event\n      agent: client_agent\n      method: startClient\n      args: {}\n\n    - type: trigger\n      triggers: [ {timeout: 60000} ]\n\n    - type: event\n      agent: client_agent\n      method: stopClient\n      trigger: clientStopped\n      args: {}  Cleanup Stream \uf0c1  The last event stream, the cleanup stream consists of two states.    First, it  waits  for all the servers to stop.  Then it enters the  exit  state.   The exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator.  It causes the Orchestrator to send agent  unload  and  disband  group messages to all of the experiment nodes and then it exits the Orchestrator.  Here is the relevant AAL code:  \ncleanup:\n    - type: trigger\n      triggers: [ {event: ServerStopped, target: exit} ]", 
            "title": "Event Streams"
        }, 
        {
            "location": "/orchestrator/simple-client-server/#running-the-experiment", 
            "text": "Step 1: Swap in the experiment \uf0c1  Swap in the experiment using this network description file:  casestudy_clientserver.tcl .  Step 2: Set up your environment \uf0c1  Assuming your experiment is named  myExp , your project is named  myProj , and the AAL file is called  casestudy_clientserver.aal , include this in your environment:  PROJ=myExp\nEXP=myProj\nAAL=casestudy_clientserver.aal  Step 3: Run the Orchestrator \uf0c1  Once the experiment is swapped in, run the Orchestrator using this AAL file:  casestudy_clientserver.aal .    /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL  Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project  montage  with experiment  caseClientServer . The output will be as follows:    The Orchestrator runs an internally defined stream called  initilization  that is responsible for establishing the  server_group  and the  client_group  and loading the agents. Once the agents are loaded, as indicated by the received trigger  AgentLoadDone , the initialization stream is complete.  Now the  serverstream ,  clientstream  and the  cleanup  stream start concurrently. The  serverstream  sends the  startServer  event to the  server_group . All members of the  server_group  start the server and fire a trigger  serverStarted .  The  clientstream  then sends the  startClient  event to the client_group. One minute later, the  clientstream  sends the event  stopClient  to the client_group and terminates the  clientstream . All members of the client_group terminate the client_agent and generate a  clientStopped  trigger which is sent back to the Orchestrator.  Once the  serverstream  receives the  clientStopped  trigger from the client_group, it sends out the  stopServer  event on the server_group. Once all the servers are stopped, the members of the server_group respond with a  serverStopped  trigger, which is forwarded to the  cleanupstream .  Upon receiving the  serverStopped  trigger, the  cleanupstream  enacts an internally define stream called  exit  that is responsible for unloading agents and tearing down the groups.   The experiment artifacts, the procedure and topology file that were used for the case study are attached below.   Procedure:  casestudy_clientserver.aal  Topology:  casestudy_clientserver.tcl  Archived Logs:  casestudy_clientserver.tar.gz", 
            "title": "Running the Experiment"
        }, 
        {
            "location": "/orchestrator/simple-client-server/#visualizing-experiment-results", 
            "text": "In order to visualize the traffic on the network, modify the above-mentioned procedure to add another stream called  monitorstream . This stream deploys a packet sensor agent on the server node to measure the traffic on the link in the experiment. The packet sensor agent records the traffic data using MAGI\u2019s data management layer.  monitor_group: [servernode]\n\nmonitor_agent:\n  group: monitor_group\n  path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz\n  execargs: {}\n\nmonitorstream:\n    - type: trigger\n      triggers: [ { event: serverStarted } ]\n\n    - type: event\n      agent: monitor_agent\n      method: startCollection\n      trigger: collectionServer\n      args: {}\n\n    - type: trigger\n      triggers: [ { event: clientStopped } ]\n\n    - type: event\n      agent: monitor_agent\n      method: stopCollection\n      args: {}  The recorded data is then pulled out by the [below mentioned] tools to create a traffic plot.  In order to populate the traffic data, re-run the experiment using the updated procedure file:  casestudy_clientserver_monitor.aal . The corresponding logs are also  available here .  The traffic may then be plotted in two ways:  Offline Plotting \uf0c1  A plot of the traffic on the link connecting the client and the server can be generated by the MAGI Graph Creation Tool.   GRAPHCONF=cs_magi_graph.conf  /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png   Real-time Plotting \uf0c1  A real time simulated traffic plot using canned data from a pre-run experiment may be visualized  here .  A similar plot using live data may be plotted by visiting the same web page and additionally passing it the hostname of the database config node of your experiment.  You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following.   cat /proj/myProject/exp/myExperiment/experiment.conf  The database config node is as follows:  \ndbdl:\n  configHost: node-1\n\nexpdl:\n  experimentName: myExperiment\n  projectName: myProject  Then edit the simulated traffic plot URL, passing it the hostname.  host=node-1.myExperiment.myProject\n\nhttp:// web-host /traffic.html?host=node-1.myExperiment.myProject  The procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below.   Procedure:  casestudy_clientserver_monitor.aal  Archived Logs:  casestudy_clientserver_monitor.tar.gz  Graph Config:  cs_magi_graph.conf", 
            "title": "Visualizing Experiment Results"
        }, 
        {
            "location": "/orchestrator/simple-client-server/#scaling-the-experiment", 
            "text": "Now suppose you wanted to generate web traffic for a larger topology. We discuss how the above AAL can be applied to a topology of 55 nodes in the  next tutorial .", 
            "title": "Scaling the Experiment"
        }, 
        {
            "location": "/orchestrator/scaled-client-server/", 
            "text": "In this example we demonstrate how to set up client server traffic generators in a larger topology.\n\n\nThis case study is identical to \nSimple Client Case Study\n except the topology is significantly larger.\n\n\nEvent Streams\n\uf0c1\n\n\nAs in the \nsimpler case\n, this example has three events streams; the server stream, the client stream, and the cleanup stream.\n\n\nMapping to the Topology\n\uf0c1\n\n\nThe groups directive in the AAL file allows mapping an agent behavior to one or more nodes.\n\n\ngroups:\n   client_group: [clientnode-1, clientnode-2, clientnode-3, clientnode-4, clientnode-5,\n                     clientnode-6, clientnode-7, clientnode-8, clientnode-9, clientnode-10,\n                     clientnode-11, clientnode-12, clientnode-13, clientnode-14, clientnode-15,\n                     clientnode-16, clientnode-17, clientnode-18, clientnode-19, clientnode-20,\n                     clientnode-21, clientnode-22, clientnode-23, clientnode-24, clientnode-25,\n                     clientnode-26, clientnode-27, clientnode-28, clientnode-29, clientnode-30,\n                     clientnode-31, clientnode-32, clientnode-33, clientnode-34, clientnode-35,\n                     clientnode-36, clientnode-37, clientnode-38, clientnode-39, clientnode-40,\n                     clientnode-41, clientnode-42, clientnode-43, clientnode-44, clientnode-45,\n                     clientnode-46, clientnode-47, clientnode-48, clientnode-49, clientnode-50 ]\n   server_group: \nslist [ servernode-1, servernode-2, servernode-3, servernode-4, servernode-5 ]\n\n\n\n\nIn this example, we observe that there are two groups: \nclient_group\n which consists of all 50 clientnodes, and \nserver_group\n which consists of 5 servernodes.\n\n\nAdditionally, we use YAML pointers to annotate the server_group as \nslist\n. The slist annotation is used to refer to the list of servers for configuring the client_agent in the section below.\n\n\nConfiguring the Agents\n\uf0c1\n\n\nThere are two types of agents, a client_agent and a server_agent. Each agent description consists of at least three directives; group, path and execargs.\n\n\n\n\ngroup\n: indicates the set of nodes that the client_agent should be deployed on.\n\n\npath\n: indicates the path to the agent code (also called an agent module).\n\n\nexecargs\n: can be used to parameterize the agents at load time. The agents may be reconfigured later in the AAL also using the \nsetConfiguration\n method.\n\n\n\n\nagents:\n  client_agent:\n    group: client_group\n    path: /share/magi/modules/http_client/http_client.tar.gz\n    execargs: {servers: *slist, interval: '5', sizes: 'minmax(1000,10000)'}\n\n  server_agent:\n    group: server_group\n    path: /share/magi/modules/apache/apache.tar.gz\n    execargs: []\n\n\n\n\nServer Stream\n\uf0c1\n\n\nThe server event stream consists of three states. \n\n\n\n\nThe \nstart\n state which generates a trigger, called \nserverStarted\n, once all the server agents are activated on the experiment nodes.\n\n\nIt then enters the \nwait\n state where it waits for a trigger from the client event stream.\n\n\nOnce the trigger is received, it enters the \nstop\n state, where the server is deactivated or terminated.\n\n\n\n\nThe AAL description is the same as the one used in the \nSimple Client case study\n.\n\n\nClient Stream\n\uf0c1\n\n\nThe client event stream consists of five states. \n\n\n\n\nFirst, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all 50 nodes.\n\n\nThe client stream then synchronizes with the server stream by waiting for the \nserverStarted\n trigger from the server nodes. \n\n\nOnce it receives the trigger the client agent is activated in the start state. Each client_agent fetches web pages for one of the listed servers.\n\n\nNext, the client stream waits for a period (\\Delta) t and then terminates the client agents in the stop state.\n\n\nOn termination, the client agents sends a \nclientStopped\n trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated.\n\n\n\n\nRunning the Experiment\n\uf0c1\n\n\nStep 1: Swap in the experiment\n\uf0c1\n\n\nSwap in the experiment using this network description file: \ncs55_topology.tcl\n\n\nStep 2: Set up your environment\n\uf0c1\n\n\nSet up your environment. Assuming your experiment is named \nmyExp\n, your DETER project is \nmyProj\n, and the AAL file is called \nprocedure.aal\n.\n\n\nPROJ=myExp\nEXP=myProj\nAAL=procedure.aal\n\n\n\n\nStep 3: Run the Orchestrator\n\uf0c1\n\n\nOnce the experiment is swapped in, run the orchestrator, giving it the AAL above and the experiment and project name. \n\n\n /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL\n\n\n\n\nOnce run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseClientServer\u201d:\n\n\n\n\n\n\nThe Orchestrator enacts an internally defined stream called \ninitilization\n that is responsible for establishing the server_group and the client_group and loading the agents. Once the agents are loaded, as indicated by the received trigger \nAgentLoadDone\n, The initialization stream is complete.\n\n\nNow the serverstream, clientstream and the cleanup stream start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted.\n\n\nThe clienstream on receiving the trigger serverStarted from the server_group, sends the startClient event to the client_group. One minute later, the clientstream sends the event stopClient to the client_group and terminates the clientstream. All members of the client_group, terminate the client_agent and generate a clientStopped trigger which is sent back to the orchestrator.\n\n\nOnce the serverstream receives the clientStopped trigger from the client_group, it sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream.\n\n\nOn receiving the serverStopped trigger, the cleanupstream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups.\n\n\n\n\nThe experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment.\n\n\n\n\nProcedure:\n \ncasestudy_clientserver55.aal\n\n\nTopology:\n \ncasestudy_clientserver55.tcl\n\n\nArchive Logs:\n \ncasestudy_clientserver55.tar.gz\n\n\nOrchestration:\n \ncasestudy_clientserver55.orch.log\n\n\n\n\nVisualizing Experiment Results\n\uf0c1\n\n\n\n\nNote\n\n\nThis process is the same as for the Simple Client case - we are reproducing here for your convenience.\n\n\n\n\nIn order to visualize the traffic on the network, modify the above mentioned procedure to add another stream called \u201cmonitorstream\u201d. This stream deploys a packet sensor agent on the router node to measure the aggregated traffic between the server nodes and the client nodes. The packet sensor agent records the traffic data using MAGI\u2019s data management layer.\n\n\nmonitor_group: [router]\n\nmonitor_agent:\n  group: monitor_group\n  path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz\n  execargs: {}\n\nmonitorstream:\n    - type: trigger\n      triggers: [ { event: serverStarted } ]\n\n    - type: event\n      agent: monitor_agent\n      method: startCollection\n      trigger: collectionServer\n      args: {}\n\n    - type: trigger\n      triggers: [ { event: clientStopped } ]\n\n    - type: event\n      agent: monitor_agent\n      method: stopCollection\n      args: {}\n\n\n\n\nThe recorded data is then pulled out by the below mentioned tools to create a traffic plot.\n\n\nIn order to populate the traffic data, re-run the experiment using the updated procedure. The updated procedure file and the corresponding logs are attached below.\n\n\nThe aggregated traffic can then be plotted in two ways:\n\n\nOffline: A plot of the traffic flowing through the router node connecting the clients and the servers can be generated using the MAGI Graph Creation Tool.\n\n\n GRAPHCONF=cs55_magi_graph.conf\n\n /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png\n\n\n\n\n\n\nReal Time: A real time simulated traffic plot using canned data from a pre-run experiment can be visualized \nhere\n.\n\n\nA similar plot using live data can be plotted by visiting the same web page, and additionally passing it the hostname of the database config node of your experiment.\n\n\nYou can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following.\n\n\n cat /proj/myProject/exp/myExperiment/experiment.conf\n\n\n\n\ndbdl:\n  configHost: node-1\n\nexpdl:\n  experimentName: myExperiment\n  projectName: myProject\n\n\n\n\nThen edit the simulated traffic plot URL, passing it the hostname.\n\n\nhost=node-1.myExperiment.myProject\n\nhttp://\nweb-host\n/traffic.html?host=node-1.myExperiment.myProject\n\n\n\n\nThe procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below.\n\n\n\n\nProcedure:\n \ncasestudy_clientserver55_monitor.aal\n\n\nTopology:\n \ncs55_topology.tcl\n\n\nArchived Logs:\n \ncasestudy_clientserver55_monitor.tar.gz\n\n\nGraph Config:\n \ncasestudy_clientserver55_magi_graph.conf", 
            "title": "Scaled Client Server Case Study"
        }, 
        {
            "location": "/orchestrator/scaled-client-server/#event-streams", 
            "text": "As in the  simpler case , this example has three events streams; the server stream, the client stream, and the cleanup stream.", 
            "title": "Event Streams"
        }, 
        {
            "location": "/orchestrator/scaled-client-server/#mapping-to-the-topology", 
            "text": "The groups directive in the AAL file allows mapping an agent behavior to one or more nodes.  groups:\n   client_group: [clientnode-1, clientnode-2, clientnode-3, clientnode-4, clientnode-5,\n                     clientnode-6, clientnode-7, clientnode-8, clientnode-9, clientnode-10,\n                     clientnode-11, clientnode-12, clientnode-13, clientnode-14, clientnode-15,\n                     clientnode-16, clientnode-17, clientnode-18, clientnode-19, clientnode-20,\n                     clientnode-21, clientnode-22, clientnode-23, clientnode-24, clientnode-25,\n                     clientnode-26, clientnode-27, clientnode-28, clientnode-29, clientnode-30,\n                     clientnode-31, clientnode-32, clientnode-33, clientnode-34, clientnode-35,\n                     clientnode-36, clientnode-37, clientnode-38, clientnode-39, clientnode-40,\n                     clientnode-41, clientnode-42, clientnode-43, clientnode-44, clientnode-45,\n                     clientnode-46, clientnode-47, clientnode-48, clientnode-49, clientnode-50 ]\n   server_group:  slist [ servernode-1, servernode-2, servernode-3, servernode-4, servernode-5 ]  In this example, we observe that there are two groups:  client_group  which consists of all 50 clientnodes, and  server_group  which consists of 5 servernodes.  Additionally, we use YAML pointers to annotate the server_group as  slist . The slist annotation is used to refer to the list of servers for configuring the client_agent in the section below.", 
            "title": "Mapping to the Topology"
        }, 
        {
            "location": "/orchestrator/scaled-client-server/#configuring-the-agents", 
            "text": "There are two types of agents, a client_agent and a server_agent. Each agent description consists of at least three directives; group, path and execargs.   group : indicates the set of nodes that the client_agent should be deployed on.  path : indicates the path to the agent code (also called an agent module).  execargs : can be used to parameterize the agents at load time. The agents may be reconfigured later in the AAL also using the  setConfiguration  method.   agents:\n  client_agent:\n    group: client_group\n    path: /share/magi/modules/http_client/http_client.tar.gz\n    execargs: {servers: *slist, interval: '5', sizes: 'minmax(1000,10000)'}\n\n  server_agent:\n    group: server_group\n    path: /share/magi/modules/apache/apache.tar.gz\n    execargs: []", 
            "title": "Configuring the Agents"
        }, 
        {
            "location": "/orchestrator/scaled-client-server/#server-stream", 
            "text": "The server event stream consists of three states.    The  start  state which generates a trigger, called  serverStarted , once all the server agents are activated on the experiment nodes.  It then enters the  wait  state where it waits for a trigger from the client event stream.  Once the trigger is received, it enters the  stop  state, where the server is deactivated or terminated.   The AAL description is the same as the one used in the  Simple Client case study .", 
            "title": "Server Stream"
        }, 
        {
            "location": "/orchestrator/scaled-client-server/#client-stream", 
            "text": "The client event stream consists of five states.    First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all 50 nodes.  The client stream then synchronizes with the server stream by waiting for the  serverStarted  trigger from the server nodes.   Once it receives the trigger the client agent is activated in the start state. Each client_agent fetches web pages for one of the listed servers.  Next, the client stream waits for a period (\\Delta) t and then terminates the client agents in the stop state.  On termination, the client agents sends a  clientStopped  trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated.", 
            "title": "Client Stream"
        }, 
        {
            "location": "/orchestrator/scaled-client-server/#running-the-experiment", 
            "text": "Step 1: Swap in the experiment \uf0c1  Swap in the experiment using this network description file:  cs55_topology.tcl  Step 2: Set up your environment \uf0c1  Set up your environment. Assuming your experiment is named  myExp , your DETER project is  myProj , and the AAL file is called  procedure.aal .  PROJ=myExp\nEXP=myProj\nAAL=procedure.aal  Step 3: Run the Orchestrator \uf0c1  Once the experiment is swapped in, run the orchestrator, giving it the AAL above and the experiment and project name.    /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL  Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseClientServer\u201d:    The Orchestrator enacts an internally defined stream called  initilization  that is responsible for establishing the server_group and the client_group and loading the agents. Once the agents are loaded, as indicated by the received trigger  AgentLoadDone , The initialization stream is complete.  Now the serverstream, clientstream and the cleanup stream start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted.  The clienstream on receiving the trigger serverStarted from the server_group, sends the startClient event to the client_group. One minute later, the clientstream sends the event stopClient to the client_group and terminates the clientstream. All members of the client_group, terminate the client_agent and generate a clientStopped trigger which is sent back to the orchestrator.  Once the serverstream receives the clientStopped trigger from the client_group, it sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream.  On receiving the serverStopped trigger, the cleanupstream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups.   The experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment.   Procedure:   casestudy_clientserver55.aal  Topology:   casestudy_clientserver55.tcl  Archive Logs:   casestudy_clientserver55.tar.gz  Orchestration:   casestudy_clientserver55.orch.log", 
            "title": "Running the Experiment"
        }, 
        {
            "location": "/orchestrator/scaled-client-server/#visualizing-experiment-results", 
            "text": "Note  This process is the same as for the Simple Client case - we are reproducing here for your convenience.   In order to visualize the traffic on the network, modify the above mentioned procedure to add another stream called \u201cmonitorstream\u201d. This stream deploys a packet sensor agent on the router node to measure the aggregated traffic between the server nodes and the client nodes. The packet sensor agent records the traffic data using MAGI\u2019s data management layer.  monitor_group: [router]\n\nmonitor_agent:\n  group: monitor_group\n  path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz\n  execargs: {}\n\nmonitorstream:\n    - type: trigger\n      triggers: [ { event: serverStarted } ]\n\n    - type: event\n      agent: monitor_agent\n      method: startCollection\n      trigger: collectionServer\n      args: {}\n\n    - type: trigger\n      triggers: [ { event: clientStopped } ]\n\n    - type: event\n      agent: monitor_agent\n      method: stopCollection\n      args: {}  The recorded data is then pulled out by the below mentioned tools to create a traffic plot.  In order to populate the traffic data, re-run the experiment using the updated procedure. The updated procedure file and the corresponding logs are attached below.  The aggregated traffic can then be plotted in two ways:  Offline: A plot of the traffic flowing through the router node connecting the clients and the servers can be generated using the MAGI Graph Creation Tool.   GRAPHCONF=cs55_magi_graph.conf  /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png   Real Time: A real time simulated traffic plot using canned data from a pre-run experiment can be visualized  here .  A similar plot using live data can be plotted by visiting the same web page, and additionally passing it the hostname of the database config node of your experiment.  You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following.   cat /proj/myProject/exp/myExperiment/experiment.conf  dbdl:\n  configHost: node-1\n\nexpdl:\n  experimentName: myExperiment\n  projectName: myProject  Then edit the simulated traffic plot URL, passing it the hostname.  host=node-1.myExperiment.myProject\n\nhttp:// web-host /traffic.html?host=node-1.myExperiment.myProject  The procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below.   Procedure:   casestudy_clientserver55_monitor.aal  Topology:   cs55_topology.tcl  Archived Logs:   casestudy_clientserver55_monitor.tar.gz  Graph Config:   casestudy_clientserver55_magi_graph.conf", 
            "title": "Visualizing Experiment Results"
        }, 
        {
            "location": "/orchestrator/feedback/", 
            "text": "Introduction\n\uf0c1\n\n\nThis case study demonstrates our system\u2019s ability to do real-time feedback from the experiment using triggers. We show how information from the experiment may be used to extend the Orchestrator autonomy and deterministically control dynamic experiments. This is an example where the active state of an experiment is a driving input to the control. The data management layer too plays an important role in enabling the flow of information.\n\n\nIn this case study we show how, in a semi-controllable environment, \nthe amount of traffic on a given link may be controlled using feedback from the experiment itself\n. The traffic on one of the links in the experiment must be maintained within a certain range; for this example, the range was 100-105 MB. We assume that the uncontrollable traffic on the link would not exceed the required maximum.\n\n\nThis experiment is set up with the following characteristics:\n\n\n\n\nthe monitored link has some noise (uncontrollable traffic) flowing through it. \n\n\nThe noise has been artificially generated. \n\n\nWe set the noise generating clients to pull randomly changing amount of traffic from the servers, in order to enact the noise.\n\n\n\n\nThe solution is not dependent on the experiment topology. We deploy a traffic monitor agent on one of the end nodes of the link to be monitored. The traffic monitoring agent continuously monitors the traffic on the link. We also deploy a traffic generator agent (control client) that coordinates with the traffic monitor agent in real time and generates exactly the amount of traffic that would help maintain the total traffic on the link within the required limits.\n\n\nTo demonstrate this case study, we set up an experiment topology similar to the one seen in the following figure, with 50 noise generating clients and 10 servers. We also tested a scaled up version of the experiment with 300 noise generating agents and 100 servers. However, due to the resource constraints on the testbed, we recommend you try this case with the smaller topology first. The scaling up of the experiment may be achieved with very simple modifications.\n\n\n\n\nWe use two kinds of agents:\n\n\n\n\nA server agent (located on the server nodes) that runs an apache server and serves random data of requested size. \n\n\nA client agent (located on the client nodes) that periodically requests data from a randomally chosen server agent. The size of data that a client agent requests is configurable. The initial configuration is set to ~1.2 MB for each of the 50 noise generating clients, and ~40 MB for the control client, adding up to a total of ~100 MB.\n\n\n\n\nNow, to synthetically generate uncontrollable traffic, the size of data requested by the noise generating clients is changed randomly. We set the probability of change as: 80% - no change, 10% - increment, and 10% - reduction. The amount by which the fetch size is changed is calculated randomly.\n\n\nThen, to add control to this experiment, specifically on the traffic on the monitored link, we add a traffic monitor and a traffic generator (control client). We deploy a traffic monitor agent on the client-side end node of the link to be monitored. This agent continuously monitors all the traffic flowing through the node. Further, we attach a node (control client) to the traffic generator, and deploy a client agent on it.\n\n\nThe Orchestrator, based on the real time traffic feedback, sends change load messages to the control client agent, in order to maintain the total traffic on the monitored link within the set limits.\nEvent Streams\n\n\nThis example has six events streams; the server stream, the noise stream, the noise modify stream, the control client stream, the control stream and the duration stream.\n\n\nMapping to the Topology\n\uf0c1\n\n\nThe groups directive in the AAL file allows mapping a agent behavior to one or more nodes.\n\n\n#!php\ngroups:\n   server_group: \nslist [s-0, s-1, s-2, s-3, s-4, s-5, s-6, s-7, s-8, s-9]\n   noise_group: [uc-0, uc-1, uc-2, uc-3, uc-4, uc-5, uc-6, uc-7, uc-8, uc-9, uc-10,\n                     uc-11, uc-12, uc-13, uc-14, uc-15, uc-16, uc-17, uc-18, uc-19, uc-20,\n                     uc-21, uc-22, uc-23, uc-24, uc-25, uc-26, uc-27, uc-28, uc-29, uc-30,\n                     uc-31, uc-32, uc-33, uc-34, uc-35, uc-36, uc-37, uc-38, uc-39, uc-40,\n                     uc-41, uc-42, uc-43, uc-44, uc-45, uc-46, uc-47, uc-48, uc-49]\n   sensor_group: [rc]\n   client_group: [c-0]\n\n\n\n\nIn this example, we observe that there are four groups server_group, noise_group, sensor_group and client_group. The server_group consists of 10 servers. The noise_group consists of 50 noise generating clients. The sensor_group consists of the lone sensor node and the client_group consists of the controlling client. Additionally,we use yaml pointers to annotate the server_group as \u201cslist\u201d. The slist annotation is used to refer to the list of servers for configuring the client agents in the section below.\nConfiguring the Agents\n\n\nThere are four types of agents, server_agent, noise_agent, monitor_agent, and client_agent.\n\n\nEach of the server agent is used to start a web server serving garbage data of requested size. The noise agents are used to create the noise of the network. Each of the noise agent periodically fetches data of a randomally changing size from a ramdonly chosen server.\n\n\nserver_agent:\n  group: server_group\n  path: /share/magi/modules/apache/apache.tar.gz\n  execargs: []\n\nnoise_agent:\n  group: noise_group\n  path: /share/magi/modules/http_client/http_client.tar.gz\n  execargs: {servers: *slist, interval: '2', sizes: '120000'}\n\nmonitor_agent:\n  group: sensor_group\n  path: /proj/montage/modules/pktcounters/pktCountersAgent.tar.gz\n  execargs: {}\n\nclient_agent:\n  group: client_group\n  path: /share/magi/modules/http_client/http_client.tar.gz\n  execargs: {servers: *slist, interval: '2', sizes: '4000000'}\n\n\n\n\nServer Stream\n\uf0c1\n\n\nThe server event stream consists of three states. The start state which generates a trigger, called serverStarted, once all the server agents are activated on the experiment nodes.\n\n\nIt then enters the wait state where it waits for a trigger from the noise stream and the client event stream.\n\n\nOnce the trigger is received, it enters the stop state, where the server is deactivated or terminated.\nNoise Stream and Control Client Stream\n\n\nThe noise and client event streams consists of five states. First, the agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all the agent nodes.\n\n\nThe streams then synchronize with the server stream by waiting for the serverStarted trigger from the server nodes. Once they receive the trigger the agents are activated in the start state. Each agent fetches web pages from one of the listed servers.\n\n\nNext, the streams wait for the monitorStopped trigger from the monitorstream. Once the trigger is received the clients are instructed to stop fetching data.\n\n\nOn termination, the agents sends a noiseStopped/clientStopped trigger that allows the server stream to synchronize and terminate the servers, which is done only after all the http client agents have terminated.\nNoise Modify Stream\n\n\nThe noise modify stream starts once the noise generating agents start. It continously instructs the noise generating agents to randomly modify the amount of noise being generated. This is done to create an uncontrolled noise generation behaviour.\nControl Stream\n\n\nThe control stream starts once the control client agent has started. It configures the traffic monitoring agent and instucts it to start monitoring. Once the monitoring starts, this stream continously monitors the amount of traffic flowing on the monitored link, and based on the traffic information, instructs the control client to modify the amount of traffic it is pulling, in order to maintain the total traffic on the monitored link within the required limits.\nDuration Stream\n\n\nThe duration stream manages the time duration for which the experiment needs to run. Its starts once the monitor agent has started. The stream then waits for \u2206t before instructing the monitor agent stop and terminating all of the agents.\n\n\nRunning the Experiment\n\uf0c1\n\n\nStep 1: Set up your environment\n\uf0c1\n\n\nSet up your environment. Assuming your experiment is named myExp, your DETER project is myProj, and the AAL file is called procedure.aal.\n\n\n\nPROJ=myExp\nEXP=myProj\nAAL=procedure.aal\n\n\n\n\n\nStep 2: Set Up Containerized Experiment\n\uf0c1\n\n\nAs this experiment requires a lot of nodes, we should try and use containerized nodes. Create a containerized version of the experiment using this network description file: \ncasestudy_feedback.tcl\n\n\n\nNS=fb_topology.tcl\n\n\n /share/containers/containerize.py $PROJ $EXP $NS --packing=8 --openvz-diskspace 15G --pnode-type MicroCloud,pc2133\n\n\n\n\nStep 3: Swap in your Experiment\n\uf0c1\n\n\nSwap in the newly created experiment.\n\n\nStep 4: Run Orchestrator\n\uf0c1\n\n\nOnce the experiment is swapped in, run the Orchestrator, giving it this AAL: \ncasestudy_feedback.aal\n, the experiment name and project name. \n\n\n /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL\n\n\n\n\nOnce run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseFeedback\u201d:\n\n\n\n\n\n\nThe Orchestrator enacts an internally defined stream called initialization that is responsible for establishing all the groups and loading the agents. Once the agents are loaded, as indicated by the received trigger \nAgentLoadDone\n, the initialization stream is complete.\n\n\nNow all of the six above mentioned streams start concurrently.\n\n\nThe serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted.\n\n\nThe noiseStream and the controlClientStream on receiving the trigger serverStarted from the server_group, send the startClient event to the noise_group and the control_group, respectively. All memebers of both the groups start http clients and fire noiseStarted and controlClientStarted triggers.\n\n\nThe noiseModifyStream on receiving the noiseStarted trigger joins a loop that sends a changeTraffic event to the noise_group, every two seconds.\n\n\nThe control stream on receiving the controlClientStarted trigger sends a startCollection event to the monitor_group. The lone member of the monitor_group starts monitoring the interfaces on the node, and fires a monitorStarted trigger. The control stream then, joins a loop that sends a sense event to the monitor_group, every two seconds, and based on the return value in the response trigger intfSensed, sends a increaseTraffic or a reduceTraffic event to the control_group, if required.\n\n\nThe duration stream after receiving the monitorStarted trigger, waits for 5 minutes. On completion, it sends a stopCollection event to the monitor_group. The monitor agent stop monitoring and sends back a monitorStopped trigger.\n\n\nOnce the noiseStream and the controlClientStream recieve the monitorStopped trigger, they send out the stopClient event to their respective members. The http clients are stopped on all the members, and the noiseStopped and the controlClientStopped triggers are sent back to the orchestrator.\n\n\nThe serverStream, on receiving the noiseStopped and the controlClientStopped triggers, sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the durationStream.\n\n\nOn receiving the serverStopped trigger, the durationStream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups.\n\n\n\n\nThe experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment.\n\n\n\n\nProcedure:\n \ncasestudy_feedback.aal\n\n\nTopology:\n \ncasestudy_feedback.tcl\n\n\nArchive Logs:\n \ncasestudy_feedback.tar.gz\n\n\nOrchestration:\n \ncasestudy_feedback.orch.log\n\n\n\n\nVisualizing Experiment Results\n\uf0c1\n\n\nOffline: A traffic plot may be generated using the \nMAGI Graph Creation Tool\n.\n\n\nReal Time: A real time simulated traffic plot using canned data from a pre-run experiment may be visualized \nhere\n.\n\n\nA similar plot using live data may be plotted by \nvisiting the same web page\n, and additionally passing it the hostname of the database config node of your experiment.\n\n\nYou can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following.\n\n\n  \n cat /proj/myProject/exp/myExperiment/experiment.conf\n\n  dbdl:\n    configHost: node-1\n\n  expdl:\n    experimentName: myExperiment\n    projectName: myProject\n\n\n\n\nThen edit the simulated traffic plot URL, passing it the hostname.\n\n\n  host=node-1.myExperiment.myProject\n\n  http://\nweb-host\n/traffic.html?host=node-1.myExperiment.myProject", 
            "title": "Feedback Case Study"
        }, 
        {
            "location": "/orchestrator/feedback/#introduction", 
            "text": "This case study demonstrates our system\u2019s ability to do real-time feedback from the experiment using triggers. We show how information from the experiment may be used to extend the Orchestrator autonomy and deterministically control dynamic experiments. This is an example where the active state of an experiment is a driving input to the control. The data management layer too plays an important role in enabling the flow of information.  In this case study we show how, in a semi-controllable environment,  the amount of traffic on a given link may be controlled using feedback from the experiment itself . The traffic on one of the links in the experiment must be maintained within a certain range; for this example, the range was 100-105 MB. We assume that the uncontrollable traffic on the link would not exceed the required maximum.  This experiment is set up with the following characteristics:   the monitored link has some noise (uncontrollable traffic) flowing through it.   The noise has been artificially generated.   We set the noise generating clients to pull randomly changing amount of traffic from the servers, in order to enact the noise.   The solution is not dependent on the experiment topology. We deploy a traffic monitor agent on one of the end nodes of the link to be monitored. The traffic monitoring agent continuously monitors the traffic on the link. We also deploy a traffic generator agent (control client) that coordinates with the traffic monitor agent in real time and generates exactly the amount of traffic that would help maintain the total traffic on the link within the required limits.  To demonstrate this case study, we set up an experiment topology similar to the one seen in the following figure, with 50 noise generating clients and 10 servers. We also tested a scaled up version of the experiment with 300 noise generating agents and 100 servers. However, due to the resource constraints on the testbed, we recommend you try this case with the smaller topology first. The scaling up of the experiment may be achieved with very simple modifications.   We use two kinds of agents:   A server agent (located on the server nodes) that runs an apache server and serves random data of requested size.   A client agent (located on the client nodes) that periodically requests data from a randomally chosen server agent. The size of data that a client agent requests is configurable. The initial configuration is set to ~1.2 MB for each of the 50 noise generating clients, and ~40 MB for the control client, adding up to a total of ~100 MB.   Now, to synthetically generate uncontrollable traffic, the size of data requested by the noise generating clients is changed randomly. We set the probability of change as: 80% - no change, 10% - increment, and 10% - reduction. The amount by which the fetch size is changed is calculated randomly.  Then, to add control to this experiment, specifically on the traffic on the monitored link, we add a traffic monitor and a traffic generator (control client). We deploy a traffic monitor agent on the client-side end node of the link to be monitored. This agent continuously monitors all the traffic flowing through the node. Further, we attach a node (control client) to the traffic generator, and deploy a client agent on it.  The Orchestrator, based on the real time traffic feedback, sends change load messages to the control client agent, in order to maintain the total traffic on the monitored link within the set limits.\nEvent Streams  This example has six events streams; the server stream, the noise stream, the noise modify stream, the control client stream, the control stream and the duration stream.", 
            "title": "Introduction"
        }, 
        {
            "location": "/orchestrator/feedback/#mapping-to-the-topology", 
            "text": "The groups directive in the AAL file allows mapping a agent behavior to one or more nodes.  #!php\ngroups:\n   server_group:  slist [s-0, s-1, s-2, s-3, s-4, s-5, s-6, s-7, s-8, s-9]\n   noise_group: [uc-0, uc-1, uc-2, uc-3, uc-4, uc-5, uc-6, uc-7, uc-8, uc-9, uc-10,\n                     uc-11, uc-12, uc-13, uc-14, uc-15, uc-16, uc-17, uc-18, uc-19, uc-20,\n                     uc-21, uc-22, uc-23, uc-24, uc-25, uc-26, uc-27, uc-28, uc-29, uc-30,\n                     uc-31, uc-32, uc-33, uc-34, uc-35, uc-36, uc-37, uc-38, uc-39, uc-40,\n                     uc-41, uc-42, uc-43, uc-44, uc-45, uc-46, uc-47, uc-48, uc-49]\n   sensor_group: [rc]\n   client_group: [c-0]  In this example, we observe that there are four groups server_group, noise_group, sensor_group and client_group. The server_group consists of 10 servers. The noise_group consists of 50 noise generating clients. The sensor_group consists of the lone sensor node and the client_group consists of the controlling client. Additionally,we use yaml pointers to annotate the server_group as \u201cslist\u201d. The slist annotation is used to refer to the list of servers for configuring the client agents in the section below.\nConfiguring the Agents  There are four types of agents, server_agent, noise_agent, monitor_agent, and client_agent.  Each of the server agent is used to start a web server serving garbage data of requested size. The noise agents are used to create the noise of the network. Each of the noise agent periodically fetches data of a randomally changing size from a ramdonly chosen server.  server_agent:\n  group: server_group\n  path: /share/magi/modules/apache/apache.tar.gz\n  execargs: []\n\nnoise_agent:\n  group: noise_group\n  path: /share/magi/modules/http_client/http_client.tar.gz\n  execargs: {servers: *slist, interval: '2', sizes: '120000'}\n\nmonitor_agent:\n  group: sensor_group\n  path: /proj/montage/modules/pktcounters/pktCountersAgent.tar.gz\n  execargs: {}\n\nclient_agent:\n  group: client_group\n  path: /share/magi/modules/http_client/http_client.tar.gz\n  execargs: {servers: *slist, interval: '2', sizes: '4000000'}", 
            "title": "Mapping to the Topology"
        }, 
        {
            "location": "/orchestrator/feedback/#server-stream", 
            "text": "The server event stream consists of three states. The start state which generates a trigger, called serverStarted, once all the server agents are activated on the experiment nodes.  It then enters the wait state where it waits for a trigger from the noise stream and the client event stream.  Once the trigger is received, it enters the stop state, where the server is deactivated or terminated.\nNoise Stream and Control Client Stream  The noise and client event streams consists of five states. First, the agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all the agent nodes.  The streams then synchronize with the server stream by waiting for the serverStarted trigger from the server nodes. Once they receive the trigger the agents are activated in the start state. Each agent fetches web pages from one of the listed servers.  Next, the streams wait for the monitorStopped trigger from the monitorstream. Once the trigger is received the clients are instructed to stop fetching data.  On termination, the agents sends a noiseStopped/clientStopped trigger that allows the server stream to synchronize and terminate the servers, which is done only after all the http client agents have terminated.\nNoise Modify Stream  The noise modify stream starts once the noise generating agents start. It continously instructs the noise generating agents to randomly modify the amount of noise being generated. This is done to create an uncontrolled noise generation behaviour.\nControl Stream  The control stream starts once the control client agent has started. It configures the traffic monitoring agent and instucts it to start monitoring. Once the monitoring starts, this stream continously monitors the amount of traffic flowing on the monitored link, and based on the traffic information, instructs the control client to modify the amount of traffic it is pulling, in order to maintain the total traffic on the monitored link within the required limits.\nDuration Stream  The duration stream manages the time duration for which the experiment needs to run. Its starts once the monitor agent has started. The stream then waits for \u2206t before instructing the monitor agent stop and terminating all of the agents.", 
            "title": "Server Stream"
        }, 
        {
            "location": "/orchestrator/feedback/#running-the-experiment", 
            "text": "Step 1: Set up your environment \uf0c1  Set up your environment. Assuming your experiment is named myExp, your DETER project is myProj, and the AAL file is called procedure.aal.  \nPROJ=myExp\nEXP=myProj\nAAL=procedure.aal  Step 2: Set Up Containerized Experiment \uf0c1  As this experiment requires a lot of nodes, we should try and use containerized nodes. Create a containerized version of the experiment using this network description file:  casestudy_feedback.tcl  \nNS=fb_topology.tcl  /share/containers/containerize.py $PROJ $EXP $NS --packing=8 --openvz-diskspace 15G --pnode-type MicroCloud,pc2133  Step 3: Swap in your Experiment \uf0c1  Swap in the newly created experiment.  Step 4: Run Orchestrator \uf0c1  Once the experiment is swapped in, run the Orchestrator, giving it this AAL:  casestudy_feedback.aal , the experiment name and project name.    /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL  Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseFeedback\u201d:    The Orchestrator enacts an internally defined stream called initialization that is responsible for establishing all the groups and loading the agents. Once the agents are loaded, as indicated by the received trigger  AgentLoadDone , the initialization stream is complete.  Now all of the six above mentioned streams start concurrently.  The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted.  The noiseStream and the controlClientStream on receiving the trigger serverStarted from the server_group, send the startClient event to the noise_group and the control_group, respectively. All memebers of both the groups start http clients and fire noiseStarted and controlClientStarted triggers.  The noiseModifyStream on receiving the noiseStarted trigger joins a loop that sends a changeTraffic event to the noise_group, every two seconds.  The control stream on receiving the controlClientStarted trigger sends a startCollection event to the monitor_group. The lone member of the monitor_group starts monitoring the interfaces on the node, and fires a monitorStarted trigger. The control stream then, joins a loop that sends a sense event to the monitor_group, every two seconds, and based on the return value in the response trigger intfSensed, sends a increaseTraffic or a reduceTraffic event to the control_group, if required.  The duration stream after receiving the monitorStarted trigger, waits for 5 minutes. On completion, it sends a stopCollection event to the monitor_group. The monitor agent stop monitoring and sends back a monitorStopped trigger.  Once the noiseStream and the controlClientStream recieve the monitorStopped trigger, they send out the stopClient event to their respective members. The http clients are stopped on all the members, and the noiseStopped and the controlClientStopped triggers are sent back to the orchestrator.  The serverStream, on receiving the noiseStopped and the controlClientStopped triggers, sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the durationStream.  On receiving the serverStopped trigger, the durationStream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups.   The experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment.   Procedure:   casestudy_feedback.aal  Topology:   casestudy_feedback.tcl  Archive Logs:   casestudy_feedback.tar.gz  Orchestration:   casestudy_feedback.orch.log", 
            "title": "Running the Experiment"
        }, 
        {
            "location": "/orchestrator/feedback/#visualizing-experiment-results", 
            "text": "Offline: A traffic plot may be generated using the  MAGI Graph Creation Tool .  Real Time: A real time simulated traffic plot using canned data from a pre-run experiment may be visualized  here .  A similar plot using live data may be plotted by  visiting the same web page , and additionally passing it the hostname of the database config node of your experiment.  You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following.      cat /proj/myProject/exp/myExperiment/experiment.conf\n\n  dbdl:\n    configHost: node-1\n\n  expdl:\n    experimentName: myExperiment\n    projectName: myProject  Then edit the simulated traffic plot URL, passing it the hostname.    host=node-1.myExperiment.myProject\n\n  http:// web-host /traffic.html?host=node-1.myExperiment.myProject", 
            "title": "Visualizing Experiment Results"
        }, 
        {
            "location": "/orchestrator/writing-agents/", 
            "text": "This page gives you a brief introduction on writing your own Magi Agent. It is designed to give you sample code, briefly explain it, then show you the pieces needed to run it. After reading this page you should be able to write and run a basic MAGI Agent. Further details and more advanced agent information may be found in the Magi Agent Library document (link).\n\n\nBasic Agent Information\n\uf0c1\n\n\nAn Agent runs in two modes: a threaded mode and a process mode.\n\n\n\n\nThreaded\n mode: The MAGI Daemon loads python codes directly, and runs the Agent in a thread in its own process space.\n\n\nProcess\n mode: The MAGI Daemon runs the agent in a process space separate from itself and communicates with the Agent via a pipe or a socket.\n\n\n\n\nDispatchAgent\n class\n\uf0c1\n\n\nIn most cases you will want to use the Orchestrator (link) and an AAL file (link) to run and coordinate your Agent actions. In order to get the basic Agent control (via remote procedure calls), you\u2019ll need to derive your agent from the \nDispatchAgent\n base class.\n\n\nThe \nDispatchAgent\n implements the simple remote procedure call (RPC) mechanism used by the Orchestrator (and available through the MAGI python API). This allows any method in a class derived from \nDispatchAgent\n to be invoked in an AAL (or by a MagiMessage if using the MAGI python interface directly). \n\n\nThe \nDispatchAgent\n code simply loops, parsing incoming messages, looking for an event message. When it finds one, it attempts to call the method specified in the message with the arguments given. You needn\u2019t worry about message handling or parsing when you derive from \nDispatchAgent\n, you can simply write your code and call that code from the AAL.\n\n\nBasic Elements of Writing a Client\n\uf0c1\n\n\nTo write and execute your agent you need the following three things:\n\n\n\n\nThe Agent code to implement the Agent. Also, every Agent must implement a method named \ngetAgent\n which returns an instance of the Agent to run. The MAGI Daemon uses this method to get an instance of the Agent to run in a local thread and communicate with the Agent instance.\n\n\nAn Interface Description Language (IDL) file to describe the Agent function and specify things the MAGI Daemon needs to know to load and execute the Agent code (among these is the location of the Agent code and the execution mode).\n\n\nAn AAL file (as described \nhere\n)\n\n\n\n\nDeploying and Executing a Sample Agent\n\uf0c1\n\n\nStep 1: Create a local directory named \u201cFileCreator\u201d\n\uf0c1\n\n\nMAGI Agents are usually contained in a single directory.\n\n\nStep 2: Create the Agent implementation code file\n\uf0c1\n\n\nCopy the following Agent implementation code to the file \u201cFileCreator/FileCreator.py\u201d.\n\n\nThis example Agent code has the following characteristics:\n\n\n\n\nIt creates a simple Agent which creates a single file on a host.\n\n\nThe agent is called FileCreator.\n\n\nIt has a single method, \ncreateFile\n, which creates the file \n/tmp/newfile\n when called\n\n\nFor this agent, we will always run in threaded-mode.\n\n\n\n\n\n    from magi.util.agent import DispatchAgent, agentmethod\n\n    # the getAgent() method must be defined somewhere for all agents.\n    # The Magi daemon invokes this mehod to get a reference to an\n    # agent. It uses this reference to run and interact with an agent\n    # instance. (The cotangent() call is generally the same for all agent\n    # implementations: it just returns an instance of the agent.)\n    def getAgent():\n        return FileCreator()\n\n    # The FileCreator agent implementation, derived from DispatchAgent.\n    class FileCreator(DispatchAgent):\n        def __init__(self):\n            DispatchAgent.__init__(self)\n            self.filename = '/tmp/newfile'\n\n        # A single method which creates the file named by self.filename.\n        # (The @agentmethod() decorator is not required, but is encouraged.\n        #  it does nothing of substance now, but may in the future.)\n        @agentmethod()\n        def createFile(self, msg):\n            **Create a file on the host.**\n            # open and immediately close the file to create it.\n            open(self.filename, 'w').close()\n\n\n\n\n\nStep 3: Create the IDL file\n\uf0c1\n\n\nCopy the IDL below to a file named \u201cFileCreator/FileCreator.idl\u201d. \n\n\n\n\nNote\n\n\nThe file and directory may be named anything, but if you deviate from the naming scheme given, make sure the mainfile setting in the IDL and the code setting in your AAL (below) matches your naming scheme.\n\n\n\n\nThe following example IDL file has the following characteristics:\n\n\n\n\nThe execution mode is \u201cthread\u201d and the inheritance is specified as \u201cDispatchAgent\u201d.\n\n\nWhen you run this, you must specify the name of your implementation file (i.e., the Agent code from the previous step). This example assumes the file is in the local directory and is named \u201cFileCreator.py\u201d.\n\n\nIt lists methods and internal variables that the author wants exposed to external configuration. In our case, we expose the variable \nfilename\n, but currently only use the default setting. Later we will describe how to set this outside of the Agent implementation.\n\n\n\n\n    name: FileCreator\n    display: File Creator Agent\n    description:  This agent creates a file on the test node.\n    execute: thread\n    mainfile: FileCreator.py\n    inherits:\n       - DispatchAgent\n\n    methods:\n       - name: createFile\n         help: Create the file\n         args: []\n\n    variables:\n       - name: filename\n         help: the full path of the file to create\n         type: string\n\n\n\n\nStep 4: Create the AAL file\n\uf0c1\n\n\nCopy the sample AAL code below to a file named \u201cFileCreator/myEvents.aal\u201d.\n\n\nMake sure to:\n\n\n\n\nReplace \nPATH\n with the full path to your \u201cFileCreator\u201d directory. Note: the PATH you use must include the NFS-mounted location on the test nodes. \n\n\nReplace \nNODES\n with the comma-separated list of nodes on your testbed on which you want to run the Agent.\n\n\n\n\n    streamstarts: [main]\n\n    groups:\n        myFileCreatorGroup [NODES]\n\n    agents:\n        myFileCreators:\n            group: myFileCreatorGroup\n            # (note: the \ncode\n argument is the Agent directory. The\n            # directory must contain an IDL and Agent implementation.)\n            code: PATH/FileCreator\n            execargs: []\n\n    eventstreams:\n        main:\n            - type: event\n              agent: myFileCreators\n              method: createFile\n              args: { }\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe AAL is in YAML format; therefore, it cannot have tabs. If you cut and paste the above code, make sure to remove any possible inserted tabs.\n\n\nBecause your Agent code is on an NFS-mounted filesystem, all MAGI Daemons may read the code directly.\n\n\n\n\n\n\nStep 5: Run Orchestrator\n\uf0c1\n\n\nRun the MAGI Orchestrator to run the event streams in your AAL file - and thus your agent code:\n\n\nmagi_orchestrator --control $control --events myEvents.aal -o run.log -v\n\n\n\n\nWhere \n$control\n is the fully qualified domain of your DETERLab node, i.e. \nmyNode.myGroup.myProject\n}\n\n\nThis command runs the Orchestrator, which connects to the \n$control\n node, runs the events in the \nmyEvents.aal\n file and writes verbose output to \nrun.log\n.\n\n\nIn this example, the method \ncreateFile\n will be called on all test nodes associated with \nmyAgentGroup\n in the AAL file.\n\n\nOn standard out, you should see Orchestrator output similar to the following:\n\n\nstream initialization : sent : joinGroup myFileCreatorGroup --\n __ALL__\nstream initialization : done : trigger GroupBuildDone myFileCreatorGroup  complete.\nstream initialization : sent : loadAgent myFileCreators --\n myFileCreatorGroup\nstream initialization : done : trigger AgentLoadDone  myFileCreators complete.\nstream initialization : DONE : complete.\nstream main           : sent : createFile(None) --\n myFileCreatorGroup\nstream main           : DONE : complete.\n\n\n\n\n\nThis output shows the two execution streams the orchestrator runs. The first, \ninitialization\n, is internal to the Orchestrator and sets up group communication and loads the agents. The second, \nmain\n, is the event stream specified in your AAL file.\n\n\nIf you do not see the \u201ccorrect\u201d output above, refer to the [#troubleshooting Troubleshooting] section below. \n\n\nTo confirm the Agent ran and executed the \ncreateFile\n method, run the following (from \nusers\n):\n\n\nssh myNode.myExperiment.myGroup ls -l /tmp/newfile\n\n\n\n\nWhere \nmyNode.myExperiment.myGroup\n is the domain name of a node on which you executed the Agent.\n\n\nYou may download the sample code as a tar file here: [FileCreator.tbz] (attach).\n\n\nRuntime Agent Configuration\n\uf0c1\n\n\nThe sample Agent \nFileCreator\n always creates the same file, each time it is run. What if you wanted to create a different file? Or a series of files? It is possible to specify Agent configuration in an AAL file - configuration that can modify the internal variables of your Agent at run time. See \nMAGI Agent Configuration\n for details.\n\n\nTroubleshooting\n\uf0c1\n\n\nLook at the Magi Daemon log file at \n/var/log/magi/daemon.log\n on your control and test nodes looking for errors. If there are syntax errors in Agent execution, they may show up here.\n\n\nError: You see a \u2018No such file\u2019 error\n\uf0c1\n\n\nYou may see an error indicating there is no such file as follows:\n\n\n\nRun-time exception in agent \nDaemon(daemon, started 140555403872000)\n on node(s) control in method __init__, line 71, in file threadInterface.py. Error: [Errno 2] No such file or directory\n\n\n\n\n\nSolution:\n You probably did not specify the correct \u201cmainfile\u201d in the IDL file. It must not be pathed out and must match the name of the main Agent implementation file, for example, \u201cmyAgent.py\u201d.\n\n\nError: You see a \u201cno attribute \u2018getAgent\u2019\u201d message\n\uf0c1\n\n\nSolution:\n The Magi Daemon needs the well-known method \ngetAgent\n to exist in the Agent module. Add it to your Agent code.\n\n\nError: 'module' object has no attribute 'getAgent'\n\uf0c1\n\n\nSolution:\n Make sure your agent defines (and exports or make available) a \ngetAgent\n method and that it returns an instance of your agent.", 
            "title": "Writing Your Own MAGI Agents"
        }, 
        {
            "location": "/orchestrator/writing-agents/#basic-agent-information", 
            "text": "An Agent runs in two modes: a threaded mode and a process mode.   Threaded  mode: The MAGI Daemon loads python codes directly, and runs the Agent in a thread in its own process space.  Process  mode: The MAGI Daemon runs the agent in a process space separate from itself and communicates with the Agent via a pipe or a socket.", 
            "title": "Basic Agent Information"
        }, 
        {
            "location": "/orchestrator/writing-agents/#dispatchagent-class", 
            "text": "In most cases you will want to use the Orchestrator (link) and an AAL file (link) to run and coordinate your Agent actions. In order to get the basic Agent control (via remote procedure calls), you\u2019ll need to derive your agent from the  DispatchAgent  base class.  The  DispatchAgent  implements the simple remote procedure call (RPC) mechanism used by the Orchestrator (and available through the MAGI python API). This allows any method in a class derived from  DispatchAgent  to be invoked in an AAL (or by a MagiMessage if using the MAGI python interface directly).   The  DispatchAgent  code simply loops, parsing incoming messages, looking for an event message. When it finds one, it attempts to call the method specified in the message with the arguments given. You needn\u2019t worry about message handling or parsing when you derive from  DispatchAgent , you can simply write your code and call that code from the AAL.", 
            "title": "DispatchAgent class"
        }, 
        {
            "location": "/orchestrator/writing-agents/#basic-elements-of-writing-a-client", 
            "text": "To write and execute your agent you need the following three things:   The Agent code to implement the Agent. Also, every Agent must implement a method named  getAgent  which returns an instance of the Agent to run. The MAGI Daemon uses this method to get an instance of the Agent to run in a local thread and communicate with the Agent instance.  An Interface Description Language (IDL) file to describe the Agent function and specify things the MAGI Daemon needs to know to load and execute the Agent code (among these is the location of the Agent code and the execution mode).  An AAL file (as described  here )", 
            "title": "Basic Elements of Writing a Client"
        }, 
        {
            "location": "/orchestrator/writing-agents/#deploying-and-executing-a-sample-agent", 
            "text": "Step 1: Create a local directory named \u201cFileCreator\u201d \uf0c1  MAGI Agents are usually contained in a single directory.  Step 2: Create the Agent implementation code file \uf0c1  Copy the following Agent implementation code to the file \u201cFileCreator/FileCreator.py\u201d.  This example Agent code has the following characteristics:   It creates a simple Agent which creates a single file on a host.  The agent is called FileCreator.  It has a single method,  createFile , which creates the file  /tmp/newfile  when called  For this agent, we will always run in threaded-mode.   \n    from magi.util.agent import DispatchAgent, agentmethod\n\n    # the getAgent() method must be defined somewhere for all agents.\n    # The Magi daemon invokes this mehod to get a reference to an\n    # agent. It uses this reference to run and interact with an agent\n    # instance. (The cotangent() call is generally the same for all agent\n    # implementations: it just returns an instance of the agent.)\n    def getAgent():\n        return FileCreator()\n\n    # The FileCreator agent implementation, derived from DispatchAgent.\n    class FileCreator(DispatchAgent):\n        def __init__(self):\n            DispatchAgent.__init__(self)\n            self.filename = '/tmp/newfile'\n\n        # A single method which creates the file named by self.filename.\n        # (The @agentmethod() decorator is not required, but is encouraged.\n        #  it does nothing of substance now, but may in the future.)\n        @agentmethod()\n        def createFile(self, msg):\n            **Create a file on the host.**\n            # open and immediately close the file to create it.\n            open(self.filename, 'w').close()  Step 3: Create the IDL file \uf0c1  Copy the IDL below to a file named \u201cFileCreator/FileCreator.idl\u201d.    Note  The file and directory may be named anything, but if you deviate from the naming scheme given, make sure the mainfile setting in the IDL and the code setting in your AAL (below) matches your naming scheme.   The following example IDL file has the following characteristics:   The execution mode is \u201cthread\u201d and the inheritance is specified as \u201cDispatchAgent\u201d.  When you run this, you must specify the name of your implementation file (i.e., the Agent code from the previous step). This example assumes the file is in the local directory and is named \u201cFileCreator.py\u201d.  It lists methods and internal variables that the author wants exposed to external configuration. In our case, we expose the variable  filename , but currently only use the default setting. Later we will describe how to set this outside of the Agent implementation.       name: FileCreator\n    display: File Creator Agent\n    description:  This agent creates a file on the test node.\n    execute: thread\n    mainfile: FileCreator.py\n    inherits:\n       - DispatchAgent\n\n    methods:\n       - name: createFile\n         help: Create the file\n         args: []\n\n    variables:\n       - name: filename\n         help: the full path of the file to create\n         type: string  Step 4: Create the AAL file \uf0c1  Copy the sample AAL code below to a file named \u201cFileCreator/myEvents.aal\u201d.  Make sure to:   Replace  PATH  with the full path to your \u201cFileCreator\u201d directory. Note: the PATH you use must include the NFS-mounted location on the test nodes.   Replace  NODES  with the comma-separated list of nodes on your testbed on which you want to run the Agent.       streamstarts: [main]\n\n    groups:\n        myFileCreatorGroup [NODES]\n\n    agents:\n        myFileCreators:\n            group: myFileCreatorGroup\n            # (note: the  code  argument is the Agent directory. The\n            # directory must contain an IDL and Agent implementation.)\n            code: PATH/FileCreator\n            execargs: []\n\n    eventstreams:\n        main:\n            - type: event\n              agent: myFileCreators\n              method: createFile\n              args: { }   Note   The AAL is in YAML format; therefore, it cannot have tabs. If you cut and paste the above code, make sure to remove any possible inserted tabs.  Because your Agent code is on an NFS-mounted filesystem, all MAGI Daemons may read the code directly.    Step 5: Run Orchestrator \uf0c1  Run the MAGI Orchestrator to run the event streams in your AAL file - and thus your agent code:  magi_orchestrator --control $control --events myEvents.aal -o run.log -v  Where  $control  is the fully qualified domain of your DETERLab node, i.e.  myNode.myGroup.myProject }  This command runs the Orchestrator, which connects to the  $control  node, runs the events in the  myEvents.aal  file and writes verbose output to  run.log .  In this example, the method  createFile  will be called on all test nodes associated with  myAgentGroup  in the AAL file.  On standard out, you should see Orchestrator output similar to the following:  stream initialization : sent : joinGroup myFileCreatorGroup --  __ALL__\nstream initialization : done : trigger GroupBuildDone myFileCreatorGroup  complete.\nstream initialization : sent : loadAgent myFileCreators --  myFileCreatorGroup\nstream initialization : done : trigger AgentLoadDone  myFileCreators complete.\nstream initialization : DONE : complete.\nstream main           : sent : createFile(None) --  myFileCreatorGroup\nstream main           : DONE : complete.  This output shows the two execution streams the orchestrator runs. The first,  initialization , is internal to the Orchestrator and sets up group communication and loads the agents. The second,  main , is the event stream specified in your AAL file.  If you do not see the \u201ccorrect\u201d output above, refer to the [#troubleshooting Troubleshooting] section below.   To confirm the Agent ran and executed the  createFile  method, run the following (from  users ):  ssh myNode.myExperiment.myGroup ls -l /tmp/newfile  Where  myNode.myExperiment.myGroup  is the domain name of a node on which you executed the Agent.  You may download the sample code as a tar file here: [FileCreator.tbz] (attach).", 
            "title": "Deploying and Executing a Sample Agent"
        }, 
        {
            "location": "/orchestrator/writing-agents/#runtime-agent-configuration", 
            "text": "The sample Agent  FileCreator  always creates the same file, each time it is run. What if you wanted to create a different file? Or a series of files? It is possible to specify Agent configuration in an AAL file - configuration that can modify the internal variables of your Agent at run time. See  MAGI Agent Configuration  for details.", 
            "title": "Runtime Agent Configuration"
        }, 
        {
            "location": "/orchestrator/writing-agents/#troubleshooting", 
            "text": "Look at the Magi Daemon log file at  /var/log/magi/daemon.log  on your control and test nodes looking for errors. If there are syntax errors in Agent execution, they may show up here.  Error: You see a \u2018No such file\u2019 error \uf0c1  You may see an error indicating there is no such file as follows:  \nRun-time exception in agent  Daemon(daemon, started 140555403872000)  on node(s) control in method __init__, line 71, in file threadInterface.py. Error: [Errno 2] No such file or directory  Solution:  You probably did not specify the correct \u201cmainfile\u201d in the IDL file. It must not be pathed out and must match the name of the main Agent implementation file, for example, \u201cmyAgent.py\u201d.  Error: You see a \u201cno attribute \u2018getAgent\u2019\u201d message \uf0c1  Solution:  The Magi Daemon needs the well-known method  getAgent  to exist in the Agent module. Add it to your Agent code.  Error: 'module' object has no attribute 'getAgent' \uf0c1  Solution:  Make sure your agent defines (and exports or make available) a  getAgent  method and that it returns an instance of your agent.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/orchestrator/agent-configuration/", 
            "text": "In \nWriting MAGI Agents\n, you saw how to create a basic agent. The sample agent created a single file on a test node. This document will explain how to use configuration in the AAL file to configure an agent at runtime.\n\n\nSetting Agent Configuration\n\uf0c1\n\n\nThis document will expand the sample code of our \nFileCreator\n example. For reference, here is the agent code:\n\n\n  from magi.util.agent import DispatchAgent, agentmethod\n  from magi.util.processAgent import initializeProcessAgent\n\n  # The FileCreator agent implementation, derived from DispatchAgent.\n  class FileCreator(DispatchAgent):\n      def __init__(self):\n          DispatchAgent.__init__(self)\n          self.filename = '/tmp/newfile'\n\n      # A single method which creates the file named by self.filename.\n      # (The @agentmethod() decorator is not required, but is encouraged.\n      #  it does nothing of substance now, but may in the future.)\n      @agentmethod()\n      def createFile(self, msg):\n          **Create a file on the host.**\n          # open and immediately close the file to create it.\n          open(self.filename, 'w').close()\n\n  # the getAgent() method must be defined somewhere for all agents.\n  # The Magi daemon invokes this mehod to get a reference to an\n  # agent. It uses this reference to run and interact with an agent\n  # instance.\n  def getAgent():\n      agent = FileCreator()\n      return agent\n\n  # In case the agent is run as a separate process, we need to\n  # create an instance of the agent, initialize the required\n  # parameters based on the received arguments, and then call the\n  # run method defined in DispatchAgent.\n  if __name__ \n__main__\n:\n      agent = FileCreator()\n      initializeProcessAgent(agent, sys.argv)\n      agent.run()\n\n\n\n\n\nIf we reset the \nself.filename\n variable in the agent \nbefore\n invoking \ncreateFile\n in the AAL, we can change the file that is created. \n\n\nThe base class \nDispatchAgent\n itself is derived from a class that will let us do this. The Agent class implements two methods: \n\n\n\n\nsetConfiguration\n -  Sets the passed parameters as class instance variables.\n\n\nconfirmConfiguration\n - This method is meant to be re-implemented in your agent if you need confirm the variables set are valid for your agent.\n\n\n\n\nTo set the \nself.filename\n variable in the FileCreator Agents, we modify the AAL to include a call to the Agent method \nsetConfiguration\n, passing in a list of key-value pairs. (In the following example, it is a single key-value pair.)\n\n\n- type: event\n  agent: myFileCreators\n  method: setConfiguration\n  args:\n     filename: /tmp/myCreatedFile\n\n\n\n\n\nNote that you do not specify self when referencing an Agent variable. We make sure to place this event in the AAL event stream prior to the \ncreateFile\n event. The complete AAL file is:\n\n\n  streamstarts: [main]\n\n  groups:\n      myFileCreatorGroup: [NODES]\n\n  agents:\n      myFileCreators:\n          group: myFileCreatorGroup\n          # (note: the \nPATH\n argument is the agent directory. The\n          # directory must contain an IDL and agent implementation. It must\n          # also contain a *__init__.py* file, which is required\n          # for it to be considered as a valid python package.)\n          path: PATH/FileCreator\n          execargs: []\n\n  eventstreams:\n      main:\n          - type: event\n            agent: myFileCreators\n            method: setConfiguration\n            args:\n               filename: /tmp/myCreatedFile\n\n          - type: event\n            agent: myFileCreators\n            method: createFile\n            args: {}\n\n\n\n\n\nNow when we run the Agent again (possibly using \nagentTool\n to restart the Magi daemons), we see the following events:\n\n\n  $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v\n  stream initialization : sent : joinGroup myFileCreatorGroup --\n __ALL__\n  stream initialization : done : trigger GroupBuildDone myFileCreatorGroup  complete.\n  stream initialization : sent : loadAgent myFileCreators --\n myFileCreatorGroup\n  stream initialization : done : trigger AgentLoadDone  myFileCreators complete.\n  stream initialization : DONE : complete.\n  stream main           : sent : setConfiguration(['/tmp/myCreatedFile ... ) --\n myFileCreatorGroup\n  stream main           : sent : createFile(None) --\n myFileCreatorGroup\n  stream main           : DONE : complete.\n  $ ssh myNode.myExperiment.myGroup ls -l /tmp/myCreatedFile\n  -rw-r--r-- 1 root root 0 Mar  5 13:55 /tmp/myCreatedFile\n  $\n\n\n\n\nAnd we see that our specified file, \n/tmp/myCreatedFile\n was created.\n\n\nConfirming Valid Configuration\n\uf0c1\n\n\nThis works well, but the input to the Agent is free-form. What if the user gives invalid input, like the wrong type or data that is not in a valid range? This is where the Agent \nconfirmConfiguration\n method comes into play.\n\n\nconfirmConfiguration\n should be written for any Agent that wants to validate its state. It gets invoked in the AAL file after the user invokes \nsetConfiguration\n.\n\n\nNote:\n The concept of an Agent confirming user input will change in future releases of MAGI. The Orchestrator (or other MAGI/Montage components) will use the interface specification in the Agent\u2019s IDL file to ensure the input to the agent is valid.\n\n\nSuppose our sample agent wanted to allow the user to create a file in only the \n/local\n directory on the host machine. The \nconfirmConfiguration\n method that does this is:\n\n\n  def confirmConfiguration(self):\n      **Make sure the user input is a string value and starts with\n      \n/local\n.**\n      if not isinstance(self.filename, (str, unicode)):\n          return False\n\n      if not self.filename.startswith('/local'):\n          return False\n\n      return True\n\n\n\n\nWhen we add this method to our sample Agent, and run the experiment with the existing AAL file, which contains configuration that does not start with \n/local\n, the Orchestrator gives us an error while executing the event stream:\n\n\n  $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v\n  stream initialization : sent : joinGroup myFileCreatorGroup --\n __ALL__\n  stream initialization : done : trigger GroupBuildDone myFileCreatorGroup  complete.\n  stream initialization : sent : loadAgent myFileCreators --\n myFileCreatorGroup\n  stream initialization : done : trigger AgentLoadDone  myFileCreators complete.\n  stream initialization : DONE : complete.\n  stream main           : sent : setConfiguration(['/tmp/myCreatedFile ... ) --\n myFileCreatorGroup\n  stream unknown        : exit : method setConfiguration returned False on agent unknown in group unknown and on node(s): moat.\n  $\n\n\n\n\nThe Orchestrator exited with an error, as it should.\n\n\nIf we now modify the AAL file to include a valid configuration, the Orchestrator succeeds. The updated AAL fragment is:\n\n\n  - type: event\n    agent: myFileCreators\n    method: setConfiguration\n    args:\n       filename: /local/myGreatFile\n\n\n\n\nWhen we run the Orchestrator with the modified AAL, it succeeds as the agent configuration is now valid:\n\n\n  $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v\n  stream initialization : sent : joinGroup myFileCreatorGroup --\n __ALL__\n  stream initialization : done : trigger GroupBuildDone myFileCreatorGroup  complete.\n  stream initialization : sent : loadAgent myFileCreators --\n myFileCreatorGroup\n  stream initialization : done : trigger AgentLoadDone  myFileCreators complete.\n  stream initialization : DONE : complete.\n  stream main           : sent : setConfiguration(['/local/myGreatFile ... ) --\n myFileCreatorGroup\n  stream main           : sent : createFile(None) --\n myFileCreatorGroup\n  stream main           : DONE : complete.\n  $\n\n\n\n\nAnd the \u201cvalid\u201d file has been created on the machine:\n\n\n  $ ssh myNode.myExperiment.myGroup ls -l /local\n  total 4\n  drwxrwxr-x 2 glawler Deter 4096 Mar  5 08:35 logs\n  -rw-r--r-- 1 root    root     0 Mar  5 14:33 myGreatFile\n  $\n\n\n\n\n\nTriggers and Event Stream Sequence Points\n\uf0c1\n\n\nIf you run the AAL and Agent code above, you may see that it does not actually work. One small needed detail has been left out of the AAL file. Normally the Orchestrator will run through the events in the AAL as fast is it can. If we used the event streams in the AAL file as it now stands:\n\n\n  eventstreams:\n      main:\n          - type: event\n            agent: myFileCreators\n            method: setConfiguration\n            args:\n                filename: /local/myGreatFile\n\n          - type: event\n            agent: myFileCreators\n            method: createFile\n            args: {}\n\n\n\n\nThe Orchestrator will send two messages to the Agents in rapid succession: the \nsetConfiguration\n and \ncreateFile\n event messages. If the \nsetConfiguration\n call returns \nFalse\n, which it will given invalid input, the Orchestrator will not receive the message because would have sent the messages and exited. \n\n\nTherefore, we need a way to tell the Orchestrator to wait for a response from \nsetConfiguration\n before continuing. This is done by inserting a small pause, using a trigger which times out after 3 seconds:\n\n\n  # Wait 3 seconds for a response to setConfiguration\n  # timeout value is in milliseconds.\n  - type: trigger\n    triggers: [{timeout: 3000}]\n\n\n\n\nIf we insert this trigger between \nsetConfiguration\n and \ncreateFile\n, the Orchestrator will receive the error message from the agent and exit on error.\n\n\nThe full AAL file now is:\n\n\n  streamstarts: [main]\n\n  groups:\n      myFileCreatorGroup: [witch, moat]\n\n  agents:\n      myFileCreators:\n          group: myFileCreatorGroup\n          path: PATH/FileCreator\n          execargs: []\n\n  eventstreams:\n      main:\n          - type: event\n            agent: myFileCreators\n            method: setConfiguration\n            args:\n                filename: /local/myGreatFile\n\n          - type: trigger\n            triggers: [{timeout: 3000}]\n\n          - type: event\n            agent: myFileCreators\n            method: createFile\n            args: {}\n\n\n\n\n\nBut how do we know that waiting for 3 seconds is a long enough time to wait? Wouldn\u2019t it be better if we could tell the Orchestrator to wait for a response from the agent before continuing? \n\n\nWe can do this using a named trigger. We add a trigger statement to the \nsetConfiguration\n event clause and modify the trigger to wait for that event before continuing to process the event stream:\n\n\n- type: event\n  agent: myFileCreators\n  trigger: configDone\n  method: setConfiguration\n  args:\n      filename: /local/myGreatFile\n\n# Wait for the event \nconfigDone\n from all fileCreator agents.\n- type: trigger\n  triggers: [{event: configDone, agent: myFileCreators}]\n\n\n\n\n\nNow when \nsetConfiguration\n is called on the Agent, the daemon will send a trigger with the event \nconfigDone\n after the method has returned. With this modified trigger, the Orchestrator will wait for the trigger event \nconfigDone\n before processing the next event in the event stream.\n\n\nHere is the Orchestrator output now. Note that \nsetConfiguration\n now \u201cfires\u201d a trigger (sends a trigger) and the Orchestrator waits until the trigger is resolved before moving on.\n\n\n  $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v\n  stream initialization : sent : joinGroup myFileCreatorGroup --\n __ALL__\n  stream initialization : done : trigger GroupBuildDone myFileCreatorGroup  complete.\n  stream initialization : sent : loadAgent myFileCreators --\n myFileCreatorGroup\n  stream initialization : done : trigger AgentLoadDone  myFileCreators complete.\n  stream initialization : DONE : complete.\n  stream main           : sent : setConfiguration(['/local/myGreatFile ... ) --\n myFileCreatorGroup  (fires trigger: configDone)\n  stream main           : done : trigger configDone  complete.\n  stream main           : sent : createFile(None) --\n myFileCreatorGroup\n  stream main           : DONE : complete.\n  $\n\n\n\n\nFor reference, the new agent implementation, AAL file, and IDL, file can be downloaded as a tar file here: \nFileCreator-withconfig.tbz\n.", 
            "title": "Configuring a MAGI Agent at Runtime"
        }, 
        {
            "location": "/orchestrator/agent-configuration/#setting-agent-configuration", 
            "text": "This document will expand the sample code of our  FileCreator  example. For reference, here is the agent code:    from magi.util.agent import DispatchAgent, agentmethod\n  from magi.util.processAgent import initializeProcessAgent\n\n  # The FileCreator agent implementation, derived from DispatchAgent.\n  class FileCreator(DispatchAgent):\n      def __init__(self):\n          DispatchAgent.__init__(self)\n          self.filename = '/tmp/newfile'\n\n      # A single method which creates the file named by self.filename.\n      # (The @agentmethod() decorator is not required, but is encouraged.\n      #  it does nothing of substance now, but may in the future.)\n      @agentmethod()\n      def createFile(self, msg):\n          **Create a file on the host.**\n          # open and immediately close the file to create it.\n          open(self.filename, 'w').close()\n\n  # the getAgent() method must be defined somewhere for all agents.\n  # The Magi daemon invokes this mehod to get a reference to an\n  # agent. It uses this reference to run and interact with an agent\n  # instance.\n  def getAgent():\n      agent = FileCreator()\n      return agent\n\n  # In case the agent is run as a separate process, we need to\n  # create an instance of the agent, initialize the required\n  # parameters based on the received arguments, and then call the\n  # run method defined in DispatchAgent.\n  if __name__  __main__ :\n      agent = FileCreator()\n      initializeProcessAgent(agent, sys.argv)\n      agent.run()  If we reset the  self.filename  variable in the agent  before  invoking  createFile  in the AAL, we can change the file that is created.   The base class  DispatchAgent  itself is derived from a class that will let us do this. The Agent class implements two methods:    setConfiguration  -  Sets the passed parameters as class instance variables.  confirmConfiguration  - This method is meant to be re-implemented in your agent if you need confirm the variables set are valid for your agent.   To set the  self.filename  variable in the FileCreator Agents, we modify the AAL to include a call to the Agent method  setConfiguration , passing in a list of key-value pairs. (In the following example, it is a single key-value pair.)  - type: event\n  agent: myFileCreators\n  method: setConfiguration\n  args:\n     filename: /tmp/myCreatedFile  Note that you do not specify self when referencing an Agent variable. We make sure to place this event in the AAL event stream prior to the  createFile  event. The complete AAL file is:    streamstarts: [main]\n\n  groups:\n      myFileCreatorGroup: [NODES]\n\n  agents:\n      myFileCreators:\n          group: myFileCreatorGroup\n          # (note: the  PATH  argument is the agent directory. The\n          # directory must contain an IDL and agent implementation. It must\n          # also contain a *__init__.py* file, which is required\n          # for it to be considered as a valid python package.)\n          path: PATH/FileCreator\n          execargs: []\n\n  eventstreams:\n      main:\n          - type: event\n            agent: myFileCreators\n            method: setConfiguration\n            args:\n               filename: /tmp/myCreatedFile\n\n          - type: event\n            agent: myFileCreators\n            method: createFile\n            args: {}  Now when we run the Agent again (possibly using  agentTool  to restart the Magi daemons), we see the following events:    $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v\n  stream initialization : sent : joinGroup myFileCreatorGroup --  __ALL__\n  stream initialization : done : trigger GroupBuildDone myFileCreatorGroup  complete.\n  stream initialization : sent : loadAgent myFileCreators --  myFileCreatorGroup\n  stream initialization : done : trigger AgentLoadDone  myFileCreators complete.\n  stream initialization : DONE : complete.\n  stream main           : sent : setConfiguration(['/tmp/myCreatedFile ... ) --  myFileCreatorGroup\n  stream main           : sent : createFile(None) --  myFileCreatorGroup\n  stream main           : DONE : complete.\n  $ ssh myNode.myExperiment.myGroup ls -l /tmp/myCreatedFile\n  -rw-r--r-- 1 root root 0 Mar  5 13:55 /tmp/myCreatedFile\n  $  And we see that our specified file,  /tmp/myCreatedFile  was created.", 
            "title": "Setting Agent Configuration"
        }, 
        {
            "location": "/orchestrator/agent-configuration/#confirming-valid-configuration", 
            "text": "This works well, but the input to the Agent is free-form. What if the user gives invalid input, like the wrong type or data that is not in a valid range? This is where the Agent  confirmConfiguration  method comes into play.  confirmConfiguration  should be written for any Agent that wants to validate its state. It gets invoked in the AAL file after the user invokes  setConfiguration .  Note:  The concept of an Agent confirming user input will change in future releases of MAGI. The Orchestrator (or other MAGI/Montage components) will use the interface specification in the Agent\u2019s IDL file to ensure the input to the agent is valid.  Suppose our sample agent wanted to allow the user to create a file in only the  /local  directory on the host machine. The  confirmConfiguration  method that does this is:    def confirmConfiguration(self):\n      **Make sure the user input is a string value and starts with\n       /local .**\n      if not isinstance(self.filename, (str, unicode)):\n          return False\n\n      if not self.filename.startswith('/local'):\n          return False\n\n      return True  When we add this method to our sample Agent, and run the experiment with the existing AAL file, which contains configuration that does not start with  /local , the Orchestrator gives us an error while executing the event stream:    $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v\n  stream initialization : sent : joinGroup myFileCreatorGroup --  __ALL__\n  stream initialization : done : trigger GroupBuildDone myFileCreatorGroup  complete.\n  stream initialization : sent : loadAgent myFileCreators --  myFileCreatorGroup\n  stream initialization : done : trigger AgentLoadDone  myFileCreators complete.\n  stream initialization : DONE : complete.\n  stream main           : sent : setConfiguration(['/tmp/myCreatedFile ... ) --  myFileCreatorGroup\n  stream unknown        : exit : method setConfiguration returned False on agent unknown in group unknown and on node(s): moat.\n  $  The Orchestrator exited with an error, as it should.  If we now modify the AAL file to include a valid configuration, the Orchestrator succeeds. The updated AAL fragment is:    - type: event\n    agent: myFileCreators\n    method: setConfiguration\n    args:\n       filename: /local/myGreatFile  When we run the Orchestrator with the modified AAL, it succeeds as the agent configuration is now valid:    $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v\n  stream initialization : sent : joinGroup myFileCreatorGroup --  __ALL__\n  stream initialization : done : trigger GroupBuildDone myFileCreatorGroup  complete.\n  stream initialization : sent : loadAgent myFileCreators --  myFileCreatorGroup\n  stream initialization : done : trigger AgentLoadDone  myFileCreators complete.\n  stream initialization : DONE : complete.\n  stream main           : sent : setConfiguration(['/local/myGreatFile ... ) --  myFileCreatorGroup\n  stream main           : sent : createFile(None) --  myFileCreatorGroup\n  stream main           : DONE : complete.\n  $  And the \u201cvalid\u201d file has been created on the machine:    $ ssh myNode.myExperiment.myGroup ls -l /local\n  total 4\n  drwxrwxr-x 2 glawler Deter 4096 Mar  5 08:35 logs\n  -rw-r--r-- 1 root    root     0 Mar  5 14:33 myGreatFile\n  $", 
            "title": "Confirming Valid Configuration"
        }, 
        {
            "location": "/orchestrator/agent-configuration/#triggers-and-event-stream-sequence-points", 
            "text": "If you run the AAL and Agent code above, you may see that it does not actually work. One small needed detail has been left out of the AAL file. Normally the Orchestrator will run through the events in the AAL as fast is it can. If we used the event streams in the AAL file as it now stands:    eventstreams:\n      main:\n          - type: event\n            agent: myFileCreators\n            method: setConfiguration\n            args:\n                filename: /local/myGreatFile\n\n          - type: event\n            agent: myFileCreators\n            method: createFile\n            args: {}  The Orchestrator will send two messages to the Agents in rapid succession: the  setConfiguration  and  createFile  event messages. If the  setConfiguration  call returns  False , which it will given invalid input, the Orchestrator will not receive the message because would have sent the messages and exited.   Therefore, we need a way to tell the Orchestrator to wait for a response from  setConfiguration  before continuing. This is done by inserting a small pause, using a trigger which times out after 3 seconds:    # Wait 3 seconds for a response to setConfiguration\n  # timeout value is in milliseconds.\n  - type: trigger\n    triggers: [{timeout: 3000}]  If we insert this trigger between  setConfiguration  and  createFile , the Orchestrator will receive the error message from the agent and exit on error.  The full AAL file now is:    streamstarts: [main]\n\n  groups:\n      myFileCreatorGroup: [witch, moat]\n\n  agents:\n      myFileCreators:\n          group: myFileCreatorGroup\n          path: PATH/FileCreator\n          execargs: []\n\n  eventstreams:\n      main:\n          - type: event\n            agent: myFileCreators\n            method: setConfiguration\n            args:\n                filename: /local/myGreatFile\n\n          - type: trigger\n            triggers: [{timeout: 3000}]\n\n          - type: event\n            agent: myFileCreators\n            method: createFile\n            args: {}  But how do we know that waiting for 3 seconds is a long enough time to wait? Wouldn\u2019t it be better if we could tell the Orchestrator to wait for a response from the agent before continuing?   We can do this using a named trigger. We add a trigger statement to the  setConfiguration  event clause and modify the trigger to wait for that event before continuing to process the event stream:  - type: event\n  agent: myFileCreators\n  trigger: configDone\n  method: setConfiguration\n  args:\n      filename: /local/myGreatFile\n\n# Wait for the event  configDone  from all fileCreator agents.\n- type: trigger\n  triggers: [{event: configDone, agent: myFileCreators}]  Now when  setConfiguration  is called on the Agent, the daemon will send a trigger with the event  configDone  after the method has returned. With this modified trigger, the Orchestrator will wait for the trigger event  configDone  before processing the next event in the event stream.  Here is the Orchestrator output now. Note that  setConfiguration  now \u201cfires\u201d a trigger (sends a trigger) and the Orchestrator waits until the trigger is resolved before moving on.    $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v\n  stream initialization : sent : joinGroup myFileCreatorGroup --  __ALL__\n  stream initialization : done : trigger GroupBuildDone myFileCreatorGroup  complete.\n  stream initialization : sent : loadAgent myFileCreators --  myFileCreatorGroup\n  stream initialization : done : trigger AgentLoadDone  myFileCreators complete.\n  stream initialization : DONE : complete.\n  stream main           : sent : setConfiguration(['/local/myGreatFile ... ) --  myFileCreatorGroup  (fires trigger: configDone)\n  stream main           : done : trigger configDone  complete.\n  stream main           : sent : createFile(None) --  myFileCreatorGroup\n  stream main           : DONE : complete.\n  $  For reference, the new agent implementation, AAL file, and IDL, file can be downloaded as a tar file here:  FileCreator-withconfig.tbz .", 
            "title": "Triggers and Event Stream Sequence Points"
        }, 
        {
            "location": "/orchestrator/orchestrator-reference/", 
            "text": "The following sections contain reference information for the MAGI orchestrator system components:\n\n\n\n    \nMAGI System Files\n\n    \nMAGI Agent Library\n\n    \nMAGI Tools", 
            "title": "Orchestrator Reference"
        }, 
        {
            "location": "/orchestrator/system-files/", 
            "text": "MAGI System Organization\n\uf0c1\n\n\nThe MAGI system is divided into \nmagicore\n and \nmagimodules\n.\n\n\nThe \nmagicore\n consists of the group communication, control, and data management infrastructure.\n\n\nThe latest available version for use on DETERLab may be found on \nusers\n at:\n\n\n/share/magi/current/\n\n\n\n\nThe \nmagimodules\n are agent function implementations that enable a particular behavior on the experiment nodes.\n\n\nThe agents along with supporting documentation and file are located on \nusers\n at:\n\n\n/share/magi/modules\n\n\n\n\nLogs\n\uf0c1\n\n\nYou can find helpful logs in these locations in \nusers\n\n\n\n\n\n\nLOG_DIR\n\n\nCheck experiment configuration file. Default: \n/var/log/magi\n\n\n\n\n\n\nMAGI Bootstrap Log\n\n\n/tmp/magi_bootstrap.log\n\n\n\n\n\n\nMAGI Daemon Log\n \n\n\n$LOG_DIR/daemon.log\n\n\n\n\n\n\nMongoDB Log\n \n\n\n$LOG_DIR/mongo.log\n\n\n\n\n\n\nConfiguration Files\n\uf0c1\n\n\nThe following configuration files are available in \nusers\n:\n\n\n\n\n\n\nAgent\n\n\nName for the set of nodes that represent the behavior\n\n\n\n\n\n\nExperiment Configuration\n\n\n/proj/\nproject_name\n/exp/\nexperiment_name\n/experiment.conf\n\n\n\n\n\n\nCONF_DIR\n\n\nCheck experiment configuration file. Default: \n/var/log/magi\n\n\n\n\n\n\nNode Configuration\n\n\n$CONF_DIR/node.conf", 
            "title": "MAGI System Files"
        }, 
        {
            "location": "/orchestrator/system-files/#magi-system-organization", 
            "text": "The MAGI system is divided into  magicore  and  magimodules .  The  magicore  consists of the group communication, control, and data management infrastructure.  The latest available version for use on DETERLab may be found on  users  at:  /share/magi/current/  The  magimodules  are agent function implementations that enable a particular behavior on the experiment nodes.  The agents along with supporting documentation and file are located on  users  at:  /share/magi/modules  Logs \uf0c1  You can find helpful logs in these locations in  users    LOG_DIR  Check experiment configuration file. Default:  /var/log/magi    MAGI Bootstrap Log  /tmp/magi_bootstrap.log    MAGI Daemon Log    $LOG_DIR/daemon.log    MongoDB Log    $LOG_DIR/mongo.log    Configuration Files \uf0c1  The following configuration files are available in  users :    Agent  Name for the set of nodes that represent the behavior    Experiment Configuration  /proj/ project_name /exp/ experiment_name /experiment.conf    CONF_DIR  Check experiment configuration file. Default:  /var/log/magi    Node Configuration  $CONF_DIR/node.conf", 
            "title": "MAGI System Organization"
        }, 
        {
            "location": "/orchestrator/agent-library/", 
            "text": "Every node has a daemon running with a series of Agents on it. These Agents are each executed in their own thread or process. They are provided with a defined interface with which to send and receive messages to other objects in the experiment. The MAGI daemon will route messages to the agent based on the routing information in the message. The daemon supports group-based, name-based, and \u201cdock\u201d-based routing. (A dock is like a port for a traditional daemon; an agent listens on a dock.) Once a message is delivered to an agent, the format of the message data is then up to the agent itself.\n\n\nMost agents will not need to parse messages directly, however, because the MAGI Agent Library supports a number of useful abstractions implemented in base classes from which Agent authors can derive. These are described in detail below.\n\n\nAgent Execution Models\n\uf0c1\n\n\nThere are two execution models supported by the daemon for Agents: \n\n \n Thread-based\n  - A thread-based Agent is loaded and runs in the process space of the daemon. The daemon communicates with a thread-based agent directly\n\n \n Process-based\n  - A process-based Agent is started as a separate process. The daemon communicates with it via standard interprocess communication techniques: a pipe or a socket.\n\n\nHere is a list outlining the differences between the execution models.\n\n\n Threads\n \n\n\n\n\n Pro\n : Lightweight\n\n\n Pro\n : Messages passed as objects without need for serialization\n\n\n Con\n : Must be written in Python\n\n\n Con\n : Must be aware of other threads when it comes to file descriptors or other shared memory\n\n\n\n\n Process\n  (Pipe or Socket)\n\n\n\n\n Pro\n : Agents may be written in languages other than Python.\n\n\n Pro\n : May kill off agent individually from the shell\n\n\n Con\n : Heavier weight if invoking a new interpreter for each Agent for scripted languages\n\n\n Con\n : Message transceiver is more complex, in particular if a library for the language has not been written. (\n\n\n\n\n\n\nNote\n\n\nAs of now, only Python is supported. We are working on adding support for other languages.)\n\n\n\n\nInterface Description Language (IDL)\n\uf0c1\n\n\nAgent authors must write an IDL that matches the interface exported by their agent. This IDL is used by MAGI to validate the interface of the agent (and in the future to generate GUIs for agent execution and configuration.) \n\n\nThe IDL should specify: \n\n agent execution model (thread or process);\n\n any public agent variables and their types, ranges, or enumerated values; \n\n any public methods and the method arguments and their types; \n\n \u201chelp\u201d strings for each method and agent variable which explain their purpose; \n* and finally any Agent library from which they derive.\n\n\nThis may seem like a lot to specify, but the Agent Library supplies IDL for base Agents --  so in practice much of the IDL specification will be supplied to the Agent author.\n\n\nThe IDL format and keywords are given in a table below. \n\n\n(TBD - Coming soon)\n\n\nAgent Library\n\uf0c1\n\n\nIn this section we describe the Agent Library and give brief examples for usage. Classes are organized from the bottom up, that is, starting with the class from which the others derive.\n\n\n\n\nNote\n\n\nWhen using the Orchestrator to run your experiment, the Orchestrator will, by default, handle a return value of \nFalse\n from an Agent method as a reason to unload all Agents, break down communication groups and exit. Thus your Agent may stop an experiment by returning \nFalse\n.\n\n\n\n\nAgent\n\uf0c1\n\n\nThis is the base Agent class. It implements a \nsetConfiguration\n method. If derived from, the user may call \nsetConfiguration\n to set any \nself\n variables in your class. \n\n\nAgent also implements an empty \nconfirmConfiguration\n method that is called once the \nself\n variables are set. You may implement your own \nconfirmConfiguration\n if you need to make sure the user has set your internal variables to match any constraints you may want to impose. Returning \nFalse\n from this method will signal to the Orchestrator that something is wrong and the Orchestrator should handle this as an error. The default implementation of \nconfirmConfiguration\n simply returns \nTrue\n. \n\n\nThe method signature for \nconfirmConfiguration()\n is\n\n\ndef confirmConfiguration(self):\n\n\n\n\nIt takes no arguments. \n\n\nIn your \nconfirmConfiguration\n method, you should confirm that your agent internal variables are the correct type and in the expected range.\n\n\nIn the following example, imagine an agent has a variable that is an integer and the range of the value must be between 1 and 10. An agent can use the \nAgent\n class to implement this as so:\n\n\nfrom magi.util.agent import Agent\n\nclass myAgent(Agent):\n    def __init__(self):\n        self.value = None\n\n    def confirmConfiguration():\n        if not isinstance(self.value, int):\n            return False\n\n        if not 1 \n= self.value \n= 10:\n            return False\n\n        return True\n\n\n\n\nIf the variable \nself.value\n is not an integer or is not between 1 and 10, \nconfirmConfiguration\n returns \nFalse\n. If running this agent with the Orchestrator, the \nFalse\n value will get returned to the Orchestrator which will unload all agents, destroy all group communications, then exit. Thus your agent may cause the experiment to stop and be reset when it is not given the correct inputs.\n\n\n\n\nNote\n\n\nIn the future, this functionality of enforcing correct input will be handled outside of the agent code. The IDL associated with the agent already specifies correct input and the Orchestrator (or other Montage/MAGI front end tool) will enforce proper input.\n\n\n\n\nAll classes in the \nAgentLibrary\n inherit from \nAgent\n.\n\n\nThe \nAgent\n documentation can seen \nhere\n.\n\n\nDispatchAgent\n\uf0c1\n\n\nThe \nDispatchAgent\n implements the simple remote procedure call (RPC) mechanism used by the Orchestrator (and available through the MAGI python API). This allows any method in a class derived from \nDispatchAgent\n to be invoked in an AAL file (or by a \nMagiMessage\n if using the MAGI python interface directly). \n\n\nYou almost always want to derive your agent from \nDispatchAgent\n. The \nDispatchAgent\n code simply loops, parsing incoming messages, looking for an event message. When it finds one, it attempts to call the method specified in the message with the arguments given in the message, thus implementing a basic RPC functionality in your agent.\n\n\nThe first argument to your RPC-enabled method is the received message. It is accompanied by the optional named-parameters, sent as part of the \nMagiMessage\n. The Agent Library exports a function decorator for \nDispatchAgent\n-callable methods named \nagentmethod\n. It is not currently used for anything, but it is suggested that agent developers use it anyway.\n\n\nThe \nDispatchAgent\n reads incoming messages and invokes the required method synchronously, i.e., it waits for a method call to return before reading the next message.\n\n\nHere is a simple example:\n\n\nfrom magi.util.agent import DispatchAgent, agentmethod\n\ndef myAgent(DispatchAgent):\n    def __init__(self):\n        DispatchAgent.__init__(self)\n\n    @agentmethod()\n    def doAction(self, msg):\n        pass\n\n\n\n\nGiven the agent \nmyAgent\n above and the AAL fragment below, the method \ndoAction\n will be called on all test nodes associated with \nmyAgentGroup\n.\n\n\neventstreams:\n    myStream:\n        - type: event\n          agent: myAgentGroup\n          method: doAction\n          args: { }\n\n\n\n\nThe \nDispatchAgent\n documentation may seen \nhere\n. \n\n\nNonBlockingDispatchAgent\n\uf0c1\n\n\nThe \nNonBlockingDispatchAgent\n is similar to \nDispatchAgent\n. The only difference is that \nNonBlockingDispatchAgent\n invokes the methods \nasynchronously\n, i.e., it forks a new thread for each method call and does not wait for the call to return. It invokes the required method and moves on to read the next message.\n\n\nReportingDispatchAgent\n\uf0c1\n\n\nYou will note that the \nDispatchAgent\n only allows an outside source to send commands to the agent. There is no communication backwards. The \nReportingDispatchAgent\n base class has a slightly different run loop. Rather than blocking forever on incoming messages, it will also call its own method, \nperiodic\n, to allow other operations to occur.\n\n\nThe call to \nperiodic\n will return the amount of time in seconds (as a float) that it will wait until calling \nperiodic\n again. The \nperiodic\n function therefore controls how often it is called. The first call will happen as soon as the run is called.\n\n\nThe method signature of the \nperiodic\n method is:\n\n\ndef periodic(self, now):\n\n\n\n\nIf \nperiodic\n is not implemented in the subclass, an exception is raised.\n\n\nThis example code writes the current time to a file once a second. Note the explicit use of the \nAgent\n class to set the file name.\n\n\nimport os.path\nfrom magi.util.agent import ReportingDispatchAgent, agentmethod\n\nclass myTimeTracker(ReportingDispatchAgent):\n    def __init__(self):\n        ReportingDispatchAgent.__init__(self)\n        self.filename = None\n\n    def confirmConfiguration(self):\n        if not os.path.exists(self.filename):\n            return False\n\n    def periodic(self, now):\n        with open(self.filename, 'a') as fd:\n            fd.write('%f\\n' % now)\n\n        # call again one second from now\n        return 1.0\n\n\n\n\n\nThe \nReportingDispatchAgent\n documentation may seen here. http://montage.deterlab.net/backend/python/util.html#magi.util.agent.ReportingDispatchAgent\n\n\nSharedServer\n\uf0c1\n\n\nThe \nSharedServer\n class inherits from \nDispatchAgent\n and expects the subclass to implement the methods \nrunserver\n and \nterminateserver\n to start or stop a local server process. \n\n\nThe \nSharedServer\n class takes care of multiple agents requesting use of the server and only calls \nrunserver\n or \nterminateserver\n when required. This ensures that there is ever only one instance of the server running at once on a given host. A canonical example of this would be a web server running a single instance of Apache. The methods \nrunserver\n and \nstopserver\n take no arguments.\n\n\nBelow is an example of a simple agent that starts and stops Apache on the local host. If there are other agents running on the machine that require Apache to be running, they may inherit from \nSharedServer\n as well, thus ensuring that there is only ever one instance of Apache running.\n\n\nfrom subprocess import check_call, CalledProcessError\nfrom magi.util.agent import SharedServer\n\nclass ApacheServerAgent(SharedServer):\n    def __init__(self):\n        SharedServer.__init__(self)\n\n    def runserver(self):\n        try:\n            check_call('apachectl start'.split())\n\n        except CalledProcessError:\n            return False\n\n        return True\n\n    def stopserver(self):\n        try:\n            check_call('apachectl stop'.split())\n\n        except CalledProcessError:\n            return False\n\n        return True\n\n\n\n\nThe \nSharedServer\n documentation may seen \nhere\n. \n\n\nTrafficClientAgent\n\uf0c1\n\n\nTrafficClientAgent\n models an agent that periodically generates traffic. It must implement the \ngetCmd\n method, returning a string to execute on the commandline to generate traffic. For example, the \ngetCmd\n could return a \ncurl\n or \nwget\n command to generate client-side HTML traffic. The signature of \ngetCmd\n is:\n\n\ndef getCmd(self, destination)\n\n\n\n\nWhere \ndestination\n is a server host name from which the agent should request traffic. \n\n\nThe \nTrafficClientAgent\n class implements the following event-callable methods: \nstartClient()\n and ```stopClient()}}. Neither method takes any arguments. These methods may be invoked from an AAL and start and stop the client respectively.\n\n\nThe base class contains a number of variables which control how often \ngetCmd\n is called and which servers should be contacted: \n\n \nservers\n: A list of server hostnames\n\n \ninterval\n: A distribution variable\n\n\n\n\nNote\n\n\nA distribution variable is any valid python expression that returns a float. It may be as simple as an integer, \u201c1\u201d, or an actual distribution function. The Agent Library provides \nminmax\n, \ngamma\n, \npareto\n, and \nexpo\n in the distributions module. Thus a valid value for the \nTrafficClientAgent\n interval value could be \nminmax(1,10)\n, which returns a value between 1 and 10 inclusive. The signatures of these distributions are:\n\n\nminmax(min, max)\ngamma(alpha, rate, cap = None)\npareto(alpha, scale = 1.0, cap = None)\nexpo(lambd, scale = 1.0, cap = None)\n\n\n\n\n\nBelow is a sample \nTrafficClientAgent\n which implements a simple HTTP client-side traffic agent. It assumes the destinations have been set correctly (via the \nAgent setConfiguration\n method) and there are web servers already running there.\n\n\nfrom magi.util.agent import TrafficClientAgent\n\nclass mySimpleHTTPClient(TrafficClientAgent):\n    def __init__(self):\n        TrafficClientAgent.__init__(self)\n\n    def getCmd(self, destination):\n        cmd = 'curl -s -o /dev/null http://%s/index.html' % destination\n        return cmd\n\n\n\n\nWhen this agent is used with the following AAL clauses, the servers \n server_1\n  and \n server_2\n  are used as HTTP traffic generation servers and traffic is generated once an interval where the interval ranges randomly between 5 and 10 seconds, inclusive. The first event sets the agent\u2019s internal configuration. The second event starts the traffic generation.\n\n\neventstreams:\n    myStream:\n        - type: event\n          agent: myHTTPClients\n          method: setConfiguration\n          args:\n                interval: 'minmax(5, 10)'\n                servers: ['server_1', 'server_2']\n\n        - type: event\n          agent: myHTTPClients\n          method: startClient\n          args: { }\n\n\n\n\nThe \nTrafficClientAgent\n documentation may seen here. http://montage.deterlab.net/backend/python/util.html#magi.util.agent.TrafficClientAgent\n\n\nProbabilisticTrafficClientAgent\n\uf0c1\n\n\nProbabilisticTrafficClientAgent\n provides the same service as \nTrafficAgent\n, but \ngetCmd\n is called only when the configured probability function evaluates to a non-zero value.\n\n\nConnectedTrafficClientAgent\n\uf0c1\n\n\nConnectedTrafficClientAgent\n is a base for an agent that controls a set of agents that have standing connections to, and traffic between, a set of servers. \n\n\nconnect()\n and \ndisconnect()\n are called periodically when a given client should connect or disconnect to a given server. \n\n\ngenerateTraffic()\n is called when the given client should generate traffic between itself and the server it is connected to. The sequence of calls is: \n\n\n[period], connect(), [period], generateTraffic(), [period], generateTraffic(), ..., disconnect()\n\n\n\n\nThis sequence may be repeated.\n\n\nDerived classes should implement \nconnect()\n, \ndisconnect()\n, and \ngenerateTraffic()\n.\n\n\nAgent Load and Execution Chain (for threaded agents)\n\uf0c1\n\n\n(TBD - Coming soon)", 
            "title": "MAGI Agent Library"
        }, 
        {
            "location": "/orchestrator/magi-tools/", 
            "text": "The following tools are available for MAGI experiments in DETERLab:\n\n\n\n  \nmagi_bootstrap.py\n\n  \nmagi_graph.py\n\n  \nmagi_orchestrator.py\n\n  \nmagi_status.py\n\n\n\n\n\nmagi_bootstrap.py\n: Install MAGI\n\uf0c1\n\n\nUsed with the testbed start command, \nmagi_bootstrap.py\n installs MAGI and supporting tools on all nodes at startup.\n\n\nIn the topology file of the desired experiment, include the following command:\n\n\ntb-set-node-startcmd $NodeName \nsudo python /share/magi/current/magi_bootstrap.py\n\n\n\n\n\nwhere \n$NodeName\n is the control node. \n\n\nmagi_graph.py\n: Create graphs for a MAGI experiment\n\uf0c1\n\n\nmagi_graph.py\n is a graph generator for experiments executed on DETERLab using MAGI. The tool fetches the required data using MAGI\u2019s data management layer and generates a graph in PNG format. This tool may be executed from either the Deter Ops machine or a remote computer with access to internet. The data to be plotted and other graph features are configurable.\n\n\nThe various command line options are as follows:\n\n\n  Usage: magi_graph.py [options]\n\n  Plots the graph for an experiment based on parameters provided.\n  Experiment Configuration File OR Project and Experiment Name\n  needs to be provided to be able to connect to the experiment.\n  Need to provide build a graph specific configuration for plotting.\n\n  Options:\n   -h, --help            show this help message and exit\n   -e EXPERIMENT, --experiment=EXPERIMENT\n                       Experiment name\n   -p PROJECT, --project=PROJECT\n                       Project name\n   -x EXPERIMENTCONFIG, --experimentConfig=EXPERIMENTCONFIG\n                       Experiment configuration file\n   -c CONFIG, --config=CONFIG\n                       Graph configuration file\n   -a AGENT, --agent=AGENT\n                       Agent name. This is used to fetch available database\n                       fields\n   -l AAL, --aal=AAL     AAL (experiment procedure) file. This is also used to\n                       fetch available database fields\n   -o OUTPUT, --output=OUTPUT\n                       Output graph file. Default: graph.png\n   -t, --tunnel          Tell the tool to tunnel request through Deter Ops\n                       (users.deterlab.net).\n   -u USERNAME, --username=USERNAME\n                       Username for creating tunnel. Required only if\n                       different from current shell username.\n\n\n\n\nThis tool expects the user to provide a configuration file. The format of the configuration file needs to be similar to the sample configuration file provided below.\n\n\n  graph:\n     type: line\n     xLabel: Time(sec)\n     yLabel: Bytes\n     title: Traffic plot\n  db:\n     agent: monitor_agent\n     filter:\n          host: servernode\n          peerNode: clientnode\n          trafficDirection: out\n     xValue: created\n     yValue: bytes\n\n\n\n\nThe configuration is divided into two parts a) Graph options and b) Database options. Graph options are used to configure the type of graph and the various labels. The database options help the tool fetch the data to be plotted.\n\n\nEach record stored in the database using MAGI\u2019s database layer has the following three fields along with any other that an agent populates.\n\n\n    agent: Agent Name\n    host: Node of which the agent is hosted\n    created: Timestamp of when the record is created\n\n\n\n\nIn the above mentioned example, data populated by the agent named \u201cmonitor_agent\u201d hosted on the node named \u201cservernode\u201d will be fetched. The data would further be filtered on the configured values of peerNode and trafficDirection, which are agent specific fields.\n\n\nAmong the fetched data, values corresponding to the fields, created and bytes, will be plotted correspoding to the x and the y axis, respectively.\n\n\nmagi_orchestrator.py\n: Run MAGI\n\uf0c1\n\n\nmagi_orchestrator.py\n reads the procedure's AAL file and orchestrates an experiment based on the specified procedures. The Orchestrator does the following in this order:\n\n\n\n\n'''Joins Groups''' - The Orchestrator iterates over the list of groups and for each group sends a request to all the mapped nodes to join the group. A corresponding reply adds the replier to the group. Messages addressed to a group are sent to all the nodes that are part of the group.\n\n\n'''Loads Agents''' - The Orchestrator iterates over the list of agents and for each agent sends an agent load request to the mapped groups. An agent load message tells the Orchestrator to start an instance of the agent implementation and to put it in a listening mode, where the instance waits for further messages.\n\n\n'''Executes Event Streams''' - Next, the Orchestrator concurrently executes all the event streams listed as part of \nstreamstarts\n.\nThe Orchestrator has a predefined event stream called \nexit\n. The purpose of this event stream is to unload all the agents and disjoin groups. All workflows should end with executing this stream for a clean exit.\n\n\n\n\nFrom your home directory on \nusers.isi.deterlab.net\n, run the following command:\n\n\n/share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal\n\n\n\n\nwhere \n\n \nclientnode\n equals the node you want to start with\n\n \nmyExp\n is the Experiment Name\n\n \nmyProj\n is the Project Name\n\n \nprocedural.aal\n is the name of the AAL file.\n\n\nThe various command line options are as follows:\n\n\n  Usage: magi_orchestrator.py [options]\n\n  Options:\n       -h, --help\n                       show this help message and exit\n       -c CONTROL, --control=CONTROL\n                       The control node to connect to (i.e. control.exp.proj)\n       -f EVENTS, --events=EVENTS\n                       The events.aal file(s) to use. Can be specified\n                       multiple times for multiple AAL files\n       -l LOGLEVEL, --loglevel=LOGLEVEL\n                       The level at which to log. Must be one of none, debug,\n                       info, warning, error, or critical. Default is info.\n       -o LOGFILE, --logfile=LOGFILE\n                       If given, log to the file instead of the console\n                       (stdout).\n      -e EXITONFAILURE, --exitOnFailure=EXITONFAILURE\n                       If any method call fails (returns False), then exit\n                       all streams, unload all agents, and exit the\n                       orchestrator. Default value is True\n      -g GROUPBUILDTIMEOUT, --groupBuildTimeout=GROUPBUILDTIMEOUT\n                       When building the initial groups for agents in the\n                       given AAL, use the timeout given (in milliseconds)\n                       when waiting for group formation to complete.\n      --nocolor\n                       If given, do not use color in output.\n      -v, --verbose\n                       Tell orchestrator to print info about what its doing\n     -n, --tunnel\n                      Tell orchestrator to tunnel data through Deter Ops\n                       (users.deterlab.net).\n\n\n\n\nmagi_status.py\n: Check status of a MAGI experiment, reboot MAGI daemon, download logs\n\uf0c1\n\n\nUse \nmagi_status.py\n to:\n\n check MAGI\u2019s status on experiment nodes\n\n reboot MAGI daemon process\n* download logs from experiment nodes\n\n\nThis tool is run for one experiment at a time. The user needs to provide the project name and the experiment name to the tool.\n\n\nThis tool, by default, works for all of the nodes corresponding to the given experiment. However, it can be made to work with a restricted set of nodes, either by directly providing the set of interested nodes, or by providing an AAL (experiment procedure) file to fetch the set of desired nodes.\n\n\nThis tool by default only informs if the MAGI daemon process on a node is reachable or not. Specific options can be used to fetch group membership details and information about active agents.\n\n\nIf you want to reboot the MAGI daemon, \nmagi_status.py\n first reboots MAGI daemon processes on the experiment nodes, and then fetches their status.\n\n\nIf the tool is asked to download logs, it just does that, and does not fetch the status.\n\n\n  Usage: magi_status.py [options]\n\n  Script to get the status of MAGI daemon processes on experiment nodes,\n  to reboot them if required, and to download logs.\n\n  Options:\n    -h, --help            show this help message and exit\n    -p PROJECT, --project=PROJECT\n                          Project name\n    -e EXPERIMENT, --experiment=EXPERIMENT\n                          Experiment name\n    -n NODES, --nodes=NODES\n                          Comma-separated list of the nodes to reboot MAGI\n                          daemon\n    -a AAL, --aal=AAL     The yaml-based procedure file to extract the list of\n                          nodes\n    -l, --logs            Fetch logs. The -o/--logoutdir option is applicable\n                          only when fetching logs.\n    -o LOGOUTDIR, --logoutdir=LOGOUTDIR\n                          Store logs under the given directory. Default: /tmp\n    -g, --groupmembership\n                          Fetch group membership detail\n    -i, --agentinfo       Fetch loaded agent information\n    -t TIMEOUT, --timeout=TIMEOUT\n                          Number of seconds to wait to receive the status reply\n                          from the nodes on the overlay\n    -r, --reboot          Reboot nodes. The following options are applicable\n                          only when rebooting.\n    -d DISTPATH, --distpath=DISTPATH\n                          Location of the distribution\n    -U, --noupdate        Do not update the system before installing MAGI\n    -N, --noinstall       Do not install MAGI and the supporting libraries", 
            "title": "MAGI Tools"
        }, 
        {
            "location": "/containers/containers-quickstart/", 
            "text": "This page describes basic information about DETERLab Containers and provides an overview of how to use it. More details are available in the \nContainers Guide\n.\n\n\nWhat are Containers?\n\uf0c1\n\n\nThe Containers system enables experimenters to create large-scale DETERLab topologies that support differing degrees of fidelity in individual elements.  In order to create an experiment larger than the 400+ computers available in DETERLab Core, experimenters must use virtualization, simulation, or some other abstraction to represent their topology.  The Containers system guides this process allowing experimenters to create large experimental environments that may be used to gather correct results.\n\n\nThe Containers system is built on top of the resource allocation that underlies the \nDETERLab testbed\n, extending it to provide multiple implementations of virtual nodes. Most DETERLab tools that run on physical experiments may be used directly on containerized experiments.  Experimenters find working in a containerized experiment very similar to working in physical DETERLab experiments.\n\n\nHow does it work?\n\uf0c1\n\n\nAn experimenter comes to DETERLab with an experimental topology of computers and networks and an experiment to carry out on that topology, and the Containers system allocates resources in the configuration specified.  The experimenter may directly access the computers in order to carry out the experiment.  The computers themselves are either physical computers or some virtual computers that emulate a computer at an acceptable level of fidelity.  Multiple experiments may be in progress at once using DETER resources, and they are protected from interfering with one another.\n\n\nContainers present researchers with more resources while preserving the DETERLab interfaces. The process of converting a topology description to an isolated collection of networked computers is basically the same as when an experimenter creates a physical topology on DETERLab.  The difference is that a containerized experiment is configured to present more experimental resources than physical ones, preserving the DETERLab interface.\n\n\nA little more completely, the Containers system lays out the virtual computers into a physical layout of computers and uses the resource allocation system to allocate that physical layout.  Then the system installs and configures the appropriate virtualization technologies in that environment to create the virtual environment.\n\n\n\n\nAs in physical DETERLab experiments, the experiment's topology is written in an extended version of DETER's ns2 syntax, or in \ntopdl\n, a topology description language.  \n\n\nCurrently experimenters pick containers directly using those languages.\n\n\nKinds of Containers\n\uf0c1\n\n\nA container is a virtualization technology, like a virtual machine implementation.  We use the term ''container'' to mean any one of the various virtualization technologies from an openvz container to a physical machine to a simulation.  The Containers system gives us a way to create interconnections of containers (in our sense) holding different experiment elements.  A containerized topology might include a physical machine, a \nqemu\n virtual machine and an \nopenvz container\n that can all communicate transparently.\n\n\nThe Containers system framework supports multiple kinds of containers, but at this point researchers may request these:\n\n\n\n\n\n\n\n\nContainer Type\n\n\nFidelity\n\n\nScalability\n\n\n\n\n\n\n\n\n\n\nPhysical Machine\n\n\nComplete fidelity\n\n\n1 per physical machine\n\n\n\n\n\n\nQemu virtual Machine\n\n\nVirtual hardware\n\n\n10's of containers per physical machine\n\n\n\n\n\n\nOpenvz container\n\n\nPartitioned resources in one Linux kernel\n\n\n100's of contatiners per physical machine\n\n\n\n\n\n\nViewOS process\n\n\nProcess with isolated network stack\n\n\n1000's of containers per physical machine\n\n\n\n\n\n\n\n\nHow do I use Containers?\n\uf0c1\n\n\nIn general, once you have a DETERLab account, you follow these steps. The \nDETERLab Containers Guide\n will walk you through a basic tutorial of these steps.\n\n\n1. Design the topology\n\uf0c1\n\n\nEvery experiment in DETERLab is based on a network topology file written in NS format and saved on the \nusers\n node. In a containerized experiment, the topology will typically be a large one with more than 400 nodes.\n\n\nsample of topology\n\n\n\n\n2. Run containerized experiment with the containerize.py command\n\uf0c1\n\n\nThe Containers system will build the containerized experiment on top of an existing \n DETERLab physical experiment\n by running the \ncontainerize.py\n command from the shell on \nusers.isi.deterlab.net\n, as in the following example:\n\n\n $ /share/containers/containerize.py DeterTest example1 ~/example1.tcl \n\n\n\n\nwhere ''DeterTest'' and ''example1'' are the project and experiment name, respectively, of the physical DETERLab experiment and ''example.tcl'' is the topology file.\n\n\nWith these default parameters, \ncontainerize.py\n will put each node into an  Openvz container with at most 10 containers per physical node.\n\n\n3. View results by accessing nodes, modify the experiment as needed.\n\uf0c1\n\n\nIn a containerized experiment, you can access the virtual nodes with the same directories mounted as in a physical DETERLab experiment. You can load and run software and conduct experiments as you would in a physical experiment. \n\n\n4. Save your work and swap out your experiment (release the resources)\n\uf0c1\n\n\nAs with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users.\n\n\nMore Information\n\uf0c1\n\n\nFor more detailed information about Containers, read the following:\n\n\n\n\nContainers Guide\n - This guide walks you through a basic example of using Containers and includes some advanced topics.\n\n\nContainers Reference\n - This reference includes Containers commands, configuration details and information about different types of containers.", 
            "title": "Containers Quickstart"
        }, 
        {
            "location": "/containers/containers-quickstart/#what-are-containers", 
            "text": "The Containers system enables experimenters to create large-scale DETERLab topologies that support differing degrees of fidelity in individual elements.  In order to create an experiment larger than the 400+ computers available in DETERLab Core, experimenters must use virtualization, simulation, or some other abstraction to represent their topology.  The Containers system guides this process allowing experimenters to create large experimental environments that may be used to gather correct results.  The Containers system is built on top of the resource allocation that underlies the  DETERLab testbed , extending it to provide multiple implementations of virtual nodes. Most DETERLab tools that run on physical experiments may be used directly on containerized experiments.  Experimenters find working in a containerized experiment very similar to working in physical DETERLab experiments.", 
            "title": "What are Containers?"
        }, 
        {
            "location": "/containers/containers-quickstart/#how-does-it-work", 
            "text": "An experimenter comes to DETERLab with an experimental topology of computers and networks and an experiment to carry out on that topology, and the Containers system allocates resources in the configuration specified.  The experimenter may directly access the computers in order to carry out the experiment.  The computers themselves are either physical computers or some virtual computers that emulate a computer at an acceptable level of fidelity.  Multiple experiments may be in progress at once using DETER resources, and they are protected from interfering with one another.  Containers present researchers with more resources while preserving the DETERLab interfaces. The process of converting a topology description to an isolated collection of networked computers is basically the same as when an experimenter creates a physical topology on DETERLab.  The difference is that a containerized experiment is configured to present more experimental resources than physical ones, preserving the DETERLab interface.  A little more completely, the Containers system lays out the virtual computers into a physical layout of computers and uses the resource allocation system to allocate that physical layout.  Then the system installs and configures the appropriate virtualization technologies in that environment to create the virtual environment.   As in physical DETERLab experiments, the experiment's topology is written in an extended version of DETER's ns2 syntax, or in  topdl , a topology description language.    Currently experimenters pick containers directly using those languages.", 
            "title": "How does it work?"
        }, 
        {
            "location": "/containers/containers-quickstart/#kinds-of-containers", 
            "text": "A container is a virtualization technology, like a virtual machine implementation.  We use the term ''container'' to mean any one of the various virtualization technologies from an openvz container to a physical machine to a simulation.  The Containers system gives us a way to create interconnections of containers (in our sense) holding different experiment elements.  A containerized topology might include a physical machine, a  qemu  virtual machine and an  openvz container  that can all communicate transparently.  The Containers system framework supports multiple kinds of containers, but at this point researchers may request these:     Container Type  Fidelity  Scalability      Physical Machine  Complete fidelity  1 per physical machine    Qemu virtual Machine  Virtual hardware  10's of containers per physical machine    Openvz container  Partitioned resources in one Linux kernel  100's of contatiners per physical machine    ViewOS process  Process with isolated network stack  1000's of containers per physical machine", 
            "title": "Kinds of Containers"
        }, 
        {
            "location": "/containers/containers-quickstart/#how-do-i-use-containers", 
            "text": "In general, once you have a DETERLab account, you follow these steps. The  DETERLab Containers Guide  will walk you through a basic tutorial of these steps.  1. Design the topology \uf0c1  Every experiment in DETERLab is based on a network topology file written in NS format and saved on the  users  node. In a containerized experiment, the topology will typically be a large one with more than 400 nodes.  sample of topology  2. Run containerized experiment with the containerize.py command \uf0c1  The Containers system will build the containerized experiment on top of an existing   DETERLab physical experiment  by running the  containerize.py  command from the shell on  users.isi.deterlab.net , as in the following example:   $ /share/containers/containerize.py DeterTest example1 ~/example1.tcl   where ''DeterTest'' and ''example1'' are the project and experiment name, respectively, of the physical DETERLab experiment and ''example.tcl'' is the topology file.  With these default parameters,  containerize.py  will put each node into an  Openvz container with at most 10 containers per physical node.  3. View results by accessing nodes, modify the experiment as needed. \uf0c1  In a containerized experiment, you can access the virtual nodes with the same directories mounted as in a physical DETERLab experiment. You can load and run software and conduct experiments as you would in a physical experiment.   4. Save your work and swap out your experiment (release the resources) \uf0c1  As with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users.", 
            "title": "How do I use Containers?"
        }, 
        {
            "location": "/containers/containers-quickstart/#more-information", 
            "text": "For more detailed information about Containers, read the following:   Containers Guide  - This guide walks you through a basic example of using Containers and includes some advanced topics.  Containers Reference  - This reference includes Containers commands, configuration details and information about different types of containers.", 
            "title": "More Information"
        }, 
        {
            "location": "/containers/containers-guide/", 
            "text": "In this tutorial we walk you through setting up a basic containerized experiment. This page also includes common advanced topics. Detailed descriptions of the commands and configuration files are available in the \nreference section\n. \n\n\n\n\nNote\n\n\nIf you are a student, go to the \nhttp://education.deterlab.net\n site for classroom-specific instructions.\n\n\n\n\nBasic Containers Tutorial\n\uf0c1\n\n\nThis tutorial will set up a containerized experiment with a star topology. We'll create a central node and connect 9 other nodes to it\n\n\nGetting started\n\uf0c1\n\n\nYou will need a DETERLab account and be a member of a project. If you need help, see \nthe Core Guide\n. \n\n\nStep 1: Design the topology\n\uf0c1\n\n\nFirst, we will describe a star topology.\n\n\n\n\nFor this example we will use the standard DETER topology descriptions.  If you are new to designing topologies, walk through the basic tutorial in the \nCore Guide\n.  The Containers system is largely compatible with the physical DETER interface.\n\n\nDownload the \nDETERLab-compatible ns2 description of this topology at this link\n to your home directory on \nusers.isi.deterlab.net\n.  It is a simple loop, along with the standard DETER boilerplate.  This file will be used to create a 10-node (9 satellites and one central node) physical experiment on DETER, although there are not many physical nodes on DETER with 10 interfaces (one interface for control traffic).\n\n\nThe following is the topology description:\n\n\n    source tb_compat.tcl\n    set ns [new Simulator]\n\n    # Create the center node (named by its variable name)\n    set center [$ns node]\n\n    # Connect 9 satellites\n    for { set i 0} { $i \n 9 } { incr i} {\n        # Create node n-1 (tcl n($i) becomes n-$i in the experiment)\n        set n($i) [$ns node]\n        # Connect center to $n($i)\n        ns duplex-link $center $n($i) 100Mb 10ms DropTail\n    }\n\n    # Creation boilerplate\n    $ns rtptoto Static\n    $ns run\n\n\n\n\n\n\nNote\n\n\n\n\nThe central node is named \"center\" and each satellite is names \"n-0\", \"n-1\"... through \"n-8\".  \n\n\nEach connection is a 100 Mb/s link with a 10ms delay.  \n\n\nThe round trip time from n-0 to center will be 20 ms and from n-0 to n-1 will be 40 ms.\n\n\n\n\n\n\nStep 2: Create the containerized experiment\n\uf0c1\n\n\nNow we will run a command so the Containers system will build the containerized experiment on top of a new DETERLab physical experiment.  \n\n\nRun the following command from the shell on \nusers.isi.deterlab.net\n and refer to the example topology you just saved in your home.  \n\n\n$ /share/containers/containerize.py DeterTest example1 ~/example1.tcl \n\n\n\n\nwhere the first two parameters are the project and experiment name to hold the DETER experiment.  \n\n\nThis command creates an experiment called \nexperiment1\n in the \nDeterTest\n project. Throughout this tutorial, we will refer to your project as \nDeterTest\n, but make sure you actually use your actual project's name. You may use the experiment name \nexperiment1\n as long as another experiment with that name doesn't already exist\n\n\n\n\nNote\n\n\nAs with any DETERLab experiment, you must be a member of the project with appropriate rights to create an experiment in it.  \ncontainerize.py\n expects there to be no experiment with that name, and it will fail if one exists.  To remove an experiment you may terminate it through the web interface or use the \nendexp\n command.  Terminating an experiment is more final than swapping one out, so be sure that you want to replace the old experiment.  You may also resolve the conflict by renaming your new containerized experiment.\n\n\n\n\nThe last parameter is the file containing the topology.  In this tutorial, we are referring to the ns2 file in \nour example\n but you may also use a \ntopdl\n description.  An ns2 description must end in \n.tcl\n or \n.ns\n.\n\n\nWith these default parameters \ncontainerize.py\n will put each node into an \nOpenvz container\n with at most 10 containers per physical node.\n\n\nThe output of the above command should be something like the following:\n\n\nusers:~$ /share/containers/containerize.py DeterTest example1 ~/example1.tcl \nContainerized experiment DeterTest/example1 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example1\n\n\n\n\nNow we can see what a containerized experiment looks like.\n\n\nThe Contents of a Containerized Experiment\n\uf0c1\n\n\nFollow the link provided in the \ncontainerize.py\n output. You will see a standard DETER experiment page that looks like this:\n\n\n\n\nYou may be surprised to see that DETER thinks the experiment has only one node:\n\n\n\n\nThe Containers system has rewritten the description file and stored additional information in the experiment's per-experiment directory that will be used to create the 10 node experiment inside the single-node DETER experiment.  \n\n\nIf you look at the ns file DETERLab has stored (click the \nNS File\n tab on the experiment page), you should see the following code:\n\n\nset ns [new Simulator]\nsource tb_compat.tcl\n\ntb-make-soft-vtype container0 {pc2133 bpc2133 MicroCloud}\nset pnode(0000) [$ns node]\ntb-set-node-os ${pnode(0000)} CentOS6-64-openvz\ntb-set-hardware ${pnode(0000)} container0\ntb-set-node-startcmd ${pnode(0000)} \nsudo /share/containers/setup/hv/bootstrap /proj/DeterTest/exp/example1/containers/site.conf \n /tmp/container.log\n\ntb-set-node-failure-action ${pnode(0000)} \nnonfatal\n\n\n$ns rtproto Static\n$ns run\n\n\n\n\nThis looks nothing like the file we gave to \ncontainerize.py\n, but it does show us a little about what the Containers system has done:\n\n\n\n\nThe single physical node (\npnode(0000)\n) will run the \nCentOS6-64-openvz\n image and run on a few kinds of node.  \n\n\nOn startup, \npnode(0000)\n will execute a command from the same \n/share/containers\n directory that \ncontainerize.py\n ran from using data in the per-experiment directory \n/proj/DeterTest/exp/example1/containers/site.conf\n.\n\n\nThere is a separate \n/proj/DeterTest/exp/example1/containers/\n directory for each experiment.  The path element after \n/proj\n is replaced with the project under which the experiment was created -- \nDeterTest\n in this example -- and the element after \nexp\n is the experiment name -- \nexample1\n in this case.  These directories are created for all DETERLab experiments.  \n\n\n\n\ncontainers\n sub-directory\n\uf0c1\n\n\nThe \ncontainers\n sub-directory of a containerized experiment holds information specific to a containerized experiment. There are a few useful bits of data in that per-experiment containers directory that we can look at.\n\n\n\n\nCopy of the topology file:\n First, a copy of the topology that we gave to \ncontainerize.py\n is available in \n/proj/DeterTest/exp/example1/containers/experiment.tcl\n.  If the experiment is created from a topdl file, the filename will be \ncontainers/experiment.tcl\n.\n\n\nVisualization of experiment:\n A simple visualization of the experiment is in \ncontainers/visualization.png\n.  This is annotated with node and network names as well as interface IP addresses.  The topology depiction \nabove\n is an example.  To view a \nlarger version, click here\n.\n\n\nIP-to-hostname mapping:\n The \ncontainers/hosts\n file is a copy of the IP-to-hostname mapping found on each virtual machine in the topology.  It can be useful in converting IP addresses back to names.  It is installed in \n/etc/hosts\n or the equivalent on each machine.\n\n\nPID/EID:\n The two files \n/var/containers/pid\n and \n/var/containers/eid\n contain the project name and experiment name.  Scripts can make use of these.\n\n\n\n\nThe rest of the contents of that directory are primarily used internally by the implementation, but a more detailed listing is in the \nContainers Reference\n.\n\n\nStep 3: Swap-in resources\n\uf0c1\n\n\nAt this point, as with any DETER experiment, the topology does not yet have any resources attached.  To get the resources, swap the experiment in from the web interface or using the \nswapexp\n command. See the \nDETERLab Core Guide\n for more information.\n\n\nStep 4: Verify virtual topology and access nodes\n\uf0c1\n\n\nOnce you have been notified that the physical experiment has finished its swap-in, the Containers system starts converting the physical topology into the virtual topology. \n\n\nAt this time, you must manually verify when the virtual topology has been created by ping-ing or trying to SSH into individual nodes of an experiment. There is also a \nworkaround suggested below\n. We are working towards offering a better notification system.\n\n\nOnce the containerized elements have all started, the nodes are available as if they were physical nodes.  For example, we may access node \nn-0\n of the experiment we swapped in by running:\n\n\n$ ssh n-0.example1.detertest\n\n\n\n\nBe sure that you replace \nexample1\n with the experiment name you passed to \ncontainerize.py\n and \nDeterTest\n with the project you created the experiment under.  This is a DNS name, so it is case-insensitive.\n\n\nWhen the SSH succeeds, you will have access to an Ubuntu 10.04 32-bit node with the same directories mounted as in a physical DETERLab experiment. Containerized nodes access the control net as well. Your home directory will be mounted, so your SSH keys will work for accessing the machine.\n\n\nUse the \nsame node naming conventions\n as physical DETERLab experiments to ping and access other nodes.\n\n\nHere is a ping from \nn-0\n to \ncenter\n and \nn-1\n that confirms the containerized experiment is working as we expect.\n\n\nn-0:~$ ping -c 3 center\nPING center-tblink-l21 (10.0.0.2) 56(84) bytes of data.\n64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=1 ttl=64 time=20.4 ms\n64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=2 ttl=64 time=20.0 ms\n64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=3 ttl=64 time=20.0 ms\n\n--- center-tblink-l21 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2002ms\nrtt min/avg/max/mdev = 20.052/20.184/20.445/0.184 ms\nn-0:~$ ping -c 3 n-1\nPING n-1-tblink-l5 (10.0.6.1) 56(84) bytes of data.\n64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=1 ttl=64 time=40.7 ms\n64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=2 ttl=64 time=40.0 ms\n64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=3 ttl=64 time=40.0 ms\n\n--- n-1-tblink-l5 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2003ms\nrtt min/avg/max/mdev = 40.094/40.318/40.764/0.355 ms\n\n\n\n\nThe nodes have the expected round trip times.\n\n\nAt this point you can load and run software and generally experiment normally.\n\n\nStart Commands\n\uf0c1\n\n\nDETERLab Core provides a facility to run a command when a physical experiment starts, called \nstart commands\n.  A containerized experiment offers a similar facility with a few differences:\n\n\n\n\nThe start commands are not coordinated across nodes.  In a physical experiment, the start commands all execute when the last node has reported to the testbed that it has completed booting.  In a containerized experiment, the start commands run when the containerized node has come up.\n\n\nLogs from the start command are available in \n/var/containers/log/start_command.out\n and \n/var/containers/log/start_command.err\n.  This is true on embedded pnodes as well.\n\n\nStart commands must be shorter than in a physical experiment because the Containers system is also using the facility.\n\n\nThe event system cannot be used to replay the start command.\n\n\n\n\n\n\nNotes\n\n\n\n\nWhile start commands that make use of shell syntax for multiple commands and simple file redirection (e.g, \n or \n) may work, errors parsing redirection or other shell commands will cause the start command to \nfail silently\n.  If you are doing anything more complex than calling a single program, we recommend that you create a simple script and run the script from the per-experiment directory or your home directory.  This makes it more likely that the log files created by containers will have useful debugging information.\n\n\nWe strongly recommend removing all shell redirection characters from the text of your start command.\n  Redirecting I/O in the text of the start command may \nfail silently\n.\n\n\n\n\n\n\nStart commands offer a simple workaround for detecting that all nodes in an experiment have started:\n\n\n    #!/bin/sh\n\n    STARTDIR=\n/proj/\n`cat /var/containers/pid`\n/exp/\n`cat /var/containers/eid`\n/startup\n\n\n    mkdir $STARTDIR\n    date \n $STARTDIR/`hostname`\n\n\n\n\nIf you make the script above the start command of all nodes, the Containers system will put the time that each local container came up in the \nstartup\n directory under the per-experiment directories.  For example, \nn-0.example1.DeterTest\n will create \n/proj/DeterTest/exp/example1/startup/n-0\n.  Then you may monitor that directory on \nusers\n to know which nodes are up.\n\n\nStep 4: Releasing Resources\n\uf0c1\n\n\nAs with a physical DETER experiment, release resources by swapping the experiment out using the web interface or the \nswapexp\n command (see the \nCore Guide\n for more information.  If you are using the \nstartcommand workaround\n to detect startup, clear the startup directory when you swap the experiment out.\n\n\nAdvanced Topics\n\uf0c1\n\n\nThe previous tutorial described how to create an experiment using only openvz containers packed 10 to a machine.  This section describes how to change those parameters.\n\n\nUsing Other Container Types\n\uf0c1\n\n\nTo change the container type that \ncontainerize.py\n assigns to nodes, use the \n--default-container\n option.  Valid choices follow the [ContainersQuickstart#KindsofContainers kinds of containers] DETERLab supports.  Specifically:\n\n\n\n\n\n\n\n\nParameter\n\n\nContainer\n\n\n\n\n\n\n\n\n\n\nembedded_pnode\n\n\nPhysical Node\n\n\n\n\n\n\nqemu\n\n\nQemu VM\n\n\n\n\n\n\nopenvz\n\n\nOpenvz Container\n\n\n\n\n\n\nprocess\n\n\nViewOS process\n\n\n\n\n\n\n\n\nYou can try this on our \nexample topology\n:\n\n\nusers:~$ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example1.tcl \n\nRequested a QEMU node with more than 7 experimental interfaces.  Qemu nodes\ncan only support 7 experimental interfaces.\n\n\n\n\nThe Containers system is now using qemu containers to build our experiment.  Unfortunately qemu containers only support 7 experimental interfaces, an internal limit on the number of interfaces the virtual hardware supports.  Run the command again but use the attached \nversion of the topology with fewer satellites\n to containerize without error.\n\n\n$ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example2.tcl \nContainerized experiment DeterTest/example2 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example2\n\n\n\n\nThe qemu experiment looks much like the openvz experiment above, at this small scale.  Qemu nodes more completely emulate hardware and the kernels are independent, unlike openvz containers.  For example, a program can load kernel modules in a qemu VM, which it cannot do in an openvz container.  The qemu containers load the Ubuntu 12.04 (32 bit) distribution by default.\n\n\nWe can also swap in the experiment using ViewOS processes, but processes cannot be manipulated from outside.  They are too lightweight to allow an SSH login, though they will run a start command.\n\n\nMixing Containers\n\uf0c1\n\n\nMixing containers requires you to assign container types in the topology description.  This is done by attaching an attribute to nodes.  The attribute is named \ncontainers:node_type\n and it takes the same values as the \n--default-container parameter to containerize.py\n.  If the experiment definition is in \ntopdl\n, the attribute can be attached using the \nstandard topdl routines\n.  Attaching the attribute in ns2 is done using the DETERLab \ntb-add-node-attribute\n command.\n\n\ntb-add-node-attribute $node containers:node_type openvz\n\n\n\n\nUsing this command in an ns2 topology description will set \nnode\n to be placed in an openvz container.  Using this feature, we can modify our \nfirst example topology\n to consist of qemu nodes and a single process container in the center.  Process nodes can have unlimited interfaces, but we cannot log into them.  The \nnew topology file\n looks like this:\n\n\n    source tb_compat.tcl\n    set ns [new Simulator]\n\n    # Create the center node (named by its variable name)\n    set center [$ns node]\n    # The center node is a process\n    tb-add-node-attribute $center containers:node_type process\n\n    # Connect 9 satellites\n    for { set i 0} { $i \n 9 } { incr i} {\n        # Create node n-1 (tcl n($i) becomes n-$i in the experiment)\n        set n($i) [$ns node]\n        # Satellites are qemu nodes\n        tb-add-node-attribute $n($i) containers:node_type qemu\n        # Connect center to $n($i)\n        ns duplex-link $center $n($i) 100Mb 10ms DropTail\n    }\n\n    # Creation boilerplate\n    $ns rtptoto Static\n    $ns run\n\n\n\n\nBecause we have explicitly set the \ncontainer_node_type\n of each node, the \n--default-container\n parameter to \ncontainerize.py\n does nothing.  Create this experiment by running:\n\n\nusers:~$ /share/containers/containerize.py  DeterTest example3 ~/example3.tcl \nContainerized experiment DeterTest/example3 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example3\n\n\n\n\nWhen we swap it in, the experiment will have 10 satellite containers in qemu VMs and a central process that only forwards packets.  Again, you cannot log in to a process container, but you can use the qemu nodes as though they were physical machines.\n\n\nAnother interesting mixture of containers is to include a physical node.  Here is a \nmodified version of our mixed topology\n that places the \nn-8\n satellite on a physical computer by setting its \ncontainers:node_type\n to \nembedded_pnode\n.\n\n\nAfter running that experiment you should have output similar to the following:\n\n\nusers:~$ /share/containers/containerize.py  DeterTest example4 ~/example4.tcl\nContainerized experiment DeterTest/example4 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example4\n\n\n\n\nFollow the url to the DETERLab experiment page and look at the \nVisualization\n tab:\n\n\n\n\nThe physical node \nn-8\n shows up in the DETERLab visualization and otherwise acts as a physical node that is in a 10-node topology.  This experiment uses three different container types: physical nodes, ViewOS processes, and Qemu VMs.\n\n\nLimitations on Mixing Containers\n\uf0c1\n\n\nAt the moment, Qemu VMs and ViewOS processes are the only containers that can share a physical node.  Physical node containers are mapped one-to-one to physical nodes by definition.  Qemu and OpenVZ use different underlying operating system images in DETERLab, therefore they cannot share physical hardware.  Neither ViewOS processes nor Qemu VMs can share a physical machine with OpenVZ VMs.\n\n\nThe first invocation of \ntb-add-node-attribute\n takes precedence.  It is best to only call \ntb-add-node-attribute\n once per attribute assigned on each node.\n\n\nSetting Openvz Parameters\n\uf0c1\n\n\nAn advantage of openvz nodes is that you can set the OS flavor and CPU bit-width across experiments and per-node.  Similarly, you can set the size of the disk allocated to each node.  \n\n\nOpenvz uses templates to look like various Linux installations.  The choices of Linux distribution that openvz supports are:\n\n\n\n\n\n\n\n\nTemplate\n\n\nDistribution\n\n\nBit-width\n\n\n\n\n\n\n\n\n\n\ncentos-6-x86\n\n\nCentOS 6\n\n\n32 bit\n\n\n\n\n\n\ncentos-6-x86_64\n\n\nCentOS 6\n\n\n64 bit\n\n\n\n\n\n\nubuntu-10.04-x86\n\n\nUbuntu 10.04 LTS\n\n\n32 bit\n\n\n\n\n\n\nubuntu-10.04-x86_64\n\n\nUbuntu 10.04 LTS\n\n\n64 bit\n\n\n\n\n\n\nubuntu-12.04-x86\n\n\nUbuntu 12.04 LTS\n\n\n32 bit\n\n\n\n\n\n\nubuntu-12.04-x86_64\n\n\nUbuntu 12.04 LTS\n\n\n64 bit\n\n\n\n\n\n\n\n\nThe default template is \nubuntu-10.04-x86\n.\n\n\nTo set a template across an entire topology, give \n--openvz-template\n and the template name from the list above.  Invoking \ncontainerize.py\n on \nour original example\n as below will instantiate the experiment under 64-bit Ubuntu 12.04:\n\n\nusers:~$ /share/containers/containerize.py --openvz-template ubuntu-12.04-x86_64 DeterTest example1 ~/example1.tcl \nContainerized experiment DeterTest/example1 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example1\n\n\n\n\nTo set the size of the file system of containers in the experiment, use \n--openvz-diskspace\n.  The value of the parameter is determined by the suffix:\n\n\n\n\n\n\n\n\nSuffix\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nG\n\n\nGigabytes\n\n\n\n\n\n\nM\n\n\nMegabytes\n\n\n\n\n\n\n\n\nThe default openvz file system size is 2GB.\n\n\nThe most practical suffix for DETERLab nodes is \"G\":\n\n\nusers:~$ /share/containers/containerize.py --openvz-diskspace 15G DeterTest example1 ~/example1.tcl \nContainerized experiment DeterTest/example1 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example1\n\n\n\n\nEach of these parameters can be set on individual nodes using attributes.  Use \ncontainers:openvz_template\n to set a template on a node and use \ncontainers:openvz_diskspace\n to set the disk space. \nThis example topology\n sets these openvz parameters per node:\n\n\n    source tb_compat.tcl\n    set ns [new Simulator]\n\n    # Create the center node (named by its variable name)\n    set center [$ns node]\n    # The center node is a process\n    tb-add-node-attribute $center containers:node_type process\n    tb-add-node-attribute $center containers:openvz_template ubuntu-12.04-x86_64\n\n    # Connect 9 satellites\n    for { set i 0} { $i \n 9 } { incr i} {\n        # Create node n-1 (tcl n($i) becomes n-$i in the experiment)\n        set n($i) [$ns node]\n        # Set satellite disk sizes to be 20 GB\n        tb-add-node-attribute $n($i) containers:openvz_diskspace 20G\n        # Connect center to $n($i)\n        ns duplex-link $center $n($i) 100Mb 10ms DropTail\n    }\n\n    # Creation boilerplate\n    $ns rtptoto Static\n    $ns run\n\n\n\n\nThe \ncenter\n node will run Ubuntu 12.04 64 bit and the satellites will have 20GB file systems.\n\n\nSetting Qemu Parameters\n\uf0c1\n\n\nThe Containers system has a more limited ability to set qemu parameters.  Right now, a custom image may be loaded using the \ncontainers::qemu_url\n attribute and the architecture of the qemu VM may be chosen using \ncontainers:qemu_arch\n.  Valid qemu architectures are:\n\n\n\n\n\n\n\n\nParam\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\ni386\n\n\n32 bit Intel\n\n\n\n\n\n\nx86_64\n\n\n64-bit Intel\n\n\n\n\n\n\n\n\nThe image URL must be reachable from inside DETERLab.  The image must be a qcow2 image, optionally bzip2ed.  Facilities to snapshot and store such images are in development.\n\n\nIf you are using a qemu image that is not booting into containers, make sure \ngrub is properly configured\n.\n\n\nQemu images also mount users' home directories the same as DETERLab physical nodes do.  In order to do this scalably, the Qemu VMs mount the users' directories from the physical node.  The DETER infrastructure cannot support exporting users' directories to thousands of containers.\n\n\nHowever, a Qemu VM can only mount a few tens of user directories this way.  The limit is 23 user directories (24 in experiments that are not instantiated in a group).  Many projects have more than 23 users, but in practice only a few experimenters need access to the containers.\n\n\nTo tell the Containers systems which user to mount, use the \n--qemu-prefer-users\n option to \ncontainerize.py\n.  That option takes a comma-separated list of usernames (no spaces).  When the Qemu nodes will always mount those users' home directories.  Others will be mounted if there is room.\n\n\nFor example:\n\n\nusers:~$ /share/containers/containerize.py  --qemu-prefer-users=faber,jjh DeterTest example4 ~/example4.tcl\n\n\n\n\nThis command makes sure that users \nfaber\n and \njjh\n have their home directories mounted in any Qemu containers.\n\n\nChanging The Packing Factor\n\uf0c1\n\n\nThe \ncontainerize.py\n program decides how many virtual nodes to put on each physical machine.  Because we have been using roughly the same number of nodes as the default packing target (10 nodes per machine) all of the examples so far have fit onto a single machine.  If we change the packing factor by using the \n--packing\n parameter to \ncontainerize.py\n, we can put fewer nodes on each machine.  For example:\n\n\nusers:~$ /share/containers/containerize.py --packing 2 DeterTest example1 ~/example1.tcl \nContainerized experiment DeterTest/example1 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example1\n\n\n\n\nThis command calls \ncontainerize.py\n on our \noriginal topology\n with a low packing factor.  The result is the same nodes spread across more physical machines, as we can see from the DETERLab web interface (on the \nVisualization\n tab):\n\n\n\n\nYou will want to balance how many physical machines you use against how precisely you want to mimic them.\n\n\nUser Packing\n\uf0c1\n\n\nYou may specify your own packing using the \ncontainers:partition\n attribute.  This attribute must be assigned an integer value. All nodes with the same partition are allocated to the same machine.  If nodes have that attribute attached to them, \ncontainers.py\n will assume that they all have been partitioned and use those.  Nodes without a partition assigned are assumed to be \nembedded_pnode\ns.\n\n\nMore Sophisticated Packing: Multiple Passes\n\uf0c1\n\n\nThe previous examples have all treated packing containers onto physical machines as a single-step process with a single parameter - the packing factor.  In fact, we can divide containers into sets and pack each set independently using different parameters.  \n\n\nFor example in an experiment with many containers dedicated only to forwarding packets and a few to modeling servers, we could create two sets and pack the forwarders tightly (using a high packing factor) and the servers loosely.\n\n\nIn exchange for providing greater control on packing, there is a price.  When a \nset\n of containers is packed, the Containers system takes into account both the nodes to be packed and their interconnections.  When \nsubsets\n of containers are packed, the system cannot consider the interconnections between subsets.  In some cases, the packing of subsets can lead to a DETERLab experiment that cannot be created successfully.  This danger is mitigated by the fact that containers that are packed together are often related in ways that limit the number of connections between that set and another.\n\n\nTo explore packing, we need to use a \nlarger topology\n:\n\n\n    source tb_compat.tcl\n    set ns [new Simulator]\n\n    set center [$ns node]\n    tb-add-node-attribute $center \ncontainers:PartitionPass\n 0\n\n\n    for { set i 0} { $i \n 3 } { incr i} {\n        set lanlist $center\n        for { set j 0 } { $j \n 20} { incr j } {\n            set idx [expr $i * 20 + $j]\n            set n($idx) [$ns node]\n            tb-add-node-attribute $n($idx) \ncontainers:PartitionPass\n [expr $i + 1]\n            lappend lanlist $n($idx)\n        }\n        set lan($i) [$ns make-lan [join $lanlist \n \n] 100Mb 0]\n    }\n\n    # Creation boilerplate\n    $ns rtptoto Static\n    $ns run\n\n\n\n\nThis creates three 20-node sub networks attached to a single central router.  It looks like this:\n\n\n\n\nEach node in the topology is assigned a \ncontainers::PackingPass\n attribute that groups them into subsets.  The \ncontainers:PackingPass\n attribute must be assigned an integer value.  The nodes in each packing pass are considered \"together\" when packing.  Each pass can be assigned different parameters.  The passes are carried out in order, though that is rarely important.\n\n\nOur example topology assigns \ncenter\n to pass 0, the nodes on \nlan-0\n (the tcl variable lan(0)) to pass 1, those on \nlan-1\n to pass 2 and those on \nlan-2\n to pass 3.  We will use the \n--pass-pack\n parameter to specify the packing factor for each pass.  Each packing factor specification looks like \npass\n:\nfactor\n where pass and factor are both integers.  We can specify more than one, separated by commas, or specify \n--pass-pack\n more than once.\n\n\nFor example, we can pack the experiment using the following factors:\n\n\n\n\n\n\n\n\nPass\n\n\nPacking Factor\n\n\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n1\n\n\n20\n\n\n\n\n\n\n2\n\n\n10\n\n\n\n\n\n\n3\n\n\n5\n\n\n\n\n\n\n\n\nBy issuing the following command:\n\n\nusers:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example6\n\n\n\n\nWe can view the packing by using \ncontainer_image.py\n to generate a visualization that includes the partitions:\n\n\nusers:~$ /share/containers/container_image.py --experiment DeterTest/example6 --partitions --out ~/example6.png\n\n\n\n\nThe output shows the topology with boxes drawn around the containers that share a physical node:\n\n\n\n\nThat partitioning is surprising in that \nlan-1\n is split into three partitions of 6 \n 7 nodes rather than two partitions of 10.  Similarly \nlan-2\n is split into five groups of 4 rather than four groups of 5.  \n\n\nThe packing system is built on the \nmetis\n graph partitioning software.  Metis takes a graph and a number of partitions and finds the most balanced partitioning that has roughly equal node counts in each partition as well as low inter-partition communication costs.  The Containers system calls metis with increasing numbers of partitions until a partitioning is found that meets the packing factor limits.\n\n\nWhen the system attempts to pack \nlan-1\n into two partitions, metis balances the node counts and the communications costs to produce a partition with 9 containers in one machine and 11 on the other.  That partitioning does not meet the 10 node limit, so it tries again with three partitions and succeeds.\n\n\nThere are two ways to fit our topology onto fewer nodes.  The first is to put slightly more slop into the packing factors:\n\n\n\n\n\n\n\n\nPass\n\n\nPacking Factor\n\n\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n1\n\n\n20\n\n\n\n\n\n\n2\n\n\n11\n\n\n\n\n\n\n3\n\n\n6\n\n\n\n\n\n\n\n\nAs in the following command:\n\n\nusers:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:11,3:6 DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example6\n\n\n\n\nThese parameters result in this packing, which fits in fewer nodes, but has the slight imbalances of splitting \nlan-1\n into 9 and 11 containers and \nlan-2\n into 4,5,and 6 container partitions.  Again, this asymmetry is an attempt to consider the internode networking costs.\n\n\n\n\nIf the packing constraints are exact - 11 containers on \nlan-1\n is unacceptable - a second choice is to use the \n--nodes-only\n option.  This sets the cost of each arc in the graph to 0.  Metis ignores such arcs altogether, so the partitions are completely even.  This may cause trouble in more complex network topologies.\n\n\nThe result of running the command with the original packing factors and \n--nodes-only\n):\n\n\nusers:~$ /share/containers/containerize.py --nodes-only --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example6\n\n\n\n\nis\n\n\n\n\nwhich has symmetric partitions.\n\n\nSometimes it is more intuitive to think in terms of the number of machines that will be used to hold containers.  The \n--size\n and \n-pass-size\n options let users express that.  The \n--size=\nexpsize\n option uses \nexpsize\n machines to hold the whole experiment.  If multiple passes are made, each is put into \nexpsize\n physical machines.  The \n--size\n option takes precedence over \n--packing\n.\n\n\nPer-pass sizing can be done using \n--pass-size\n which uses the same syntax as \n--pass-pack\n.  Therefore the command:\n\n\nusers:~$ /share/containers/containerize.py --pass-size 0:1,1:1,2:2,3:4 DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example6\n\n\n\n\npacks pass 0 into one physical machine, pass 1 into one physical machine, pass 2 into two physical machines and pass 3 into four physcial machines.  The result looks like:\n\n\n\n\nPartitions have different numbers of containers in them because metis is considering network constraints as well.  As with using packing, adding \n--nodes-only\n restores symmetry:\n\n\n\n\nThe \n--pass-pack\n option is a per-pass generalization of the \n--packing\n option.  The options that can be specified per-pass are:\n\n\n\n\n\n\n\n\nSingle-pass\n\n\nPer-Pass\n\n\nPer-Pass Format\n\n\nPer-Pass Example\n\n\n\n\n\n\n\n\n\n\n--packing\n\n\n--pass-pack\n\n\npass\n:\npacking\n  (comma-separated)\n\n\n--pass-pack 0:1,1:20,2:11,3:6\n\n\n\n\n\n\n--size\n\n\n--pass-size\n\n\npass\n:\nsize\n  (comma-separated)\n\n\n--pass-size 0:1,1:1,2:2,3:4\n\n\n\n\n\n\n--pnode-types\n\n\n--pass-pnodes\n\n\npass\n:\npnode\n[,\npnode\n...] (semicolon separated)\n\n\n--pass-pnodes 0:MicroCloud;1:bpc2133,pc2133\n\n\n\n\n\n\n--nodes-only\n\n\n--pass-nodes-only\n\n\npass\n (comma-separated)\n\n\n--pass-nodes-only 1,3,5\n\n\n\n\n\n\n\n\nThe single-pass version sets a default so that this invocation on \nour 4 pass topology\n:\n\n\nusers:~$ /share/containers/containerize.py --packing 5 --pass-pack 0:1,1:20 DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example6\n\n\n\n\nwill pack pass 0 with a factor of 1, pass 1 with a factor of 20 and passes 2 and 3 with a factor of 5.\n\n\nSimilarly:\n\n\nusers:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes '0:pc2133,bpc2133;1:MicroCloud' DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example6\n\n\n\n\nwill allocate either bpc2133 or pc2133 nodes to containers assigned by pass 0 and Microcloud physical nodes to the containers partitioned in pass 1.  The rest will be allocated as the \nsite configuration\n specifies.  The single quotes around the \n--pass-pnodes\n option protects the semi-colon from the shell.  Another choice is to specify the command as:\n\n\nusers:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes 0:pc2133,bpc2133 --pass-pnodes 1:MicroCloud DeterTest example6 ~/example6.tcl \n\n\n\n\nThat formulation avoids the quotes by avoiding the semicolon.  All the per-pass options may be specified multiple times on the command line.\n\n\nYou can mix and match sizes and packing factors.  This invocation:\n\n\n/share/containers/containerize.py --pass-size 1:10 --pass-pack 2:5,3:10 DeterTest example6 ~/example6.tcl\nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest\neid=example6\n\n\n\n\nProduces:\n\n\n\n\nRemember that \n--size\n sets a default pass size and that sizes have precedence over packing.  If you specify \n--size\n, no \n--packing\n or \n--pass-packing\n value will take effect.  To mix packing and sizes, use \n--pack-size\n for each sized pass, rather than \n--size\n.\n\n\nThese per-pass variables and user-specified pass specifications give users fine grained control over the paritioning process, even if they do not want to do the partitioning themselves.\n\n\nIf no \ncontainers:PartitionPass\n attributes are specified in the topology, and no \ncontainers:Partition\n attributes are specified either, ```containerize.py}} carries out -- at most -- two passes.  Pass 0 paritions all openvz containers and pass 1 partitions all qemu and process containers.\n\n\nFurther Reading\n\uf0c1\n\n\nHopefully these illustrative examples have given you an idea of how to use the containers system and what it is capable of.  More details are available from \nthe reference guide\n.  Please see \n Getting Help\n if you have difficulties.", 
            "title": "Containers Guide"
        }, 
        {
            "location": "/containers/containers-guide/#basic-containers-tutorial", 
            "text": "This tutorial will set up a containerized experiment with a star topology. We'll create a central node and connect 9 other nodes to it  Getting started \uf0c1  You will need a DETERLab account and be a member of a project. If you need help, see  the Core Guide .   Step 1: Design the topology \uf0c1  First, we will describe a star topology.   For this example we will use the standard DETER topology descriptions.  If you are new to designing topologies, walk through the basic tutorial in the  Core Guide .  The Containers system is largely compatible with the physical DETER interface.  Download the  DETERLab-compatible ns2 description of this topology at this link  to your home directory on  users.isi.deterlab.net .  It is a simple loop, along with the standard DETER boilerplate.  This file will be used to create a 10-node (9 satellites and one central node) physical experiment on DETER, although there are not many physical nodes on DETER with 10 interfaces (one interface for control traffic).  The following is the topology description:      source tb_compat.tcl\n    set ns [new Simulator]\n\n    # Create the center node (named by its variable name)\n    set center [$ns node]\n\n    # Connect 9 satellites\n    for { set i 0} { $i   9 } { incr i} {\n        # Create node n-1 (tcl n($i) becomes n-$i in the experiment)\n        set n($i) [$ns node]\n        # Connect center to $n($i)\n        ns duplex-link $center $n($i) 100Mb 10ms DropTail\n    }\n\n    # Creation boilerplate\n    $ns rtptoto Static\n    $ns run   Note   The central node is named \"center\" and each satellite is names \"n-0\", \"n-1\"... through \"n-8\".    Each connection is a 100 Mb/s link with a 10ms delay.    The round trip time from n-0 to center will be 20 ms and from n-0 to n-1 will be 40 ms.    Step 2: Create the containerized experiment \uf0c1  Now we will run a command so the Containers system will build the containerized experiment on top of a new DETERLab physical experiment.    Run the following command from the shell on  users.isi.deterlab.net  and refer to the example topology you just saved in your home.    $ /share/containers/containerize.py DeterTest example1 ~/example1.tcl   where the first two parameters are the project and experiment name to hold the DETER experiment.    This command creates an experiment called  experiment1  in the  DeterTest  project. Throughout this tutorial, we will refer to your project as  DeterTest , but make sure you actually use your actual project's name. You may use the experiment name  experiment1  as long as another experiment with that name doesn't already exist   Note  As with any DETERLab experiment, you must be a member of the project with appropriate rights to create an experiment in it.   containerize.py  expects there to be no experiment with that name, and it will fail if one exists.  To remove an experiment you may terminate it through the web interface or use the  endexp  command.  Terminating an experiment is more final than swapping one out, so be sure that you want to replace the old experiment.  You may also resolve the conflict by renaming your new containerized experiment.   The last parameter is the file containing the topology.  In this tutorial, we are referring to the ns2 file in  our example  but you may also use a  topdl  description.  An ns2 description must end in  .tcl  or  .ns .  With these default parameters  containerize.py  will put each node into an  Openvz container  with at most 10 containers per physical node.  The output of the above command should be something like the following:  users:~$ /share/containers/containerize.py DeterTest example1 ~/example1.tcl \nContainerized experiment DeterTest/example1 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example1  Now we can see what a containerized experiment looks like.  The Contents of a Containerized Experiment \uf0c1  Follow the link provided in the  containerize.py  output. You will see a standard DETER experiment page that looks like this:   You may be surprised to see that DETER thinks the experiment has only one node:   The Containers system has rewritten the description file and stored additional information in the experiment's per-experiment directory that will be used to create the 10 node experiment inside the single-node DETER experiment.    If you look at the ns file DETERLab has stored (click the  NS File  tab on the experiment page), you should see the following code:  set ns [new Simulator]\nsource tb_compat.tcl\n\ntb-make-soft-vtype container0 {pc2133 bpc2133 MicroCloud}\nset pnode(0000) [$ns node]\ntb-set-node-os ${pnode(0000)} CentOS6-64-openvz\ntb-set-hardware ${pnode(0000)} container0\ntb-set-node-startcmd ${pnode(0000)}  sudo /share/containers/setup/hv/bootstrap /proj/DeterTest/exp/example1/containers/site.conf   /tmp/container.log \ntb-set-node-failure-action ${pnode(0000)}  nonfatal \n\n$ns rtproto Static\n$ns run  This looks nothing like the file we gave to  containerize.py , but it does show us a little about what the Containers system has done:   The single physical node ( pnode(0000) ) will run the  CentOS6-64-openvz  image and run on a few kinds of node.    On startup,  pnode(0000)  will execute a command from the same  /share/containers  directory that  containerize.py  ran from using data in the per-experiment directory  /proj/DeterTest/exp/example1/containers/site.conf .  There is a separate  /proj/DeterTest/exp/example1/containers/  directory for each experiment.  The path element after  /proj  is replaced with the project under which the experiment was created --  DeterTest  in this example -- and the element after  exp  is the experiment name --  example1  in this case.  These directories are created for all DETERLab experiments.     containers  sub-directory \uf0c1  The  containers  sub-directory of a containerized experiment holds information specific to a containerized experiment. There are a few useful bits of data in that per-experiment containers directory that we can look at.   Copy of the topology file:  First, a copy of the topology that we gave to  containerize.py  is available in  /proj/DeterTest/exp/example1/containers/experiment.tcl .  If the experiment is created from a topdl file, the filename will be  containers/experiment.tcl .  Visualization of experiment:  A simple visualization of the experiment is in  containers/visualization.png .  This is annotated with node and network names as well as interface IP addresses.  The topology depiction  above  is an example.  To view a  larger version, click here .  IP-to-hostname mapping:  The  containers/hosts  file is a copy of the IP-to-hostname mapping found on each virtual machine in the topology.  It can be useful in converting IP addresses back to names.  It is installed in  /etc/hosts  or the equivalent on each machine.  PID/EID:  The two files  /var/containers/pid  and  /var/containers/eid  contain the project name and experiment name.  Scripts can make use of these.   The rest of the contents of that directory are primarily used internally by the implementation, but a more detailed listing is in the  Containers Reference .  Step 3: Swap-in resources \uf0c1  At this point, as with any DETER experiment, the topology does not yet have any resources attached.  To get the resources, swap the experiment in from the web interface or using the  swapexp  command. See the  DETERLab Core Guide  for more information.  Step 4: Verify virtual topology and access nodes \uf0c1  Once you have been notified that the physical experiment has finished its swap-in, the Containers system starts converting the physical topology into the virtual topology.   At this time, you must manually verify when the virtual topology has been created by ping-ing or trying to SSH into individual nodes of an experiment. There is also a  workaround suggested below . We are working towards offering a better notification system.  Once the containerized elements have all started, the nodes are available as if they were physical nodes.  For example, we may access node  n-0  of the experiment we swapped in by running:  $ ssh n-0.example1.detertest  Be sure that you replace  example1  with the experiment name you passed to  containerize.py  and  DeterTest  with the project you created the experiment under.  This is a DNS name, so it is case-insensitive.  When the SSH succeeds, you will have access to an Ubuntu 10.04 32-bit node with the same directories mounted as in a physical DETERLab experiment. Containerized nodes access the control net as well. Your home directory will be mounted, so your SSH keys will work for accessing the machine.  Use the  same node naming conventions  as physical DETERLab experiments to ping and access other nodes.  Here is a ping from  n-0  to  center  and  n-1  that confirms the containerized experiment is working as we expect.  n-0:~$ ping -c 3 center\nPING center-tblink-l21 (10.0.0.2) 56(84) bytes of data.\n64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=1 ttl=64 time=20.4 ms\n64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=2 ttl=64 time=20.0 ms\n64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=3 ttl=64 time=20.0 ms\n\n--- center-tblink-l21 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2002ms\nrtt min/avg/max/mdev = 20.052/20.184/20.445/0.184 ms\nn-0:~$ ping -c 3 n-1\nPING n-1-tblink-l5 (10.0.6.1) 56(84) bytes of data.\n64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=1 ttl=64 time=40.7 ms\n64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=2 ttl=64 time=40.0 ms\n64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=3 ttl=64 time=40.0 ms\n\n--- n-1-tblink-l5 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2003ms\nrtt min/avg/max/mdev = 40.094/40.318/40.764/0.355 ms  The nodes have the expected round trip times.  At this point you can load and run software and generally experiment normally.  Start Commands \uf0c1  DETERLab Core provides a facility to run a command when a physical experiment starts, called  start commands .  A containerized experiment offers a similar facility with a few differences:   The start commands are not coordinated across nodes.  In a physical experiment, the start commands all execute when the last node has reported to the testbed that it has completed booting.  In a containerized experiment, the start commands run when the containerized node has come up.  Logs from the start command are available in  /var/containers/log/start_command.out  and  /var/containers/log/start_command.err .  This is true on embedded pnodes as well.  Start commands must be shorter than in a physical experiment because the Containers system is also using the facility.  The event system cannot be used to replay the start command.    Notes   While start commands that make use of shell syntax for multiple commands and simple file redirection (e.g,   or  ) may work, errors parsing redirection or other shell commands will cause the start command to  fail silently .  If you are doing anything more complex than calling a single program, we recommend that you create a simple script and run the script from the per-experiment directory or your home directory.  This makes it more likely that the log files created by containers will have useful debugging information.  We strongly recommend removing all shell redirection characters from the text of your start command.   Redirecting I/O in the text of the start command may  fail silently .    Start commands offer a simple workaround for detecting that all nodes in an experiment have started:      #!/bin/sh\n\n    STARTDIR= /proj/ `cat /var/containers/pid` /exp/ `cat /var/containers/eid` /startup \n\n    mkdir $STARTDIR\n    date   $STARTDIR/`hostname`  If you make the script above the start command of all nodes, the Containers system will put the time that each local container came up in the  startup  directory under the per-experiment directories.  For example,  n-0.example1.DeterTest  will create  /proj/DeterTest/exp/example1/startup/n-0 .  Then you may monitor that directory on  users  to know which nodes are up.  Step 4: Releasing Resources \uf0c1  As with a physical DETER experiment, release resources by swapping the experiment out using the web interface or the  swapexp  command (see the  Core Guide  for more information.  If you are using the  startcommand workaround  to detect startup, clear the startup directory when you swap the experiment out.", 
            "title": "Basic Containers Tutorial"
        }, 
        {
            "location": "/containers/containers-guide/#advanced-topics", 
            "text": "The previous tutorial described how to create an experiment using only openvz containers packed 10 to a machine.  This section describes how to change those parameters.  Using Other Container Types \uf0c1  To change the container type that  containerize.py  assigns to nodes, use the  --default-container  option.  Valid choices follow the [ContainersQuickstart#KindsofContainers kinds of containers] DETERLab supports.  Specifically:     Parameter  Container      embedded_pnode  Physical Node    qemu  Qemu VM    openvz  Openvz Container    process  ViewOS process     You can try this on our  example topology :  users:~$ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example1.tcl \n\nRequested a QEMU node with more than 7 experimental interfaces.  Qemu nodes\ncan only support 7 experimental interfaces.  The Containers system is now using qemu containers to build our experiment.  Unfortunately qemu containers only support 7 experimental interfaces, an internal limit on the number of interfaces the virtual hardware supports.  Run the command again but use the attached  version of the topology with fewer satellites  to containerize without error.  $ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example2.tcl \nContainerized experiment DeterTest/example2 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example2  The qemu experiment looks much like the openvz experiment above, at this small scale.  Qemu nodes more completely emulate hardware and the kernels are independent, unlike openvz containers.  For example, a program can load kernel modules in a qemu VM, which it cannot do in an openvz container.  The qemu containers load the Ubuntu 12.04 (32 bit) distribution by default.  We can also swap in the experiment using ViewOS processes, but processes cannot be manipulated from outside.  They are too lightweight to allow an SSH login, though they will run a start command.  Mixing Containers \uf0c1  Mixing containers requires you to assign container types in the topology description.  This is done by attaching an attribute to nodes.  The attribute is named  containers:node_type  and it takes the same values as the  --default-container parameter to containerize.py .  If the experiment definition is in  topdl , the attribute can be attached using the  standard topdl routines .  Attaching the attribute in ns2 is done using the DETERLab  tb-add-node-attribute  command.  tb-add-node-attribute $node containers:node_type openvz  Using this command in an ns2 topology description will set  node  to be placed in an openvz container.  Using this feature, we can modify our  first example topology  to consist of qemu nodes and a single process container in the center.  Process nodes can have unlimited interfaces, but we cannot log into them.  The  new topology file  looks like this:      source tb_compat.tcl\n    set ns [new Simulator]\n\n    # Create the center node (named by its variable name)\n    set center [$ns node]\n    # The center node is a process\n    tb-add-node-attribute $center containers:node_type process\n\n    # Connect 9 satellites\n    for { set i 0} { $i   9 } { incr i} {\n        # Create node n-1 (tcl n($i) becomes n-$i in the experiment)\n        set n($i) [$ns node]\n        # Satellites are qemu nodes\n        tb-add-node-attribute $n($i) containers:node_type qemu\n        # Connect center to $n($i)\n        ns duplex-link $center $n($i) 100Mb 10ms DropTail\n    }\n\n    # Creation boilerplate\n    $ns rtptoto Static\n    $ns run  Because we have explicitly set the  container_node_type  of each node, the  --default-container  parameter to  containerize.py  does nothing.  Create this experiment by running:  users:~$ /share/containers/containerize.py  DeterTest example3 ~/example3.tcl \nContainerized experiment DeterTest/example3 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example3  When we swap it in, the experiment will have 10 satellite containers in qemu VMs and a central process that only forwards packets.  Again, you cannot log in to a process container, but you can use the qemu nodes as though they were physical machines.  Another interesting mixture of containers is to include a physical node.  Here is a  modified version of our mixed topology  that places the  n-8  satellite on a physical computer by setting its  containers:node_type  to  embedded_pnode .  After running that experiment you should have output similar to the following:  users:~$ /share/containers/containerize.py  DeterTest example4 ~/example4.tcl\nContainerized experiment DeterTest/example4 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example4  Follow the url to the DETERLab experiment page and look at the  Visualization  tab:   The physical node  n-8  shows up in the DETERLab visualization and otherwise acts as a physical node that is in a 10-node topology.  This experiment uses three different container types: physical nodes, ViewOS processes, and Qemu VMs.  Limitations on Mixing Containers \uf0c1  At the moment, Qemu VMs and ViewOS processes are the only containers that can share a physical node.  Physical node containers are mapped one-to-one to physical nodes by definition.  Qemu and OpenVZ use different underlying operating system images in DETERLab, therefore they cannot share physical hardware.  Neither ViewOS processes nor Qemu VMs can share a physical machine with OpenVZ VMs.  The first invocation of  tb-add-node-attribute  takes precedence.  It is best to only call  tb-add-node-attribute  once per attribute assigned on each node.  Setting Openvz Parameters \uf0c1  An advantage of openvz nodes is that you can set the OS flavor and CPU bit-width across experiments and per-node.  Similarly, you can set the size of the disk allocated to each node.    Openvz uses templates to look like various Linux installations.  The choices of Linux distribution that openvz supports are:     Template  Distribution  Bit-width      centos-6-x86  CentOS 6  32 bit    centos-6-x86_64  CentOS 6  64 bit    ubuntu-10.04-x86  Ubuntu 10.04 LTS  32 bit    ubuntu-10.04-x86_64  Ubuntu 10.04 LTS  64 bit    ubuntu-12.04-x86  Ubuntu 12.04 LTS  32 bit    ubuntu-12.04-x86_64  Ubuntu 12.04 LTS  64 bit     The default template is  ubuntu-10.04-x86 .  To set a template across an entire topology, give  --openvz-template  and the template name from the list above.  Invoking  containerize.py  on  our original example  as below will instantiate the experiment under 64-bit Ubuntu 12.04:  users:~$ /share/containers/containerize.py --openvz-template ubuntu-12.04-x86_64 DeterTest example1 ~/example1.tcl \nContainerized experiment DeterTest/example1 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example1  To set the size of the file system of containers in the experiment, use  --openvz-diskspace .  The value of the parameter is determined by the suffix:     Suffix  Value      G  Gigabytes    M  Megabytes     The default openvz file system size is 2GB.  The most practical suffix for DETERLab nodes is \"G\":  users:~$ /share/containers/containerize.py --openvz-diskspace 15G DeterTest example1 ~/example1.tcl \nContainerized experiment DeterTest/example1 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example1  Each of these parameters can be set on individual nodes using attributes.  Use  containers:openvz_template  to set a template on a node and use  containers:openvz_diskspace  to set the disk space.  This example topology  sets these openvz parameters per node:      source tb_compat.tcl\n    set ns [new Simulator]\n\n    # Create the center node (named by its variable name)\n    set center [$ns node]\n    # The center node is a process\n    tb-add-node-attribute $center containers:node_type process\n    tb-add-node-attribute $center containers:openvz_template ubuntu-12.04-x86_64\n\n    # Connect 9 satellites\n    for { set i 0} { $i   9 } { incr i} {\n        # Create node n-1 (tcl n($i) becomes n-$i in the experiment)\n        set n($i) [$ns node]\n        # Set satellite disk sizes to be 20 GB\n        tb-add-node-attribute $n($i) containers:openvz_diskspace 20G\n        # Connect center to $n($i)\n        ns duplex-link $center $n($i) 100Mb 10ms DropTail\n    }\n\n    # Creation boilerplate\n    $ns rtptoto Static\n    $ns run  The  center  node will run Ubuntu 12.04 64 bit and the satellites will have 20GB file systems.  Setting Qemu Parameters \uf0c1  The Containers system has a more limited ability to set qemu parameters.  Right now, a custom image may be loaded using the  containers::qemu_url  attribute and the architecture of the qemu VM may be chosen using  containers:qemu_arch .  Valid qemu architectures are:     Param  Meaning      i386  32 bit Intel    x86_64  64-bit Intel     The image URL must be reachable from inside DETERLab.  The image must be a qcow2 image, optionally bzip2ed.  Facilities to snapshot and store such images are in development.  If you are using a qemu image that is not booting into containers, make sure  grub is properly configured .  Qemu images also mount users' home directories the same as DETERLab physical nodes do.  In order to do this scalably, the Qemu VMs mount the users' directories from the physical node.  The DETER infrastructure cannot support exporting users' directories to thousands of containers.  However, a Qemu VM can only mount a few tens of user directories this way.  The limit is 23 user directories (24 in experiments that are not instantiated in a group).  Many projects have more than 23 users, but in practice only a few experimenters need access to the containers.  To tell the Containers systems which user to mount, use the  --qemu-prefer-users  option to  containerize.py .  That option takes a comma-separated list of usernames (no spaces).  When the Qemu nodes will always mount those users' home directories.  Others will be mounted if there is room.  For example:  users:~$ /share/containers/containerize.py  --qemu-prefer-users=faber,jjh DeterTest example4 ~/example4.tcl  This command makes sure that users  faber  and  jjh  have their home directories mounted in any Qemu containers.  Changing The Packing Factor \uf0c1  The  containerize.py  program decides how many virtual nodes to put on each physical machine.  Because we have been using roughly the same number of nodes as the default packing target (10 nodes per machine) all of the examples so far have fit onto a single machine.  If we change the packing factor by using the  --packing  parameter to  containerize.py , we can put fewer nodes on each machine.  For example:  users:~$ /share/containers/containerize.py --packing 2 DeterTest example1 ~/example1.tcl \nContainerized experiment DeterTest/example1 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example1  This command calls  containerize.py  on our  original topology  with a low packing factor.  The result is the same nodes spread across more physical machines, as we can see from the DETERLab web interface (on the  Visualization  tab):   You will want to balance how many physical machines you use against how precisely you want to mimic them.  User Packing \uf0c1  You may specify your own packing using the  containers:partition  attribute.  This attribute must be assigned an integer value. All nodes with the same partition are allocated to the same machine.  If nodes have that attribute attached to them,  containers.py  will assume that they all have been partitioned and use those.  Nodes without a partition assigned are assumed to be  embedded_pnode s.  More Sophisticated Packing: Multiple Passes \uf0c1  The previous examples have all treated packing containers onto physical machines as a single-step process with a single parameter - the packing factor.  In fact, we can divide containers into sets and pack each set independently using different parameters.    For example in an experiment with many containers dedicated only to forwarding packets and a few to modeling servers, we could create two sets and pack the forwarders tightly (using a high packing factor) and the servers loosely.  In exchange for providing greater control on packing, there is a price.  When a  set  of containers is packed, the Containers system takes into account both the nodes to be packed and their interconnections.  When  subsets  of containers are packed, the system cannot consider the interconnections between subsets.  In some cases, the packing of subsets can lead to a DETERLab experiment that cannot be created successfully.  This danger is mitigated by the fact that containers that are packed together are often related in ways that limit the number of connections between that set and another.  To explore packing, we need to use a  larger topology :      source tb_compat.tcl\n    set ns [new Simulator]\n\n    set center [$ns node]\n    tb-add-node-attribute $center  containers:PartitionPass  0\n\n\n    for { set i 0} { $i   3 } { incr i} {\n        set lanlist $center\n        for { set j 0 } { $j   20} { incr j } {\n            set idx [expr $i * 20 + $j]\n            set n($idx) [$ns node]\n            tb-add-node-attribute $n($idx)  containers:PartitionPass  [expr $i + 1]\n            lappend lanlist $n($idx)\n        }\n        set lan($i) [$ns make-lan [join $lanlist    ] 100Mb 0]\n    }\n\n    # Creation boilerplate\n    $ns rtptoto Static\n    $ns run  This creates three 20-node sub networks attached to a single central router.  It looks like this:   Each node in the topology is assigned a  containers::PackingPass  attribute that groups them into subsets.  The  containers:PackingPass  attribute must be assigned an integer value.  The nodes in each packing pass are considered \"together\" when packing.  Each pass can be assigned different parameters.  The passes are carried out in order, though that is rarely important.  Our example topology assigns  center  to pass 0, the nodes on  lan-0  (the tcl variable lan(0)) to pass 1, those on  lan-1  to pass 2 and those on  lan-2  to pass 3.  We will use the  --pass-pack  parameter to specify the packing factor for each pass.  Each packing factor specification looks like  pass : factor  where pass and factor are both integers.  We can specify more than one, separated by commas, or specify  --pass-pack  more than once.  For example, we can pack the experiment using the following factors:     Pass  Packing Factor      0  1    1  20    2  10    3  5     By issuing the following command:  users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example6  We can view the packing by using  container_image.py  to generate a visualization that includes the partitions:  users:~$ /share/containers/container_image.py --experiment DeterTest/example6 --partitions --out ~/example6.png  The output shows the topology with boxes drawn around the containers that share a physical node:   That partitioning is surprising in that  lan-1  is split into three partitions of 6   7 nodes rather than two partitions of 10.  Similarly  lan-2  is split into five groups of 4 rather than four groups of 5.    The packing system is built on the  metis  graph partitioning software.  Metis takes a graph and a number of partitions and finds the most balanced partitioning that has roughly equal node counts in each partition as well as low inter-partition communication costs.  The Containers system calls metis with increasing numbers of partitions until a partitioning is found that meets the packing factor limits.  When the system attempts to pack  lan-1  into two partitions, metis balances the node counts and the communications costs to produce a partition with 9 containers in one machine and 11 on the other.  That partitioning does not meet the 10 node limit, so it tries again with three partitions and succeeds.  There are two ways to fit our topology onto fewer nodes.  The first is to put slightly more slop into the packing factors:     Pass  Packing Factor      0  1    1  20    2  11    3  6     As in the following command:  users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:11,3:6 DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example6  These parameters result in this packing, which fits in fewer nodes, but has the slight imbalances of splitting  lan-1  into 9 and 11 containers and  lan-2  into 4,5,and 6 container partitions.  Again, this asymmetry is an attempt to consider the internode networking costs.   If the packing constraints are exact - 11 containers on  lan-1  is unacceptable - a second choice is to use the  --nodes-only  option.  This sets the cost of each arc in the graph to 0.  Metis ignores such arcs altogether, so the partitions are completely even.  This may cause trouble in more complex network topologies.  The result of running the command with the original packing factors and  --nodes-only ):  users:~$ /share/containers/containerize.py --nodes-only --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example6  is   which has symmetric partitions.  Sometimes it is more intuitive to think in terms of the number of machines that will be used to hold containers.  The  --size  and  -pass-size  options let users express that.  The  --size= expsize  option uses  expsize  machines to hold the whole experiment.  If multiple passes are made, each is put into  expsize  physical machines.  The  --size  option takes precedence over  --packing .  Per-pass sizing can be done using  --pass-size  which uses the same syntax as  --pass-pack .  Therefore the command:  users:~$ /share/containers/containerize.py --pass-size 0:1,1:1,2:2,3:4 DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example6  packs pass 0 into one physical machine, pass 1 into one physical machine, pass 2 into two physical machines and pass 3 into four physcial machines.  The result looks like:   Partitions have different numbers of containers in them because metis is considering network constraints as well.  As with using packing, adding  --nodes-only  restores symmetry:   The  --pass-pack  option is a per-pass generalization of the  --packing  option.  The options that can be specified per-pass are:     Single-pass  Per-Pass  Per-Pass Format  Per-Pass Example      --packing  --pass-pack  pass : packing   (comma-separated)  --pass-pack 0:1,1:20,2:11,3:6    --size  --pass-size  pass : size   (comma-separated)  --pass-size 0:1,1:1,2:2,3:4    --pnode-types  --pass-pnodes  pass : pnode [, pnode ...] (semicolon separated)  --pass-pnodes 0:MicroCloud;1:bpc2133,pc2133    --nodes-only  --pass-nodes-only  pass  (comma-separated)  --pass-nodes-only 1,3,5     The single-pass version sets a default so that this invocation on  our 4 pass topology :  users:~$ /share/containers/containerize.py --packing 5 --pass-pack 0:1,1:20 DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example6  will pack pass 0 with a factor of 1, pass 1 with a factor of 20 and passes 2 and 3 with a factor of 5.  Similarly:  users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes '0:pc2133,bpc2133;1:MicroCloud' DeterTest example6 ~/example6.tcl \nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example6  will allocate either bpc2133 or pc2133 nodes to containers assigned by pass 0 and Microcloud physical nodes to the containers partitioned in pass 1.  The rest will be allocated as the  site configuration  specifies.  The single quotes around the  --pass-pnodes  option protects the semi-colon from the shell.  Another choice is to specify the command as:  users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes 0:pc2133,bpc2133 --pass-pnodes 1:MicroCloud DeterTest example6 ~/example6.tcl   That formulation avoids the quotes by avoiding the semicolon.  All the per-pass options may be specified multiple times on the command line.  You can mix and match sizes and packing factors.  This invocation:  /share/containers/containerize.py --pass-size 1:10 --pass-pack 2:5,3:10 DeterTest example6 ~/example6.tcl\nContainerized experiment DeterTest/example6 successfully created!\nAccess it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest eid=example6  Produces:   Remember that  --size  sets a default pass size and that sizes have precedence over packing.  If you specify  --size , no  --packing  or  --pass-packing  value will take effect.  To mix packing and sizes, use  --pack-size  for each sized pass, rather than  --size .  These per-pass variables and user-specified pass specifications give users fine grained control over the paritioning process, even if they do not want to do the partitioning themselves.  If no  containers:PartitionPass  attributes are specified in the topology, and no  containers:Partition  attributes are specified either, ```containerize.py}} carries out -- at most -- two passes.  Pass 0 paritions all openvz containers and pass 1 partitions all qemu and process containers.", 
            "title": "Advanced Topics"
        }, 
        {
            "location": "/containers/containers-guide/#further-reading", 
            "text": "Hopefully these illustrative examples have given you an idea of how to use the containers system and what it is capable of.  More details are available from  the reference guide .  Please see   Getting Help  if you have difficulties.", 
            "title": "Further Reading"
        }, 
        {
            "location": "/containers/containers-reference/", 
            "text": "This document describes the details of the commands and data structures that make up the Containers system.  The \nContainers Guide\n provides useful context about the workflows and goals of the system that inform these technical details.\n\n\nCommands\n\uf0c1\n\n\nThis section describes the command line interface to the Containers system.\n\n\ncontainerize.py\n\uf0c1\n\n\nThe \ncontainerize.py\n command creates a DETERLab experiment made up of containers.  The \ncontainerize.py\n program is available from \n/share/containers/containerize.py\n on \nusers.isi.deterlab.net\n.  A sample invocation is:\n\n\n$ /share/containers/containerize.py MyProject MyExperiment ~/mytopology.tcl\n\n\n\n\nIt will create a new experiment in \nMyProject\n called \nMyExperiment\n containing the experiment topology in \nmytopology.tcl\n.  All the topology creation commands supported by DETERLab are supported by the Containers system, but DETERLab program agents are not.  \nDETERLab start commands\n \nare\n supported.\n\n\nContainers will create an experiment in a group if the project parameter is of the form \nproject\n/\ngroup\n.  To start an experiment in the \ntesting\n group of the \nDETER\n project, the first parameter is specified as \nDETER/testing\n.\n\n\nContainers supports \nns2 file\n or \ntopdl\n descriptions.  Ns2 descriptions must end with \n.tcl\n or \n.ns\n.  Other files are assumed to be topdl descriptions.\n\n\nNames of substrates and nodes in ns2 files are restricted to valid tcl variable names.  Names of substrates and nodes in topdl files are restricted to the characters A-Z, a-z, digits, the underscore and the hyphen (-).\n\n\nBy default, \ncontainerize.py\n program will partition the topology into openvz containers, packing 10 containers per physical computer.  If the topology is already partitioned - meaning at least one element has a \ncontainers::partition\n attribute - \ncontainerize.py\n will not partition it.  The \n--force-partition\n flag causes \ncontainerize.py\n to partition the experiment regardless of the presence of \ncontainers:partition\n attributes.\n\n\nIf container types have been assigned to nodes using the \ncontainers:node_type\n attribute, \ncontainerize.py\n will respect them.  Valid container types for the \ncontainers:node_type\n attribute or the \n--default-container\n parameter are:\n\n\n\n\n\n\n\n\nParameter\n\n\nContainer\n\n\n\n\n\n\n\n\n\n\nembedded_pnode\n\n\nPhysical Node\n\n\n\n\n\n\nqemu\n\n\nQemu VM\n\n\n\n\n\n\nopenvz\n\n\nOpenvz Container\n\n\n\n\n\n\nprocess\n\n\nViewOS process\n\n\n\n\n\n\n\n\nThe \ncontainerize.py\n command takes several parameters that can change its behavior:\n\n\n\n\n\n\n--default-container\n=\nkind\n\n\nContainerize nodes without a container type into \nkind\n.  If no nodes have been assigned containers, this puts all them into \nkind\n containers.\n\n\n\n\n\n\n--force-partition\n\n\nPartition the experiment whether or not it has been partitioned already\n\n\n\n\n\n\n--packing=\nint\n\n\nAttempt to put \nint\n containers into each physical node.  The default \n--packing\n is 10.\n\n\n\n\n\n\n--size=\nint\n\n\nAttempt to divide the experiment into \nint\n physical nodes.  The default is to use packing.  There are some nuances to this with mixed containers.  See the \nContainers Guide\n for more details.\n\n\n\n\n\n\n--config=\nfilename\n\n\nRead configuration variables from \nfilename\n. Configuration values are discussed \nbelow\n.\n\n\n\n\n\n\n--pnode-types=\ntype1[,type2...]\n\n\nOverride the site configuration and request nodes of \ntype1\n (or \ntype2\n etc.) as host nodes.\n\n\n\n\n\n\n--end-node-shaping\n\n\nAttempt to do end node traffic shaping even in containers connected by VDE switches.  This works with qemu nodes, but not process nodes.  Topologies that include both openvz nodes and qemu nodes that shape traffic should use this.  See the \ndiscussion below\n.\n\n\n\n\n\n\n--vde-switch-shaping\n\n\nDo traffic shaping in VDE switches.  Probably the default, but that is controlled in \nthe site configuration\n. See the \ndiscussion below\n.\n\n\n\n\n\n\n--openvz-diskspace\n\n\nSet the default openvz disk space size.  The suffixes \nG\n and \nM\n stand for 'gigabytes' and 'megabytes'.\n\n\n\n\n\n\n--openvz-template\n\n\nSet the default openvz template.  Templates are described in the \nContainers Guide\n.\n\n\n\n\n\n\n--openvz-template-dir\n\n\nAdd a directory to be searched for openvz templates.  Templates must end in \ntar.gz\n and be accessible to the user at creation and swap time.  They can only be located under the \n/proj\n or \n/share\n directories.\n\n\n\n\n\n\n--image\n\n\nConstruct a visualization of the virtual topology and leave it in the experiment directories (default).\n\n\n\n\n\n\n--nodes-only\n\n\nIgnore network constraints when partitioning nodes.\n\n\n\n\n\n\n--no-image\n\n\nDo not construct a visualization of the virtual topology.\n\n\n\n\n\n\n--pass-pack=\npass\n:\npacking\n[,\npass\n:\npacking\n...]\n\n\nSpecify the packing factor for each partitioning pass.  The [ContainersGuide#MoreSophisticatedPacking:MultiplePasses Containers Guide] describes this in detail.\n\n\n\n\n\n\n--pass-size=\npass\n:\nsize\n[,\npass\n:\nsize\n...]\n\n\nSpecify the number of physical machines to use for each partitioning pass.  The \nContainers Guide\n describes this in detail.\n\n\n\n\n\n\n--pass-pnodes=\npass\n:\ntype\n[,\ntype\n...][;\npass\n:\ntype\n[,\ntype\n...]...]\n\n\nSpecify the pnode types on which nodes packed in partitioning pass \npass\n can be placed.  The \nContainers Guide\n describes this in detail.\n\n\n\n\n\n\n--pass-nodes-only=\npass\n[,\npass\n...]\n\n\nSpecify the partitioning passes on which network connectivity is ignored.  The \nContainers Guide\n describes this in detail.\n\n\n\n\n\n\n--prefer-qemu-users=\nuser[,user...]\n\n\nMake sure that Qemu images mount the given users' home directories.  Qemu nodes can mount at most 19 users' home directories and this ensures that the experimenters using the containers can reach their home directories.\n\n\n\n\n\n\n--debug\n\n\nPrint additional diagnostics and leave failed DETER experiments on the testbed.\n\n\n\n\n\n\n--keep-tmp\n\n\nDo not remove temporary files - used for debugging only.\n\n\n\n\n\n\nThis invocation:\n\n\n$ ./containerize.py --packing 25 --default-container=qemu --force-partition DeterTest faber-packem ~/experiment.xml\n\n\n\n\ntakes the topology in \n~/experiment.xml\n (which must be a topdl description), packs it into 25 qemu containers per physical node, and creates an experiment called 'DeterTest/faber-packem' that can be swapped-in.  If \nexperiment.xml\n is already partitioned, it will be re-partitioned.  If some nodes in that topology are assigned to openvz nodes already, those nodes will be still be in openvz containers.\n\n\nThe result of a successful \ncontainerize.py\n run is a DETERLab experiment that can be swapped in.\n\n\nMore detailed examples are available in the \nContainers Guide\n.\n\n\ncontainer_image.py\n\uf0c1\n\n\nThe \ncontainer_image.py\n command draws a picture of the topology of an experiment.  This is helpful in keeping track of how virtual nodes are connected.  \ncontainerize.py\n calls this internally and stores the output in the \nper-experiment directory\n (unless \n--no-image\n is used).\n\n\nA researcher may call \ncontainer_image.py\n directly to generate an image later or to generate one with the partitioning drawn.\n\n\nThe simplest way to call \ncontainer_image.py\n is:\n\n\n/share/containers/container_image.py topology.xml output.png\n\n\n\n\nThe first parameter is a \ntopdl\n description, for example the one in the \nper-experiment directory\n.  The second parameter is the output file for the image.  When drawing an experiment that has been containerized, the \n--experiment\n option is very useful.\n\n\nOptions include:\n\n\n\n\n\n\n--experiment=\nproject\n/\nexperiment\n\n\nDraw the experiment in \nproject\n/\nexperiment\n, if it exists.  Note that this is just the DETERLab experiment and project names.  Omit any sub-group.\n\n\n\n\n\n\n--topo=\nfilename\n\n\nDraw the topology in the \nfilename\n indicated.\n\n\n\n\n\n\n--attr-prefix=\nprefix\n\n\nPrefix for containers attributes.  Deprecated.\n\n\n\n\n\n\n--partitions\n\n\nDraw labeled boxes around nodes that share a physical node.\n\n\n\n\n\n\n--out=\nfilename\n\n\nSave the image in the \nfilename\n indicated.\n\n\n\n\n\n\n--program=\nprogramname\n\n      Use \nprogramname\n to lay out the graph.  \nprogramname\n must take a file in \ngraphviz's dot language\n.  This is given as the \n--program\n option to \nfedd_image.py\n internally.  The default is \nfdp\n which works well when \n--partitions\n is given.\n\n\n\n\n\n\nIf neither \n--topo\n nor \n--experiment\n is given, the first positional parameter is the topdl topology to draw.  If \n--out\n is not given the next positional parameter (the first if neither \n--topo\n nor \n--experiment\n is given) is the output file.\n\n\nA common invocation looks like:\n\n\n/share/containers/container_image.py --experiment SAFER/testbed-containers ~/drawing.png\n\n\n\n\nTopdl Attributes For Containers\n\uf0c1\n\n\nSeveral \ntopdl\n attributes influence how an experiment is containerized.  These can be added to nodes using the ns2 \ntb-add-node-attribute\n command (used throughout the \nContainers Guide\n) or directly to the topdl.\n\n\nThese attributes are all attached to nodes/Computers:\n\n\n\n\n\n\ncontainers:node_type\n\n\nThe container that will hold this node.  \nThe full list\n is available here.\n\n\n\n\n\n\ncontainers:partition\n\n\nAn identifier grouping nodes together in containers that will share a physical node.  Generally assigned by \ncontainerize.py\n, but researchers can also directly assign them.  The \ncontainerize.py\n command assigns integers, so if a researcher assigns other partition identifiers, \ncontainerize.py\n will not overwrite them.\n\n\n\n\n\n\ncontainers:openvz_template\n\n\nThe flavor of Linux distribution to emulate on openvz.  There is a list of valid choices in \nthe Containers Guide\n.\n\n\n\n\n\n\ncontainers:openvz_diskspace\n\n\n\n\n\n\nAmount of disk space to allocate to an openvz container.  Be sure to include the \nG\n (gigabyte) or \nM\n (megabyte) suffix or the size will be taken as disk blocks.\n\n\n\n\ncontainers:ghost\n\n\n\n\nIf this attribute is true, resources will be allocated for this node, but it will not be started when the topology is created.\n\n\n\n\n\n\ncontainers:maverick_url\n\n\nA location to download the QEMU image for this container.  The name is a legacy that will disappear.  This is deprecated.\n\n\n\n\n\n\nThere are a few other attributes that are meaningful to more applications.  Users specifying ns2 files will not need to set these directly, as the DETERLab ns2 interpreter does so.\n\n\nOn Computers:\n\n\n\n\n\n\nstartup\n\n\nThe start command.  \ntb-set-node-startcmd\n sets this.\n\n\n\n\n\n\nOn interfaces:\n\n\n\n\n\n\nip4_address\n\n\nThe IPv4 address of this interface.  Set by the ns2 commands for fixing addresses.\n\n\n\n\n\n\nip4_netmask\n\n\nThe IPv4 netmask.  ns2 sets this.\n\n\n\n\n\n\nConfiguration Files\n\uf0c1\n\n\nThese files control the operation of the containers system.\n\n\nPer-experiment Directory\n\uf0c1\n\n\nWhen an experiment is containerized, the data necessary to create it is stored in \n/proj/\nproject\n/exp/\nexperiment\n/containers\n.  The path \n/proj/\nproject\n/exp/\nexperiment\n is created by DETERLab when the experiment is created, and used by experimenters for a variety of things.  This directory is replicated on nodes under \n/var/containers/config\n.\n\n\nThere are a few files in the per-experiment directory that most experimenters can use:\n\n\n\n\n\n\nexperiment.tcl\n\n\nIf the topology was passed to \ncontainerize.py\n as an ns file, this is a copy of that input file.  Useful for seeing what the experimenter asked for, or as a basis for new experiments.\n\n\n\n\n\n\nexperiment.xml\n\n\nThe analog of \nexperiment.tcl\n, this is the topology given as \ntopdl\n.  The topdl input file.\n\n\n\n\n\n\nvisualization.png\n\n\nA drawing of the virtual topology in png format.  Generated by \ncontainer_image.py\n\n\n\n\n\n\nhosts\n\n\nThe host to IP mapping that will be installed on each node as \n/etc/hosts\n.\n\n\n\n\n\n\nsite.conf\n\n     A clone of the \nsite configuration file\n that holds the global variables that the container creation will use.  Values overridden on the command line invocation of \ncontainerize.py\n will be present in this file.\n\n\n\n\n\n\nThe rest of this directory is primarily of interest to developers.  It includes:\n\n\n\n\n\n\nannotated.xml\n\n\nFirst version of the input topology after default container types have been added.  Input to the partitioning step.\n\n\n\n\n\n\nassignment\n\n\nA yaml representation of the partition to virtual node mapping.\n\n\n\n\n\n\nbackend_config.yaml\n\n\n\n\n\n\nThe server and channel to use for grandstand communication.  Encoded in YAML.\n\n\n\n\n\n\nchildren\n\n\nDirectory containing the assignment, including all the levels of nested hypervisors.\n\n\n\n\n\n\nconfig.tgz\n\n\nThe contents of the per-experiment directory (except \nconfig.tgz\n) for distribution into the experiment.\n\n\n\n\n\n\nembedding.yaml\n\n\nA yaml-encoded representation of the children sub-directory\n\n\n\n\n\n\nghosts\n\n\nContainers that are initially not started in the experiment.\n\n\n\n\n\n\nmaverick_url\n\n\nYaml encoding of the qemu images to be used on each node.\n\n\n\n\n\n\nopenvz_guest_url\n\n\nYaml encoding of the openvz templates to be used on each node.\n\n\n\n\n\n\npartitioned.xml\n\n\nOutput of the partitioning process.  A copy of \nannotated.xml\n that has been decorated with the partitions.\n\n\n\n\n\n\nphys_topo.ns\n\n\nThe ns2 file used to create the DETERLab experiment.\n\n\n\n\n\n\nphys_topo.xml\n\n\nThe topdl file used to generate \nphystopo.ns\n.\n\n\n\n\n\n\npid_eid\n\n\nThe DETERLab project and experiment name under which this topology will be created.  Broken out into \n/var/containers/pid\n and \n/var/containers/eid\n on virtual nodes inside the topology.\n\n\n\n\n\n\nroute\n\n\nA directory containing the routing tables for each node.\n\n\n\n\n\n\nshaping.yaml\n\n\nYaml-encoded data about the per-network and per-node loss, delay, and capacity parameters.\n\n\n\n\n\n\nswitch\n\n\nA directory containing the \nVDE switch\n topology for the experiment.\n\n\n\n\n\n\nswitch_extra.yaml\n\n\nYaml-encoded extra switch configuration information.  Mostly VDE switch configuration esoterica.\n\n\n\n\n\n\ntopo.xml\n\n\nThe final topology representation from which the physical topology is extracted.  Includes the virtual topology as well.  This file can be used as input to \ncontainer_image.py\n.\n\n\n\n\n\n\ntraffic_shaping.pickle\n\n\nPickled\n information for configuring endnode traffic shaping.\n\n\n\n\n\n\nwirefilters.yaml\n\n\nSpecific parameters for configuring the delay elements in VDE switched topologies that implement traffic shaping. \nSee below\n.\n\n\n\n\n\n\nSite Configuration File\n\uf0c1\n\n\nThe site configuration file controls how all experiments are containerized across DETERLab.  The contents are primarily of interest to developers, but researchers may occasionally find the need to specify their own.  The \n--config\n parameter to \ncontainerize.py\n does that.\n\n\nThe site configuration file is an attribute-value pair file parsed by a python ConfigParser that sets overall container parameters.  Many of these have legacy internal names.\n\n\nThe default site configuration is in \n/share/containers/site.conf\n on \nusers.isi.deterlab.net\n.\n\n\nAcceptable values (and their DETERLab defaults) are:\n\n\n\n\n\n\nmaverick_url\n\n\nDefault image used by qemu containers.  Default: \nhttp://scratch/benito/pangolinbz.img.bz2\n\n\n\n\n\n\nurl_base\n\n\nBase URL of the DETERLab web interface on which users can see experiments.  Default: \nhttp://www.isi.deterlab.net/\n\n\n\n\n\n\nqemu_host_hw\n\n\nHardware used by containers.  Default: \npc2133,bpc2133,MicroCloud\n\n\n\n\n\n\nxmlrpc_server\n\n\nHost and port from which to request experiment creation. Default: \nboss.isi.deterlab.net:3069\n\n\n\n\n\n\nqemu_host_os\n\n\nOSID to request for qemu container nodess. Default: \nUbuntu1204-64-STD\n\n\n\n\n\n\nexec_root\n\n\nRoot of the directory tree holding containers software and libraries.  Developers often change this. Default: \n/share/containers\n\n\n\n\n\n\nopenvz_host_os\n\n\nOSID to request for openvz nodes. Default \nCentOS6-64-openvz\n\n\n\n\n\n\nopenvz_guest_url\n\n\nLocation to load the openvz template from.  Default: \n%(exec_root)s/images/ubuntu-10.04-x86.tar.gz\n\n\n\n\n\n\nswitch_shaping\n\n\nTrue if switched containers (see below) should do traffic shaping in the VDE switch that connects them.  Default: \ntrue\n\n\n\n\n\n\nswitched_containers\n\n\nA list of the containers that are networked with VDE switches.  Default: \nqemu,process\n\n\n\n\n\n\nopenvz_template_dir\n\n\nThe directory that stores openvz template files.  Default:  \n%(exec_root)s/images/\n (that is the \nimages\n directory in the \nexec_root\n directory defined in the site config file.  This can be a comma-separated list that will be searched in order, after any template directories given on the command line.\n\n\n\n\n\n\nnode_log\n\n\nThe name of the file on experiment nodes used to log containers creation.  Default is \n/tmp/containers.log\n\n\n\n\n\n\ntopdl_converter\n\n\nThe program used to convert ns2 descriptions to topdl.  The default is \nfedd_ns2topdl.py --file\n but any program that takes a single ns2 file as a parameter and prints the topdl to standard output is viable.  On DETERLab installations \n/usr/testbed/lib/ns2ir/parse.tcl -t -x 3 -m dummy dummy dummy dummy\n can be used to decouple containers from needing a running fedd.\n\n\n\n\n\n\ndefault_router\n\n\nThe IP address of a router needed to reach testbed infrastructure\n\n\n\n\n\n\ndefault_dest\n\n\nThe network on which testbed infrastructure lives that needs to be routed through \ndefault_router\n.\n\n\n\n\n\n\nbackend_server\n\n\nDeprecated\n\n\n\n\n\n\ngrandstand_port\n\n\nDeprecated\n\n\n\n\n\n\nContainer Notes\n\uf0c1\n\n\nDifferent container types have some quirks.  This section lists limitations of each container, as well as issues in interconnecting them.\n\n\nQemu\n\uf0c1\n\n\nQemu nodes are limited to 7 experimental interfaces.  They currently run only Ubuntu 12.04 32 bit operating systems.\n\n\nViewOS Processes\n\uf0c1\n\n\nThese have no way to log in or work as conventional machines.  Process tree rooted in the \nstart command\n is created, so a service will run with its own view of the network.  It does not have an address on the control net.\n\n\nBecause of a bug in their internal routing, multi-homed processes do not respond correctly for requests on some interfaces.  A ViewOS process does not recognize its other addresses when a packet arrives on a different interface.  A picture makes this clearer:\n\n\n\n\nContainer A can ping Interface X (10.0.0.1) of the ViewOS container successfully, but if Container A tries to ping Interface Y (10.0.1.2), the ViewOS container will not reply. In fact it will send ARP requests on Interface Y looking for its own address.\n\n\nFor this reason, ViewOS processes are best used as lightweight forwarders.\n\n\nPhysical Nodes\n\uf0c1\n\n\nPhysical nodes can be incorporated into experiments, but should only use modern versions of Ubuntu, to allow the Containers system to run their \nstart commands\n correctly and to initialize their routing tables.\n\n\nInterconnections: VDE switches and local networking\n\uf0c1\n\n\nThe various containers are interconnected using either local kernel virtual networking or \nVDE switches\n.  Kernel networking is lower overhead because it does not require process context switching, but VDE switches are a more general solution.\n\n\nNetwork behavior changes such as loss, delay or rate limits  are introduced into a network of containers using one of two mechanisms: inserting elements into a VDE switch topology or end node traffic shaping.  \n\n\nInserting elements into the VDE switch topology allows the system to modify the behavior for all packets passing through it.  Generally this means all packets to or from a host, as the Containers system inserts these elements in the path between the node and the switch.\n\n\nThis figure shows three containers sharing a virtual LAN (VLAN) on a VDE switch with no traffic shaping:\n\n\n\n\nThe blue containers connect to the switch and the switch has interconnected their VDE ports into the red shared VLAN.  To add delays to two of the nodes on that VLAN, the following VDE switch configuration would be used:\n\n\n\n\nThe VDE switch connects the containers with shaped traffic to the delay elements, not to the shared VLAN.  The delay elements are on the VLAN and delay all traffic passing through them.  The Container system configures the delay elements to delay traffic symmetrically - traffic from the LAN and traffic from the container are both delayed.  The VDE tools can be configured asymmetrically as well.  This is a very flexible way to interconnect containers.\n\n\nThat flexibility incurs a cost in overhead.  Each delay element and the VDE switch is a process, do traffic passing from one delayed nodes to the other experiences 7 context switches: container -\n switch, switch -\n delay, delay -\n switch, switch -\n delay, delay -\n switch, and switch -\n container.\n\n\nThe alternative mechanism is to do the traffic shaping inside the nodes, using \nLinux traffic shaping\n.  In this case, traffic outbound from a container is delayed in the container for the full transit time to the next hop.  The next node does the same.  End-node shaping all happens in the kernel so it is relatively inexpensive at run time.\n\n\nQemu nodes can make use of either end-node shaping or VDE shaping, and use VDE shaping by default.  The \n--end-node-shaping\n and \n--vde-switch-shaping\n options to \ncontainerize.py\n force the choice in qemu.\n\n\nViewOS processes can only use VDE shaping.  Their network stack emulation is not rich enough to include traffic shaping.\n\n\nOpenvz nodes only use end-node traffic shaping.  They have no native VDE support so interconnecting openvz containers to VDE switches would include both extra kernel crossings and extra context switches.  Because a primary attraction of VDE switches is their efficiency, the Containers system does not implement VDE interconnections to openvz.\n\n\nSimilarly embedded physical nodes use only endnode traffic shaping, as routing outgoing traffic through a virtual switch infrastructure that just connects to its physical interfaces is at best confusing.\n\n\nUnfortunately, endnode traffic shaping and VDE shaping are incompatible.  Because endnode shaping does not impose delays on arriving traffic, it cannot delay traffic from a VDE delayed node correctly.\n\n\nThis is primarily of academic interest, unless a researcher wants to impose traffic shaping between containers using incompatible traffic shaping.  There needs to be an unshaped link between the two kinds of traffic shaping.\n\n\nBootable Qemu Images\n\uf0c1\n\n\nFor qemu images to boot reliably, they should not wait for a keypress at the \ngrub\n command, which is distressingly common.\n\n\nTo ensure that your image does not wait for \ngrub\n, do the following:\n\n\nFor Ubuntu 12.04 (and any system that uses grub2) edit \n/etc/default/grub\n. For example:\n\n\nGRUB_DEFAULT=0\nGRUB_HIDDEN_TIMEOUT=0\nGRUB_HIDDEN_TIMEOUT_QUIET=true\nGRUB_TIMEOUT=1\nGRUB_DISTRIBUTOR=`lsb_release -i -s 2\n /dev/null || echo Debian`\nGRUB_CMDLINE_LINUX_DEFAULT=\nquiet splash\n\nGRUB_CMDLINE_LINUX=\n\n\n\n\n\nJust make sure the HIDDENs are not commented out and have true/0 values.\n\n\nYou then must run a command on the system which generates all the new grub configurations:\n\n\n$ sudo update-grub\n\n\n\n\nsudo configuration\n\uf0c1\n\n\nThe Containers system adds all users to the admin group so that group should be able to use \nsudo\n without providing a password.", 
            "title": "Containers Reference"
        }, 
        {
            "location": "/containers/containers-reference/#commands", 
            "text": "This section describes the command line interface to the Containers system.  containerize.py \uf0c1  The  containerize.py  command creates a DETERLab experiment made up of containers.  The  containerize.py  program is available from  /share/containers/containerize.py  on  users.isi.deterlab.net .  A sample invocation is:  $ /share/containers/containerize.py MyProject MyExperiment ~/mytopology.tcl  It will create a new experiment in  MyProject  called  MyExperiment  containing the experiment topology in  mytopology.tcl .  All the topology creation commands supported by DETERLab are supported by the Containers system, but DETERLab program agents are not.   DETERLab start commands   are  supported.  Containers will create an experiment in a group if the project parameter is of the form  project / group .  To start an experiment in the  testing  group of the  DETER  project, the first parameter is specified as  DETER/testing .  Containers supports  ns2 file  or  topdl  descriptions.  Ns2 descriptions must end with  .tcl  or  .ns .  Other files are assumed to be topdl descriptions.  Names of substrates and nodes in ns2 files are restricted to valid tcl variable names.  Names of substrates and nodes in topdl files are restricted to the characters A-Z, a-z, digits, the underscore and the hyphen (-).  By default,  containerize.py  program will partition the topology into openvz containers, packing 10 containers per physical computer.  If the topology is already partitioned - meaning at least one element has a  containers::partition  attribute -  containerize.py  will not partition it.  The  --force-partition  flag causes  containerize.py  to partition the experiment regardless of the presence of  containers:partition  attributes.  If container types have been assigned to nodes using the  containers:node_type  attribute,  containerize.py  will respect them.  Valid container types for the  containers:node_type  attribute or the  --default-container  parameter are:     Parameter  Container      embedded_pnode  Physical Node    qemu  Qemu VM    openvz  Openvz Container    process  ViewOS process     The  containerize.py  command takes several parameters that can change its behavior:    --default-container = kind  Containerize nodes without a container type into  kind .  If no nodes have been assigned containers, this puts all them into  kind  containers.    --force-partition  Partition the experiment whether or not it has been partitioned already    --packing= int  Attempt to put  int  containers into each physical node.  The default  --packing  is 10.    --size= int  Attempt to divide the experiment into  int  physical nodes.  The default is to use packing.  There are some nuances to this with mixed containers.  See the  Containers Guide  for more details.    --config= filename  Read configuration variables from  filename . Configuration values are discussed  below .    --pnode-types= type1[,type2...]  Override the site configuration and request nodes of  type1  (or  type2  etc.) as host nodes.    --end-node-shaping  Attempt to do end node traffic shaping even in containers connected by VDE switches.  This works with qemu nodes, but not process nodes.  Topologies that include both openvz nodes and qemu nodes that shape traffic should use this.  See the  discussion below .    --vde-switch-shaping  Do traffic shaping in VDE switches.  Probably the default, but that is controlled in  the site configuration . See the  discussion below .    --openvz-diskspace  Set the default openvz disk space size.  The suffixes  G  and  M  stand for 'gigabytes' and 'megabytes'.    --openvz-template  Set the default openvz template.  Templates are described in the  Containers Guide .    --openvz-template-dir  Add a directory to be searched for openvz templates.  Templates must end in  tar.gz  and be accessible to the user at creation and swap time.  They can only be located under the  /proj  or  /share  directories.    --image  Construct a visualization of the virtual topology and leave it in the experiment directories (default).    --nodes-only  Ignore network constraints when partitioning nodes.    --no-image  Do not construct a visualization of the virtual topology.    --pass-pack= pass : packing [, pass : packing ...]  Specify the packing factor for each partitioning pass.  The [ContainersGuide#MoreSophisticatedPacking:MultiplePasses Containers Guide] describes this in detail.    --pass-size= pass : size [, pass : size ...]  Specify the number of physical machines to use for each partitioning pass.  The  Containers Guide  describes this in detail.    --pass-pnodes= pass : type [, type ...][; pass : type [, type ...]...]  Specify the pnode types on which nodes packed in partitioning pass  pass  can be placed.  The  Containers Guide  describes this in detail.    --pass-nodes-only= pass [, pass ...]  Specify the partitioning passes on which network connectivity is ignored.  The  Containers Guide  describes this in detail.    --prefer-qemu-users= user[,user...]  Make sure that Qemu images mount the given users' home directories.  Qemu nodes can mount at most 19 users' home directories and this ensures that the experimenters using the containers can reach their home directories.    --debug  Print additional diagnostics and leave failed DETER experiments on the testbed.    --keep-tmp  Do not remove temporary files - used for debugging only.    This invocation:  $ ./containerize.py --packing 25 --default-container=qemu --force-partition DeterTest faber-packem ~/experiment.xml  takes the topology in  ~/experiment.xml  (which must be a topdl description), packs it into 25 qemu containers per physical node, and creates an experiment called 'DeterTest/faber-packem' that can be swapped-in.  If  experiment.xml  is already partitioned, it will be re-partitioned.  If some nodes in that topology are assigned to openvz nodes already, those nodes will be still be in openvz containers.  The result of a successful  containerize.py  run is a DETERLab experiment that can be swapped in.  More detailed examples are available in the  Containers Guide .  container_image.py \uf0c1  The  container_image.py  command draws a picture of the topology of an experiment.  This is helpful in keeping track of how virtual nodes are connected.   containerize.py  calls this internally and stores the output in the  per-experiment directory  (unless  --no-image  is used).  A researcher may call  container_image.py  directly to generate an image later or to generate one with the partitioning drawn.  The simplest way to call  container_image.py  is:  /share/containers/container_image.py topology.xml output.png  The first parameter is a  topdl  description, for example the one in the  per-experiment directory .  The second parameter is the output file for the image.  When drawing an experiment that has been containerized, the  --experiment  option is very useful.  Options include:    --experiment= project / experiment  Draw the experiment in  project / experiment , if it exists.  Note that this is just the DETERLab experiment and project names.  Omit any sub-group.    --topo= filename  Draw the topology in the  filename  indicated.    --attr-prefix= prefix  Prefix for containers attributes.  Deprecated.    --partitions  Draw labeled boxes around nodes that share a physical node.    --out= filename  Save the image in the  filename  indicated.    --program= programname \n      Use  programname  to lay out the graph.   programname  must take a file in  graphviz's dot language .  This is given as the  --program  option to  fedd_image.py  internally.  The default is  fdp  which works well when  --partitions  is given.    If neither  --topo  nor  --experiment  is given, the first positional parameter is the topdl topology to draw.  If  --out  is not given the next positional parameter (the first if neither  --topo  nor  --experiment  is given) is the output file.  A common invocation looks like:  /share/containers/container_image.py --experiment SAFER/testbed-containers ~/drawing.png", 
            "title": "Commands"
        }, 
        {
            "location": "/containers/containers-reference/#topdl-attributes-for-containers", 
            "text": "Several  topdl  attributes influence how an experiment is containerized.  These can be added to nodes using the ns2  tb-add-node-attribute  command (used throughout the  Containers Guide ) or directly to the topdl.  These attributes are all attached to nodes/Computers:    containers:node_type  The container that will hold this node.   The full list  is available here.    containers:partition  An identifier grouping nodes together in containers that will share a physical node.  Generally assigned by  containerize.py , but researchers can also directly assign them.  The  containerize.py  command assigns integers, so if a researcher assigns other partition identifiers,  containerize.py  will not overwrite them.    containers:openvz_template  The flavor of Linux distribution to emulate on openvz.  There is a list of valid choices in  the Containers Guide .    containers:openvz_diskspace    Amount of disk space to allocate to an openvz container.  Be sure to include the  G  (gigabyte) or  M  (megabyte) suffix or the size will be taken as disk blocks.   containers:ghost   If this attribute is true, resources will be allocated for this node, but it will not be started when the topology is created.    containers:maverick_url  A location to download the QEMU image for this container.  The name is a legacy that will disappear.  This is deprecated.    There are a few other attributes that are meaningful to more applications.  Users specifying ns2 files will not need to set these directly, as the DETERLab ns2 interpreter does so.  On Computers:    startup  The start command.   tb-set-node-startcmd  sets this.    On interfaces:    ip4_address  The IPv4 address of this interface.  Set by the ns2 commands for fixing addresses.    ip4_netmask  The IPv4 netmask.  ns2 sets this.", 
            "title": "Topdl Attributes For Containers"
        }, 
        {
            "location": "/containers/containers-reference/#configuration-files", 
            "text": "These files control the operation of the containers system.  Per-experiment Directory \uf0c1  When an experiment is containerized, the data necessary to create it is stored in  /proj/ project /exp/ experiment /containers .  The path  /proj/ project /exp/ experiment  is created by DETERLab when the experiment is created, and used by experimenters for a variety of things.  This directory is replicated on nodes under  /var/containers/config .  There are a few files in the per-experiment directory that most experimenters can use:    experiment.tcl  If the topology was passed to  containerize.py  as an ns file, this is a copy of that input file.  Useful for seeing what the experimenter asked for, or as a basis for new experiments.    experiment.xml  The analog of  experiment.tcl , this is the topology given as  topdl .  The topdl input file.    visualization.png  A drawing of the virtual topology in png format.  Generated by  container_image.py    hosts  The host to IP mapping that will be installed on each node as  /etc/hosts .    site.conf \n     A clone of the  site configuration file  that holds the global variables that the container creation will use.  Values overridden on the command line invocation of  containerize.py  will be present in this file.    The rest of this directory is primarily of interest to developers.  It includes:    annotated.xml  First version of the input topology after default container types have been added.  Input to the partitioning step.    assignment  A yaml representation of the partition to virtual node mapping.    backend_config.yaml    The server and channel to use for grandstand communication.  Encoded in YAML.    children  Directory containing the assignment, including all the levels of nested hypervisors.    config.tgz  The contents of the per-experiment directory (except  config.tgz ) for distribution into the experiment.    embedding.yaml  A yaml-encoded representation of the children sub-directory    ghosts  Containers that are initially not started in the experiment.    maverick_url  Yaml encoding of the qemu images to be used on each node.    openvz_guest_url  Yaml encoding of the openvz templates to be used on each node.    partitioned.xml  Output of the partitioning process.  A copy of  annotated.xml  that has been decorated with the partitions.    phys_topo.ns  The ns2 file used to create the DETERLab experiment.    phys_topo.xml  The topdl file used to generate  phystopo.ns .    pid_eid  The DETERLab project and experiment name under which this topology will be created.  Broken out into  /var/containers/pid  and  /var/containers/eid  on virtual nodes inside the topology.    route  A directory containing the routing tables for each node.    shaping.yaml  Yaml-encoded data about the per-network and per-node loss, delay, and capacity parameters.    switch  A directory containing the  VDE switch  topology for the experiment.    switch_extra.yaml  Yaml-encoded extra switch configuration information.  Mostly VDE switch configuration esoterica.    topo.xml  The final topology representation from which the physical topology is extracted.  Includes the virtual topology as well.  This file can be used as input to  container_image.py .    traffic_shaping.pickle  Pickled  information for configuring endnode traffic shaping.    wirefilters.yaml  Specific parameters for configuring the delay elements in VDE switched topologies that implement traffic shaping.  See below .    Site Configuration File \uf0c1  The site configuration file controls how all experiments are containerized across DETERLab.  The contents are primarily of interest to developers, but researchers may occasionally find the need to specify their own.  The  --config  parameter to  containerize.py  does that.  The site configuration file is an attribute-value pair file parsed by a python ConfigParser that sets overall container parameters.  Many of these have legacy internal names.  The default site configuration is in  /share/containers/site.conf  on  users.isi.deterlab.net .  Acceptable values (and their DETERLab defaults) are:    maverick_url  Default image used by qemu containers.  Default:  http://scratch/benito/pangolinbz.img.bz2    url_base  Base URL of the DETERLab web interface on which users can see experiments.  Default:  http://www.isi.deterlab.net/    qemu_host_hw  Hardware used by containers.  Default:  pc2133,bpc2133,MicroCloud    xmlrpc_server  Host and port from which to request experiment creation. Default:  boss.isi.deterlab.net:3069    qemu_host_os  OSID to request for qemu container nodess. Default:  Ubuntu1204-64-STD    exec_root  Root of the directory tree holding containers software and libraries.  Developers often change this. Default:  /share/containers    openvz_host_os  OSID to request for openvz nodes. Default  CentOS6-64-openvz    openvz_guest_url  Location to load the openvz template from.  Default:  %(exec_root)s/images/ubuntu-10.04-x86.tar.gz    switch_shaping  True if switched containers (see below) should do traffic shaping in the VDE switch that connects them.  Default:  true    switched_containers  A list of the containers that are networked with VDE switches.  Default:  qemu,process    openvz_template_dir  The directory that stores openvz template files.  Default:   %(exec_root)s/images/  (that is the  images  directory in the  exec_root  directory defined in the site config file.  This can be a comma-separated list that will be searched in order, after any template directories given on the command line.    node_log  The name of the file on experiment nodes used to log containers creation.  Default is  /tmp/containers.log    topdl_converter  The program used to convert ns2 descriptions to topdl.  The default is  fedd_ns2topdl.py --file  but any program that takes a single ns2 file as a parameter and prints the topdl to standard output is viable.  On DETERLab installations  /usr/testbed/lib/ns2ir/parse.tcl -t -x 3 -m dummy dummy dummy dummy  can be used to decouple containers from needing a running fedd.    default_router  The IP address of a router needed to reach testbed infrastructure    default_dest  The network on which testbed infrastructure lives that needs to be routed through  default_router .    backend_server  Deprecated    grandstand_port  Deprecated", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/containers/containers-reference/#container-notes", 
            "text": "Different container types have some quirks.  This section lists limitations of each container, as well as issues in interconnecting them.  Qemu \uf0c1  Qemu nodes are limited to 7 experimental interfaces.  They currently run only Ubuntu 12.04 32 bit operating systems.  ViewOS Processes \uf0c1  These have no way to log in or work as conventional machines.  Process tree rooted in the  start command  is created, so a service will run with its own view of the network.  It does not have an address on the control net.  Because of a bug in their internal routing, multi-homed processes do not respond correctly for requests on some interfaces.  A ViewOS process does not recognize its other addresses when a packet arrives on a different interface.  A picture makes this clearer:   Container A can ping Interface X (10.0.0.1) of the ViewOS container successfully, but if Container A tries to ping Interface Y (10.0.1.2), the ViewOS container will not reply. In fact it will send ARP requests on Interface Y looking for its own address.  For this reason, ViewOS processes are best used as lightweight forwarders.  Physical Nodes \uf0c1  Physical nodes can be incorporated into experiments, but should only use modern versions of Ubuntu, to allow the Containers system to run their  start commands  correctly and to initialize their routing tables.  Interconnections: VDE switches and local networking \uf0c1  The various containers are interconnected using either local kernel virtual networking or  VDE switches .  Kernel networking is lower overhead because it does not require process context switching, but VDE switches are a more general solution.  Network behavior changes such as loss, delay or rate limits  are introduced into a network of containers using one of two mechanisms: inserting elements into a VDE switch topology or end node traffic shaping.    Inserting elements into the VDE switch topology allows the system to modify the behavior for all packets passing through it.  Generally this means all packets to or from a host, as the Containers system inserts these elements in the path between the node and the switch.  This figure shows three containers sharing a virtual LAN (VLAN) on a VDE switch with no traffic shaping:   The blue containers connect to the switch and the switch has interconnected their VDE ports into the red shared VLAN.  To add delays to two of the nodes on that VLAN, the following VDE switch configuration would be used:   The VDE switch connects the containers with shaped traffic to the delay elements, not to the shared VLAN.  The delay elements are on the VLAN and delay all traffic passing through them.  The Container system configures the delay elements to delay traffic symmetrically - traffic from the LAN and traffic from the container are both delayed.  The VDE tools can be configured asymmetrically as well.  This is a very flexible way to interconnect containers.  That flexibility incurs a cost in overhead.  Each delay element and the VDE switch is a process, do traffic passing from one delayed nodes to the other experiences 7 context switches: container -  switch, switch -  delay, delay -  switch, switch -  delay, delay -  switch, and switch -  container.  The alternative mechanism is to do the traffic shaping inside the nodes, using  Linux traffic shaping .  In this case, traffic outbound from a container is delayed in the container for the full transit time to the next hop.  The next node does the same.  End-node shaping all happens in the kernel so it is relatively inexpensive at run time.  Qemu nodes can make use of either end-node shaping or VDE shaping, and use VDE shaping by default.  The  --end-node-shaping  and  --vde-switch-shaping  options to  containerize.py  force the choice in qemu.  ViewOS processes can only use VDE shaping.  Their network stack emulation is not rich enough to include traffic shaping.  Openvz nodes only use end-node traffic shaping.  They have no native VDE support so interconnecting openvz containers to VDE switches would include both extra kernel crossings and extra context switches.  Because a primary attraction of VDE switches is their efficiency, the Containers system does not implement VDE interconnections to openvz.  Similarly embedded physical nodes use only endnode traffic shaping, as routing outgoing traffic through a virtual switch infrastructure that just connects to its physical interfaces is at best confusing.  Unfortunately, endnode traffic shaping and VDE shaping are incompatible.  Because endnode shaping does not impose delays on arriving traffic, it cannot delay traffic from a VDE delayed node correctly.  This is primarily of academic interest, unless a researcher wants to impose traffic shaping between containers using incompatible traffic shaping.  There needs to be an unshaped link between the two kinds of traffic shaping.", 
            "title": "Container Notes"
        }, 
        {
            "location": "/containers/containers-reference/#bootable-qemu-images", 
            "text": "For qemu images to boot reliably, they should not wait for a keypress at the  grub  command, which is distressingly common.  To ensure that your image does not wait for  grub , do the following:  For Ubuntu 12.04 (and any system that uses grub2) edit  /etc/default/grub . For example:  GRUB_DEFAULT=0\nGRUB_HIDDEN_TIMEOUT=0\nGRUB_HIDDEN_TIMEOUT_QUIET=true\nGRUB_TIMEOUT=1\nGRUB_DISTRIBUTOR=`lsb_release -i -s 2  /dev/null || echo Debian`\nGRUB_CMDLINE_LINUX_DEFAULT= quiet splash \nGRUB_CMDLINE_LINUX=   Just make sure the HIDDENs are not commented out and have true/0 values.  You then must run a command on the system which generates all the new grub configurations:  $ sudo update-grub  sudo configuration \uf0c1  The Containers system adds all users to the admin group so that group should be able to use  sudo  without providing a password.", 
            "title": "Bootable Qemu Images"
        }, 
        {
            "location": "/federation/", 
            "text": "What is Federation?\n\uf0c1\n\n\nTBD\n\n\nFor more information, go to the Federation site.", 
            "title": "Federation"
        }, 
        {
            "location": "/federation/#what-is-federation", 
            "text": "TBD  For more information, go to the Federation site.", 
            "title": "What is Federation?"
        }, 
        {
            "location": "/abac/", 
            "text": "What is ABAC?\n\uf0c1\n\n\nTBD\n\n\nFor more information, go to the ABAC site.", 
            "title": "ABAC"
        }, 
        {
            "location": "/abac/#what-is-abac", 
            "text": "TBD  For more information, go to the ABAC site.", 
            "title": "What is ABAC?"
        }, 
        {
            "location": "/ISIUCB/", 
            "text": "DETERLab has two machine rooms.  Nodes are split more or less evenly between the two, which are seamlessly bridged together over a Gigabit link.\n\n\nISI\n\uf0c1\n\n\nThe primary machine room is located in Southern California at \nThe University of Southern California Information Sciences Institute (ISI)\n.  This is where \nusers.isi.deterlab.net\n and \nwww.isi.deterlab.net\n are located.\n\n\nUCB\n\uf0c1\n\n\nThe other machine room is located in Northern California at \nThe University of California Berkeley (UCB)\n.  Nodes that start with the letter \nb\n are located at UCB.", 
            "title": "DETERLab Rooms"
        }, 
        {
            "location": "/ISIUCB/#isi", 
            "text": "The primary machine room is located in Southern California at  The University of Southern California Information Sciences Institute (ISI) .  This is where  users.isi.deterlab.net  and  www.isi.deterlab.net  are located.", 
            "title": "ISI"
        }, 
        {
            "location": "/ISIUCB/#ucb", 
            "text": "The other machine room is located in Northern California at  The University of California Berkeley (UCB) .  Nodes that start with the letter  b  are located at UCB.", 
            "title": "UCB"
        }, 
        {
            "location": "/about/release-notes/", 
            "text": "TBD", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/contributing/", 
            "text": "TBD", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/license/", 
            "text": "TBD", 
            "title": "License"
        }
    ]
}