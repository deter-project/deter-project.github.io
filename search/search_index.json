{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"These pages constitute the end-user documentation for DETERLab documentation. What is DETERLab? \u00b6 DETERLab is a state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. To date, DETERLab-based projects have included behavior analysis and defensive technologies including DDoS attacks, worm and botnet attacks, encryption, pattern detection, and intrusion-tolerant storage protocols. Important Links \u00b6 Testbed - this is the web interface to DETERLab (requires registration ) Support - this website provides support for DETERLab using a ticketing system. Organization \u00b6 DETERLab docs are organized by the dominant systems (Core, Orchestrator and Containers) and then each system includes: Quickstart - Gives a summary of the system and an overview of the steps involved to use it. Guide - Provides details on the most common, basic ways to use the system. It serves as a basic tutorial and reference. Reference - Includes references to configuration, commands and other reference materials Ad hoc topics - More advanced topics for more complicated features of the system.","title":"Welcome to the DETERLab Documentation"},{"location":"#what-is-deterlab","text":"DETERLab is a state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. To date, DETERLab-based projects have included behavior analysis and defensive technologies including DDoS attacks, worm and botnet attacks, encryption, pattern detection, and intrusion-tolerant storage protocols.","title":"What is DETERLab?"},{"location":"#important-links","text":"Testbed - this is the web interface to DETERLab (requires registration ) Support - this website provides support for DETERLab using a ticketing system.","title":"Important Links"},{"location":"#organization","text":"DETERLab docs are organized by the dominant systems (Core, Orchestrator and Containers) and then each system includes: Quickstart - Gives a summary of the system and an overview of the steps involved to use it. Guide - Provides details on the most common, basic ways to use the system. It serves as a basic tutorial and reference. Reference - Includes references to configuration, commands and other reference materials Ad hoc topics - More advanced topics for more complicated features of the system.","title":"Organization"},{"location":"Glossary/","text":"DETERLab Glossary \u00b6 Agent Abstraction Language (AAL) A YAML based descriptive language that the MAGI Orchestrator uses to describe an experiment\u2019s procedural workflow. The entire experiment procedure needs to be expressed as part of an AAL file. Find more information in the Orchestrator Guide . agent In MAGI Orchestrator , a piece of code that instruments a given functional behavior. Find more information in the Orchestrator Guide . Boss network ( myboss.isi.deterlab.net ) The main testbed server that runs DETERLab. Users are not allowed to log directly into it. collectors/collector nodes In MAGI Orchestrator , a set of nodes that capture and store experiment status and data. container_image.py In the Containers (Virtualization) system , a command that draws a picture of the topology of an experiment. This is helpful in keeping track of how virtual nodes are connected. For more information, see the Containers Reference . containerize.py In Containers (Virtualization) system , the command that creates a DETER experiment made up of containers. For more information, see containerize.py . container In the Containers (Virtualization) system , any one of various virtualization technologies, from an Openvz container to a physical machine to a simulation. The Containers system allows you to create interconnections of containers (in our sense) holding different experiment elements. data management layer In the MAGI Orchestrator , helps agents collect data. Users can query for the collected data by connecting to the data management layer. DBDL In the MAGI Orchestrator , the section of the experiment config file that configures the data layer. DETER web interface (isi.deterlab.net) The browser-based web portal for starting and defining experiments in DETERLab. DETER Stands for cyber DEfense Technology Experimental Research and is an organization out of ISI/USC conducting research (the DETER Project) as well as the operator of a leading cyber security experimentation lab, DETERLab. DETER's mission is to readily enable the research community to conduct advanced research in cyber security through use of DETERLab's innovative methods and advanced tools -- that allow for repeatable, scalable and scientifically verifiable experimentation -- for homeland security and critical infrastructure protection. DETER Project A research project run by DETER that focuses on answering key questions about how best to conduct cyber-security research, what are the best methods and tools to carry out this kind of research, and how to conduct cyber research in a repeatable, archivable, hypothesis-based way. For more information, see https://www.deter-project.org/about_deter_project . DETERLab A state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. For more information, see https://www.deter-project.org/about_deterlab . Emulab A network testbed designed to provide researchers a wide range of environments in which to develop, debug, and evaluate their systems. The DETERLab system is based on Emulab. endnode tracing/monitoring Refers to putting the trace hooks on the end nodes of a link, instead of on delay nodes. This happens when there are no delay nodes in use or if you have explicitly requested end node shaping to reduce the number of nodes you need for an experiment. event groups In MAGI Orchestrator , a group of events that enable a behavior in the experiment, for example web server events. event streams In MAGI Orchestrator , the order of events in a particular MAGI (orchestrated) experiment. Events are one of two things, events or triggers. Events are similar to remote procedure calls on agents. Triggers are synchronization and/or branching points in your event stream. A simple example of an event stream could be: configure the agent, tell it to begin monitoring the local node, wait for 60 seconds, then tell the agent to stop monitoring. When the event stream has no more events or triggers, the orchestrator will exit. event system In MAGI Orchestrator , a messaging system to send and receive orchestration messages. events In MAGI Orchestrator , events are one of two things, events or triggers. Events are similar to remote procedure calls on agents. Triggers are synchronization and/or branching points in your event stream. ExpDL In the MAGI Orchestrator , the section of the experiment config file that configures common experiment wide configuration that is used by the various MAGI tools. Most of the configuration in this section is automatically generated. experiment Work in DETERLab is organized by experiments within project. experiment.conf In the MAGI Orchestrator , the experiment-wide, YAML-based configuration file. federation In general, refers to the ability to join disparate resources as if they were the same resource. DETERLab offers federated architecture for creating experiments that span multiple testbeds through dynamically acquiring resources from other testbeds. fidelity In general, means the degree to which something is faithfully reproduced. In DETERLab, refers to varying degrees different elements of a large-scale experiment (in the Containers (Virtualization) system , for example) need to be fully reproduced. Some processes require high degree of fidelity while others do not. gatekeeper Protects the internet facing side of the testbed and serves as a NAT machine for the Private Internet Network using a bridging firewall. image IDs A descriptor for a disk image. This can be an image of an entire disk, a partition of a disk, or a set of partitions of a disk. image Refers to a disk image. interface description file (IDL) In the MAGI Orchestrator , refers to a YAML-based file that describes an agent's interface. This is required when writing your own agents and allows an agent to be integrated with the experiment GUIs. link tracing/monitoring The ability to follow the path of a link or LAN in a DETERLab experiment. MAGI A DETERLab system that allows you to orchestrate very complex experiments. Stands for Montage AGent Infrastructure (Montage was originally an experiment lifecycle manager in DETERLab). For more information, see MAGI Orchestrator docs . MAGI Graph In the MAGI Orchestrator , a graph generator for experiments executed on DETER Lab using MAGI. The tool fetches the required data using MAGI\u2019s data management layer and generates a graph in PNG format. MAGI Status In the MAGI Orchestrator , a status tool that allows you to check MAGI\u2019s status on experiment nodes, reboot MAGI daemon process, and download logs from experiment nodes. magi_bootstrap.py In the MAGI Orchestrator , installs the MAGI distribution along with the supporting tools when setting up a MAGI-enabled experiment in DETERLab. magi_orchestrator.py In the MAGI Orchestrator , a tool that parses an AAL file and orchestrates an experiment based on the specified procedures. Messaging Description Language (MESDL) In the MAGI Orchestrator , the section of the experiment config file that defines all the overlays and bridges for the experiment. The MAGI \u201coverlay\u201d is built on top of the control network on the DETERLab testbed. Nodes in the overlay can be thought of as being connected by virtual point-to-point links. The overlay provides a way to communicate with all the MAGI-enabled nodes across the testbed boundaries and even over the internet. \u201cBridges\u201d act as meeting places for all the members of the overlay it serves. node.conf In the MAGI Orchestrator , a node-specific configuration file used by the MAGI daemon process. As part of the bootstrap process, the experiment-wide config file (experiment.conf) is already broken down into node-level configuration. But you may customize node configuration via this file and it may be provided as an input to the MAGI bootstrap script, or to the MAGI daemon startup script, directly. nodes A node simply means any computer allocated to an experiment. NS (Network Simulators) files syntax Used to describe topologies in network experiments. DETERLab-specific information may be found in the Core Guide and further documentation is available at http://www.isi.edu/nsnam/ns/ . operating system images (OSIDs) Describes an operating system which resides on a partition of a disk image. Every ImageID will have at least one OSID associated with it. orchestrator see [orchestrator.py], in the MAGI Orchestrator . project Each group of users (or team) using DETERLab is grouped into 'projects', identified by a project ID (PID). swapin The process where DETERLab allocates resources for your experiment and runs it. swapout The process where DETERLab frees up the resources that were being used for your experiment. It is very important to do so when you are no longer using your experiment so that these resources are available for other experiments. topology A description of the various elements of a computer network. In DETERLab, your experiment requires a topology in NS syntax that describes the links, nodes, etc of your experiment. triggers In the MAGI Orchestrator , these are synchronization and/or branching points in a stream of events. users network ( users.deterlab.net ) DETERLab's file server that serves as a shell host for testbed users. YAML A simple format for describing data, used as the syntax for many configuration files throughout DETERLab systems. In general, just follow configuration file documentation, but if you are curious about specifications, you may find the latest here: http://www.yaml.org/spec/1.2/spec.html","title":"Glossary"},{"location":"Glossary/#deterlab-glossary","text":"Agent Abstraction Language (AAL) A YAML based descriptive language that the MAGI Orchestrator uses to describe an experiment\u2019s procedural workflow. The entire experiment procedure needs to be expressed as part of an AAL file. Find more information in the Orchestrator Guide . agent In MAGI Orchestrator , a piece of code that instruments a given functional behavior. Find more information in the Orchestrator Guide . Boss network ( myboss.isi.deterlab.net ) The main testbed server that runs DETERLab. Users are not allowed to log directly into it. collectors/collector nodes In MAGI Orchestrator , a set of nodes that capture and store experiment status and data. container_image.py In the Containers (Virtualization) system , a command that draws a picture of the topology of an experiment. This is helpful in keeping track of how virtual nodes are connected. For more information, see the Containers Reference . containerize.py In Containers (Virtualization) system , the command that creates a DETER experiment made up of containers. For more information, see containerize.py . container In the Containers (Virtualization) system , any one of various virtualization technologies, from an Openvz container to a physical machine to a simulation. The Containers system allows you to create interconnections of containers (in our sense) holding different experiment elements. data management layer In the MAGI Orchestrator , helps agents collect data. Users can query for the collected data by connecting to the data management layer. DBDL In the MAGI Orchestrator , the section of the experiment config file that configures the data layer. DETER web interface (isi.deterlab.net) The browser-based web portal for starting and defining experiments in DETERLab. DETER Stands for cyber DEfense Technology Experimental Research and is an organization out of ISI/USC conducting research (the DETER Project) as well as the operator of a leading cyber security experimentation lab, DETERLab. DETER's mission is to readily enable the research community to conduct advanced research in cyber security through use of DETERLab's innovative methods and advanced tools -- that allow for repeatable, scalable and scientifically verifiable experimentation -- for homeland security and critical infrastructure protection. DETER Project A research project run by DETER that focuses on answering key questions about how best to conduct cyber-security research, what are the best methods and tools to carry out this kind of research, and how to conduct cyber research in a repeatable, archivable, hypothesis-based way. For more information, see https://www.deter-project.org/about_deter_project . DETERLab A state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. For more information, see https://www.deter-project.org/about_deterlab . Emulab A network testbed designed to provide researchers a wide range of environments in which to develop, debug, and evaluate their systems. The DETERLab system is based on Emulab. endnode tracing/monitoring Refers to putting the trace hooks on the end nodes of a link, instead of on delay nodes. This happens when there are no delay nodes in use or if you have explicitly requested end node shaping to reduce the number of nodes you need for an experiment. event groups In MAGI Orchestrator , a group of events that enable a behavior in the experiment, for example web server events. event streams In MAGI Orchestrator , the order of events in a particular MAGI (orchestrated) experiment. Events are one of two things, events or triggers. Events are similar to remote procedure calls on agents. Triggers are synchronization and/or branching points in your event stream. A simple example of an event stream could be: configure the agent, tell it to begin monitoring the local node, wait for 60 seconds, then tell the agent to stop monitoring. When the event stream has no more events or triggers, the orchestrator will exit. event system In MAGI Orchestrator , a messaging system to send and receive orchestration messages. events In MAGI Orchestrator , events are one of two things, events or triggers. Events are similar to remote procedure calls on agents. Triggers are synchronization and/or branching points in your event stream. ExpDL In the MAGI Orchestrator , the section of the experiment config file that configures common experiment wide configuration that is used by the various MAGI tools. Most of the configuration in this section is automatically generated. experiment Work in DETERLab is organized by experiments within project. experiment.conf In the MAGI Orchestrator , the experiment-wide, YAML-based configuration file. federation In general, refers to the ability to join disparate resources as if they were the same resource. DETERLab offers federated architecture for creating experiments that span multiple testbeds through dynamically acquiring resources from other testbeds. fidelity In general, means the degree to which something is faithfully reproduced. In DETERLab, refers to varying degrees different elements of a large-scale experiment (in the Containers (Virtualization) system , for example) need to be fully reproduced. Some processes require high degree of fidelity while others do not. gatekeeper Protects the internet facing side of the testbed and serves as a NAT machine for the Private Internet Network using a bridging firewall. image IDs A descriptor for a disk image. This can be an image of an entire disk, a partition of a disk, or a set of partitions of a disk. image Refers to a disk image. interface description file (IDL) In the MAGI Orchestrator , refers to a YAML-based file that describes an agent's interface. This is required when writing your own agents and allows an agent to be integrated with the experiment GUIs. link tracing/monitoring The ability to follow the path of a link or LAN in a DETERLab experiment. MAGI A DETERLab system that allows you to orchestrate very complex experiments. Stands for Montage AGent Infrastructure (Montage was originally an experiment lifecycle manager in DETERLab). For more information, see MAGI Orchestrator docs . MAGI Graph In the MAGI Orchestrator , a graph generator for experiments executed on DETER Lab using MAGI. The tool fetches the required data using MAGI\u2019s data management layer and generates a graph in PNG format. MAGI Status In the MAGI Orchestrator , a status tool that allows you to check MAGI\u2019s status on experiment nodes, reboot MAGI daemon process, and download logs from experiment nodes. magi_bootstrap.py In the MAGI Orchestrator , installs the MAGI distribution along with the supporting tools when setting up a MAGI-enabled experiment in DETERLab. magi_orchestrator.py In the MAGI Orchestrator , a tool that parses an AAL file and orchestrates an experiment based on the specified procedures. Messaging Description Language (MESDL) In the MAGI Orchestrator , the section of the experiment config file that defines all the overlays and bridges for the experiment. The MAGI \u201coverlay\u201d is built on top of the control network on the DETERLab testbed. Nodes in the overlay can be thought of as being connected by virtual point-to-point links. The overlay provides a way to communicate with all the MAGI-enabled nodes across the testbed boundaries and even over the internet. \u201cBridges\u201d act as meeting places for all the members of the overlay it serves. node.conf In the MAGI Orchestrator , a node-specific configuration file used by the MAGI daemon process. As part of the bootstrap process, the experiment-wide config file (experiment.conf) is already broken down into node-level configuration. But you may customize node configuration via this file and it may be provided as an input to the MAGI bootstrap script, or to the MAGI daemon startup script, directly. nodes A node simply means any computer allocated to an experiment. NS (Network Simulators) files syntax Used to describe topologies in network experiments. DETERLab-specific information may be found in the Core Guide and further documentation is available at http://www.isi.edu/nsnam/ns/ . operating system images (OSIDs) Describes an operating system which resides on a partition of a disk image. Every ImageID will have at least one OSID associated with it. orchestrator see [orchestrator.py], in the MAGI Orchestrator . project Each group of users (or team) using DETERLab is grouped into 'projects', identified by a project ID (PID). swapin The process where DETERLab allocates resources for your experiment and runs it. swapout The process where DETERLab frees up the resources that were being used for your experiment. It is very important to do so when you are no longer using your experiment so that these resources are available for other experiments. topology A description of the various elements of a computer network. In DETERLab, your experiment requires a topology in NS syntax that describes the links, nodes, etc of your experiment. triggers In the MAGI Orchestrator , these are synchronization and/or branching points in a stream of events. users network ( users.deterlab.net ) DETERLab's file server that serves as a shell host for testbed users. YAML A simple format for describing data, used as the syntax for many configuration files throughout DETERLab systems. In general, just follow configuration file documentation, but if you are curious about specifications, you may find the latest here: http://www.yaml.org/spec/1.2/spec.html","title":"DETERLab Glossary"},{"location":"ISIUCB/","text":"DETERLab Rooms \u00b6 DETERLab has three machine rooms. Nodes are split more or less evenly between the ISI, USC, and Berkeley. A 10 Gigabit link from ISI to USC and another 10 Gigabit link from ISI to Berkeley provides complete connectivity. ISI \u00b6 The primary machine room is located in Southern California at The University of Southern California Information Sciences Institute (ISI) . This is where users.deterlab.net and www.isi.deterlab.net are located. USC \u00b6 Another machine room is located in a Southern California colocation facility at The University of Southern California Information Technology Services (USC ITS) . UCB \u00b6 The other machine room is located in Northern California at The University of California Berkeley (UCB) . Nodes that start with the letter b are located at UCB.","title":"DETERLab Rooms"},{"location":"ISIUCB/#deterlab-rooms","text":"DETERLab has three machine rooms. Nodes are split more or less evenly between the ISI, USC, and Berkeley. A 10 Gigabit link from ISI to USC and another 10 Gigabit link from ISI to Berkeley provides complete connectivity.","title":"DETERLab Rooms"},{"location":"ISIUCB/#isi","text":"The primary machine room is located in Southern California at The University of Southern California Information Sciences Institute (ISI) . This is where users.deterlab.net and www.isi.deterlab.net are located.","title":"ISI"},{"location":"ISIUCB/#usc","text":"Another machine room is located in a Southern California colocation facility at The University of Southern California Information Technology Services (USC ITS) .","title":"USC"},{"location":"ISIUCB/#ucb","text":"The other machine room is located in Northern California at The University of California Berkeley (UCB) . Nodes that start with the letter b are located at UCB.","title":"UCB"},{"location":"about_these_pages/","text":"About These Pages \u00b6 Note about porting \u00b6 This information should be adjusted for the ported docs and turned into a readme file (ie, not publicly viewable) Software Used \u00b6 These pages are created with mkdocs version 0.14.0, a static documentation site generator. Site Location and Directory Layout \u00b6 This site is kept in the deter-project repository, under the directory docs . The pages are generated from markdown files in site/docs/ . Updating a Page \u00b6 To update a page, edit the file in docs/. Run the command mkdocs serve in the site/ directory. This will start a local web browser which you can use to preview your changes. If you are making multiple changes, it is convenient to keep this preview server running, because it will automatically pick up, render, and display changes as you make them. Once you are happy with your changes, commit them and then push them to the live server (see below ). Creating a New Page \u00b6 To create a new page, create a file in docs/ and then add a corresponding line in the pages section of sysadmin.deterlab.net/mkdocs.yml . This configuration file is of the format: site_name: DETERLab Docs site_description: User documentation for the DETERLab testbed theme: readthedocs theme_dir: 'readthedocs_custom' site_favicon: favicon.ico pages: - Welcome to the DETERLab Documentation: index.md #- About These Pages: about_these_pages.md #- Style Guide: style-guide.md - Glossary: glossary.md - DETERLab Core: - Core Quickstart: core/core-quickstart.md In the pages section, the left half of the line is the page's human-readable name, while the right half is the name of the file in docs/. You can change the order of the page listing in the sidebar by changing the order of the items in the .yml file. Once you are happy with your changes, add your new file and the mkdocs.yml to the git repository, commit them, and then push the changes to the live server (see below ). Renaming a Page \u00b6 This would simply be a matter of renaming the file in docs/, and making corresponding changes in mkdocs.yml, as well as in any other files in docs/ which reference the original name. Note that if you rename a page, you may want to run mkdocs build --clean instead of mkdocs build when you regenerate the site . Customizing Docs \u00b6 If you are customizing these docs for your own organization, here are some general guidelines for making them your own. Changing the logo and colors requires a small amount of HTML and CSS knowledge: In general, you can do a find/replace in the site/docs directory replacing 'DETERLab' with your own organization name. The base.html file in the `readthedocs_custom' directory includes customizations to the layout and is where the doc-logo.png is included. Replace the png with your own using a PNG file around 24px by 24px. Colors are controlled in the docs/css/extra.css file. Pushing Changes to the Live Server \u00b6 This site is hosted on tardis.deterlab.net , in the directory /var/www-sysadmin/. Authentication is provided by apache Basic Auth, configured in /etc/apache2/sites-enabled/sysadmin.deterlab.net , with an htpasswd file at /var/www-sysadmin/.htpasswd . You can add user accounts to this, or change their password, by running htpasswd /var/www-sysadmin/.htpasswd USERNAME (where you replace USERNAME with the username you want) with root privileges on tardis. Once you are satisfied with your changes and have committed them, build a new static HTML version of the site by running mkdocs build in the sysadmin.deterlab.net directory. You may see some python errors about unicode literals. These can safely be ignored. The mkdocs build command populates the directory sysadmin.deterlab.net/site/ with a new version of the site. Now all you have to do is copy the files over to tardis: ~/operations/sysadmin.deterlab.net$ scp -r site/* tardis.deterlab.net:/var/www-sysadmin/ jross@tardis.deterlab.net's password: index.html 100% 13KB 12.6KB/s 00:00 index.html 100% 10KB 10.1KB/s 00:00 index.html 100% 15KB 14.9KB/s 00:00 index.html 100% 9774 9.5KB/s 00:00 index.html 100% 12KB 11.8KB/s 00:00 index.html 100% 10KB 10.3KB/s 00:00 theme.css 100% 87KB 86.9KB/s 00:00 highlight.css 100% 1682 1.6KB/s 00:00 theme_extra.css 100% 2310 2.3KB/s 00:00 index.html 100% 11KB 11.2KB/s 00:00 index.html 100% 18KB 17.9KB/s 00:00 index.html 100% 11KB 10.7KB/s 00:00 fontawesome-webfont.woff 100% 43KB 42.6KB/s 00:00 fontawesome-webfont.ttf 100% 77KB 77.2KB/s 00:00 fontawesome-webfont.eot 100% 37KB 36.5KB/s 00:00 fontawesome-webfont.svg 100% 193KB 193.2KB/s 00:00 index.html 100% 11KB 10.7KB/s 00:00 index.html 100% 10KB 9.9KB/s 00:00 index.html 100% 10KB 10.0KB/s 00:00 index.html 100% 9993 9.8KB/s 00:00 index.html 100% 10KB 10.1KB/s 00:00 index.html 100% 10KB 10.5KB/s 00:00 invoice.png 100% 151KB 151.3KB/s 00:00 invoice_2.png 100% 152KB 151.9KB/s 00:00 firefox_proxy_config.png 100% 57KB 57.3KB/s 00:00 invoice_3.png 100% 150KB 150.1KB/s 00:00 favicon.ico 100% 1150 1.1KB/s 00:00 index.html 100% 12KB 12.1KB/s 00:00 index.html 100% 11KB 10.7KB/s 00:00 index.html 100% 10KB 10.1KB/s 00:00 modernizr-2.8.3.min.js 100% 11KB 10.8KB/s 00:00 jquery-2.1.1.min.js 100% 82KB 82.3KB/s 00:00 highlight.pack.js 100% 294KB 293.7KB/s 00:00 theme.js 100% 1751 1.7KB/s 00:00 LICENSE 100% 1498 1.5KB/s 00:00 index.html 100% 11KB 11.0KB/s 00:00 search.js 100% 2593 2.5KB/s 00:00 text.js 100% 15KB 15.3KB/s 00:00 lunr-0.5.7.min.js 100% 14KB 14.2KB/s 00:00 mustache.min.js 100% 8835 8.6KB/s 00:00 search-results-template.mustache 100% 90 0.1KB/s 00:00 require.js 100% 15KB 14.9KB/s 00:00 search_index.json 100% 99KB 99.3KB/s 00:00 index.html 100% 10KB 10.1KB/s 00:00 index.html 100% 11KB 11.0KB/s 00:00 index.html 100% 10KB 10.3KB/s 00:00 index.html 100% 11KB 10.5KB/s 00:00 index.html 100% 10KB 9.8KB/s 00:00 index.html 100% 9906 9.7KB/s 00:00 index.html 100% 9754 9.5KB/s 00:00 index.html 100% 9547 9.3KB/s 00:00 index.html 100% 9830 9.6KB/s 00:00 index.html 100% 11KB 10.6KB/s 00:00 index.html 100% 15KB 15.0KB/s 00:00 search.html 100% 9121 8.9KB/s 00:00 index.html 100% 17KB 17.4KB/s 00:00 sitemap.xml 100% 5732 5.6KB/s 00:00 index.html 100% 13KB 12.7KB/s 00:00 index.html 100% 11KB 11.0KB/s 00:00 index.html 100% 10KB 10.0KB/s 00:00 index.html 100% 10KB 10.1KB/s 00:00 index.html 100% 14KB 14.3KB/s 00:00 index.html 100% 12KB 12.1KB/s 00:00 index.html 100% 10KB 9.9KB/s 00:00 index.html 100% 13KB 13.1KB/s 00:00 index.html 100% 11KB 10.9KB/s 00:00 index.html 100% 10KB 10.2KB/s 00:00 index.html 100% 10KB 10.3KB/s 00:00","title":"About These Pages"},{"location":"about_these_pages/#about-these-pages","text":"","title":"About These Pages"},{"location":"about_these_pages/#note-about-porting","text":"This information should be adjusted for the ported docs and turned into a readme file (ie, not publicly viewable)","title":"Note about porting"},{"location":"about_these_pages/#software-used","text":"These pages are created with mkdocs version 0.14.0, a static documentation site generator.","title":"Software Used"},{"location":"about_these_pages/#site-location-and-directory-layout","text":"This site is kept in the deter-project repository, under the directory docs . The pages are generated from markdown files in site/docs/ .","title":"Site Location and Directory Layout"},{"location":"about_these_pages/#updating-a-page","text":"To update a page, edit the file in docs/. Run the command mkdocs serve in the site/ directory. This will start a local web browser which you can use to preview your changes. If you are making multiple changes, it is convenient to keep this preview server running, because it will automatically pick up, render, and display changes as you make them. Once you are happy with your changes, commit them and then push them to the live server (see below ).","title":"Updating a Page"},{"location":"about_these_pages/#creating-a-new-page","text":"To create a new page, create a file in docs/ and then add a corresponding line in the pages section of sysadmin.deterlab.net/mkdocs.yml . This configuration file is of the format: site_name: DETERLab Docs site_description: User documentation for the DETERLab testbed theme: readthedocs theme_dir: 'readthedocs_custom' site_favicon: favicon.ico pages: - Welcome to the DETERLab Documentation: index.md #- About These Pages: about_these_pages.md #- Style Guide: style-guide.md - Glossary: glossary.md - DETERLab Core: - Core Quickstart: core/core-quickstart.md In the pages section, the left half of the line is the page's human-readable name, while the right half is the name of the file in docs/. You can change the order of the page listing in the sidebar by changing the order of the items in the .yml file. Once you are happy with your changes, add your new file and the mkdocs.yml to the git repository, commit them, and then push the changes to the live server (see below ).","title":"Creating a New Page"},{"location":"about_these_pages/#renaming-a-page","text":"This would simply be a matter of renaming the file in docs/, and making corresponding changes in mkdocs.yml, as well as in any other files in docs/ which reference the original name. Note that if you rename a page, you may want to run mkdocs build --clean instead of mkdocs build when you regenerate the site .","title":"Renaming a Page"},{"location":"about_these_pages/#customizing-docs","text":"If you are customizing these docs for your own organization, here are some general guidelines for making them your own. Changing the logo and colors requires a small amount of HTML and CSS knowledge: In general, you can do a find/replace in the site/docs directory replacing 'DETERLab' with your own organization name. The base.html file in the `readthedocs_custom' directory includes customizations to the layout and is where the doc-logo.png is included. Replace the png with your own using a PNG file around 24px by 24px. Colors are controlled in the docs/css/extra.css file.","title":"Customizing Docs"},{"location":"about_these_pages/#pushing-changes-to-the-live-server","text":"This site is hosted on tardis.deterlab.net , in the directory /var/www-sysadmin/. Authentication is provided by apache Basic Auth, configured in /etc/apache2/sites-enabled/sysadmin.deterlab.net , with an htpasswd file at /var/www-sysadmin/.htpasswd . You can add user accounts to this, or change their password, by running htpasswd /var/www-sysadmin/.htpasswd USERNAME (where you replace USERNAME with the username you want) with root privileges on tardis. Once you are satisfied with your changes and have committed them, build a new static HTML version of the site by running mkdocs build in the sysadmin.deterlab.net directory. You may see some python errors about unicode literals. These can safely be ignored. The mkdocs build command populates the directory sysadmin.deterlab.net/site/ with a new version of the site. Now all you have to do is copy the files over to tardis: ~/operations/sysadmin.deterlab.net$ scp -r site/* tardis.deterlab.net:/var/www-sysadmin/ jross@tardis.deterlab.net's password: index.html 100% 13KB 12.6KB/s 00:00 index.html 100% 10KB 10.1KB/s 00:00 index.html 100% 15KB 14.9KB/s 00:00 index.html 100% 9774 9.5KB/s 00:00 index.html 100% 12KB 11.8KB/s 00:00 index.html 100% 10KB 10.3KB/s 00:00 theme.css 100% 87KB 86.9KB/s 00:00 highlight.css 100% 1682 1.6KB/s 00:00 theme_extra.css 100% 2310 2.3KB/s 00:00 index.html 100% 11KB 11.2KB/s 00:00 index.html 100% 18KB 17.9KB/s 00:00 index.html 100% 11KB 10.7KB/s 00:00 fontawesome-webfont.woff 100% 43KB 42.6KB/s 00:00 fontawesome-webfont.ttf 100% 77KB 77.2KB/s 00:00 fontawesome-webfont.eot 100% 37KB 36.5KB/s 00:00 fontawesome-webfont.svg 100% 193KB 193.2KB/s 00:00 index.html 100% 11KB 10.7KB/s 00:00 index.html 100% 10KB 9.9KB/s 00:00 index.html 100% 10KB 10.0KB/s 00:00 index.html 100% 9993 9.8KB/s 00:00 index.html 100% 10KB 10.1KB/s 00:00 index.html 100% 10KB 10.5KB/s 00:00 invoice.png 100% 151KB 151.3KB/s 00:00 invoice_2.png 100% 152KB 151.9KB/s 00:00 firefox_proxy_config.png 100% 57KB 57.3KB/s 00:00 invoice_3.png 100% 150KB 150.1KB/s 00:00 favicon.ico 100% 1150 1.1KB/s 00:00 index.html 100% 12KB 12.1KB/s 00:00 index.html 100% 11KB 10.7KB/s 00:00 index.html 100% 10KB 10.1KB/s 00:00 modernizr-2.8.3.min.js 100% 11KB 10.8KB/s 00:00 jquery-2.1.1.min.js 100% 82KB 82.3KB/s 00:00 highlight.pack.js 100% 294KB 293.7KB/s 00:00 theme.js 100% 1751 1.7KB/s 00:00 LICENSE 100% 1498 1.5KB/s 00:00 index.html 100% 11KB 11.0KB/s 00:00 search.js 100% 2593 2.5KB/s 00:00 text.js 100% 15KB 15.3KB/s 00:00 lunr-0.5.7.min.js 100% 14KB 14.2KB/s 00:00 mustache.min.js 100% 8835 8.6KB/s 00:00 search-results-template.mustache 100% 90 0.1KB/s 00:00 require.js 100% 15KB 14.9KB/s 00:00 search_index.json 100% 99KB 99.3KB/s 00:00 index.html 100% 10KB 10.1KB/s 00:00 index.html 100% 11KB 11.0KB/s 00:00 index.html 100% 10KB 10.3KB/s 00:00 index.html 100% 11KB 10.5KB/s 00:00 index.html 100% 10KB 9.8KB/s 00:00 index.html 100% 9906 9.7KB/s 00:00 index.html 100% 9754 9.5KB/s 00:00 index.html 100% 9547 9.3KB/s 00:00 index.html 100% 9830 9.6KB/s 00:00 index.html 100% 11KB 10.6KB/s 00:00 index.html 100% 15KB 15.0KB/s 00:00 search.html 100% 9121 8.9KB/s 00:00 index.html 100% 17KB 17.4KB/s 00:00 sitemap.xml 100% 5732 5.6KB/s 00:00 index.html 100% 13KB 12.7KB/s 00:00 index.html 100% 11KB 11.0KB/s 00:00 index.html 100% 10KB 10.0KB/s 00:00 index.html 100% 10KB 10.1KB/s 00:00 index.html 100% 14KB 14.3KB/s 00:00 index.html 100% 12KB 12.1KB/s 00:00 index.html 100% 10KB 9.9KB/s 00:00 index.html 100% 13KB 13.1KB/s 00:00 index.html 100% 11KB 10.9KB/s 00:00 index.html 100% 10KB 10.2KB/s 00:00 index.html 100% 10KB 10.3KB/s 00:00","title":"Pushing Changes to the Live Server"},{"location":"style-guide/","text":"Style guide \u00b6 The following style guide should be followed when writing documentation for this site. When writing documentation for Wolf CMS, we highly appreciate people following this guide as it allows us to more easily incorporate contributed documentation. If the guide is lacking or faulty somehow, please do bring that to our attention. Thank you. Document title \u00b6 Every document is starts with its own title and it is written as follows: Document title ============== Sub headings \u00b6 The various levels of sub headings should be written as: Level two heading ----------------- ### Level three heading #### Level four heading ##### Level five heading ###### Level six heading Notes \u00b6 Various types of notes can and should be used throughout the documentation to inform people of special cases, danger areas, give tips, etc. However, notes should not be used overly much. The various note types are: Note The informational note, written as: !!! note Some extra piece of information. Tip A note that gives a tip, written as: !!! tip A handy tip! Warning A note that gives a warning, written as: !!! warning Please make sure you backup your system before... Danger A note that warns of dangerous actions or settins, written as: !!! danger If you do this you will likely screw up your system! Code \u00b6 When applicable, you should enhance your explanations using code examples. Settings, file names and code in general should either be written as short, inline entries using single backticks ( ` ) surrounding the code or as so called code blocks: ``` This is a code block. ``` Inline markup \u00b6 one asterisk: *text* for emphasis ; two asterisks: **text** for strong emphasis ; Lists \u00b6 List markup is natural: just place an asterisk at the start of a paragraph and indent properly. The same goes for numbered lists. * This is a bulleted list. * It has two items, the second item uses several lines which are all indented. 1. This is a numbered list. 2. It has two items too. Nested lists can be achieved by simply indenting them. Note When adding code blocks or similar structures to a list, make sure they are all indented, including any empty lines. Hyperlinks \u00b6 When additional information is needed or when referencing other parts of the documentation, a link is required. You can link in several different ways depending on the requirements of the situation: Basic raw links <http://www.example.com/> Link using descriptive text [Link text](http://www.example.com/) Link to other pages within the documentation [Link text](path/to/doc.md) Link to an anchor within a page [Link text](yourpage.md#anchorname) Link to an same-page anchor [Link text](#anchorname) Please note that all headings are automatically anchors. Warning Due to a bug in the current MkDocs implementation, same-page anchor links should be written as [Link text](samepage.md#anchor) instead of the correct [Link text](#anchor) . Images \u00b6 Images can be inserted either using markdown syntax or as basic HTML code when alignment or special styling is required. Whenever possible, stick to the markdown syntax. ![Alternate text](images/example-image.png) ![Alternate text](images/example-image.png \"Optional title\") <p align=\"center\"><img src=\"docs/images/example-image.png\" alt=\"Alternate text\"></p> Before you do this, you need to prepare image (crop, resize, mark...) and put it in the docs/images folder.","title":"Style guide"},{"location":"style-guide/#style-guide","text":"The following style guide should be followed when writing documentation for this site. When writing documentation for Wolf CMS, we highly appreciate people following this guide as it allows us to more easily incorporate contributed documentation. If the guide is lacking or faulty somehow, please do bring that to our attention. Thank you.","title":"Style guide"},{"location":"style-guide/#document-title","text":"Every document is starts with its own title and it is written as follows: Document title ==============","title":"Document title"},{"location":"style-guide/#sub-headings","text":"The various levels of sub headings should be written as: Level two heading ----------------- ### Level three heading #### Level four heading ##### Level five heading ###### Level six heading","title":"Sub headings"},{"location":"style-guide/#notes","text":"Various types of notes can and should be used throughout the documentation to inform people of special cases, danger areas, give tips, etc. However, notes should not be used overly much. The various note types are: Note The informational note, written as: !!! note Some extra piece of information. Tip A note that gives a tip, written as: !!! tip A handy tip! Warning A note that gives a warning, written as: !!! warning Please make sure you backup your system before... Danger A note that warns of dangerous actions or settins, written as: !!! danger If you do this you will likely screw up your system!","title":"Notes"},{"location":"style-guide/#code","text":"When applicable, you should enhance your explanations using code examples. Settings, file names and code in general should either be written as short, inline entries using single backticks ( ` ) surrounding the code or as so called code blocks: ``` This is a code block. ```","title":"Code"},{"location":"style-guide/#inline-markup","text":"one asterisk: *text* for emphasis ; two asterisks: **text** for strong emphasis ;","title":"Inline markup"},{"location":"style-guide/#lists","text":"List markup is natural: just place an asterisk at the start of a paragraph and indent properly. The same goes for numbered lists. * This is a bulleted list. * It has two items, the second item uses several lines which are all indented. 1. This is a numbered list. 2. It has two items too. Nested lists can be achieved by simply indenting them. Note When adding code blocks or similar structures to a list, make sure they are all indented, including any empty lines.","title":"Lists"},{"location":"style-guide/#hyperlinks","text":"When additional information is needed or when referencing other parts of the documentation, a link is required. You can link in several different ways depending on the requirements of the situation: Basic raw links <http://www.example.com/> Link using descriptive text [Link text](http://www.example.com/) Link to other pages within the documentation [Link text](path/to/doc.md) Link to an anchor within a page [Link text](yourpage.md#anchorname) Link to an same-page anchor [Link text](#anchorname) Please note that all headings are automatically anchors. Warning Due to a bug in the current MkDocs implementation, same-page anchor links should be written as [Link text](samepage.md#anchor) instead of the correct [Link text](#anchor) .","title":"Hyperlinks"},{"location":"style-guide/#images","text":"Images can be inserted either using markdown syntax or as basic HTML code when alignment or special styling is required. Whenever possible, stick to the markdown syntax. ![Alternate text](images/example-image.png) ![Alternate text](images/example-image.png \"Optional title\") <p align=\"center\"><img src=\"docs/images/example-image.png\" alt=\"Alternate text\"></p> Before you do this, you need to prepare image (crop, resize, mark...) and put it in the docs/images folder.","title":"Images"},{"location":"abac/","text":"ABAC \u00b6 What is ABAC? \u00b6 The ABAC project has designed and implemented tools for using Attribute-Based Access Control, a scalable authorization system based on formal logic. It maps principals to attributes and uses the attribute to make an authorization decision, e.g., if user1 has the login attribute the login program will allow them to log in. This library, libabac, is a base on which to build those tools. It is in use in the DETER Federation system and being integrated with the GENI network testbed . For more information, go to the ABAC site at: abac.deterlab.net/","title":"ABAC"},{"location":"abac/#abac","text":"","title":"ABAC"},{"location":"abac/#what-is-abac","text":"The ABAC project has designed and implemented tools for using Attribute-Based Access Control, a scalable authorization system based on formal logic. It maps principals to attributes and uses the attribute to make an authorization decision, e.g., if user1 has the login attribute the login program will allow them to log in. This library, libabac, is a base on which to build those tools. It is in use in the DETER Federation system and being integrated with the GENI network testbed . For more information, go to the ABAC site at: abac.deterlab.net/","title":"What is ABAC?"},{"location":"about/contributing/","text":"TBD","title":"Contributing"},{"location":"about/license/","text":"TBD","title":"License"},{"location":"about/release-notes/","text":"TBD","title":"Release notes"},{"location":"competitions/","text":"Competitions on DETERLab \u00b6 DETERLab can be used to run attack/defense exercises between teams of students or researchers. In such an exercise, a given experiment will be accessed by two teams - a blue (defense) team and a red (offense) team. DETERLab competitions UI enables: Easy creation of experiments and teams Specification of which teams can access which experiments and which machines in an experiment Experiment setup (swap in, application installation) Competition scoring Experiment termination Enabling competitions \u00b6 If you wish to run competitions within your DETERLab project, please submit a ticket and provide rough estimates of how often you would run and how many machines you would need. We will review and enable competitions feature for your project. New competition \u00b6 You can create a new competition from your \"My DETERLab\" view. There will be a \"Competitions\" tab once you have a competition-enabled project. From that tab, you can click on the left-hand menu option \"New competition\". The dialogue will ask you for the competition name and folder path. You should specify a unique name, such that no current experiments of your contain it as a prefix (e.g., you can specify 'comp' if you have no experiments whose name starts with 'comp'). The folder path should lead to a folder which has at least two files inside it: (1) cctf.ns file, describing the topology of an experiment that will be used for the competition and (2) start.pl, specifying a script to set up software on and limit access to experimental nodes. There are several competitions in shared materials, which you can reuse. You should also read our guide to writing competition scripts . The dialogue will also ask you how many copies of the competition you need and whether team assignment should be \"paired\" or \"circular\". The figures below illustrate these two assignment types. In a circular assignment there are as many teams as there are experiments. Each team defends one experiment and attacks one other experiment. This team assignment enables participants on each team to play both defensive and offensive roles. In a paired assignment the number of teams is twice that of the experiments. Each team either defends one experiment or attacks one experiment. This team assignment places each participant into either offensive or defensive role. Once you click \"Submit\" DETERLab will create a number of experiments and teams for your competition. Writing start up scripts for competitions \u00b6 A good start up script for a competition has the following properties: - Is project and experiment agnostic - it takes project and experiment names as arguments from command line. This enables scaling and portability. - Limits access to specific experiment's nodes to red/blue team or removes access to all teams - Installs needed software - Starts scoring - Makes any changes persist through reboots - Ends up with node reboot Please start from our sample script with these features and version it to satisfy your needs. Note Don't forget to make your script executable Managing your competition \u00b6 Once your competition is created you can see it in the competitions tab. You can set it up, run it or destroy it. Set up \u00b6 Set up includes: (1) specifying which teams can access which machines, (2) assigning participants to teams. Team access \u00b6 Each experiment has a number of machines. You can specify which team can access each machine: defense (blue) team, offense (red) team or none. Blue team machines will be defended in a competition, red team machines will be used to launch attacks on blue team. You can use machines with no-team access for scoring or infrastructure set up (e.g. set up DNS root server that can be queried but not modified by teams). When you are finished, click \"Assign\" then \"Done\". You can always repeat this action, as long as the competition is not swapped in. Otherwise, you have to swap it out to change team access. Bulding teams \u00b6 You can assign a participant to a team by dragging him/her into the gray blocks for the desired team. A participant can belong to at most one team. When you are finished click \"Assign\" then \"Done\". You can always repeat this action, as long as the competition is not swapped in. If a participant shows up after you have swapped in the competition, you have to swap out, add the participant, and swap in again. Run \u00b6 Running a competition requires that machines be assigned to it, and software set up on those machines. Allocating machines \u00b6 When you click \"Allocate\" in the \"Run\" screen for your competition all related experiments will start swap-in. All experiments must successfully swap in before you can proceed. Setting up software \u00b6 You can set up necessary software for the competition manually but this does not scale. A better option is to put all your set up commands into start.pl script in your competition folder path. This script will be run when you click \"Install\" button in the \"Run\" screen. It will be run once per each experiment and the project and experiment names will be passed to the script. Our sample start.pl script in guide to writing competition scripts sets up limited access to machines, as specified in \"Team access\" option and makes sure that these changes remain in effect after reboots. After all the set up actions are completed, the script reboots the machines. It is also a good idea to start your scoring script at the time when you install the software. The scoring script should run periodically and update the \"score\" file in each experiment's \"tbdata\" directory. Please see guide to writing competition scripts for a full example. Starting the competition \u00b6 When teams are ready to start please click the \"Start\" button in the \"Run\" screen to zero out the score. You can also use this button to reset the score. Scoring the competition \u00b6 If you followed our guide to writing competition scripts team scores will be continuously updated. You can see them by clicking on the \"Score\" button. The score will be shown per experiment and broken into \"red\" and \"blue\" parts for offense and defense teams. Retiring the competition \u00b6 You can release the machines (swap out) when your competition is done. This action does not affect any of the competition's software or logs. Destroy \u00b6 When you are completely done with your competition, click the \"Destroy\" button. This will remove all the experiments and teams. It is important to clean up your competitions when you don't need them anymore to preserve DETERLab's resources.","title":"Competitions"},{"location":"competitions/#competitions-on-deterlab","text":"DETERLab can be used to run attack/defense exercises between teams of students or researchers. In such an exercise, a given experiment will be accessed by two teams - a blue (defense) team and a red (offense) team. DETERLab competitions UI enables: Easy creation of experiments and teams Specification of which teams can access which experiments and which machines in an experiment Experiment setup (swap in, application installation) Competition scoring Experiment termination","title":"Competitions on DETERLab"},{"location":"competitions/#enabling-competitions","text":"If you wish to run competitions within your DETERLab project, please submit a ticket and provide rough estimates of how often you would run and how many machines you would need. We will review and enable competitions feature for your project.","title":"Enabling competitions"},{"location":"competitions/#new-competition","text":"You can create a new competition from your \"My DETERLab\" view. There will be a \"Competitions\" tab once you have a competition-enabled project. From that tab, you can click on the left-hand menu option \"New competition\". The dialogue will ask you for the competition name and folder path. You should specify a unique name, such that no current experiments of your contain it as a prefix (e.g., you can specify 'comp' if you have no experiments whose name starts with 'comp'). The folder path should lead to a folder which has at least two files inside it: (1) cctf.ns file, describing the topology of an experiment that will be used for the competition and (2) start.pl, specifying a script to set up software on and limit access to experimental nodes. There are several competitions in shared materials, which you can reuse. You should also read our guide to writing competition scripts . The dialogue will also ask you how many copies of the competition you need and whether team assignment should be \"paired\" or \"circular\". The figures below illustrate these two assignment types. In a circular assignment there are as many teams as there are experiments. Each team defends one experiment and attacks one other experiment. This team assignment enables participants on each team to play both defensive and offensive roles. In a paired assignment the number of teams is twice that of the experiments. Each team either defends one experiment or attacks one experiment. This team assignment places each participant into either offensive or defensive role. Once you click \"Submit\" DETERLab will create a number of experiments and teams for your competition.","title":"New competition"},{"location":"competitions/#writing-start-up-scripts-for-competitions","text":"A good start up script for a competition has the following properties: - Is project and experiment agnostic - it takes project and experiment names as arguments from command line. This enables scaling and portability. - Limits access to specific experiment's nodes to red/blue team or removes access to all teams - Installs needed software - Starts scoring - Makes any changes persist through reboots - Ends up with node reboot Please start from our sample script with these features and version it to satisfy your needs. Note Don't forget to make your script executable","title":" Writing start up scripts for competitions"},{"location":"competitions/#managing-your-competition","text":"Once your competition is created you can see it in the competitions tab. You can set it up, run it or destroy it.","title":"Managing your competition"},{"location":"competitions/#set-up","text":"Set up includes: (1) specifying which teams can access which machines, (2) assigning participants to teams.","title":"Set up"},{"location":"competitions/#team-access","text":"Each experiment has a number of machines. You can specify which team can access each machine: defense (blue) team, offense (red) team or none. Blue team machines will be defended in a competition, red team machines will be used to launch attacks on blue team. You can use machines with no-team access for scoring or infrastructure set up (e.g. set up DNS root server that can be queried but not modified by teams). When you are finished, click \"Assign\" then \"Done\". You can always repeat this action, as long as the competition is not swapped in. Otherwise, you have to swap it out to change team access.","title":"Team access"},{"location":"competitions/#bulding-teams","text":"You can assign a participant to a team by dragging him/her into the gray blocks for the desired team. A participant can belong to at most one team. When you are finished click \"Assign\" then \"Done\". You can always repeat this action, as long as the competition is not swapped in. If a participant shows up after you have swapped in the competition, you have to swap out, add the participant, and swap in again.","title":"Bulding teams"},{"location":"competitions/#run","text":"Running a competition requires that machines be assigned to it, and software set up on those machines.","title":"Run"},{"location":"competitions/#allocating-machines","text":"When you click \"Allocate\" in the \"Run\" screen for your competition all related experiments will start swap-in. All experiments must successfully swap in before you can proceed.","title":"Allocating machines"},{"location":"competitions/#setting-up-software","text":"You can set up necessary software for the competition manually but this does not scale. A better option is to put all your set up commands into start.pl script in your competition folder path. This script will be run when you click \"Install\" button in the \"Run\" screen. It will be run once per each experiment and the project and experiment names will be passed to the script. Our sample start.pl script in guide to writing competition scripts sets up limited access to machines, as specified in \"Team access\" option and makes sure that these changes remain in effect after reboots. After all the set up actions are completed, the script reboots the machines. It is also a good idea to start your scoring script at the time when you install the software. The scoring script should run periodically and update the \"score\" file in each experiment's \"tbdata\" directory. Please see guide to writing competition scripts for a full example.","title":"Setting up software"},{"location":"competitions/#starting-the-competition","text":"When teams are ready to start please click the \"Start\" button in the \"Run\" screen to zero out the score. You can also use this button to reset the score.","title":"Starting the competition"},{"location":"competitions/#scoring-the-competition","text":"If you followed our guide to writing competition scripts team scores will be continuously updated. You can see them by clicking on the \"Score\" button. The score will be shown per experiment and broken into \"red\" and \"blue\" parts for offense and defense teams.","title":"Scoring the competition"},{"location":"competitions/#retiring-the-competition","text":"You can release the machines (swap out) when your competition is done. This action does not affect any of the competition's software or logs.","title":"Retiring the competition"},{"location":"competitions/#destroy","text":"When you are completely done with your competition, click the \"Destroy\" button. This will remove all the experiments and teams. It is important to clean up your competitions when you don't need them anymore to preserve DETERLab's resources.","title":"Destroy"},{"location":"competitions/comp-script-guide/","text":"Writing start up scripts for competitions \u00b6 A good start up script for a competition has the following properties: - Is project and experiment agnostic - it takes project and experiment names as arguments from command line. This enables scaling and portability. - Limits access to specific experiment's nodes to red/blue team or removes access to all teams - Installs needed software - Starts scoring - Makes any changes persist through reboots - Ends up with node reboot Please start from our sample script with these features and version it to satisfy your needs. !!!tip Don't forget to make your script executable","title":"Comp script guide"},{"location":"competitions/comp-script-guide/#writing-start-up-scripts-for-competitions","text":"A good start up script for a competition has the following properties: - Is project and experiment agnostic - it takes project and experiment names as arguments from command line. This enables scaling and portability. - Limits access to specific experiment's nodes to red/blue team or removes access to all teams - Installs needed software - Starts scoring - Makes any changes persist through reboots - Ends up with node reboot Please start from our sample script with these features and version it to satisfy your needs. !!!tip Don't forget to make your script executable","title":"Writing start up scripts for competitions"},{"location":"containers/containers-guide/","text":"Containers Guide \u00b6 In this tutorial we walk you through setting up a basic containerized experiment. This page also includes common advanced topics. Detailed descriptions of the commands and configuration files are available in the reference section . Note If you are a student, go to the http://education.deterlab.net site for classroom-specific instructions. Basic Containers Tutorial \u00b6 This tutorial will set up a containerized experiment with a star topology. We'll create a central node and connect 9 other nodes to it Getting started \u00b6 You will need a DETERLab account and be a member of a project. If you need help, see the Core Guide . Step 1: Design the topology \u00b6 First, we will describe a star topology. For this example we will use the standard DETER topology descriptions. If you are new to designing topologies, walk through the basic tutorial in the Core Guide . The Containers system is largely compatible with the physical DETER interface. Download the DETERLab-compatible ns2 description of this topology at this link to your home directory on users.deterlab.net . It is a simple loop, along with the standard DETER boilerplate. This file will be used to create a 10-node (9 satellites and one central node) physical experiment on DETER, although there are not many physical nodes on DETER with 10 interfaces (one interface for control traffic). The following is the topology description: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtproto Static $ns run Note The central node is named \"center\" and each satellite is names \"n-0\", \"n-1\"... through \"n-8\". Each connection is a 100 Mb/s link with a 10ms delay. The round trip time from n-0 to center will be 20 ms and from n-0 to n-1 will be 40 ms. Step 2: Create the containerized experiment \u00b6 Now we will run a command so the Containers system will build the containerized experiment on top of a new DETERLab physical experiment. Run the following command from the shell on users.deterlab.net and refer to the example topology you just saved in your home. $ /share/containers/containerize.py DeterTest example1 ~/example1.tcl where the first two parameters are the project and experiment name to hold the DETER experiment. This command creates an experiment called experiment1 in the DeterTest project. Throughout this tutorial, we will refer to your project as DeterTest , but make sure you actually use your actual project's name. You may use the experiment name experiment1 as long as another experiment with that name doesn't already exist Note As with any DETERLab experiment, you must be a member of the project with appropriate rights to create an experiment in it. containerize.py expects there to be no experiment with that name, and it will fail if one exists. To remove an experiment you may terminate it through the web interface or use the endexp command. Terminating an experiment is more final than swapping one out, so be sure that you want to replace the old experiment. You may also resolve the conflict by renaming your new containerized experiment. The last parameter is the file containing the topology. In this tutorial, we are referring to the ns2 file in our example but you may also use a topdl description. An ns2 description must end in .tcl or .ns . With these default parameters containerize.py will put each node into an Openvz container with at most 10 containers per physical node. The output of the above command should be something like the following: users:~$ /share/containers/containerize.py DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 Now we can see what a containerized experiment looks like. The Contents of a Containerized Experiment \u00b6 Follow the link provided in the containerize.py output. You will see a standard DETER experiment page that looks like this: You may be surprised to see that DETER thinks the experiment has only one node: The Containers system has rewritten the description file and stored additional information in the experiment's per-experiment directory that will be used to create the 10 node experiment inside the single-node DETER experiment. If you look at the ns file DETERLab has stored (click the NS File tab on the experiment page), you should see the following code: set ns [new Simulator] source tb_compat.tcl tb-make-soft-vtype container0 {pc2133 bpc2133 MicroCloud} set pnode(0000) [$ns node] tb-set-node-os ${pnode(0000)} CentOS6-64-openvz tb-set-hardware ${pnode(0000)} container0 tb-set-node-startcmd ${pnode(0000)} \"sudo /share/containers/setup/hv/bootstrap /proj/DeterTest/exp/example1/containers/site.conf >& /tmp/container.log\" tb-set-node-failure-action ${pnode(0000)} \"nonfatal\" $ns rtproto Static $ns run This looks nothing like the file we gave to containerize.py , but it does show us a little about what the Containers system has done: The single physical node ( pnode(0000) ) will run the CentOS6-64-openvz image and run on a few kinds of node. On startup, pnode(0000) will execute a command from the same /share/containers directory that containerize.py ran from using data in the per-experiment directory /proj/DeterTest/exp/example1/containers/site.conf . There is a separate /proj/DeterTest/exp/example1/containers/ directory for each experiment. The path element after /proj is replaced with the project under which the experiment was created -- DeterTest in this example -- and the element after exp is the experiment name -- example1 in this case. These directories are created for all DETERLab experiments. containers sub-directory \u00b6 The containers sub-directory of a containerized experiment holds information specific to a containerized experiment. There are a few useful bits of data in that per-experiment containers directory that we can look at. Copy of the topology file: First, a copy of the topology that we gave to containerize.py is available in /proj/DeterTest/exp/example1/containers/experiment.tcl . If the experiment is created from a topdl file, the filename will be containers/experiment.tcl . Visualization of experiment: A simple visualization of the experiment is in containers/visualization.png . This is annotated with node and network names as well as interface IP addresses. The topology depiction above is an example. To view a larger version, click here . IP-to-hostname mapping: The containers/hosts file is a copy of the IP-to-hostname mapping found on each virtual machine in the topology. It can be useful in converting IP addresses back to names. It is installed in /etc/hosts or the equivalent on each machine. PID/EID: The two files /var/containers/pid and /var/containers/eid contain the project name and experiment name. Scripts can make use of these. The rest of the contents of that directory are primarily used internally by the implementation, but a more detailed listing is in the Containers Reference . Step 3: Swap-in resources \u00b6 At this point, as with any DETER experiment, the topology does not yet have any resources attached. To get the resources, swap the experiment in from the web interface or using the swapexp command. See the DETERLab Core Guide for more information. Step 4: Verify virtual topology and access nodes \u00b6 Once you have been notified that the physical experiment has finished its swap-in, the Containers system starts converting the physical topology into the virtual topology. At this time, you must manually verify when the virtual topology has been created by ping-ing or trying to SSH into individual nodes of an experiment. There is also a workaround suggested below . We are working towards offering a better notification system. Once the containerized elements have all started, the nodes are available as if they were physical nodes. For example, we may access node n-0 of the experiment we swapped in by running: $ ssh n-0.example1.detertest Be sure that you replace example1 with the experiment name you passed to containerize.py and DeterTest with the project you created the experiment under. This is a DNS name, so it is case-insensitive. When the SSH succeeds, you will have access to an Ubuntu 10.04 32-bit node with the same directories mounted as in a physical DETERLab experiment. Containerized nodes access the control net as well. Your home directory will be mounted, so your SSH keys will work for accessing the machine. Use the same node naming conventions as physical DETERLab experiments to ping and access other nodes. Here is a ping from n-0 to center and n-1 that confirms the containerized experiment is working as we expect. n-0:~$ ping -c 3 center PING center-tblink-l21 (10.0.0.2) 56(84) bytes of data. 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=1 ttl=64 time=20.4 ms 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=2 ttl=64 time=20.0 ms 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=3 ttl=64 time=20.0 ms --- center-tblink-l21 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2002ms rtt min/avg/max/mdev = 20.052/20.184/20.445/0.184 ms n-0:~$ ping -c 3 n-1 PING n-1-tblink-l5 (10.0.6.1) 56(84) bytes of data. 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=1 ttl=64 time=40.7 ms 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=2 ttl=64 time=40.0 ms 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=3 ttl=64 time=40.0 ms --- n-1-tblink-l5 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2003ms rtt min/avg/max/mdev = 40.094/40.318/40.764/0.355 ms The nodes have the expected round trip times. At this point you can load and run software and generally experiment normally. Start Commands \u00b6 DETERLab Core provides a facility to run a command when a physical experiment starts, called start commands . A containerized experiment offers a similar facility with a few differences: The start commands are not coordinated across nodes. In a physical experiment, the start commands all execute when the last node has reported to the testbed that it has completed booting. In a containerized experiment, the start commands run when the containerized node has come up. Logs from the start command are available in /var/containers/log/start_command.out and /var/containers/log/start_command.err . This is true on embedded pnodes as well. Start commands must be shorter than in a physical experiment because the Containers system is also using the facility. The event system cannot be used to replay the start command. Notes While start commands that make use of shell syntax for multiple commands and simple file redirection (e.g, > or <) may work, errors parsing redirection or other shell commands will cause the start command to fail silently . If you are doing anything more complex than calling a single program, we recommend that you create a simple script and run the script from the per-experiment directory or your home directory. This makes it more likely that the log files created by containers will have useful debugging information. We strongly recommend removing all shell redirection characters from the text of your start command. Redirecting I/O in the text of the start command may fail silently . Start commands offer a simple workaround for detecting that all nodes in an experiment have started: #!/bin/sh STARTDIR=\"/proj/\"`cat /var/containers/pid`\"/exp/\"`cat /var/containers/eid`\"/startup\" mkdir $STARTDIR date > $STARTDIR/`hostname` If you make the script above the start command of all nodes, the Containers system will put the time that each local container came up in the startup directory under the per-experiment directories. For example, n-0.example1.DeterTest will create /proj/DeterTest/exp/example1/startup/n-0 . Then you may monitor that directory on users to know which nodes are up. Step 4: Releasing Resources \u00b6 As with a physical DETER experiment, release resources by swapping the experiment out using the web interface or the swapexp command (see the Core Guide for more information. If you are using the startcommand workaround to detect startup, clear the startup directory when you swap the experiment out. Advanced Topics \u00b6 The previous tutorial described how to create an experiment using only openvz containers packed 10 to a machine. This section describes how to change those parameters. Using Other Container Types \u00b6 To change the container type that containerize.py assigns to nodes, use the --default-container option. Valid choices follow the [ContainersQuickstart#KindsofContainers kinds of containers] DETERLab supports. Specifically: Parameter Container embedded_pnode Physical Node qemu Qemu VM openvz Openvz Container You can try this on our example topology : users:~$ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example1.tcl Requested a QEMU node with more than 7 experimental interfaces. Qemu nodes can only support 7 experimental interfaces. The Containers system is now using qemu containers to build our experiment. Unfortunately qemu containers only support 7 experimental interfaces, an internal limit on the number of interfaces the virtual hardware supports. Run the command again but use the attached <a href=\"/downloads/example2.tcl\" version of the topology with fewer satellites to containerize without error. $ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example2.tcl Containerized experiment DeterTest/example2 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example2 The qemu experiment looks much like the openvz experiment above, at this small scale. Qemu nodes more completely emulate hardware and the kernels are independent, unlike openvz containers. For example, a program can load kernel modules in a qemu VM, which it cannot do in an openvz container. The qemu containers load the Ubuntu 12.04 (32 bit) distribution by default. Mixing Containers \u00b6 Mixing containers requires you to assign container types in the topology description. This is done by attaching an attribute to nodes. The attribute is named containers:node_type and it takes the same values as the --default-container parameter to containerize.py . If the experiment definition is in topdl , the attribute can be attached using the standard topdl routines . Attaching the attribute in ns2 is done using the DETERLab tb-add-node-attribute command. tb-add-node-attribute $node containers:node_type openvz Using this command in an ns2 topology description will set node to be placed in an openvz container. Using this feature, we can modify our first example topology to consist of qemu nodes and a single OpenVZ container in the center. The new topology file looks like this: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] # The center node is an openvz VM tb-add-node-attribute $center containers:node_type openvz # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Satellites are qemu nodes tb-add-node-attribute $n($i) containers:node_type qemu # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtproto Static $ns run Because we have explicitly set the container_node_type of each node, the --default-container parameter to containerize.py does nothing. Create this experiment by running: users:~$ /share/containers/containerize.py DeterTest example3 ~/example3.tcl Containerized experiment DeterTest/example3 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example3 When we swap it in, the experiment will have 10 satellite containers in qemu VMs and a central OpenVZ VM that forwards packets. Another interesting mixture of containers is to include a physical node. Here is a modified version of our mixed topology that places the n-8 satellite on a physical computer by setting its containers:node_type to embedded_pnode . After running that experiment you should have output similar to the following: users:~$ /share/containers/containerize.py DeterTest example4 ~/example4.tcl Containerized experiment DeterTest/example4 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example4 Follow the url to the DETERLab experiment page and look at the Visualization tab: The physical node n-8 shows up in the DETERLab visualization and otherwise acts as a physical node that is in a 10-node topology. This experiment uses three different container types: physical nodes, ViewOS processes, and Qemu VMs. (Note that ViewOS containers are not currently supported on DETER.) Limitations on Mixing Containers \u00b6 Physical node containers are mapped one-to-one to physical nodes by definition. Qemu and OpenVZ use different underlying operating system images in DETERLab, therefore they cannot share physical hardware. Qemu VMs cannot share a physical machine with OpenVZ VMs. If these container types are mixed in an experiment, they will always be assigned different physical nodes. The first invocation of tb-add-node-attribute takes precedence. It is best to only call tb-add-node-attribute once per attribute assigned on each node. Setting Openvz Parameters \u00b6 An advantage of openvz nodes is that you can set the OS flavor and CPU bit-width across experiments and per-node. Similarly, you can set the size of the disk allocated to each node. Openvz uses templates to look like various Linux installations. The choices of Linux distribution that openvz supports are: Template Distribution Bit-width centos-6-x86 CentOS 6 32 bit centos-6-x86_64 CentOS 6 64 bit ubuntu-12.04-x86 Ubuntu 12.04 LTS 32 bit ubuntu-12.04-x86_64 Ubuntu 12.04 LTS 64 bit ubuntu-14.04-x86 Ubuntu 14.04 LTS 32 bit ubuntu-14.04-x86_64 Ubuntu 14.04 LTS 64 bit The default template is ubuntu-14.04-x86_64 . To set a template across an entire topology, give --openvz-template and the template name from the list above. Invoking containerize.py on our original example as below will instantiate the experiment under 64-bit Ubuntu 12.04: users:~$ /share/containers/containerize.py --openvz-template ubuntu-12.04-x86_64 DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 To set the size of the file system of containers in the experiment, use --openvz-diskspace . The value of the parameter is determined by the suffix: Suffix Value G Gigabytes M Megabytes The default openvz file system size is 2GB. The most practical suffix for DETERLab nodes is \"G\": users:~$ /share/containers/containerize.py --openvz-diskspace 15G DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 Each of these parameters can be set on individual nodes using attributes. Use containers:openvz_template to set a template on a node and use containers:openvz_diskspace to set the disk space. This example topology sets these openvz parameters per node: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] tb-add-node-attribute $center containers:openvz_template ubuntu-12.04-x86_64 # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Set satellite disk sizes to be 20 GB tb-add-node-attribute $n($i) containers:openvz_diskspace 20G # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtptoto Static $ns run The center node will run Ubuntu 12.04 64 bit and the satellites will have 20GB file systems. Setting Qemu Parameters \u00b6 The Containers system has a more limited ability to set qemu parameters. Right now, a custom image may be loaded using the containers::qemu_url attribute and the architecture of the qemu VM may be chosen using containers:qemu_arch . Valid qemu architectures are: Param Meaning i386 32 bit Intel x86_64 64-bit Intel The image URL must be reachable from inside DETERLab. The image must be a qcow2 image, optionally bzip2ed. Facilities to snapshot and store such images are in development. If you are using a qemu image that is not booting into containers, make sure grub is properly configured . Qemu images also mount users' home directories the same as DETERLab physical nodes do. In order to do this scalably, the Qemu VMs mount the users' directories from the physical node. The DETER infrastructure cannot support exporting users' directories to thousands of containers. However, a Qemu VM can only mount a few tens of user directories this way. The limit is 23 user directories (24 in experiments that are not instantiated in a group). Many projects have more than 23 users, but in practice only a few experimenters need access to the containers. To tell the Containers systems which user to mount, use the --qemu-prefer-users option to containerize.py . That option takes a comma-separated list of usernames (no spaces). When the Qemu nodes will always mount those users' home directories. Others will be mounted if there is room. For example: users:~$ /share/containers/containerize.py --qemu-prefer-users=faber,jjh DeterTest example4 ~/example4.tcl This command makes sure that users faber and jjh have their home directories mounted in any Qemu containers. Changing The Packing Factor \u00b6 The containerize.py program decides how many virtual nodes to put on each physical machine. Because we have been using roughly the same number of nodes as the default packing target (10 nodes per machine) all of the examples so far have fit onto a single machine. If we change the packing factor by using the --packing parameter to containerize.py , we can put fewer nodes on each machine. For example: users:~$ /share/containers/containerize.py --packing 2 DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 This command calls containerize.py on our original topology with a low packing factor. The result is the same nodes spread across more physical machines, as we can see from the DETERLab web interface (on the Visualization tab): You will want to balance how many physical machines you use against how precisely you want to mimic them. User Packing \u00b6 You may specify your own packing using the containers:partition attribute. This attribute must be assigned an integer value. All nodes with the same partition are allocated to the same machine. If nodes have that attribute attached to them, containers.py will assume that they all have been partitioned and use those. Nodes without a partition assigned are assumed to be embedded_pnode s. More Sophisticated Packing: Multiple Passes \u00b6 The previous examples have all treated packing containers onto physical machines as a single-step process with a single parameter - the packing factor. In fact, we can divide containers into sets and pack each set independently using different parameters. For example in an experiment with many containers dedicated only to forwarding packets and a few to modeling servers, we could create two sets and pack the forwarders tightly (using a high packing factor) and the servers loosely. In exchange for providing greater control on packing, there is a price. When a set of containers is packed, the Containers system takes into account both the nodes to be packed and their interconnections. When subsets of containers are packed, the system cannot consider the interconnections between subsets. In some cases, the packing of subsets can lead to a DETERLab experiment that cannot be created successfully. This danger is mitigated by the fact that containers that are packed together are often related in ways that limit the number of connections between that set and another. To explore packing, we need to use a larger topology : source tb_compat.tcl set ns [new Simulator] set center [$ns node] tb-add-node-attribute $center \"containers:PartitionPass\" 0 for { set i 0} { $i < 3 } { incr i} { set lanlist $center for { set j 0 } { $j < 20} { incr j } { set idx [expr $i * 20 + $j] set n($idx) [$ns node] tb-add-node-attribute $n($idx) \"containers:PartitionPass\" [expr $i + 1] lappend lanlist $n($idx) } set lan($i) [$ns make-lan [join $lanlist \" \"] 100Mb 0] } # Creation boilerplate $ns rtptoto Static $ns run This creates three 20-node sub networks attached to a single central router. It looks like this: Each node in the topology is assigned a containers::PackingPass attribute that groups them into subsets. The containers:PackingPass attribute must be assigned an integer value. The nodes in each packing pass are considered \"together\" when packing. Each pass can be assigned different parameters. The passes are carried out in order, though that is rarely important. Our example topology assigns center to pass 0, the nodes on lan-0 (the tcl variable lan(0)) to pass 1, those on lan-1 to pass 2 and those on lan-2 to pass 3. We will use the --pass-pack parameter to specify the packing factor for each pass. Each packing factor specification looks like pass : factor where pass and factor are both integers. We can specify more than one, separated by commas, or specify --pass-pack more than once. For example, we can pack the experiment using the following factors: Pass Packing Factor 0 1 1 20 2 10 3 5 By issuing the following command: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 We can view the packing by using container_image.py to generate a visualization that includes the partitions: users:~$ /share/containers/container_image.py --experiment DeterTest/example6 --partitions --out ~/example6.png The output shows the topology with boxes drawn around the containers that share a physical node: That partitioning is surprising in that lan-1 is split into three partitions of 6 & 7 nodes rather than two partitions of 10. Similarly lan-2 is split into five groups of 4 rather than four groups of 5. The packing system is built on the metis graph partitioning software. Metis takes a graph and a number of partitions and finds the most balanced partitioning that has roughly equal node counts in each partition as well as low inter-partition communication costs. The Containers system calls metis with increasing numbers of partitions until a partitioning is found that meets the packing factor limits. When the system attempts to pack lan-1 into two partitions, metis balances the node counts and the communications costs to produce a partition with 9 containers in one machine and 11 on the other. That partitioning does not meet the 10 node limit, so it tries again with three partitions and succeeds. There are two ways to fit our topology onto fewer nodes. The first is to put slightly more slop into the packing factors: Pass Packing Factor 0 1 1 20 2 11 3 6 As in the following command: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:11,3:6 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 These parameters result in this packing, which fits in fewer nodes, but has the slight imbalances of splitting lan-1 into 9 and 11 containers and lan-2 into 4,5,and 6 container partitions. Again, this asymmetry is an attempt to consider the internode networking costs. If the packing constraints are exact - 11 containers on lan-1 is unacceptable - a second choice is to use the --nodes-only option. This sets the cost of each arc in the graph to 0. Metis ignores such arcs altogether, so the partitions are completely even. This may cause trouble in more complex network topologies. The result of running the command with the original packing factors and --nodes-only ): users:~$ /share/containers/containerize.py --nodes-only --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 is which has symmetric partitions. Sometimes it is more intuitive to think in terms of the number of machines that will be used to hold containers. The --size and -pass-size options let users express that. The --size= expsize option uses expsize machines to hold the whole experiment. If multiple passes are made, each is put into expsize physical machines. The --size option takes precedence over --packing . Per-pass sizing can be done using --pass-size which uses the same syntax as --pass-pack . Therefore the command: users:~$ /share/containers/containerize.py --pass-size 0:1,1:1,2:2,3:4 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 packs pass 0 into one physical machine, pass 1 into one physical machine, pass 2 into two physical machines and pass 3 into four physcial machines. The result looks like: Partitions have different numbers of containers in them because metis is considering network constraints as well. As with using packing, adding --nodes-only restores symmetry: The --pass-pack option is a per-pass generalization of the --packing option. The options that can be specified per-pass are: Single-pass Per-Pass Per-Pass Format Per-Pass Example --packing --pass-pack pass : packing (comma-separated) --pass-pack 0:1,1:20,2:11,3:6 --size --pass-size pass : size (comma-separated) --pass-size 0:1,1:1,2:2,3:4 --pnode-types --pass-pnodes pass : pnode [, pnode ...] (semicolon separated) --pass-pnodes 0:MicroCloud;1:bpc2133,pc2133 --nodes-only --pass-nodes-only pass (comma-separated) --pass-nodes-only 1,3,5 The single-pass version sets a default so that this invocation on our 4 pass topology : users:~$ /share/containers/containerize.py --packing 5 --pass-pack 0:1,1:20 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 will pack pass 0 with a factor of 1, pass 1 with a factor of 20 and passes 2 and 3 with a factor of 5. Similarly: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes '0:pc2133,bpc2133;1:MicroCloud' DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 will allocate either bpc2133 or pc2133 nodes to containers assigned by pass 0 and Microcloud physical nodes to the containers partitioned in pass 1. The rest will be allocated as the site configuration specifies. The single quotes around the --pass-pnodes option protects the semi-colon from the shell. Another choice is to specify the command as: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes 0:pc2133,bpc2133 --pass-pnodes 1:MicroCloud DeterTest example6 ~/example6.tcl That formulation avoids the quotes by avoiding the semicolon. All the per-pass options may be specified multiple times on the command line. You can mix and match sizes and packing factors. This invocation: /share/containers/containerize.py --pass-size 1:10 --pass-pack 2:5,3:10 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 Produces: Remember that --size sets a default pass size and that sizes have precedence over packing. If you specify --size , no --packing or --pass-packing value will take effect. To mix packing and sizes, use --pack-size for each sized pass, rather than --size . These per-pass variables and user-specified pass specifications give users fine grained control over the paritioning process, even if they do not want to do the partitioning themselves. If no containers:PartitionPass attributes are specified in the topology, and no containers:Partition attributes are specified either, ```containerize.py}} carries out -- at most -- two passes. Pass 0 paritions all openvz containers and pass 1 partitions all qemu containers. Further Reading \u00b6 Hopefully these illustrative examples have given you an idea of how to use the containers system and what it is capable of. More details are available from the reference guide . Please see Getting Help if you have difficulties.","title":"Containers Guide"},{"location":"containers/containers-guide/#containers-guide","text":"In this tutorial we walk you through setting up a basic containerized experiment. This page also includes common advanced topics. Detailed descriptions of the commands and configuration files are available in the reference section . Note If you are a student, go to the http://education.deterlab.net site for classroom-specific instructions.","title":"Containers Guide"},{"location":"containers/containers-guide/#basic-containers-tutorial","text":"This tutorial will set up a containerized experiment with a star topology. We'll create a central node and connect 9 other nodes to it","title":"Basic Containers Tutorial"},{"location":"containers/containers-guide/#getting-started","text":"You will need a DETERLab account and be a member of a project. If you need help, see the Core Guide .","title":"Getting started"},{"location":"containers/containers-guide/#step-1-design-the-topology","text":"First, we will describe a star topology. For this example we will use the standard DETER topology descriptions. If you are new to designing topologies, walk through the basic tutorial in the Core Guide . The Containers system is largely compatible with the physical DETER interface. Download the DETERLab-compatible ns2 description of this topology at this link to your home directory on users.deterlab.net . It is a simple loop, along with the standard DETER boilerplate. This file will be used to create a 10-node (9 satellites and one central node) physical experiment on DETER, although there are not many physical nodes on DETER with 10 interfaces (one interface for control traffic). The following is the topology description: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtproto Static $ns run Note The central node is named \"center\" and each satellite is names \"n-0\", \"n-1\"... through \"n-8\". Each connection is a 100 Mb/s link with a 10ms delay. The round trip time from n-0 to center will be 20 ms and from n-0 to n-1 will be 40 ms.","title":"Step 1: Design the topology"},{"location":"containers/containers-guide/#step-2-create-the-containerized-experiment","text":"Now we will run a command so the Containers system will build the containerized experiment on top of a new DETERLab physical experiment. Run the following command from the shell on users.deterlab.net and refer to the example topology you just saved in your home. $ /share/containers/containerize.py DeterTest example1 ~/example1.tcl where the first two parameters are the project and experiment name to hold the DETER experiment. This command creates an experiment called experiment1 in the DeterTest project. Throughout this tutorial, we will refer to your project as DeterTest , but make sure you actually use your actual project's name. You may use the experiment name experiment1 as long as another experiment with that name doesn't already exist Note As with any DETERLab experiment, you must be a member of the project with appropriate rights to create an experiment in it. containerize.py expects there to be no experiment with that name, and it will fail if one exists. To remove an experiment you may terminate it through the web interface or use the endexp command. Terminating an experiment is more final than swapping one out, so be sure that you want to replace the old experiment. You may also resolve the conflict by renaming your new containerized experiment. The last parameter is the file containing the topology. In this tutorial, we are referring to the ns2 file in our example but you may also use a topdl description. An ns2 description must end in .tcl or .ns . With these default parameters containerize.py will put each node into an Openvz container with at most 10 containers per physical node. The output of the above command should be something like the following: users:~$ /share/containers/containerize.py DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 Now we can see what a containerized experiment looks like.","title":"Step 2: Create the containerized experiment"},{"location":"containers/containers-guide/#the-contents-of-a-containerized-experiment","text":"Follow the link provided in the containerize.py output. You will see a standard DETER experiment page that looks like this: You may be surprised to see that DETER thinks the experiment has only one node: The Containers system has rewritten the description file and stored additional information in the experiment's per-experiment directory that will be used to create the 10 node experiment inside the single-node DETER experiment. If you look at the ns file DETERLab has stored (click the NS File tab on the experiment page), you should see the following code: set ns [new Simulator] source tb_compat.tcl tb-make-soft-vtype container0 {pc2133 bpc2133 MicroCloud} set pnode(0000) [$ns node] tb-set-node-os ${pnode(0000)} CentOS6-64-openvz tb-set-hardware ${pnode(0000)} container0 tb-set-node-startcmd ${pnode(0000)} \"sudo /share/containers/setup/hv/bootstrap /proj/DeterTest/exp/example1/containers/site.conf >& /tmp/container.log\" tb-set-node-failure-action ${pnode(0000)} \"nonfatal\" $ns rtproto Static $ns run This looks nothing like the file we gave to containerize.py , but it does show us a little about what the Containers system has done: The single physical node ( pnode(0000) ) will run the CentOS6-64-openvz image and run on a few kinds of node. On startup, pnode(0000) will execute a command from the same /share/containers directory that containerize.py ran from using data in the per-experiment directory /proj/DeterTest/exp/example1/containers/site.conf . There is a separate /proj/DeterTest/exp/example1/containers/ directory for each experiment. The path element after /proj is replaced with the project under which the experiment was created -- DeterTest in this example -- and the element after exp is the experiment name -- example1 in this case. These directories are created for all DETERLab experiments.","title":"The Contents of a Containerized Experiment"},{"location":"containers/containers-guide/#containers-sub-directory","text":"The containers sub-directory of a containerized experiment holds information specific to a containerized experiment. There are a few useful bits of data in that per-experiment containers directory that we can look at. Copy of the topology file: First, a copy of the topology that we gave to containerize.py is available in /proj/DeterTest/exp/example1/containers/experiment.tcl . If the experiment is created from a topdl file, the filename will be containers/experiment.tcl . Visualization of experiment: A simple visualization of the experiment is in containers/visualization.png . This is annotated with node and network names as well as interface IP addresses. The topology depiction above is an example. To view a larger version, click here . IP-to-hostname mapping: The containers/hosts file is a copy of the IP-to-hostname mapping found on each virtual machine in the topology. It can be useful in converting IP addresses back to names. It is installed in /etc/hosts or the equivalent on each machine. PID/EID: The two files /var/containers/pid and /var/containers/eid contain the project name and experiment name. Scripts can make use of these. The rest of the contents of that directory are primarily used internally by the implementation, but a more detailed listing is in the Containers Reference .","title":"containers sub-directory"},{"location":"containers/containers-guide/#step-3-swap-in-resources","text":"At this point, as with any DETER experiment, the topology does not yet have any resources attached. To get the resources, swap the experiment in from the web interface or using the swapexp command. See the DETERLab Core Guide for more information.","title":"Step 3: Swap-in resources"},{"location":"containers/containers-guide/#step-4-verify-virtual-topology-and-access-nodes","text":"Once you have been notified that the physical experiment has finished its swap-in, the Containers system starts converting the physical topology into the virtual topology. At this time, you must manually verify when the virtual topology has been created by ping-ing or trying to SSH into individual nodes of an experiment. There is also a workaround suggested below . We are working towards offering a better notification system. Once the containerized elements have all started, the nodes are available as if they were physical nodes. For example, we may access node n-0 of the experiment we swapped in by running: $ ssh n-0.example1.detertest Be sure that you replace example1 with the experiment name you passed to containerize.py and DeterTest with the project you created the experiment under. This is a DNS name, so it is case-insensitive. When the SSH succeeds, you will have access to an Ubuntu 10.04 32-bit node with the same directories mounted as in a physical DETERLab experiment. Containerized nodes access the control net as well. Your home directory will be mounted, so your SSH keys will work for accessing the machine. Use the same node naming conventions as physical DETERLab experiments to ping and access other nodes. Here is a ping from n-0 to center and n-1 that confirms the containerized experiment is working as we expect. n-0:~$ ping -c 3 center PING center-tblink-l21 (10.0.0.2) 56(84) bytes of data. 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=1 ttl=64 time=20.4 ms 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=2 ttl=64 time=20.0 ms 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=3 ttl=64 time=20.0 ms --- center-tblink-l21 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2002ms rtt min/avg/max/mdev = 20.052/20.184/20.445/0.184 ms n-0:~$ ping -c 3 n-1 PING n-1-tblink-l5 (10.0.6.1) 56(84) bytes of data. 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=1 ttl=64 time=40.7 ms 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=2 ttl=64 time=40.0 ms 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=3 ttl=64 time=40.0 ms --- n-1-tblink-l5 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2003ms rtt min/avg/max/mdev = 40.094/40.318/40.764/0.355 ms The nodes have the expected round trip times. At this point you can load and run software and generally experiment normally.","title":"Step 4: Verify virtual topology and access nodes"},{"location":"containers/containers-guide/#start-commands","text":"DETERLab Core provides a facility to run a command when a physical experiment starts, called start commands . A containerized experiment offers a similar facility with a few differences: The start commands are not coordinated across nodes. In a physical experiment, the start commands all execute when the last node has reported to the testbed that it has completed booting. In a containerized experiment, the start commands run when the containerized node has come up. Logs from the start command are available in /var/containers/log/start_command.out and /var/containers/log/start_command.err . This is true on embedded pnodes as well. Start commands must be shorter than in a physical experiment because the Containers system is also using the facility. The event system cannot be used to replay the start command. Notes While start commands that make use of shell syntax for multiple commands and simple file redirection (e.g, > or <) may work, errors parsing redirection or other shell commands will cause the start command to fail silently . If you are doing anything more complex than calling a single program, we recommend that you create a simple script and run the script from the per-experiment directory or your home directory. This makes it more likely that the log files created by containers will have useful debugging information. We strongly recommend removing all shell redirection characters from the text of your start command. Redirecting I/O in the text of the start command may fail silently . Start commands offer a simple workaround for detecting that all nodes in an experiment have started: #!/bin/sh STARTDIR=\"/proj/\"`cat /var/containers/pid`\"/exp/\"`cat /var/containers/eid`\"/startup\" mkdir $STARTDIR date > $STARTDIR/`hostname` If you make the script above the start command of all nodes, the Containers system will put the time that each local container came up in the startup directory under the per-experiment directories. For example, n-0.example1.DeterTest will create /proj/DeterTest/exp/example1/startup/n-0 . Then you may monitor that directory on users to know which nodes are up.","title":"Start Commands"},{"location":"containers/containers-guide/#step-4-releasing-resources","text":"As with a physical DETER experiment, release resources by swapping the experiment out using the web interface or the swapexp command (see the Core Guide for more information. If you are using the startcommand workaround to detect startup, clear the startup directory when you swap the experiment out.","title":"Step 4: Releasing Resources"},{"location":"containers/containers-guide/#advanced-topics","text":"The previous tutorial described how to create an experiment using only openvz containers packed 10 to a machine. This section describes how to change those parameters.","title":"Advanced Topics"},{"location":"containers/containers-guide/#using-other-container-types","text":"To change the container type that containerize.py assigns to nodes, use the --default-container option. Valid choices follow the [ContainersQuickstart#KindsofContainers kinds of containers] DETERLab supports. Specifically: Parameter Container embedded_pnode Physical Node qemu Qemu VM openvz Openvz Container You can try this on our example topology : users:~$ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example1.tcl Requested a QEMU node with more than 7 experimental interfaces. Qemu nodes can only support 7 experimental interfaces. The Containers system is now using qemu containers to build our experiment. Unfortunately qemu containers only support 7 experimental interfaces, an internal limit on the number of interfaces the virtual hardware supports. Run the command again but use the attached <a href=\"/downloads/example2.tcl\" version of the topology with fewer satellites to containerize without error. $ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example2.tcl Containerized experiment DeterTest/example2 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example2 The qemu experiment looks much like the openvz experiment above, at this small scale. Qemu nodes more completely emulate hardware and the kernels are independent, unlike openvz containers. For example, a program can load kernel modules in a qemu VM, which it cannot do in an openvz container. The qemu containers load the Ubuntu 12.04 (32 bit) distribution by default.","title":"Using Other Container Types"},{"location":"containers/containers-guide/#mixing-containers","text":"Mixing containers requires you to assign container types in the topology description. This is done by attaching an attribute to nodes. The attribute is named containers:node_type and it takes the same values as the --default-container parameter to containerize.py . If the experiment definition is in topdl , the attribute can be attached using the standard topdl routines . Attaching the attribute in ns2 is done using the DETERLab tb-add-node-attribute command. tb-add-node-attribute $node containers:node_type openvz Using this command in an ns2 topology description will set node to be placed in an openvz container. Using this feature, we can modify our first example topology to consist of qemu nodes and a single OpenVZ container in the center. The new topology file looks like this: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] # The center node is an openvz VM tb-add-node-attribute $center containers:node_type openvz # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Satellites are qemu nodes tb-add-node-attribute $n($i) containers:node_type qemu # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtproto Static $ns run Because we have explicitly set the container_node_type of each node, the --default-container parameter to containerize.py does nothing. Create this experiment by running: users:~$ /share/containers/containerize.py DeterTest example3 ~/example3.tcl Containerized experiment DeterTest/example3 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example3 When we swap it in, the experiment will have 10 satellite containers in qemu VMs and a central OpenVZ VM that forwards packets. Another interesting mixture of containers is to include a physical node. Here is a modified version of our mixed topology that places the n-8 satellite on a physical computer by setting its containers:node_type to embedded_pnode . After running that experiment you should have output similar to the following: users:~$ /share/containers/containerize.py DeterTest example4 ~/example4.tcl Containerized experiment DeterTest/example4 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example4 Follow the url to the DETERLab experiment page and look at the Visualization tab: The physical node n-8 shows up in the DETERLab visualization and otherwise acts as a physical node that is in a 10-node topology. This experiment uses three different container types: physical nodes, ViewOS processes, and Qemu VMs. (Note that ViewOS containers are not currently supported on DETER.)","title":"Mixing Containers"},{"location":"containers/containers-guide/#limitations-on-mixing-containers","text":"Physical node containers are mapped one-to-one to physical nodes by definition. Qemu and OpenVZ use different underlying operating system images in DETERLab, therefore they cannot share physical hardware. Qemu VMs cannot share a physical machine with OpenVZ VMs. If these container types are mixed in an experiment, they will always be assigned different physical nodes. The first invocation of tb-add-node-attribute takes precedence. It is best to only call tb-add-node-attribute once per attribute assigned on each node.","title":"Limitations on Mixing Containers"},{"location":"containers/containers-guide/#setting-openvz-parameters","text":"An advantage of openvz nodes is that you can set the OS flavor and CPU bit-width across experiments and per-node. Similarly, you can set the size of the disk allocated to each node. Openvz uses templates to look like various Linux installations. The choices of Linux distribution that openvz supports are: Template Distribution Bit-width centos-6-x86 CentOS 6 32 bit centos-6-x86_64 CentOS 6 64 bit ubuntu-12.04-x86 Ubuntu 12.04 LTS 32 bit ubuntu-12.04-x86_64 Ubuntu 12.04 LTS 64 bit ubuntu-14.04-x86 Ubuntu 14.04 LTS 32 bit ubuntu-14.04-x86_64 Ubuntu 14.04 LTS 64 bit The default template is ubuntu-14.04-x86_64 . To set a template across an entire topology, give --openvz-template and the template name from the list above. Invoking containerize.py on our original example as below will instantiate the experiment under 64-bit Ubuntu 12.04: users:~$ /share/containers/containerize.py --openvz-template ubuntu-12.04-x86_64 DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 To set the size of the file system of containers in the experiment, use --openvz-diskspace . The value of the parameter is determined by the suffix: Suffix Value G Gigabytes M Megabytes The default openvz file system size is 2GB. The most practical suffix for DETERLab nodes is \"G\": users:~$ /share/containers/containerize.py --openvz-diskspace 15G DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 Each of these parameters can be set on individual nodes using attributes. Use containers:openvz_template to set a template on a node and use containers:openvz_diskspace to set the disk space. This example topology sets these openvz parameters per node: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] tb-add-node-attribute $center containers:openvz_template ubuntu-12.04-x86_64 # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Set satellite disk sizes to be 20 GB tb-add-node-attribute $n($i) containers:openvz_diskspace 20G # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtptoto Static $ns run The center node will run Ubuntu 12.04 64 bit and the satellites will have 20GB file systems.","title":"Setting Openvz Parameters"},{"location":"containers/containers-guide/#setting-qemu-parameters","text":"The Containers system has a more limited ability to set qemu parameters. Right now, a custom image may be loaded using the containers::qemu_url attribute and the architecture of the qemu VM may be chosen using containers:qemu_arch . Valid qemu architectures are: Param Meaning i386 32 bit Intel x86_64 64-bit Intel The image URL must be reachable from inside DETERLab. The image must be a qcow2 image, optionally bzip2ed. Facilities to snapshot and store such images are in development. If you are using a qemu image that is not booting into containers, make sure grub is properly configured . Qemu images also mount users' home directories the same as DETERLab physical nodes do. In order to do this scalably, the Qemu VMs mount the users' directories from the physical node. The DETER infrastructure cannot support exporting users' directories to thousands of containers. However, a Qemu VM can only mount a few tens of user directories this way. The limit is 23 user directories (24 in experiments that are not instantiated in a group). Many projects have more than 23 users, but in practice only a few experimenters need access to the containers. To tell the Containers systems which user to mount, use the --qemu-prefer-users option to containerize.py . That option takes a comma-separated list of usernames (no spaces). When the Qemu nodes will always mount those users' home directories. Others will be mounted if there is room. For example: users:~$ /share/containers/containerize.py --qemu-prefer-users=faber,jjh DeterTest example4 ~/example4.tcl This command makes sure that users faber and jjh have their home directories mounted in any Qemu containers.","title":"Setting Qemu Parameters"},{"location":"containers/containers-guide/#changing-the-packing-factor","text":"The containerize.py program decides how many virtual nodes to put on each physical machine. Because we have been using roughly the same number of nodes as the default packing target (10 nodes per machine) all of the examples so far have fit onto a single machine. If we change the packing factor by using the --packing parameter to containerize.py , we can put fewer nodes on each machine. For example: users:~$ /share/containers/containerize.py --packing 2 DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 This command calls containerize.py on our original topology with a low packing factor. The result is the same nodes spread across more physical machines, as we can see from the DETERLab web interface (on the Visualization tab): You will want to balance how many physical machines you use against how precisely you want to mimic them.","title":"Changing The Packing Factor"},{"location":"containers/containers-guide/#user-packing","text":"You may specify your own packing using the containers:partition attribute. This attribute must be assigned an integer value. All nodes with the same partition are allocated to the same machine. If nodes have that attribute attached to them, containers.py will assume that they all have been partitioned and use those. Nodes without a partition assigned are assumed to be embedded_pnode s.","title":"User Packing"},{"location":"containers/containers-guide/#more-sophisticated-packing-multiple-passes","text":"The previous examples have all treated packing containers onto physical machines as a single-step process with a single parameter - the packing factor. In fact, we can divide containers into sets and pack each set independently using different parameters. For example in an experiment with many containers dedicated only to forwarding packets and a few to modeling servers, we could create two sets and pack the forwarders tightly (using a high packing factor) and the servers loosely. In exchange for providing greater control on packing, there is a price. When a set of containers is packed, the Containers system takes into account both the nodes to be packed and their interconnections. When subsets of containers are packed, the system cannot consider the interconnections between subsets. In some cases, the packing of subsets can lead to a DETERLab experiment that cannot be created successfully. This danger is mitigated by the fact that containers that are packed together are often related in ways that limit the number of connections between that set and another. To explore packing, we need to use a larger topology : source tb_compat.tcl set ns [new Simulator] set center [$ns node] tb-add-node-attribute $center \"containers:PartitionPass\" 0 for { set i 0} { $i < 3 } { incr i} { set lanlist $center for { set j 0 } { $j < 20} { incr j } { set idx [expr $i * 20 + $j] set n($idx) [$ns node] tb-add-node-attribute $n($idx) \"containers:PartitionPass\" [expr $i + 1] lappend lanlist $n($idx) } set lan($i) [$ns make-lan [join $lanlist \" \"] 100Mb 0] } # Creation boilerplate $ns rtptoto Static $ns run This creates three 20-node sub networks attached to a single central router. It looks like this: Each node in the topology is assigned a containers::PackingPass attribute that groups them into subsets. The containers:PackingPass attribute must be assigned an integer value. The nodes in each packing pass are considered \"together\" when packing. Each pass can be assigned different parameters. The passes are carried out in order, though that is rarely important. Our example topology assigns center to pass 0, the nodes on lan-0 (the tcl variable lan(0)) to pass 1, those on lan-1 to pass 2 and those on lan-2 to pass 3. We will use the --pass-pack parameter to specify the packing factor for each pass. Each packing factor specification looks like pass : factor where pass and factor are both integers. We can specify more than one, separated by commas, or specify --pass-pack more than once. For example, we can pack the experiment using the following factors: Pass Packing Factor 0 1 1 20 2 10 3 5 By issuing the following command: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 We can view the packing by using container_image.py to generate a visualization that includes the partitions: users:~$ /share/containers/container_image.py --experiment DeterTest/example6 --partitions --out ~/example6.png The output shows the topology with boxes drawn around the containers that share a physical node: That partitioning is surprising in that lan-1 is split into three partitions of 6 & 7 nodes rather than two partitions of 10. Similarly lan-2 is split into five groups of 4 rather than four groups of 5. The packing system is built on the metis graph partitioning software. Metis takes a graph and a number of partitions and finds the most balanced partitioning that has roughly equal node counts in each partition as well as low inter-partition communication costs. The Containers system calls metis with increasing numbers of partitions until a partitioning is found that meets the packing factor limits. When the system attempts to pack lan-1 into two partitions, metis balances the node counts and the communications costs to produce a partition with 9 containers in one machine and 11 on the other. That partitioning does not meet the 10 node limit, so it tries again with three partitions and succeeds. There are two ways to fit our topology onto fewer nodes. The first is to put slightly more slop into the packing factors: Pass Packing Factor 0 1 1 20 2 11 3 6 As in the following command: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:11,3:6 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 These parameters result in this packing, which fits in fewer nodes, but has the slight imbalances of splitting lan-1 into 9 and 11 containers and lan-2 into 4,5,and 6 container partitions. Again, this asymmetry is an attempt to consider the internode networking costs. If the packing constraints are exact - 11 containers on lan-1 is unacceptable - a second choice is to use the --nodes-only option. This sets the cost of each arc in the graph to 0. Metis ignores such arcs altogether, so the partitions are completely even. This may cause trouble in more complex network topologies. The result of running the command with the original packing factors and --nodes-only ): users:~$ /share/containers/containerize.py --nodes-only --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 is which has symmetric partitions. Sometimes it is more intuitive to think in terms of the number of machines that will be used to hold containers. The --size and -pass-size options let users express that. The --size= expsize option uses expsize machines to hold the whole experiment. If multiple passes are made, each is put into expsize physical machines. The --size option takes precedence over --packing . Per-pass sizing can be done using --pass-size which uses the same syntax as --pass-pack . Therefore the command: users:~$ /share/containers/containerize.py --pass-size 0:1,1:1,2:2,3:4 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 packs pass 0 into one physical machine, pass 1 into one physical machine, pass 2 into two physical machines and pass 3 into four physcial machines. The result looks like: Partitions have different numbers of containers in them because metis is considering network constraints as well. As with using packing, adding --nodes-only restores symmetry: The --pass-pack option is a per-pass generalization of the --packing option. The options that can be specified per-pass are: Single-pass Per-Pass Per-Pass Format Per-Pass Example --packing --pass-pack pass : packing (comma-separated) --pass-pack 0:1,1:20,2:11,3:6 --size --pass-size pass : size (comma-separated) --pass-size 0:1,1:1,2:2,3:4 --pnode-types --pass-pnodes pass : pnode [, pnode ...] (semicolon separated) --pass-pnodes 0:MicroCloud;1:bpc2133,pc2133 --nodes-only --pass-nodes-only pass (comma-separated) --pass-nodes-only 1,3,5 The single-pass version sets a default so that this invocation on our 4 pass topology : users:~$ /share/containers/containerize.py --packing 5 --pass-pack 0:1,1:20 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 will pack pass 0 with a factor of 1, pass 1 with a factor of 20 and passes 2 and 3 with a factor of 5. Similarly: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes '0:pc2133,bpc2133;1:MicroCloud' DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 will allocate either bpc2133 or pc2133 nodes to containers assigned by pass 0 and Microcloud physical nodes to the containers partitioned in pass 1. The rest will be allocated as the site configuration specifies. The single quotes around the --pass-pnodes option protects the semi-colon from the shell. Another choice is to specify the command as: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes 0:pc2133,bpc2133 --pass-pnodes 1:MicroCloud DeterTest example6 ~/example6.tcl That formulation avoids the quotes by avoiding the semicolon. All the per-pass options may be specified multiple times on the command line. You can mix and match sizes and packing factors. This invocation: /share/containers/containerize.py --pass-size 1:10 --pass-pack 2:5,3:10 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 Produces: Remember that --size sets a default pass size and that sizes have precedence over packing. If you specify --size , no --packing or --pass-packing value will take effect. To mix packing and sizes, use --pack-size for each sized pass, rather than --size . These per-pass variables and user-specified pass specifications give users fine grained control over the paritioning process, even if they do not want to do the partitioning themselves. If no containers:PartitionPass attributes are specified in the topology, and no containers:Partition attributes are specified either, ```containerize.py}} carries out -- at most -- two passes. Pass 0 paritions all openvz containers and pass 1 partitions all qemu containers.","title":"More Sophisticated Packing: Multiple Passes"},{"location":"containers/containers-guide/#further-reading","text":"Hopefully these illustrative examples have given you an idea of how to use the containers system and what it is capable of. More details are available from the reference guide . Please see Getting Help if you have difficulties.","title":"Further Reading"},{"location":"containers/containers-quickstart/","text":"Containers Quickstart \u00b6 This page describes basic information about DETERLab Containers and provides an overview of how to use it. More details are available in the Containers Guide . What are Containers? \u00b6 The Containers system enables experimenters to create large-scale DETERLab topologies that support differing degrees of fidelity in individual elements. In order to create an experiment larger than the 400+ computers available in DETERLab Core, experimenters must use virtualization, simulation, or some other abstraction to represent their topology. The Containers system guides this process allowing experimenters to create large experimental environments that may be used to gather correct results. The Containers system is built on top of the resource allocation that underlies the DETERLab testbed , extending it to provide multiple implementations of virtual nodes. Most DETERLab tools that run on physical experiments may be used directly on containerized experiments. Experimenters find working in a containerized experiment very similar to working in physical DETERLab experiments. How does it work? \u00b6 An experimenter comes to DETERLab with an experimental topology of computers and networks and an experiment to carry out on that topology, and the Containers system allocates resources in the configuration specified. The experimenter may directly access the computers in order to carry out the experiment. The computers themselves are either physical computers or some virtual computers that emulate a computer at an acceptable level of fidelity. Multiple experiments may be in progress at once using DETER resources, and they are protected from interfering with one another. Containers present researchers with more resources while preserving the DETERLab interfaces. The process of converting a topology description to an isolated collection of networked computers is basically the same as when an experimenter creates a physical topology on DETERLab. The difference is that a containerized experiment is configured to present more experimental resources than physical ones, preserving the DETERLab interface. A little more completely, the Containers system lays out the virtual computers into a physical layout of computers and uses the resource allocation system to allocate that physical layout. Then the system installs and configures the appropriate virtualization technologies in that environment to create the virtual environment. As in physical DETERLab experiments, the experiment's topology is written in an extended version of DETER's ns2 syntax, or in topdl , a topology description language. Currently experimenters pick containers directly using those languages. Kinds of Containers \u00b6 A container is a virtualization technology, like a virtual machine implementation. We use the term ''container'' to mean any one of the various virtualization technologies from an openvz container to a physical machine to a simulation. The Containers system gives us a way to create interconnections of containers (in our sense) holding different experiment elements. A containerized topology might include a physical machine, a qemu virtual machine and an openvz container that can all communicate transparently. The Containers system framework supports multiple kinds of containers, but at this point researchers may request these: Container Type Fidelity Scalability Physical Machine Complete fidelity 1 per physical machine Qemu virtual Machine Virtual hardware 10's of containers per physical machine Openvz container Partitioned resources in one Linux kernel 100's of contatiners per physical machine How do I use Containers? \u00b6 In general, once you have a DETERLab account, you follow these steps. The DETERLab Containers Guide will walk you through a basic tutorial of these steps. 1. Design the topology \u00b6 Every experiment in DETERLab is based on a network topology file written in NS format and saved on the users node. In a containerized experiment, the topology will typically be a large one with more than 400 nodes. sample of topology 2. Run containerized experiment with the containerize.py command \u00b6 The Containers system will build the containerized experiment on top of an existing DETERLab physical experiment by running the containerize.py command from the shell on users.deterlab.net , as in the following example: $ /share/containers/containerize.py DeterTest example1 ~/example1.tcl where ''DeterTest'' and ''example1'' are the project and experiment name, respectively, of the physical DETERLab experiment and ''example.tcl'' is the topology file. With these default parameters, containerize.py will put each node into an Openvz container with at most 10 containers per physical node. 3. View results by accessing nodes, modify the experiment as needed. \u00b6 In a containerized experiment, you can access the virtual nodes with the same directories mounted as in a physical DETERLab experiment. You can load and run software and conduct experiments as you would in a physical experiment. 4. Save your work and swap out your experiment (release the resources) \u00b6 As with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users. More Information \u00b6 For more detailed information about Containers, read the following: Containers Guide - This guide walks you through a basic example of using Containers and includes some advanced topics. Containers Reference - This reference includes Containers commands, configuration details and information about different types of containers.","title":"Containers Quickstart"},{"location":"containers/containers-quickstart/#containers-quickstart","text":"This page describes basic information about DETERLab Containers and provides an overview of how to use it. More details are available in the Containers Guide .","title":"Containers Quickstart"},{"location":"containers/containers-quickstart/#what-are-containers","text":"The Containers system enables experimenters to create large-scale DETERLab topologies that support differing degrees of fidelity in individual elements. In order to create an experiment larger than the 400+ computers available in DETERLab Core, experimenters must use virtualization, simulation, or some other abstraction to represent their topology. The Containers system guides this process allowing experimenters to create large experimental environments that may be used to gather correct results. The Containers system is built on top of the resource allocation that underlies the DETERLab testbed , extending it to provide multiple implementations of virtual nodes. Most DETERLab tools that run on physical experiments may be used directly on containerized experiments. Experimenters find working in a containerized experiment very similar to working in physical DETERLab experiments.","title":"What are Containers?"},{"location":"containers/containers-quickstart/#how-does-it-work","text":"An experimenter comes to DETERLab with an experimental topology of computers and networks and an experiment to carry out on that topology, and the Containers system allocates resources in the configuration specified. The experimenter may directly access the computers in order to carry out the experiment. The computers themselves are either physical computers or some virtual computers that emulate a computer at an acceptable level of fidelity. Multiple experiments may be in progress at once using DETER resources, and they are protected from interfering with one another. Containers present researchers with more resources while preserving the DETERLab interfaces. The process of converting a topology description to an isolated collection of networked computers is basically the same as when an experimenter creates a physical topology on DETERLab. The difference is that a containerized experiment is configured to present more experimental resources than physical ones, preserving the DETERLab interface. A little more completely, the Containers system lays out the virtual computers into a physical layout of computers and uses the resource allocation system to allocate that physical layout. Then the system installs and configures the appropriate virtualization technologies in that environment to create the virtual environment. As in physical DETERLab experiments, the experiment's topology is written in an extended version of DETER's ns2 syntax, or in topdl , a topology description language. Currently experimenters pick containers directly using those languages.","title":"How does it work?"},{"location":"containers/containers-quickstart/#kinds-of-containers","text":"A container is a virtualization technology, like a virtual machine implementation. We use the term ''container'' to mean any one of the various virtualization technologies from an openvz container to a physical machine to a simulation. The Containers system gives us a way to create interconnections of containers (in our sense) holding different experiment elements. A containerized topology might include a physical machine, a qemu virtual machine and an openvz container that can all communicate transparently. The Containers system framework supports multiple kinds of containers, but at this point researchers may request these: Container Type Fidelity Scalability Physical Machine Complete fidelity 1 per physical machine Qemu virtual Machine Virtual hardware 10's of containers per physical machine Openvz container Partitioned resources in one Linux kernel 100's of contatiners per physical machine","title":"Kinds of Containers"},{"location":"containers/containers-quickstart/#how-do-i-use-containers","text":"In general, once you have a DETERLab account, you follow these steps. The DETERLab Containers Guide will walk you through a basic tutorial of these steps.","title":"How do I use Containers?"},{"location":"containers/containers-quickstart/#1-design-the-topology","text":"Every experiment in DETERLab is based on a network topology file written in NS format and saved on the users node. In a containerized experiment, the topology will typically be a large one with more than 400 nodes. sample of topology","title":"1. Design the topology"},{"location":"containers/containers-quickstart/#2-run-containerized-experiment-with-the-containerizepy-command","text":"The Containers system will build the containerized experiment on top of an existing DETERLab physical experiment by running the containerize.py command from the shell on users.deterlab.net , as in the following example: $ /share/containers/containerize.py DeterTest example1 ~/example1.tcl where ''DeterTest'' and ''example1'' are the project and experiment name, respectively, of the physical DETERLab experiment and ''example.tcl'' is the topology file. With these default parameters, containerize.py will put each node into an Openvz container with at most 10 containers per physical node.","title":"2. Run containerized experiment with the containerize.py command"},{"location":"containers/containers-quickstart/#3-view-results-by-accessing-nodes-modify-the-experiment-as-needed","text":"In a containerized experiment, you can access the virtual nodes with the same directories mounted as in a physical DETERLab experiment. You can load and run software and conduct experiments as you would in a physical experiment.","title":"3. View results by accessing nodes, modify the experiment as needed."},{"location":"containers/containers-quickstart/#4-save-your-work-and-swap-out-your-experiment-release-the-resources","text":"As with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users.","title":"4. Save your work and swap out your experiment (release the resources)"},{"location":"containers/containers-quickstart/#more-information","text":"For more detailed information about Containers, read the following: Containers Guide - This guide walks you through a basic example of using Containers and includes some advanced topics. Containers Reference - This reference includes Containers commands, configuration details and information about different types of containers.","title":"More Information"},{"location":"containers/containers-reference/","text":"Containers Reference \u00b6 This document describes the details of the commands and data structures that make up the Containers system. The Containers Guide provides useful context about the workflows and goals of the system that inform these technical details. Commands \u00b6 This section describes the command line interface to the Containers system. containerize.py \u00b6 The containerize.py command creates a DETERLab experiment made up of containers. The containerize.py program is available from /share/containers/containerize.py on users.deterlab.net . A sample invocation is: $ /share/containers/containerize.py MyProject MyExperiment ~/mytopology.tcl It will create a new experiment in MyProject called MyExperiment containing the experiment topology in mytopology.tcl . All the topology creation commands supported by DETERLab are supported by the Containers system, but DETERLab program agents are not. DETERLab start commands are supported. Containers will create an experiment in a group if the project parameter is of the form project / group . To start an experiment in the testing group of the DETER project, the first parameter is specified as DETER/testing . Containers supports ns2 file or topdl descriptions. Ns2 descriptions must end with .tcl or .ns . Other files are assumed to be topdl descriptions. Names of substrates and nodes in ns2 files are restricted to valid tcl variable names. Names of substrates and nodes in topdl files are restricted to the characters A-Z, a-z, digits, the underscore and the hyphen (-). By default, containerize.py program will partition the topology into openvz containers, packing 10 containers per physical computer. If the topology is already partitioned - meaning at least one element has a containers::partition attribute - containerize.py will not partition it. The --force-partition flag causes containerize.py to partition the experiment regardless of the presence of containers:partition attributes. If container types have been assigned to nodes using the containers:node_type attribute, containerize.py will respect them. Valid container types for the containers:node_type attribute or the --default-container parameter are: Parameter Container embedded_pnode Physical Node qemu Qemu VM openvz Openvz Container The containerize.py command takes several parameters that can change its behavior: --default-container = kind Containerize nodes without a container type into kind . If no nodes have been assigned containers, this puts all them into kind containers. --force-partition Partition the experiment whether or not it has been partitioned already --packing= int Attempt to put int containers into each physical node. The default --packing is 10. --size= int Attempt to divide the experiment into int physical nodes. The default is to use packing. There are some nuances to this with mixed containers. See the Containers Guide for more details. --config= filename Read configuration variables from filename . Configuration values are discussed below . --pnode-types= type1[,type2...] Override the site configuration and request nodes of type1 (or type2 etc.) as host nodes. --end-node-shaping Attempt to do end node traffic shaping even in containers connected by VDE switches. This works with qemu nodes. Topologies that include both openvz nodes and qemu nodes that shape traffic should use this. See the discussion below . --vde-switch-shaping Do traffic shaping in VDE switches. Probably the default, but that is controlled in the site configuration . See the discussion below . --openvz-diskspace Set the default openvz disk space size. The suffixes G and M stand for 'gigabytes' and 'megabytes'. --openvz-template Set the default openvz template. Templates are described in the Containers Guide . --openvz-template-dir Add a directory to be searched for openvz templates. Templates must end in tar.gz and be accessible to the user at creation and swap time. They can only be located under the /proj or /share directories. --image Construct a visualization of the virtual topology and leave it in the experiment directories (default). --nodes-only Ignore network constraints when partitioning nodes. --no-image Do not construct a visualization of the virtual topology. --pass-pack= pass : packing [, pass : packing ...] Specify the packing factor for each partitioning pass. The [ContainersGuide#MoreSophisticatedPacking:MultiplePasses Containers Guide] describes this in detail. --pass-size= pass : size [, pass : size ...] Specify the number of physical machines to use for each partitioning pass. The Containers Guide describes this in detail. --pass-pnodes= pass : type [, type ...][; pass : type [, type ...]...] Specify the pnode types on which nodes packed in partitioning pass pass can be placed. The Containers Guide describes this in detail. --pass-nodes-only= pass [, pass ...] Specify the partitioning passes on which network connectivity is ignored. The Containers Guide describes this in detail. --prefer-qemu-users= user[,user...] Make sure that Qemu images mount the given users' home directories. Qemu nodes can mount at most 19 users' home directories and this ensures that the experimenters using the containers can reach their home directories. --debug Print additional diagnostics and leave failed DETER experiments on the testbed. --keep-tmp Do not remove temporary files - used for debugging only. This invocation: $ ./containerize.py --packing 25 --default-container=qemu --force-partition DeterTest faber-packem ~/experiment.xml takes the topology in ~/experiment.xml (which must be a topdl description), packs it into 25 qemu containers per physical node, and creates an experiment called 'DeterTest/faber-packem' that can be swapped-in. If experiment.xml is already partitioned, it will be re-partitioned. If some nodes in that topology are assigned to openvz nodes already, those nodes will be still be in openvz containers. The result of a successful containerize.py run is a DETERLab experiment that can be swapped in. More detailed examples are available in the Containers Guide . container_image.py \u00b6 The container_image.py command draws a picture of the topology of an experiment. This is helpful in keeping track of how virtual nodes are connected. containerize.py calls this internally and stores the output in the per-experiment directory (unless --no-image is used). A researcher may call container_image.py directly to generate an image later or to generate one with the partitioning drawn. The simplest way to call container_image.py is: /share/containers/container_image.py topology.xml output.png The first parameter is a topdl description, for example the one in the per-experiment directory . The second parameter is the output file for the image. When drawing an experiment that has been containerized, the --experiment option is very useful. Options include: --experiment= project / experiment Draw the experiment in project / experiment , if it exists. Note that this is just the DETERLab experiment and project names. Omit any sub-group. --topo= filename Draw the topology in the filename indicated. --attr-prefix= prefix Prefix for containers attributes. Deprecated. --partitions Draw labeled boxes around nodes that share a physical node. --out= filename Save the image in the filename indicated. --program= programname Use programname to lay out the graph. programname must take a file in graphviz's dot language . This is given as the --program option to fedd_image.py internally. The default is fdp which works well when --partitions is given. If neither --topo nor --experiment is given, the first positional parameter is the topdl topology to draw. If --out is not given the next positional parameter (the first if neither --topo nor --experiment is given) is the output file. A common invocation looks like: /share/containers/container_image.py --experiment SAFER/testbed-containers ~/drawing.png Topdl Attributes For Containers \u00b6 Several topdl attributes influence how an experiment is containerized. These can be added to nodes using the ns2 tb-add-node-attribute command (used throughout the Containers Guide ) or directly to the topdl. These attributes are all attached to nodes/Computers: containers:node_type The container that will hold this node. The full list is available here. containers:partition An identifier grouping nodes together in containers that will share a physical node. Generally assigned by containerize.py , but researchers can also directly assign them. The containerize.py command assigns integers, so if a researcher assigns other partition identifiers, containerize.py will not overwrite them. containers:openvz_template The flavor of Linux distribution to emulate on openvz. There is a list of valid choices in the Containers Guide . containers:openvz_diskspace Amount of disk space to allocate to an openvz container. Be sure to include the G (gigabyte) or M (megabyte) suffix or the size will be taken as disk blocks. containers:ghost If this attribute is true, resources will be allocated for this node, but it will not be started when the topology is created. containers:maverick_url A location to download the QEMU image for this container. The name is a legacy that will disappear. This is deprecated. There are a few other attributes that are meaningful to more applications. Users specifying ns2 files will not need to set these directly, as the DETERLab ns2 interpreter does so. On Computers: startup The start command. tb-set-node-startcmd sets this. On interfaces: ip4_address The IPv4 address of this interface. Set by the ns2 commands for fixing addresses. ip4_netmask The IPv4 netmask. ns2 sets this. Configuration Files \u00b6 These files control the operation of the containers system. Per-experiment Directory \u00b6 When an experiment is containerized, the data necessary to create it is stored in /proj/ project /exp/ experiment /containers . The path /proj/ project /exp/ experiment is created by DETERLab when the experiment is created, and used by experimenters for a variety of things. This directory is replicated on nodes under /var/containers/config . There are a few files in the per-experiment directory that most experimenters can use: experiment.tcl If the topology was passed to containerize.py as an ns file, this is a copy of that input file. Useful for seeing what the experimenter asked for, or as a basis for new experiments. experiment.xml The analog of experiment.tcl , this is the topology given as topdl . The topdl input file. visualization.png A drawing of the virtual topology in png format. Generated by container_image.py hosts The host to IP mapping that will be installed on each node as /etc/hosts . site.conf A clone of the site configuration file that holds the global variables that the container creation will use. Values overridden on the command line invocation of containerize.py will be present in this file. The rest of this directory is primarily of interest to developers. It includes: annotated.xml First version of the input topology after default container types have been added. Input to the partitioning step. assignment A yaml representation of the partition to virtual node mapping. backend_config.yaml The server and channel to use for grandstand communication. Encoded in YAML. children Directory containing the assignment, including all the levels of nested hypervisors. config.tgz The contents of the per-experiment directory (except config.tgz ) for distribution into the experiment. embedding.yaml A yaml-encoded representation of the children sub-directory ghosts Containers that are initially not started in the experiment. maverick_url Yaml encoding of the qemu images to be used on each node. openvz_guest_url Yaml encoding of the openvz templates to be used on each node. partitioned.xml Output of the partitioning process. A copy of annotated.xml that has been decorated with the partitions. phys_topo.ns The ns2 file used to create the DETERLab experiment. phys_topo.xml The topdl file used to generate phystopo.ns . pid_eid The DETERLab project and experiment name under which this topology will be created. Broken out into /var/containers/pid and /var/containers/eid on virtual nodes inside the topology. route A directory containing the routing tables for each node. shaping.yaml Yaml-encoded data about the per-network and per-node loss, delay, and capacity parameters. switch A directory containing the VDE switch topology for the experiment. switch_extra.yaml Yaml-encoded extra switch configuration information. Mostly VDE switch configuration esoterica. topo.xml The final topology representation from which the physical topology is extracted. Includes the virtual topology as well. This file can be used as input to container_image.py . traffic_shaping.pickle Pickled information for configuring endnode traffic shaping. wirefilters.yaml Specific parameters for configuring the delay elements in VDE switched topologies that implement traffic shaping. See below . Site Configuration File \u00b6 The site configuration file controls how all experiments are containerized across DETERLab. The contents are primarily of interest to developers, but researchers may occasionally find the need to specify their own. The --config parameter to containerize.py does that. The site configuration file is an attribute-value pair file parsed by a python ConfigParser that sets overall container parameters. Many of these have legacy internal names. The default site configuration is in /share/containers/site.conf on users.deterlab.net . Acceptable values (and their DETERLab defaults) are: maverick_url Default image used by qemu containers. Default: http://scratch/benito/pangolinbz.img.bz2 url_base Base URL of the DETERLab web interface on which users can see experiments. Default: http://www.isi.deterlab.net/ qemu_host_hw Hardware used by containers. Default: pc2133,bpc2133,MicroCloud xmlrpc_server Host and port from which to request experiment creation. Default: boss.isi.deterlab.net:3069 qemu_host_os OSID to request for qemu container nodess. Default: Ubuntu1604-STD exec_root Root of the directory tree holding containers software and libraries. Developers often change this. Default: /share/containers openvz_host_os OSID to request for openvz nodes. Default CentOS6-64-openvz openvz_guest_url Location to load the openvz template from. Default: %(exec_root)s/images/ubuntu-10.04-x86.tar.gz switch_shaping True if switched containers (see below) should do traffic shaping in the VDE switch that connects them. Default: true switched_containers A list of the containers that are networked with VDE switches. Default: qemus openvz_template_dir The directory that stores openvz template files. Default: %(exec_root)s/images/ (that is the images directory in the exec_root directory defined in the site config file. This can be a comma-separated list that will be searched in order, after any template directories given on the command line. node_log The name of the file on experiment nodes used to log containers creation. Default is /tmp/containers.log topdl_converter The program used to convert ns2 descriptions to topdl. The default is fedd_ns2topdl.py --file but any program that takes a single ns2 file as a parameter and prints the topdl to standard output is viable. On DETERLab installations /usr/testbed/lib/ns2ir/parse.tcl -t -x 3 -m dummy dummy dummy dummy can be used to decouple containers from needing a running fedd. default_router The IP address of a router needed to reach testbed infrastructure default_dest The network on which testbed infrastructure lives that needs to be routed through default_router . backend_server Deprecated grandstand_port Deprecated Container Notes \u00b6 Different container types have some quirks. This section lists limitations of each container, as well as issues in interconnecting them. Qemu \u00b6 Qemu nodes are limited to 7 experimental interfaces. They currently run only Ubuntu 12.04 32 bit operating systems. Physical Nodes \u00b6 Physical nodes can be incorporated into experiments, but should only use modern versions of Ubuntu, to allow the Containers system to run their start commands correctly and to initialize their routing tables. Interconnections: VDE switches and local networking \u00b6 The various containers are interconnected using either local kernel virtual networking or VDE switches . Kernel networking is lower overhead because it does not require process context switching, but VDE switches are a more general solution. Network behavior changes such as loss, delay or rate limits are introduced into a network of containers using one of two mechanisms: inserting elements into a VDE switch topology or end node traffic shaping. Inserting elements into the VDE switch topology allows the system to modify the behavior for all packets passing through it. Generally this means all packets to or from a host, as the Containers system inserts these elements in the path between the node and the switch. This figure shows three containers sharing a virtual LAN (VLAN) on a VDE switch with no traffic shaping: The blue containers connect to the switch and the switch has interconnected their VDE ports into the red shared VLAN. To add delays to two of the nodes on that VLAN, the following VDE switch configuration would be used: The VDE switch connects the containers with shaped traffic to the delay elements, not to the shared VLAN. The delay elements are on the VLAN and delay all traffic passing through them. The Container system configures the delay elements to delay traffic symmetrically - traffic from the LAN and traffic from the container are both delayed. The VDE tools can be configured asymmetrically as well. This is a very flexible way to interconnect containers. That flexibility incurs a cost in overhead. Each delay element and the VDE switch is a process, do traffic passing from one delayed nodes to the other experiences 7 context switches: container -> switch, switch -> delay, delay -> switch, switch -> delay, delay -> switch, and switch -> container. The alternative mechanism is to do the traffic shaping inside the nodes, using Linux traffic shaping . In this case, traffic outbound from a container is delayed in the container for the full transit time to the next hop. The next node does the same. End-node shaping all happens in the kernel so it is relatively inexpensive at run time. Qemu nodes can make use of either end-node shaping or VDE shaping, and use VDE shaping by default. The --end-node-shaping and --vde-switch-shaping options to containerize.py force the choice in qemu. Openvz nodes only use end-node traffic shaping. They have no native VDE support so interconnecting openvz containers to VDE switches would include both extra kernel crossings and extra context switches. Because a primary attraction of VDE switches is their efficiency, the Containers system does not implement VDE interconnections to openvz. Similarly embedded physical nodes use only endnode traffic shaping, as routing outgoing traffic through a virtual switch infrastructure that just connects to its physical interfaces is at best confusing. Unfortunately, endnode traffic shaping and VDE shaping are incompatible. Because endnode shaping does not impose delays on arriving traffic, it cannot delay traffic from a VDE delayed node correctly. This is primarily of academic interest, unless a researcher wants to impose traffic shaping between containers using incompatible traffic shaping. There needs to be an unshaped link between the two kinds of traffic shaping. Bootable Qemu Images \u00b6 For qemu images to boot reliably, they should not wait for a keypress at the grub command, which is distressingly common. To ensure that your image does not wait for grub , do the following: For Ubuntu 12.04 (and any system that uses grub2) edit /etc/default/grub . For example: GRUB_DEFAULT=0 GRUB_HIDDEN_TIMEOUT=0 GRUB_HIDDEN_TIMEOUT_QUIET=true GRUB_TIMEOUT=1 GRUB_DISTRIBUTOR=`lsb_release -i -s 2> /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\" GRUB_CMDLINE_LINUX=\"\" Just make sure the HIDDENs are not commented out and have true/0 values. You then must run a command on the system which generates all the new grub configurations: $ sudo update-grub sudo configuration \u00b6 The Containers system adds all users to the admin group so that group should be able to use sudo without providing a password.","title":"Containers Reference"},{"location":"containers/containers-reference/#containers-reference","text":"This document describes the details of the commands and data structures that make up the Containers system. The Containers Guide provides useful context about the workflows and goals of the system that inform these technical details.","title":"Containers Reference"},{"location":"containers/containers-reference/#commands","text":"This section describes the command line interface to the Containers system.","title":"Commands"},{"location":"containers/containers-reference/#containerizepy","text":"The containerize.py command creates a DETERLab experiment made up of containers. The containerize.py program is available from /share/containers/containerize.py on users.deterlab.net . A sample invocation is: $ /share/containers/containerize.py MyProject MyExperiment ~/mytopology.tcl It will create a new experiment in MyProject called MyExperiment containing the experiment topology in mytopology.tcl . All the topology creation commands supported by DETERLab are supported by the Containers system, but DETERLab program agents are not. DETERLab start commands are supported. Containers will create an experiment in a group if the project parameter is of the form project / group . To start an experiment in the testing group of the DETER project, the first parameter is specified as DETER/testing . Containers supports ns2 file or topdl descriptions. Ns2 descriptions must end with .tcl or .ns . Other files are assumed to be topdl descriptions. Names of substrates and nodes in ns2 files are restricted to valid tcl variable names. Names of substrates and nodes in topdl files are restricted to the characters A-Z, a-z, digits, the underscore and the hyphen (-). By default, containerize.py program will partition the topology into openvz containers, packing 10 containers per physical computer. If the topology is already partitioned - meaning at least one element has a containers::partition attribute - containerize.py will not partition it. The --force-partition flag causes containerize.py to partition the experiment regardless of the presence of containers:partition attributes. If container types have been assigned to nodes using the containers:node_type attribute, containerize.py will respect them. Valid container types for the containers:node_type attribute or the --default-container parameter are: Parameter Container embedded_pnode Physical Node qemu Qemu VM openvz Openvz Container The containerize.py command takes several parameters that can change its behavior: --default-container = kind Containerize nodes without a container type into kind . If no nodes have been assigned containers, this puts all them into kind containers. --force-partition Partition the experiment whether or not it has been partitioned already --packing= int Attempt to put int containers into each physical node. The default --packing is 10. --size= int Attempt to divide the experiment into int physical nodes. The default is to use packing. There are some nuances to this with mixed containers. See the Containers Guide for more details. --config= filename Read configuration variables from filename . Configuration values are discussed below . --pnode-types= type1[,type2...] Override the site configuration and request nodes of type1 (or type2 etc.) as host nodes. --end-node-shaping Attempt to do end node traffic shaping even in containers connected by VDE switches. This works with qemu nodes. Topologies that include both openvz nodes and qemu nodes that shape traffic should use this. See the discussion below . --vde-switch-shaping Do traffic shaping in VDE switches. Probably the default, but that is controlled in the site configuration . See the discussion below . --openvz-diskspace Set the default openvz disk space size. The suffixes G and M stand for 'gigabytes' and 'megabytes'. --openvz-template Set the default openvz template. Templates are described in the Containers Guide . --openvz-template-dir Add a directory to be searched for openvz templates. Templates must end in tar.gz and be accessible to the user at creation and swap time. They can only be located under the /proj or /share directories. --image Construct a visualization of the virtual topology and leave it in the experiment directories (default). --nodes-only Ignore network constraints when partitioning nodes. --no-image Do not construct a visualization of the virtual topology. --pass-pack= pass : packing [, pass : packing ...] Specify the packing factor for each partitioning pass. The [ContainersGuide#MoreSophisticatedPacking:MultiplePasses Containers Guide] describes this in detail. --pass-size= pass : size [, pass : size ...] Specify the number of physical machines to use for each partitioning pass. The Containers Guide describes this in detail. --pass-pnodes= pass : type [, type ...][; pass : type [, type ...]...] Specify the pnode types on which nodes packed in partitioning pass pass can be placed. The Containers Guide describes this in detail. --pass-nodes-only= pass [, pass ...] Specify the partitioning passes on which network connectivity is ignored. The Containers Guide describes this in detail. --prefer-qemu-users= user[,user...] Make sure that Qemu images mount the given users' home directories. Qemu nodes can mount at most 19 users' home directories and this ensures that the experimenters using the containers can reach their home directories. --debug Print additional diagnostics and leave failed DETER experiments on the testbed. --keep-tmp Do not remove temporary files - used for debugging only. This invocation: $ ./containerize.py --packing 25 --default-container=qemu --force-partition DeterTest faber-packem ~/experiment.xml takes the topology in ~/experiment.xml (which must be a topdl description), packs it into 25 qemu containers per physical node, and creates an experiment called 'DeterTest/faber-packem' that can be swapped-in. If experiment.xml is already partitioned, it will be re-partitioned. If some nodes in that topology are assigned to openvz nodes already, those nodes will be still be in openvz containers. The result of a successful containerize.py run is a DETERLab experiment that can be swapped in. More detailed examples are available in the Containers Guide .","title":"containerize.py"},{"location":"containers/containers-reference/#container_imagepy","text":"The container_image.py command draws a picture of the topology of an experiment. This is helpful in keeping track of how virtual nodes are connected. containerize.py calls this internally and stores the output in the per-experiment directory (unless --no-image is used). A researcher may call container_image.py directly to generate an image later or to generate one with the partitioning drawn. The simplest way to call container_image.py is: /share/containers/container_image.py topology.xml output.png The first parameter is a topdl description, for example the one in the per-experiment directory . The second parameter is the output file for the image. When drawing an experiment that has been containerized, the --experiment option is very useful. Options include: --experiment= project / experiment Draw the experiment in project / experiment , if it exists. Note that this is just the DETERLab experiment and project names. Omit any sub-group. --topo= filename Draw the topology in the filename indicated. --attr-prefix= prefix Prefix for containers attributes. Deprecated. --partitions Draw labeled boxes around nodes that share a physical node. --out= filename Save the image in the filename indicated. --program= programname Use programname to lay out the graph. programname must take a file in graphviz's dot language . This is given as the --program option to fedd_image.py internally. The default is fdp which works well when --partitions is given. If neither --topo nor --experiment is given, the first positional parameter is the topdl topology to draw. If --out is not given the next positional parameter (the first if neither --topo nor --experiment is given) is the output file. A common invocation looks like: /share/containers/container_image.py --experiment SAFER/testbed-containers ~/drawing.png","title":"container_image.py"},{"location":"containers/containers-reference/#topdl-attributes-for-containers","text":"Several topdl attributes influence how an experiment is containerized. These can be added to nodes using the ns2 tb-add-node-attribute command (used throughout the Containers Guide ) or directly to the topdl. These attributes are all attached to nodes/Computers: containers:node_type The container that will hold this node. The full list is available here. containers:partition An identifier grouping nodes together in containers that will share a physical node. Generally assigned by containerize.py , but researchers can also directly assign them. The containerize.py command assigns integers, so if a researcher assigns other partition identifiers, containerize.py will not overwrite them. containers:openvz_template The flavor of Linux distribution to emulate on openvz. There is a list of valid choices in the Containers Guide . containers:openvz_diskspace Amount of disk space to allocate to an openvz container. Be sure to include the G (gigabyte) or M (megabyte) suffix or the size will be taken as disk blocks. containers:ghost If this attribute is true, resources will be allocated for this node, but it will not be started when the topology is created. containers:maverick_url A location to download the QEMU image for this container. The name is a legacy that will disappear. This is deprecated. There are a few other attributes that are meaningful to more applications. Users specifying ns2 files will not need to set these directly, as the DETERLab ns2 interpreter does so. On Computers: startup The start command. tb-set-node-startcmd sets this. On interfaces: ip4_address The IPv4 address of this interface. Set by the ns2 commands for fixing addresses. ip4_netmask The IPv4 netmask. ns2 sets this.","title":"Topdl Attributes For Containers"},{"location":"containers/containers-reference/#configuration-files","text":"These files control the operation of the containers system.","title":"Configuration Files"},{"location":"containers/containers-reference/#per-experiment-directory","text":"When an experiment is containerized, the data necessary to create it is stored in /proj/ project /exp/ experiment /containers . The path /proj/ project /exp/ experiment is created by DETERLab when the experiment is created, and used by experimenters for a variety of things. This directory is replicated on nodes under /var/containers/config . There are a few files in the per-experiment directory that most experimenters can use: experiment.tcl If the topology was passed to containerize.py as an ns file, this is a copy of that input file. Useful for seeing what the experimenter asked for, or as a basis for new experiments. experiment.xml The analog of experiment.tcl , this is the topology given as topdl . The topdl input file. visualization.png A drawing of the virtual topology in png format. Generated by container_image.py hosts The host to IP mapping that will be installed on each node as /etc/hosts . site.conf A clone of the site configuration file that holds the global variables that the container creation will use. Values overridden on the command line invocation of containerize.py will be present in this file. The rest of this directory is primarily of interest to developers. It includes: annotated.xml First version of the input topology after default container types have been added. Input to the partitioning step. assignment A yaml representation of the partition to virtual node mapping. backend_config.yaml The server and channel to use for grandstand communication. Encoded in YAML. children Directory containing the assignment, including all the levels of nested hypervisors. config.tgz The contents of the per-experiment directory (except config.tgz ) for distribution into the experiment. embedding.yaml A yaml-encoded representation of the children sub-directory ghosts Containers that are initially not started in the experiment. maverick_url Yaml encoding of the qemu images to be used on each node. openvz_guest_url Yaml encoding of the openvz templates to be used on each node. partitioned.xml Output of the partitioning process. A copy of annotated.xml that has been decorated with the partitions. phys_topo.ns The ns2 file used to create the DETERLab experiment. phys_topo.xml The topdl file used to generate phystopo.ns . pid_eid The DETERLab project and experiment name under which this topology will be created. Broken out into /var/containers/pid and /var/containers/eid on virtual nodes inside the topology. route A directory containing the routing tables for each node. shaping.yaml Yaml-encoded data about the per-network and per-node loss, delay, and capacity parameters. switch A directory containing the VDE switch topology for the experiment. switch_extra.yaml Yaml-encoded extra switch configuration information. Mostly VDE switch configuration esoterica. topo.xml The final topology representation from which the physical topology is extracted. Includes the virtual topology as well. This file can be used as input to container_image.py . traffic_shaping.pickle Pickled information for configuring endnode traffic shaping. wirefilters.yaml Specific parameters for configuring the delay elements in VDE switched topologies that implement traffic shaping. See below .","title":"Per-experiment Directory"},{"location":"containers/containers-reference/#site-configuration-file","text":"The site configuration file controls how all experiments are containerized across DETERLab. The contents are primarily of interest to developers, but researchers may occasionally find the need to specify their own. The --config parameter to containerize.py does that. The site configuration file is an attribute-value pair file parsed by a python ConfigParser that sets overall container parameters. Many of these have legacy internal names. The default site configuration is in /share/containers/site.conf on users.deterlab.net . Acceptable values (and their DETERLab defaults) are: maverick_url Default image used by qemu containers. Default: http://scratch/benito/pangolinbz.img.bz2 url_base Base URL of the DETERLab web interface on which users can see experiments. Default: http://www.isi.deterlab.net/ qemu_host_hw Hardware used by containers. Default: pc2133,bpc2133,MicroCloud xmlrpc_server Host and port from which to request experiment creation. Default: boss.isi.deterlab.net:3069 qemu_host_os OSID to request for qemu container nodess. Default: Ubuntu1604-STD exec_root Root of the directory tree holding containers software and libraries. Developers often change this. Default: /share/containers openvz_host_os OSID to request for openvz nodes. Default CentOS6-64-openvz openvz_guest_url Location to load the openvz template from. Default: %(exec_root)s/images/ubuntu-10.04-x86.tar.gz switch_shaping True if switched containers (see below) should do traffic shaping in the VDE switch that connects them. Default: true switched_containers A list of the containers that are networked with VDE switches. Default: qemus openvz_template_dir The directory that stores openvz template files. Default: %(exec_root)s/images/ (that is the images directory in the exec_root directory defined in the site config file. This can be a comma-separated list that will be searched in order, after any template directories given on the command line. node_log The name of the file on experiment nodes used to log containers creation. Default is /tmp/containers.log topdl_converter The program used to convert ns2 descriptions to topdl. The default is fedd_ns2topdl.py --file but any program that takes a single ns2 file as a parameter and prints the topdl to standard output is viable. On DETERLab installations /usr/testbed/lib/ns2ir/parse.tcl -t -x 3 -m dummy dummy dummy dummy can be used to decouple containers from needing a running fedd. default_router The IP address of a router needed to reach testbed infrastructure default_dest The network on which testbed infrastructure lives that needs to be routed through default_router . backend_server Deprecated grandstand_port Deprecated","title":"Site Configuration File"},{"location":"containers/containers-reference/#container-notes","text":"Different container types have some quirks. This section lists limitations of each container, as well as issues in interconnecting them.","title":"Container Notes"},{"location":"containers/containers-reference/#qemu","text":"Qemu nodes are limited to 7 experimental interfaces. They currently run only Ubuntu 12.04 32 bit operating systems.","title":"Qemu"},{"location":"containers/containers-reference/#physical-nodes","text":"Physical nodes can be incorporated into experiments, but should only use modern versions of Ubuntu, to allow the Containers system to run their start commands correctly and to initialize their routing tables.","title":"Physical Nodes"},{"location":"containers/containers-reference/#interconnections-vde-switches-and-local-networking","text":"The various containers are interconnected using either local kernel virtual networking or VDE switches . Kernel networking is lower overhead because it does not require process context switching, but VDE switches are a more general solution. Network behavior changes such as loss, delay or rate limits are introduced into a network of containers using one of two mechanisms: inserting elements into a VDE switch topology or end node traffic shaping. Inserting elements into the VDE switch topology allows the system to modify the behavior for all packets passing through it. Generally this means all packets to or from a host, as the Containers system inserts these elements in the path between the node and the switch. This figure shows three containers sharing a virtual LAN (VLAN) on a VDE switch with no traffic shaping: The blue containers connect to the switch and the switch has interconnected their VDE ports into the red shared VLAN. To add delays to two of the nodes on that VLAN, the following VDE switch configuration would be used: The VDE switch connects the containers with shaped traffic to the delay elements, not to the shared VLAN. The delay elements are on the VLAN and delay all traffic passing through them. The Container system configures the delay elements to delay traffic symmetrically - traffic from the LAN and traffic from the container are both delayed. The VDE tools can be configured asymmetrically as well. This is a very flexible way to interconnect containers. That flexibility incurs a cost in overhead. Each delay element and the VDE switch is a process, do traffic passing from one delayed nodes to the other experiences 7 context switches: container -> switch, switch -> delay, delay -> switch, switch -> delay, delay -> switch, and switch -> container. The alternative mechanism is to do the traffic shaping inside the nodes, using Linux traffic shaping . In this case, traffic outbound from a container is delayed in the container for the full transit time to the next hop. The next node does the same. End-node shaping all happens in the kernel so it is relatively inexpensive at run time. Qemu nodes can make use of either end-node shaping or VDE shaping, and use VDE shaping by default. The --end-node-shaping and --vde-switch-shaping options to containerize.py force the choice in qemu. Openvz nodes only use end-node traffic shaping. They have no native VDE support so interconnecting openvz containers to VDE switches would include both extra kernel crossings and extra context switches. Because a primary attraction of VDE switches is their efficiency, the Containers system does not implement VDE interconnections to openvz. Similarly embedded physical nodes use only endnode traffic shaping, as routing outgoing traffic through a virtual switch infrastructure that just connects to its physical interfaces is at best confusing. Unfortunately, endnode traffic shaping and VDE shaping are incompatible. Because endnode shaping does not impose delays on arriving traffic, it cannot delay traffic from a VDE delayed node correctly. This is primarily of academic interest, unless a researcher wants to impose traffic shaping between containers using incompatible traffic shaping. There needs to be an unshaped link between the two kinds of traffic shaping.","title":"Interconnections: VDE switches and local networking"},{"location":"containers/containers-reference/#bootable-qemu-images","text":"For qemu images to boot reliably, they should not wait for a keypress at the grub command, which is distressingly common. To ensure that your image does not wait for grub , do the following: For Ubuntu 12.04 (and any system that uses grub2) edit /etc/default/grub . For example: GRUB_DEFAULT=0 GRUB_HIDDEN_TIMEOUT=0 GRUB_HIDDEN_TIMEOUT_QUIET=true GRUB_TIMEOUT=1 GRUB_DISTRIBUTOR=`lsb_release -i -s 2> /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\" GRUB_CMDLINE_LINUX=\"\" Just make sure the HIDDENs are not commented out and have true/0 values. You then must run a command on the system which generates all the new grub configurations: $ sudo update-grub","title":"Bootable Qemu Images"},{"location":"containers/containers-reference/#sudo-configuration","text":"The Containers system adds all users to the admin group so that group should be able to use sudo without providing a password.","title":"sudo configuration"},{"location":"core/DETERSSH/","text":"Accessing testbeds using SSH \u00b6 Each node on the testbed is reachable via SSH . One main difference between DETER and Emulab is that DETER nodes are not accessible directly from the internet. In order to log into your nodes, you must first log into '''users.deterlab.net''' using your DETER username (not your email address) and password (or your SSH public key). From users you can log into your nodes. To save on connections, you might want to look into using GNU screen on users . Also refer to the Tips and Tricks section below for ways to make accessing DETER easier. Uploading files to DETER \u00b6 You can upload files to users.deterlab.net via SCP or SFTP . Files in your home directory and in your project directory will be made available to you on all of your testbed nodes via NFS . An Example Session with Windows and Putty \u00b6 Putty is a free, lightweight SSH client for Windows. Here is an example session in which I connect to my experimental node \"node0\" in my experiment \"jjh-ubuntu1004\" in the project \"DeterTest\". First we connect to '''users.deterlab.net''': We then enter in our just our username and password (the same password as the DETERLab web interface). Trying to use your email address or something like jjh@users.deterlab.net will '''not''' work: Now we have successfully logged into users.deterlab.net: From users, we now ssh into our experimental node, \"node0.jjh-ubuntu.detertest\": Tips and Tricks \u00b6 Listing your nodes from the command line \u00b6 When logged into users.deterlab.net , the node_list command will list the names of all your nodes. You can log into your nodes using either the pcXXX name or the full experimental name. [jhickey@users ~]$ node_list DeterTest/jjh-ubuntu node1.jjh-ubuntu.DeterTest / pc118 node0.jjh-ubuntu.DeterTest / pc054 node2.jjh-ubuntu.DeterTest / pc167 DeterTest/jjh-ubuntu1004 node0.jjh-ubuntu1004.DeterTest / pc026 [jhickey@users ~]$ SSH port forwarding \u00b6 If you are, for example, running an internal web server on one of your DETER nodes, you can access it via SSH through users. For example to redirect port 80 on pcXXX to your local machine on port 8080 you would do: ssh -L 8080:pcXXX:80 username@users.deterlab.net Once logged in, you should be able to access the web server on your DETER node by going to http://localhost:8080. For more information on port forwarding with SSH, please refer to the SSH man page. SSH port forwarding with Putty \u00b6 To use putty for port forwarding, configure putty to open a connection to users.deterlab.net before you make that connection, set up the tunneling parameters. This example forwards local port 12345 to a remode desktop protocol server (port 3389) on a testbed node. select the Tunnels menu from under the SSH choice in the Connection menu on the left hand side. Add a forwarded port using the Local type, a local port number (12345 in the image) and the DETER hostname and port in the Destination field. In the example we are forwarding the connection to port 3389 (the remote desktop protocol) on pc102 . Be sure to press Add to add the port. The putty window will look like this: Now open that connection. You will see a login prompt, and you should log in to users. Now you should be able to point your local remote desktop viewer to localhost port 12345 and see the login screen of pc102. If the node is a Windows node, you will see something like this: Be sure that your local machine does not firewall the local port 12345. Replace pc102 with a node in your experiment and the forwarded port with the port your service uses. Uploading your SSH key from OS X \u00b6 The Upload File dialog in Macintosh OS X does not show hidden directories by default. This creates and extra hassle when uploading SSH public keys from an OS X machine. In the \"Upload File\" dialog, use the shortcut '''Shift-Command-G''' and type in \"~/.ssh\" to navigate to the contents of your .ssh directory. Then you will be presented with the contents of your .ssh directory and will be able to upload your id_rsa.pub file to DETER: OpenSSH Configuration for Directly Logging into testbed nodes \u00b6 These configuration tweaks should work for any operating system that runs OpenSSH (Linux, BSD, and OS X typically use OpenSSH as the default SSH client). It is possible to log directly into testbed nodes with a little SSH configuration tweaking. Adding the following statement to '''~/.ssh/config''' will allow you to skip logging into users in order to access a particular testbed node. Change MyProject to the name of your project. Host pc*.isi.deterlab.net ProxyCommand ssh users.deterlab.net nc %h %p StrictHostKeyChecking no Host *.MyProject.isi.deterlab.net ProxyCommand ssh users.deterlab.net nc %h %p StrictHostKeyChecking no With this configuration change and a proper SSH key setup, you will be able to directly log into nodes in your experiment. You will now be able to log into nodes in your experiment using either the actual node name, e.g. pc025.isi.deterlab.net, or [host].[experiment].[project].isi.deterlab.net. For example: jjhs-mac-mini:~ jjh$ ssh node0.jjh-ubuntu1004.DeterTest.isi.deterlab.net Warning: Permanently added 'node0.jjh-ubuntu1004.detertest.isi.deterlab.net' (RSA) to the list of known hosts. Linux node0.jjh-ubuntu1004.detertest.isi.deterlab.net 2.6.32-25-generic-pae #45-Ubuntu SMP Sat Oct 16 21:01:33 UTC 2010 i686 GNU/Linux Ubuntu 10.04.1 LTS Welcome to Ubuntu! * Documentation: https://help.ubuntu.com/ System information as of Wed Nov 10 20:41:19 PST 2010 System load: 0.0 Processes: 116 Usage of /: 12.3% of 14.67GB Users logged in: 0 Memory usage: 1% IP address for eth1: 192.168.1.26 Swap usage: 0% Graph this data and manage this system at https://landscape.canonical.com/ 4 packages can be updated. 2 updates are security updates. Last login: Wed Nov 10 20:13:15 2010 from users.deterlab.net node0:~> Accelerating Multiple Connections using OpenSSH Connection Multplexing \u00b6 You can log in multiple times using the same SSH connection. This dramatically speeds up creating new connections. To enable SSH connection multiplexing, add the following lines to ~/.ssh/config. If you are on a multiuser machine, you may want to store the control socket someplace other than /tmp. Host users.deterlab.net ControlMaster auto ControlPath /tmp/%r@%h:%p To verify that it is working, you can use the '''-v''' option: jjhs-mac-mini:~ jjh$ ssh -v pc026.isi.deterlab.net OpenSSH_5.2p1, OpenSSL 0.9.8l 5 Nov 2009 debug1: Reading configuration data /Users/jjh/.ssh/config debug1: Applying options for *isi.deterlab.net debug1: Applying options for pc*.isi.deterlab.net debug1: Applying options for * debug1: Reading configuration data /etc/ssh_config debug1: auto-mux: Trying existing master Last login: Wed Nov 10 20:51:43 2010 from users.deterlab.net node0:~> If you try to close your master connection while other connections are active, the connection will stay running until the other sessions end.","title":"Accessing testbeds using SSH"},{"location":"core/DETERSSH/#accessing-testbeds-using-ssh","text":"Each node on the testbed is reachable via SSH . One main difference between DETER and Emulab is that DETER nodes are not accessible directly from the internet. In order to log into your nodes, you must first log into '''users.deterlab.net''' using your DETER username (not your email address) and password (or your SSH public key). From users you can log into your nodes. To save on connections, you might want to look into using GNU screen on users . Also refer to the Tips and Tricks section below for ways to make accessing DETER easier.","title":"Accessing testbeds using SSH"},{"location":"core/DETERSSH/#uploading-files-to-deter","text":"You can upload files to users.deterlab.net via SCP or SFTP . Files in your home directory and in your project directory will be made available to you on all of your testbed nodes via NFS .","title":"Uploading files to DETER"},{"location":"core/DETERSSH/#an-example-session-with-windows-and-putty","text":"Putty is a free, lightweight SSH client for Windows. Here is an example session in which I connect to my experimental node \"node0\" in my experiment \"jjh-ubuntu1004\" in the project \"DeterTest\". First we connect to '''users.deterlab.net''': We then enter in our just our username and password (the same password as the DETERLab web interface). Trying to use your email address or something like jjh@users.deterlab.net will '''not''' work: Now we have successfully logged into users.deterlab.net: From users, we now ssh into our experimental node, \"node0.jjh-ubuntu.detertest\":","title":"An Example Session with Windows and Putty"},{"location":"core/DETERSSH/#tips-and-tricks","text":"","title":"Tips and Tricks"},{"location":"core/DETERSSH/#listing-your-nodes-from-the-command-line","text":"When logged into users.deterlab.net , the node_list command will list the names of all your nodes. You can log into your nodes using either the pcXXX name or the full experimental name. [jhickey@users ~]$ node_list DeterTest/jjh-ubuntu node1.jjh-ubuntu.DeterTest / pc118 node0.jjh-ubuntu.DeterTest / pc054 node2.jjh-ubuntu.DeterTest / pc167 DeterTest/jjh-ubuntu1004 node0.jjh-ubuntu1004.DeterTest / pc026 [jhickey@users ~]$","title":"Listing your nodes from the command line"},{"location":"core/DETERSSH/#ssh-port-forwarding","text":"If you are, for example, running an internal web server on one of your DETER nodes, you can access it via SSH through users. For example to redirect port 80 on pcXXX to your local machine on port 8080 you would do: ssh -L 8080:pcXXX:80 username@users.deterlab.net Once logged in, you should be able to access the web server on your DETER node by going to http://localhost:8080. For more information on port forwarding with SSH, please refer to the SSH man page.","title":"SSH port forwarding"},{"location":"core/DETERSSH/#ssh-port-forwarding-with-putty","text":"To use putty for port forwarding, configure putty to open a connection to users.deterlab.net before you make that connection, set up the tunneling parameters. This example forwards local port 12345 to a remode desktop protocol server (port 3389) on a testbed node. select the Tunnels menu from under the SSH choice in the Connection menu on the left hand side. Add a forwarded port using the Local type, a local port number (12345 in the image) and the DETER hostname and port in the Destination field. In the example we are forwarding the connection to port 3389 (the remote desktop protocol) on pc102 . Be sure to press Add to add the port. The putty window will look like this: Now open that connection. You will see a login prompt, and you should log in to users. Now you should be able to point your local remote desktop viewer to localhost port 12345 and see the login screen of pc102. If the node is a Windows node, you will see something like this: Be sure that your local machine does not firewall the local port 12345. Replace pc102 with a node in your experiment and the forwarded port with the port your service uses.","title":"SSH port forwarding with Putty"},{"location":"core/DETERSSH/#uploading-your-ssh-key-from-os-x","text":"The Upload File dialog in Macintosh OS X does not show hidden directories by default. This creates and extra hassle when uploading SSH public keys from an OS X machine. In the \"Upload File\" dialog, use the shortcut '''Shift-Command-G''' and type in \"~/.ssh\" to navigate to the contents of your .ssh directory. Then you will be presented with the contents of your .ssh directory and will be able to upload your id_rsa.pub file to DETER:","title":"Uploading your SSH key from OS X "},{"location":"core/DETERSSH/#openssh-configuration-for-directly-logging-into-testbed-nodes","text":"These configuration tweaks should work for any operating system that runs OpenSSH (Linux, BSD, and OS X typically use OpenSSH as the default SSH client). It is possible to log directly into testbed nodes with a little SSH configuration tweaking. Adding the following statement to '''~/.ssh/config''' will allow you to skip logging into users in order to access a particular testbed node. Change MyProject to the name of your project. Host pc*.isi.deterlab.net ProxyCommand ssh users.deterlab.net nc %h %p StrictHostKeyChecking no Host *.MyProject.isi.deterlab.net ProxyCommand ssh users.deterlab.net nc %h %p StrictHostKeyChecking no With this configuration change and a proper SSH key setup, you will be able to directly log into nodes in your experiment. You will now be able to log into nodes in your experiment using either the actual node name, e.g. pc025.isi.deterlab.net, or [host].[experiment].[project].isi.deterlab.net. For example: jjhs-mac-mini:~ jjh$ ssh node0.jjh-ubuntu1004.DeterTest.isi.deterlab.net Warning: Permanently added 'node0.jjh-ubuntu1004.detertest.isi.deterlab.net' (RSA) to the list of known hosts. Linux node0.jjh-ubuntu1004.detertest.isi.deterlab.net 2.6.32-25-generic-pae #45-Ubuntu SMP Sat Oct 16 21:01:33 UTC 2010 i686 GNU/Linux Ubuntu 10.04.1 LTS Welcome to Ubuntu! * Documentation: https://help.ubuntu.com/ System information as of Wed Nov 10 20:41:19 PST 2010 System load: 0.0 Processes: 116 Usage of /: 12.3% of 14.67GB Users logged in: 0 Memory usage: 1% IP address for eth1: 192.168.1.26 Swap usage: 0% Graph this data and manage this system at https://landscape.canonical.com/ 4 packages can be updated. 2 updates are security updates. Last login: Wed Nov 10 20:13:15 2010 from users.deterlab.net node0:~>","title":"OpenSSH Configuration for Directly Logging into testbed nodes"},{"location":"core/DETERSSH/#accelerating-multiple-connections-using-openssh-connection-multplexing","text":"You can log in multiple times using the same SSH connection. This dramatically speeds up creating new connections. To enable SSH connection multiplexing, add the following lines to ~/.ssh/config. If you are on a multiuser machine, you may want to store the control socket someplace other than /tmp. Host users.deterlab.net ControlMaster auto ControlPath /tmp/%r@%h:%p To verify that it is working, you can use the '''-v''' option: jjhs-mac-mini:~ jjh$ ssh -v pc026.isi.deterlab.net OpenSSH_5.2p1, OpenSSL 0.9.8l 5 Nov 2009 debug1: Reading configuration data /Users/jjh/.ssh/config debug1: Applying options for *isi.deterlab.net debug1: Applying options for pc*.isi.deterlab.net debug1: Applying options for * debug1: Reading configuration data /etc/ssh_config debug1: auto-mux: Trying existing master Last login: Wed Nov 10 20:51:43 2010 from users.deterlab.net node0:~> If you try to close your master connection while other connections are active, the connection will stay running until the other sessions end.","title":"Accelerating Multiple Connections using OpenSSH Connection Multplexing"},{"location":"core/core-guide/","text":"Core Guide \u00b6 In this tutorial we begin with a small 3-5 node experiment, so that you will become familiar with NS syntax and the practical aspects of DETERLab operation. Usually, you will want to incorporate another system such as the MAGI Orchestrator for more fully fleshed out experiments. But this is a good starting point for those new to DETERLab. Note If you are a student, go to the http://education.deterlab.net site for classroom-specific instructions. Node Use Policy \u00b6 Please make sure to read our guidelines for using nodes in DETERLab . These guidelines help keep DETERLab an effective environment for all users. DETERLab Environment \u00b6 Your experiment is made up of one or more machines on the internal DETERLab network, which is behind a firewall. users.deterlab.net (or users for short) is the \"control server\" for DETERLab. From users , you can contact all your nodes, reboot them, connect to their serial ports, etc. Each user has a home directory on this server and you may SSH into it with your username and password for your DETERLab account. myboss.isi.deterlab.net ( or boss for short) is the main testbed server that runs DETERLab. Users are not allowed to log directly into it. Basic Tutorial \u00b6 Getting Started \u00b6 Work in DETERLab is organized by experiments within projects . Each project is created and managed by a leader - usually the Principal Investigator (PI) of a research project or the instructor of a class on cybersecurity. The project leader then invites members to join by providing them with the project name and sending them the link to the 'Join a Project' page. Before you can take the following tutorial, you need an active account in a project in DETERLab. See How to Register to make sure if you're qualified, and then follow the directions to create a project or ask to join an existing project - if you go through either process for the first time, your account is created as a result. If you already have an account, proceed to the next step. Step 1: Design the topology \u00b6 Part of DETERLab's power lies in its ability to assume many different topologies; the description of a such a topology is a necessary part of an experiment. Before you can start your experiment, you must model the elements of the experiment's network with a topology. For this basic tutorial, use this NS file which includes a simple topology and save it to a directory called basicExp in your local directory on users.deterlab.net . The rest of this section describes NS format and walks you through the different parts of the sample file. NS Format \u00b6 DETERLab uses the \"NS\" (\"Network Simulator\") format to describe network topologies. This is substantially the same Tcl-based format used by ns-2 . Since DETERLab offers emulation, rather than simulation, these files are interpreted in a somewhat different manner than ns-2. Therefore, some ns-2 functionality may work differently than you expect, or may not be implemented at all. Please look for warnings of the form: *** WARNING: Unsupported NS Statement! Link type BAZ, using DropTail! If you feel there is useful functionality missing, please let us know . Also, some testbed-specific syntax has been added, which, with the inclusion of the compatibility module tb_compat.tcl , will be ignored by the NS simulator. This allows the same NS file to work on both DETERLab and ns-2, most of the time. NS Example \u00b6 In our example, we are creating a test network which looks like the following: Figure 1: A is connected to B, and B to C and D with a LAN. Here's how to describe this topology: Declare a simulator and include a file that allows you to use the special tb- commands. First off, all NS files start with a simple prologue, declaring a simulator and including a file that allows you to use the special tb- commands: # This is a simple ns script. Comments start with #. set ns [new Simulator] source tb_compat.tcl Define the 4 nodes in the topology. set nodeA [$ns node] set nodeB [$ns node] set nodeC [$ns node] set nodeD [$ns node] nodeA and so on are the virtual names ( vnames ) of the nodes in your topology. When your experiment is swapped in (has allocated resources), they will be assigned to physical node names like pc45 , probably different ones each time. NOTE: Avoid vnames that clash with the physical node names in the testbed.** Define the link and the LAN that connect the nodes. NS syntax permits you to specify the bandwidth, latency, and queue type. Note that since NS can't impose artificial losses like DETERLab can, we use a separate tb- command to add loss on a link. For our example, we will define a full speed LAN between B, C, and D, and a shaped link from node A to B. set link0 [$ns duplex-link $nodeB $nodeA 30Mb 50ms DropTail] tb-set-link-loss $link0 0.01 set lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms] In addition to the standard NS syntax above, a number of extensions are available in DETERLab that allow you to better control your experiment. For example, you may specify what Operating System is booted on your nodes. For the versions of FreeBSD, Linux, and Windows we currently support, please refer to the Operating System Images page. Click List ImageIDs in the DETERLab web interface Interaction pane to see the current list of DETERLab-supplied operating systems. By default, our most recent Linux image is selected. tb-set-node-os $nodeA FBSD11-STD tb-set-node-os $nodeC Ubuntu1604-STD tb-set-node-os $nodeC WINXP-UPDATE Enable routing. In a topology like this, you will likely want to communicate between all the nodes, including nodes that aren't directly connected, like A and C . In order for that to happen, we must enable routing in our experiment, so B can route packets for the other nodes. The typical way to do this is with Static routing. (Other options are detailed in the Routing section below ). $ns rtproto Static End with an epilogue that instructs the simulator to start. # Go! $ns run Step 2: Create a new experiment \u00b6 For this tutorial, we will use the web interface to create a new experiment. You could also use the DETERLab Shell Commands . Log into DETERLab with your account credentials (see How to Register ). Click the Experimentation menu item, then click Begin an Experiment . Click Select Project and choose your project. This is also know as your project name or Project ID (PID). This is used as an argument in many commands. Most people will be a member of just one project, and will not have a choice. If you are a member of multiple projects, be sure to select the correct project from the menu. In this example, we will refer to the project as DeterTest . Leave the Group field set to Default Group unless otherwise instructed. Enter the Name field with an easily identifiable name for the experiment. The Name should be a single word (no spaces) identifier. For this tutorial, use basic-experiment . This is also known as your experiment name or Experiment ID (EID) and is used as an argument in many commands. Enter the Description field with a brief description of the experiment. In the Your NS File field, enter the local path to the basic.ns file you downloaded. This file will be uploaded through your browser when you choose \"Submit.\" The rest of the settings depend on the goals of your experiment. In this case, you may simply set the Idle Swap field to 1 h and leave the rest of the settings for Swapping , Linktest Option , and BatchMode at their default for now. Check the Swap In Immediately box to start your lab now. If you did not check this box, you would follow the directions for [starting an experiment] to allocate resources later. Click Submit . After submission, DETERLab will begin processing your request. This process can take several minutes, depending on how large your topology is, and what other features (such as delay nodes and bandwidth limits) you are using. While you are waiting, you may watch the swap in process displayed in your web browser. Assuming all goes well, you will receive an email message indicating success or failure, and if successful, a listing of the nodes and IP address that were allocated to your experiment. For the NS file in this example, you should receive a listing that looks similar to this: Experiment: DeterTest/basic-experiment State: swapped Virtual Node Info: ID Type OS Qualified Name --------------- ------------ --------------- -------------------- nodeA pc FBSD11-STD nodeA.basic-experiment.DeterTest.isi.deterlab.net nodeB pc nodeB.basic-experiment.DeterTest.isi.deterlab.net nodeC pc Ubuntu1604-STD nodeC.basic-experiment.DeterTest.isi.deterlab.net nodeD pc nodeD.basic-experiment.DeterTest.isi.deterlab.net Virtual Lan/Link Info: ID Member/Proto IP/Mask Delay BW (Kbs) Loss Rate --------------- --------------- --------------- --------- --------- --------- lan0 nodeB:1 10.1.2.4 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 lan0 nodeC:0 10.1.2.3 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 lan0 nodeD:0 10.1.2.2 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 link0 nodeA:0 10.1.1.3 25.00 30000 0.00501256 ethernet 255.255.255.0 25.00 30000 0.00501256 link0 nodeB:0 10.1.1.2 25.00 30000 0.00501256 ethernet 255.255.255.0 25.00 30000 0.00501256 Virtual Queue Info: ID Member Q Limit Type weight/min_th/max_th/linterm --------------- --------------- ---------- ------- ---------------------------- lan0 nodeB:1 100 slots Tail 0/0/0/0 lan0 nodeC:0 100 slots Tail 0/0/0/0 lan0 nodeD:0 100 slots Tail 0/0/0/0 link0 nodeA:0 100 slots Tail 0/0/0/0 link0 nodeB:0 100 slots Tail 0/0/0/0 Event Groups: Group Name Members --------------- --------------------------------------------------------------- link0-tracemon link0-nodeB-tracemon,link0-nodeA-tracemon __all_lans lan0,link0 __all_tracemon link0-nodeB-tracemon,link0-nodeA-tracemon,lan0-nodeD-tracemon,lan0-nodeC-tracemon,lan0-nodeB-tracemon lan0-tracemon lan0-nodeB-tracemon,lan0-nodeC-tracemon,lan0-nodeD-tracemon Here is a breakdown of the results: * A single delay node was allocated and inserted into the link between nodeA and nodeB . This link is invisible from your perspective, except for the fact that it adds latency, error, or reduced bandwidth. However, the information for the delay links are included so that you can modify the delay parameters after the experiment has been created (Note that you cannot convert a non-shaped link into a shaped link; you can only modify the traffic shaping parameters of a link that is already being shaped). [[BR]] * Delays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. A delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other. [[BR]] * Each link in the Virtual Lan/Link section has its delay, etc., split between two entries. One is for traffic coming into the link from the node, and the other is for traffic leaving the link to the node. In the case of links, the four entries often get optimized to two entries in a Physical Lan/Link section. [[BR]] * The names in the Qualified Name column refer to the control network interfaces for each of your allocated nodes. These names are added to the DETERLab nameserver map on the fly, and are immediately available for you to use so that you do not have to worry about the actual physical node names that were chosen. In the names listed above, DeterTest is the name of the project that you chose to work in, and basic-experiment is the name of the experiment that you provided on the Begin an Experiment page. [[BR]] * Please don't use the Qualified Name from within nodes in your experiment, since it will contact them over the control network, bypassing the link shaping we configured. Starting an experiment (Swap-in) \u00b6 If you want to go back to an existing experiment to start it and swap-in (allocate resources): Go to your dashboard by clicking the My DETERLab link in the top menu. In the Current Experiments table, click on the EID (Experiment ID) of the experiment you want to start. In the left sidebar, click Swap Experiment In , then click Confirm . The swap in process will take 5 to 10 minutes; you will receive an email notification when it is complete. While you are waiting, you can watch the swap in process displayed in your web browser. Step 3: Access nodes in your lab environment \u00b6 To access your experimental nodes, you'll need to first SSH into users.deterlab.net using your DETERLab username and password. Once you log in to users , you'll need to SSH again to your actual experimental nodes. Since your nodes addresses may change every time you swap them in, it's best to SSH to the permanent network names of the nodes. As we mentioned in the previous step, the Qualified Names are included in the output after the experiment is swapped in. Here is another way to find them after swap-in: a. Navigate to the experiment you just created in the web interface . This location is usually called the experiment page . * If you just swapped in your experiment, the quickest way to find your node names is to click on the experiment name in the table under Swap Control . * You can also get there by clicking My DETERLab in the top navigation. Your experiment is listed as \"active\" in the State column. Click on the experiment's name in the EID column to display the experiment page.. b. Click on the Details tab . * Your nodes' network names are listed under the heading Qualified Name . For example, node1.basic-experiment.DeterTest.isi.deterlab.net . * You should familiarize yourself with the information available on this page, but for now we just need to know the long DNS qualified name(s) node(s) you just swapped in. * If you are curious, you should also look at the Settings (generic info), Visualization , and NS File tabs. (The topology mapplet may be disabled for some labs, so these last two may not be visible). c. SSH from users to your experimental nodes by running a command with the following syntax : ssh node1.ExperimentName.ProjectName.isi.deterlab.net * You will not need to re-authenticate. * You may need to wait a few more minutes. Once DETERLab is finished setting up the experiment, the nodes still need a minute or two to boot and complete their configuration. If you get a message about \"server configuration\" when you try to log in, wait a few minutes and try again. d. If you need to create new users on your experimental nodes, you may log in as them by running the following from the experimental node: ssh newuser@node1.basicExp.ProjectName.isi.deterlab.net or ssh newuser@localhost Step 4: View results and modify the experiment \u00b6 You can visualize the experiment by going to your experiment page (from My DETERLab, click the EID link for your experiment) and clicking the Visualization tab. From this page you can also change the NS file by clicking on the NS File tab or modify parameters by clicking Modify Traffic Shaping in the left sidebar. An alternative method is to log into users.deterlab.net and use the delay_config program. This program requires that you know the symbolic names of the individual links. This information is available on the experiment page. Step 5: Configure and run your experiment. \u00b6 Once you have all link modifications to your liking, you now need to install any additional tools you need (tools not included in the OS images you chose in Step 1), configure your tools and coordinate these tools to create events in your experiment. For simple experiments, installation, configuration and triggering events can be done by hand or through small scripts. To accomplish this, log into your machines (see Step 3), perform the OS-specific steps needed to install and configure your tools, and run these tools by hand or through scripts, such as shell scripts or remote scripts such as Fabric-based scripts http://www.fabfile.org . For more complicated experiments, you may need more automated ways to install and configure tools as well as coordinate events within your experiment. For fine-grained control over events and event triggers, see the MAGI Orchestrator . A large part of many experiments is traffic generation: the generation and modulation of packets on experiment links. Tools for such generation include the MAGI Orchestrator and LegoTG , as well as many other possibilities . Step 6: Save your work and swap-out (release resources) \u00b6 When you are done working with your nodes, it is best practice to save your work and swap out the experiment so that other users have access to the physical machines. Saving and securing your files on DETERLab \u00b6 Every user on DETERLab has a home directory on users.deterlab.net which is mounted via NFS to experimental nodes. This means that anything you place in your home directory on one experimental node (or the users machine) is visible in your home directory on your other experimental nodes. Your home directory is private and will not be overwritten, so you may save your work in that directory. However, everything else on experimental nodes is permanently lost when an experiment is swapped out. Remember: Make sure you save your work in your home directory before swapping out your experiment! Another place you may save your files would be /proj/YourProject . This directory is also NFS-mounted to all experimental nodes, so the same rules apply about writing to it a lot, as for your home directory. It is shared by all members of your project/class. Again, on DETERLab, files ARE NOT SAVED between swap-ins. Additionally, experiments may be forcibly swapped out after a certain number of idle hours (or some maximum amount of time). You must manually save copies of any files you want to keep in your home directory. Any files left elsewhere on the experimental nodes will be erased and lost forever. This means that if you want to store progress for a lab and come back to it later, you will need to put it in your home directory before swapping out the experiment. Swap Out vs Terminate \u00b6 When to Swap Out When you are done with your experiment for the time being, make sure you save your work into an appropriate location and then swap out your experiment. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you are taking a break from the work, but coming back later. To do this, click Swap Experiment Out link on the experiment page. This allows the resources to be de-allocated so that someone else can use them. When to Terminate When you are completely finished with your experiment and have no intention of running it again, use the Terminate Experiment link in the sidebar of the experiment page. Be careful: termination will erase the experiment and you won't be able to swap it back in without recreating it. DETERLab will then tear down your experiment, and send you an email message when the process is complete. At this point you are allowed to reuse the experiment name (say, if you wanted to create a similar experiment with different parameters). Terminating says \"I won't need this experiment ever again.\" Just remember to Swap In/Out, and never \"Terminate\" unless you're sure you're completely done with the experiment. If you do end up terminating an experiment, you can always go back and recreate it. Scheduling experiment swapout/termination \u00b6 If you expect that your experiment should run for a set period of time, but you will not be around to terminate or swap the experiment out, then you should use the scheduled swapout/termination feature. This allows you to specify a maximum running time in your NS file so that you will not hold scarce resources when you are offline. To schedule a swapout or termination in your NS file: $ns at 2000.0 \"$ns terminate\" or $ns at 2000.0 \"$ns swapout\" This will cause your experiment to either be terminated or swapped out after 2000 seconds of wallclock time. Why can't I log in to DETERLab? \u00b6 DETERLab has an automatic blacklist mechanism. If you enter the wrong username and password combination too many times, your account will no longer be accessible from your current IP address. If you think that this has happened to you, try logging in from another address (if you know how), or create an issue (see Getting Help ), which will relay the request to the testbed-ops group that this specific blacklist entry should be erased. Installing RPMs automatically \u00b6 The DETERLab NS extension tb-set-node-rpms allows you to specify a (space-separated) list of RPMs to install on each of your nodes when it boots: tb-set-node-rpms $nodeA /proj/myproj/rpms/silly-freebsd.rpm tb-set-node-rpms $nodeB /proj/myproj/rpms/silly-linux.rpm tb-set-node-rpms $nodeC /proj/myproj/rpms/silly-windows.rpm The above NS code says to install the silly-freebsd.rpm file on nodeA , the silly-linux.rpm on nodeB , and the silly-windows.rpm on nodeC . RPMs are installed as root, and must reside in either the project's /proj directory, or if the experiment has been created in a subgroup, in the /groups directory. You may not place your RPMs in your home directory. Installing TAR files automatically \u00b6 The DETERLab NS extension tb-set-node-tarfiles allows you to specify a set of tarfiles to install on each of your nodes when it boots. While similar to the tb-set-node-rpms command, the format of this command is slightly different in that you must specify a directory in which to unpack the tar file. This avoids problems with having to specify absolute pathnames in your tarfile, which many modern tar programs balk at. tb-set-node-tarfiles $nodeA /usr/site /proj/projectName/tarfiles/silly.tar.gz The above NS code says to install the silly.tar.gz tar file on nodeA from the working directory /usr/site when the node first boots. The tarfile must reside in either the project's /proj directory, or if the experiment has been created in a subgroup, in the /groups directory. You may not place your tarfiles in your home directory. You may specify as many tarfiles as you wish, as long as each one is preceded by the directory it should be unpacked in, all separated by spaces. Starting your application automatically \u00b6 You may start your application automatically when your nodes boot for the first time (when an experiment is started or swapped in) by using the tb-set-node-startcmd NS extension. The argument is a command string (pathname of a script or program, plus arguments) that is run as the UID of the experiment creator, after the node has reached multiuser mode. The command is invoked using /bin/csh , and the working directory is undefined (your script should cd to the directory you need). You can specify the same program for each node, or a different program. For example: tb-set-node-startcmd $nodeA \"/proj/projectName/runme.nodeA\" tb-set-node-startcmd $nodeB \"/proj/projectName/runme.nodeB\" will run /proj/projectName/runme.nodeA on nodeA and /proj/projectName/runme.nodeB on nodeB. The programs must reside on the node's local filesystem, or in a directory that can be reached via NFS. This is either the project's /proj directory, in the /groups directory if the experiment has been created in a subgroup, or a project member's home directory in /users . If you need to see the output of your command, be sure to redirect the output into a file. You may place the file on the local node, or in one of the NFS mounted directories mentioned above. For example: tb-set-node-startcmd $nodeB \"/proj/myproj/runme >& /tmp/foo.log\" Note that the syntax and function of /bin/csh differs from other shells (including bash), specifically in redirection syntax. Be sure to use csh syntax or your start command will fail silently. The exit value of the start command is reported back to the Web Interface, and is made available to you via the experiment page. There is a listing for all of the nodes in the experiment, and the exit value is recorded in this listing. The special symbol none indicates that the node is still running the start command. Notifying the start program when all other nodes have started \u00b6 It is often necessary for your start program to determine when all of the other nodes in the experiment have started, and are ready to proceed. Sometimes called a barrier , this allows programs to wait at a specific point, and then all proceed at once. DETERLab provides a simple form of this mechanism using a synchronization server that runs on a node of your choice. Specify the node in your NS file: tb-set-sync-server $nodeB When nodeB boots, the synchronization server will automatically start. Your software can then synchronize using the emulab-sync program that is installed on your nodes. For example, your node start command might look like this: #!/bin/sh if [ \"$1\" = \"master\" ]; then /usr/testbed/bin/emulab-sync -i 4 else /usr/testbed/bin/emulab-sync fi /usr/site/bin/dosilly In this example, there are five nodes in the experiment, one of which must be configured to operate as the master, initializing the barrier to the number of clients (four in the above example) that are expected to rendezvous at the barrier. The master will by default wait for all of the clients to reach the barrier. Each client of the barrier also waits until all of the clients have reached the barrier (and of course, until the master initializes the barrier to the proper count). Any number of clients may be specified (any subset of nodes in your experiment can wait). If the master does not need to wait for the clients, you may use the async option which releases the master immediately: /usr/testbed/bin/emulab-sync -a -i 4 You may also specify the name of the barrier. /usr/testbed/bin/emulab-sync -a -i 4 -n mybarrier This allows multiple barriers to be in use at the same time. Scripts on nodeA and nodeB can be waiting on a barrier named \"foo\" while (other) scripts on nodeA and nodeC can be waiting on a barrier named \"bar.\" You may reuse an existing barrier (including the default barrier) once it has been released (all clients arrived and woken up). Setting up IP routing between nodes \u00b6 As DETER strives to make all aspects of the network controllable by the user, we do not attempt to impose any IP routing architecture or protocol by default. However, many users are more interested in end-to-end aspects and don't want to be bothered with setting up routes. For those users we provide an option to automatically set up routes on nodes which run one of our provided FreeBSD, Linux or Windows XP disk images. You can use the NS rtproto syntax in your NS file to enable routing: $ns rtproto protocolOption where the protocolOption is limited to one of Session , Static , Static-old , or Manual . Session routing provides fully automated routing support, and is implemented by enabling gated running of the OSPF protocol on all nodes in the experiment. This is not supported on Windows XP nodes. Static routing also provides automatic routing support, but rather than computing the routes dynamically, the routes are precomputed by a distributed route computation algorithm running in parallel on the experiment nodes. Static-old specifies use of the older centralized route computation algorithm, precomputing the nodes when the experiment is created, and then loading them onto each node when it boots. Manual routing allows you to explicitly specify per-node routing information in the NS file. To do this, use the Manual routing option to rtproto , followed by a list of routes using the add-route command: $node add-route $dst $nexthop where the dst can be either a node, a link, or a LAN. For example: $client add-route $server $router $client add-route [$ns link $server $router] $router $client add-route $serverlan $router Note that you would need a separate add-route command to establish a route for the reverse direction; thus allowing you to specify differing forward and reverse routes if so desired. These statements are converted into appropriate route(8) commands on your experimental nodes when they boot. In the above examples, the first form says to set up a manual route between $client and $server , using $router as the nexthop; $client and $router should be directly connected, and the interface on $server should be unambiguous; either directly connected to the router, or an edge node that has just a single interface. If the destination has multiple interfaces configured, and it is not connected directly to the nexthop, the interface that you are intending to route to is ambiguous. In the topology shown to the right, $nodeD has two interfaces configured. If you attempted to set up a route like this: $nodeA add-route $nodeD $nodeB you would receive an error since DETERLab staff would not easily be able to determine which of the two links on $nodeD you are referring to. Fortunately, there is an easy solution. Instead of a node, specify the link directly: $nodeA add-route [$ns link $nodeD $nodeC] $nodeB This tells us exactly which link you mean, enabling us to convert that information into a proper route command on $nodeA . The last form of the add-route command is used when adding a route to an entire LAN. It would be tedious and error prone to specify a route to each node in a LAN by hand. Instead, just route to the entire network: set clientlan [$ns make-lan \"$nodeE $nodeF $nodeG\" 100Mb 0ms] $nodeA add-route $clientlan $nodeB In general, it is still best practice to use either Session or Static routing for all but small, simple topologies. Explicitly setting up all the routes in even a moderately-sized experiment is extremely error prone. Consider this: a recently created experiment with 17 nodes and 10 subnets required 140 hand-created routes in the NS file . Two final, cautionary notes on routing: * The default route must be set to use the control network interface. You might be tempted to set the default route on your nodes to reduce the number of explicit routes used. Please avoid this. That would prevent nodes from contacting the outside world, i.e., you. * If you use your own routing daemon, you must avoid using the control network interface in the configuration. Since every node in the testbed is directly connected to the control network LAN, a naive routing daemon configuration will discover that any node is just one hop away, via the control network, from any other node and all inter-node traffic will be routed via that interface.","title":"Core Guide"},{"location":"core/core-guide/#core-guide","text":"In this tutorial we begin with a small 3-5 node experiment, so that you will become familiar with NS syntax and the practical aspects of DETERLab operation. Usually, you will want to incorporate another system such as the MAGI Orchestrator for more fully fleshed out experiments. But this is a good starting point for those new to DETERLab. Note If you are a student, go to the http://education.deterlab.net site for classroom-specific instructions.","title":"Core Guide"},{"location":"core/core-guide/#node-use-policy","text":"Please make sure to read our guidelines for using nodes in DETERLab . These guidelines help keep DETERLab an effective environment for all users.","title":"Node Use Policy"},{"location":"core/core-guide/#deterlab-environment","text":"Your experiment is made up of one or more machines on the internal DETERLab network, which is behind a firewall. users.deterlab.net (or users for short) is the \"control server\" for DETERLab. From users , you can contact all your nodes, reboot them, connect to their serial ports, etc. Each user has a home directory on this server and you may SSH into it with your username and password for your DETERLab account. myboss.isi.deterlab.net ( or boss for short) is the main testbed server that runs DETERLab. Users are not allowed to log directly into it.","title":"DETERLab Environment"},{"location":"core/core-guide/#basic-tutorial","text":"","title":"Basic Tutorial"},{"location":"core/core-guide/#getting-started","text":"Work in DETERLab is organized by experiments within projects . Each project is created and managed by a leader - usually the Principal Investigator (PI) of a research project or the instructor of a class on cybersecurity. The project leader then invites members to join by providing them with the project name and sending them the link to the 'Join a Project' page. Before you can take the following tutorial, you need an active account in a project in DETERLab. See How to Register to make sure if you're qualified, and then follow the directions to create a project or ask to join an existing project - if you go through either process for the first time, your account is created as a result. If you already have an account, proceed to the next step.","title":"Getting Started"},{"location":"core/core-guide/#step-1-design-the-topology","text":"Part of DETERLab's power lies in its ability to assume many different topologies; the description of a such a topology is a necessary part of an experiment. Before you can start your experiment, you must model the elements of the experiment's network with a topology. For this basic tutorial, use this NS file which includes a simple topology and save it to a directory called basicExp in your local directory on users.deterlab.net . The rest of this section describes NS format and walks you through the different parts of the sample file.","title":"Step 1: Design the topology"},{"location":"core/core-guide/#ns-format","text":"DETERLab uses the \"NS\" (\"Network Simulator\") format to describe network topologies. This is substantially the same Tcl-based format used by ns-2 . Since DETERLab offers emulation, rather than simulation, these files are interpreted in a somewhat different manner than ns-2. Therefore, some ns-2 functionality may work differently than you expect, or may not be implemented at all. Please look for warnings of the form: *** WARNING: Unsupported NS Statement! Link type BAZ, using DropTail! If you feel there is useful functionality missing, please let us know . Also, some testbed-specific syntax has been added, which, with the inclusion of the compatibility module tb_compat.tcl , will be ignored by the NS simulator. This allows the same NS file to work on both DETERLab and ns-2, most of the time.","title":"NS Format"},{"location":"core/core-guide/#ns-example","text":"In our example, we are creating a test network which looks like the following: Figure 1: A is connected to B, and B to C and D with a LAN. Here's how to describe this topology: Declare a simulator and include a file that allows you to use the special tb- commands. First off, all NS files start with a simple prologue, declaring a simulator and including a file that allows you to use the special tb- commands: # This is a simple ns script. Comments start with #. set ns [new Simulator] source tb_compat.tcl Define the 4 nodes in the topology. set nodeA [$ns node] set nodeB [$ns node] set nodeC [$ns node] set nodeD [$ns node] nodeA and so on are the virtual names ( vnames ) of the nodes in your topology. When your experiment is swapped in (has allocated resources), they will be assigned to physical node names like pc45 , probably different ones each time. NOTE: Avoid vnames that clash with the physical node names in the testbed.** Define the link and the LAN that connect the nodes. NS syntax permits you to specify the bandwidth, latency, and queue type. Note that since NS can't impose artificial losses like DETERLab can, we use a separate tb- command to add loss on a link. For our example, we will define a full speed LAN between B, C, and D, and a shaped link from node A to B. set link0 [$ns duplex-link $nodeB $nodeA 30Mb 50ms DropTail] tb-set-link-loss $link0 0.01 set lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms] In addition to the standard NS syntax above, a number of extensions are available in DETERLab that allow you to better control your experiment. For example, you may specify what Operating System is booted on your nodes. For the versions of FreeBSD, Linux, and Windows we currently support, please refer to the Operating System Images page. Click List ImageIDs in the DETERLab web interface Interaction pane to see the current list of DETERLab-supplied operating systems. By default, our most recent Linux image is selected. tb-set-node-os $nodeA FBSD11-STD tb-set-node-os $nodeC Ubuntu1604-STD tb-set-node-os $nodeC WINXP-UPDATE Enable routing. In a topology like this, you will likely want to communicate between all the nodes, including nodes that aren't directly connected, like A and C . In order for that to happen, we must enable routing in our experiment, so B can route packets for the other nodes. The typical way to do this is with Static routing. (Other options are detailed in the Routing section below ). $ns rtproto Static End with an epilogue that instructs the simulator to start. # Go! $ns run","title":"NS Example"},{"location":"core/core-guide/#step-2-create-a-new-experiment","text":"For this tutorial, we will use the web interface to create a new experiment. You could also use the DETERLab Shell Commands . Log into DETERLab with your account credentials (see How to Register ). Click the Experimentation menu item, then click Begin an Experiment . Click Select Project and choose your project. This is also know as your project name or Project ID (PID). This is used as an argument in many commands. Most people will be a member of just one project, and will not have a choice. If you are a member of multiple projects, be sure to select the correct project from the menu. In this example, we will refer to the project as DeterTest . Leave the Group field set to Default Group unless otherwise instructed. Enter the Name field with an easily identifiable name for the experiment. The Name should be a single word (no spaces) identifier. For this tutorial, use basic-experiment . This is also known as your experiment name or Experiment ID (EID) and is used as an argument in many commands. Enter the Description field with a brief description of the experiment. In the Your NS File field, enter the local path to the basic.ns file you downloaded. This file will be uploaded through your browser when you choose \"Submit.\" The rest of the settings depend on the goals of your experiment. In this case, you may simply set the Idle Swap field to 1 h and leave the rest of the settings for Swapping , Linktest Option , and BatchMode at their default for now. Check the Swap In Immediately box to start your lab now. If you did not check this box, you would follow the directions for [starting an experiment] to allocate resources later. Click Submit . After submission, DETERLab will begin processing your request. This process can take several minutes, depending on how large your topology is, and what other features (such as delay nodes and bandwidth limits) you are using. While you are waiting, you may watch the swap in process displayed in your web browser. Assuming all goes well, you will receive an email message indicating success or failure, and if successful, a listing of the nodes and IP address that were allocated to your experiment. For the NS file in this example, you should receive a listing that looks similar to this: Experiment: DeterTest/basic-experiment State: swapped Virtual Node Info: ID Type OS Qualified Name --------------- ------------ --------------- -------------------- nodeA pc FBSD11-STD nodeA.basic-experiment.DeterTest.isi.deterlab.net nodeB pc nodeB.basic-experiment.DeterTest.isi.deterlab.net nodeC pc Ubuntu1604-STD nodeC.basic-experiment.DeterTest.isi.deterlab.net nodeD pc nodeD.basic-experiment.DeterTest.isi.deterlab.net Virtual Lan/Link Info: ID Member/Proto IP/Mask Delay BW (Kbs) Loss Rate --------------- --------------- --------------- --------- --------- --------- lan0 nodeB:1 10.1.2.4 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 lan0 nodeC:0 10.1.2.3 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 lan0 nodeD:0 10.1.2.2 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 link0 nodeA:0 10.1.1.3 25.00 30000 0.00501256 ethernet 255.255.255.0 25.00 30000 0.00501256 link0 nodeB:0 10.1.1.2 25.00 30000 0.00501256 ethernet 255.255.255.0 25.00 30000 0.00501256 Virtual Queue Info: ID Member Q Limit Type weight/min_th/max_th/linterm --------------- --------------- ---------- ------- ---------------------------- lan0 nodeB:1 100 slots Tail 0/0/0/0 lan0 nodeC:0 100 slots Tail 0/0/0/0 lan0 nodeD:0 100 slots Tail 0/0/0/0 link0 nodeA:0 100 slots Tail 0/0/0/0 link0 nodeB:0 100 slots Tail 0/0/0/0 Event Groups: Group Name Members --------------- --------------------------------------------------------------- link0-tracemon link0-nodeB-tracemon,link0-nodeA-tracemon __all_lans lan0,link0 __all_tracemon link0-nodeB-tracemon,link0-nodeA-tracemon,lan0-nodeD-tracemon,lan0-nodeC-tracemon,lan0-nodeB-tracemon lan0-tracemon lan0-nodeB-tracemon,lan0-nodeC-tracemon,lan0-nodeD-tracemon Here is a breakdown of the results: * A single delay node was allocated and inserted into the link between nodeA and nodeB . This link is invisible from your perspective, except for the fact that it adds latency, error, or reduced bandwidth. However, the information for the delay links are included so that you can modify the delay parameters after the experiment has been created (Note that you cannot convert a non-shaped link into a shaped link; you can only modify the traffic shaping parameters of a link that is already being shaped). [[BR]] * Delays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. A delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other. [[BR]] * Each link in the Virtual Lan/Link section has its delay, etc., split between two entries. One is for traffic coming into the link from the node, and the other is for traffic leaving the link to the node. In the case of links, the four entries often get optimized to two entries in a Physical Lan/Link section. [[BR]] * The names in the Qualified Name column refer to the control network interfaces for each of your allocated nodes. These names are added to the DETERLab nameserver map on the fly, and are immediately available for you to use so that you do not have to worry about the actual physical node names that were chosen. In the names listed above, DeterTest is the name of the project that you chose to work in, and basic-experiment is the name of the experiment that you provided on the Begin an Experiment page. [[BR]] * Please don't use the Qualified Name from within nodes in your experiment, since it will contact them over the control network, bypassing the link shaping we configured.","title":"Step 2: Create a new experiment"},{"location":"core/core-guide/#starting-an-experiment-swap-in","text":"If you want to go back to an existing experiment to start it and swap-in (allocate resources): Go to your dashboard by clicking the My DETERLab link in the top menu. In the Current Experiments table, click on the EID (Experiment ID) of the experiment you want to start. In the left sidebar, click Swap Experiment In , then click Confirm . The swap in process will take 5 to 10 minutes; you will receive an email notification when it is complete. While you are waiting, you can watch the swap in process displayed in your web browser.","title":"Starting an experiment (Swap-in)"},{"location":"core/core-guide/#step-3-access-nodes-in-your-lab-environment","text":"To access your experimental nodes, you'll need to first SSH into users.deterlab.net using your DETERLab username and password. Once you log in to users , you'll need to SSH again to your actual experimental nodes. Since your nodes addresses may change every time you swap them in, it's best to SSH to the permanent network names of the nodes. As we mentioned in the previous step, the Qualified Names are included in the output after the experiment is swapped in. Here is another way to find them after swap-in: a. Navigate to the experiment you just created in the web interface . This location is usually called the experiment page . * If you just swapped in your experiment, the quickest way to find your node names is to click on the experiment name in the table under Swap Control . * You can also get there by clicking My DETERLab in the top navigation. Your experiment is listed as \"active\" in the State column. Click on the experiment's name in the EID column to display the experiment page.. b. Click on the Details tab . * Your nodes' network names are listed under the heading Qualified Name . For example, node1.basic-experiment.DeterTest.isi.deterlab.net . * You should familiarize yourself with the information available on this page, but for now we just need to know the long DNS qualified name(s) node(s) you just swapped in. * If you are curious, you should also look at the Settings (generic info), Visualization , and NS File tabs. (The topology mapplet may be disabled for some labs, so these last two may not be visible). c. SSH from users to your experimental nodes by running a command with the following syntax : ssh node1.ExperimentName.ProjectName.isi.deterlab.net * You will not need to re-authenticate. * You may need to wait a few more minutes. Once DETERLab is finished setting up the experiment, the nodes still need a minute or two to boot and complete their configuration. If you get a message about \"server configuration\" when you try to log in, wait a few minutes and try again. d. If you need to create new users on your experimental nodes, you may log in as them by running the following from the experimental node: ssh newuser@node1.basicExp.ProjectName.isi.deterlab.net or ssh newuser@localhost","title":"Step 3: Access nodes in your lab environment"},{"location":"core/core-guide/#step-4-view-results-and-modify-the-experiment","text":"You can visualize the experiment by going to your experiment page (from My DETERLab, click the EID link for your experiment) and clicking the Visualization tab. From this page you can also change the NS file by clicking on the NS File tab or modify parameters by clicking Modify Traffic Shaping in the left sidebar. An alternative method is to log into users.deterlab.net and use the delay_config program. This program requires that you know the symbolic names of the individual links. This information is available on the experiment page.","title":"Step 4: View results and modify the experiment"},{"location":"core/core-guide/#step-5-configure-and-run-your-experiment","text":"Once you have all link modifications to your liking, you now need to install any additional tools you need (tools not included in the OS images you chose in Step 1), configure your tools and coordinate these tools to create events in your experiment. For simple experiments, installation, configuration and triggering events can be done by hand or through small scripts. To accomplish this, log into your machines (see Step 3), perform the OS-specific steps needed to install and configure your tools, and run these tools by hand or through scripts, such as shell scripts or remote scripts such as Fabric-based scripts http://www.fabfile.org . For more complicated experiments, you may need more automated ways to install and configure tools as well as coordinate events within your experiment. For fine-grained control over events and event triggers, see the MAGI Orchestrator . A large part of many experiments is traffic generation: the generation and modulation of packets on experiment links. Tools for such generation include the MAGI Orchestrator and LegoTG , as well as many other possibilities .","title":"Step 5: Configure and run your experiment."},{"location":"core/core-guide/#step-6-save-your-work-and-swap-out-release-resources","text":"When you are done working with your nodes, it is best practice to save your work and swap out the experiment so that other users have access to the physical machines.","title":"Step 6: Save your work and swap-out (release resources)"},{"location":"core/core-guide/#saving-and-securing-your-files-on-deterlab","text":"Every user on DETERLab has a home directory on users.deterlab.net which is mounted via NFS to experimental nodes. This means that anything you place in your home directory on one experimental node (or the users machine) is visible in your home directory on your other experimental nodes. Your home directory is private and will not be overwritten, so you may save your work in that directory. However, everything else on experimental nodes is permanently lost when an experiment is swapped out. Remember: Make sure you save your work in your home directory before swapping out your experiment! Another place you may save your files would be /proj/YourProject . This directory is also NFS-mounted to all experimental nodes, so the same rules apply about writing to it a lot, as for your home directory. It is shared by all members of your project/class. Again, on DETERLab, files ARE NOT SAVED between swap-ins. Additionally, experiments may be forcibly swapped out after a certain number of idle hours (or some maximum amount of time). You must manually save copies of any files you want to keep in your home directory. Any files left elsewhere on the experimental nodes will be erased and lost forever. This means that if you want to store progress for a lab and come back to it later, you will need to put it in your home directory before swapping out the experiment.","title":"Saving and securing your files on DETERLab"},{"location":"core/core-guide/#swap-out-vs-terminate","text":"When to Swap Out When you are done with your experiment for the time being, make sure you save your work into an appropriate location and then swap out your experiment. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you are taking a break from the work, but coming back later. To do this, click Swap Experiment Out link on the experiment page. This allows the resources to be de-allocated so that someone else can use them. When to Terminate When you are completely finished with your experiment and have no intention of running it again, use the Terminate Experiment link in the sidebar of the experiment page. Be careful: termination will erase the experiment and you won't be able to swap it back in without recreating it. DETERLab will then tear down your experiment, and send you an email message when the process is complete. At this point you are allowed to reuse the experiment name (say, if you wanted to create a similar experiment with different parameters). Terminating says \"I won't need this experiment ever again.\" Just remember to Swap In/Out, and never \"Terminate\" unless you're sure you're completely done with the experiment. If you do end up terminating an experiment, you can always go back and recreate it.","title":"Swap Out vs Terminate"},{"location":"core/core-guide/#scheduling-experiment-swapouttermination","text":"If you expect that your experiment should run for a set period of time, but you will not be around to terminate or swap the experiment out, then you should use the scheduled swapout/termination feature. This allows you to specify a maximum running time in your NS file so that you will not hold scarce resources when you are offline. To schedule a swapout or termination in your NS file: $ns at 2000.0 \"$ns terminate\" or $ns at 2000.0 \"$ns swapout\" This will cause your experiment to either be terminated or swapped out after 2000 seconds of wallclock time.","title":"Scheduling experiment swapout/termination"},{"location":"core/core-guide/#why-cant-i-log-in-to-deterlab","text":"DETERLab has an automatic blacklist mechanism. If you enter the wrong username and password combination too many times, your account will no longer be accessible from your current IP address. If you think that this has happened to you, try logging in from another address (if you know how), or create an issue (see Getting Help ), which will relay the request to the testbed-ops group that this specific blacklist entry should be erased.","title":"Why can't I log in to DETERLab?"},{"location":"core/core-guide/#installing-rpms-automatically","text":"The DETERLab NS extension tb-set-node-rpms allows you to specify a (space-separated) list of RPMs to install on each of your nodes when it boots: tb-set-node-rpms $nodeA /proj/myproj/rpms/silly-freebsd.rpm tb-set-node-rpms $nodeB /proj/myproj/rpms/silly-linux.rpm tb-set-node-rpms $nodeC /proj/myproj/rpms/silly-windows.rpm The above NS code says to install the silly-freebsd.rpm file on nodeA , the silly-linux.rpm on nodeB , and the silly-windows.rpm on nodeC . RPMs are installed as root, and must reside in either the project's /proj directory, or if the experiment has been created in a subgroup, in the /groups directory. You may not place your RPMs in your home directory.","title":"Installing RPMs automatically"},{"location":"core/core-guide/#installing-tar-files-automatically","text":"The DETERLab NS extension tb-set-node-tarfiles allows you to specify a set of tarfiles to install on each of your nodes when it boots. While similar to the tb-set-node-rpms command, the format of this command is slightly different in that you must specify a directory in which to unpack the tar file. This avoids problems with having to specify absolute pathnames in your tarfile, which many modern tar programs balk at. tb-set-node-tarfiles $nodeA /usr/site /proj/projectName/tarfiles/silly.tar.gz The above NS code says to install the silly.tar.gz tar file on nodeA from the working directory /usr/site when the node first boots. The tarfile must reside in either the project's /proj directory, or if the experiment has been created in a subgroup, in the /groups directory. You may not place your tarfiles in your home directory. You may specify as many tarfiles as you wish, as long as each one is preceded by the directory it should be unpacked in, all separated by spaces.","title":"Installing TAR files automatically"},{"location":"core/core-guide/#starting-your-application-automatically","text":"You may start your application automatically when your nodes boot for the first time (when an experiment is started or swapped in) by using the tb-set-node-startcmd NS extension. The argument is a command string (pathname of a script or program, plus arguments) that is run as the UID of the experiment creator, after the node has reached multiuser mode. The command is invoked using /bin/csh , and the working directory is undefined (your script should cd to the directory you need). You can specify the same program for each node, or a different program. For example: tb-set-node-startcmd $nodeA \"/proj/projectName/runme.nodeA\" tb-set-node-startcmd $nodeB \"/proj/projectName/runme.nodeB\" will run /proj/projectName/runme.nodeA on nodeA and /proj/projectName/runme.nodeB on nodeB. The programs must reside on the node's local filesystem, or in a directory that can be reached via NFS. This is either the project's /proj directory, in the /groups directory if the experiment has been created in a subgroup, or a project member's home directory in /users . If you need to see the output of your command, be sure to redirect the output into a file. You may place the file on the local node, or in one of the NFS mounted directories mentioned above. For example: tb-set-node-startcmd $nodeB \"/proj/myproj/runme >& /tmp/foo.log\" Note that the syntax and function of /bin/csh differs from other shells (including bash), specifically in redirection syntax. Be sure to use csh syntax or your start command will fail silently. The exit value of the start command is reported back to the Web Interface, and is made available to you via the experiment page. There is a listing for all of the nodes in the experiment, and the exit value is recorded in this listing. The special symbol none indicates that the node is still running the start command.","title":"Starting your application automatically"},{"location":"core/core-guide/#notifying-the-start-program-when-all-other-nodes-have-started","text":"It is often necessary for your start program to determine when all of the other nodes in the experiment have started, and are ready to proceed. Sometimes called a barrier , this allows programs to wait at a specific point, and then all proceed at once. DETERLab provides a simple form of this mechanism using a synchronization server that runs on a node of your choice. Specify the node in your NS file: tb-set-sync-server $nodeB When nodeB boots, the synchronization server will automatically start. Your software can then synchronize using the emulab-sync program that is installed on your nodes. For example, your node start command might look like this: #!/bin/sh if [ \"$1\" = \"master\" ]; then /usr/testbed/bin/emulab-sync -i 4 else /usr/testbed/bin/emulab-sync fi /usr/site/bin/dosilly In this example, there are five nodes in the experiment, one of which must be configured to operate as the master, initializing the barrier to the number of clients (four in the above example) that are expected to rendezvous at the barrier. The master will by default wait for all of the clients to reach the barrier. Each client of the barrier also waits until all of the clients have reached the barrier (and of course, until the master initializes the barrier to the proper count). Any number of clients may be specified (any subset of nodes in your experiment can wait). If the master does not need to wait for the clients, you may use the async option which releases the master immediately: /usr/testbed/bin/emulab-sync -a -i 4 You may also specify the name of the barrier. /usr/testbed/bin/emulab-sync -a -i 4 -n mybarrier This allows multiple barriers to be in use at the same time. Scripts on nodeA and nodeB can be waiting on a barrier named \"foo\" while (other) scripts on nodeA and nodeC can be waiting on a barrier named \"bar.\" You may reuse an existing barrier (including the default barrier) once it has been released (all clients arrived and woken up).","title":"Notifying the start program when all other nodes have started"},{"location":"core/core-guide/#setting-up-ip-routing-between-nodes","text":"As DETER strives to make all aspects of the network controllable by the user, we do not attempt to impose any IP routing architecture or protocol by default. However, many users are more interested in end-to-end aspects and don't want to be bothered with setting up routes. For those users we provide an option to automatically set up routes on nodes which run one of our provided FreeBSD, Linux or Windows XP disk images. You can use the NS rtproto syntax in your NS file to enable routing: $ns rtproto protocolOption where the protocolOption is limited to one of Session , Static , Static-old , or Manual . Session routing provides fully automated routing support, and is implemented by enabling gated running of the OSPF protocol on all nodes in the experiment. This is not supported on Windows XP nodes. Static routing also provides automatic routing support, but rather than computing the routes dynamically, the routes are precomputed by a distributed route computation algorithm running in parallel on the experiment nodes. Static-old specifies use of the older centralized route computation algorithm, precomputing the nodes when the experiment is created, and then loading them onto each node when it boots. Manual routing allows you to explicitly specify per-node routing information in the NS file. To do this, use the Manual routing option to rtproto , followed by a list of routes using the add-route command: $node add-route $dst $nexthop where the dst can be either a node, a link, or a LAN. For example: $client add-route $server $router $client add-route [$ns link $server $router] $router $client add-route $serverlan $router Note that you would need a separate add-route command to establish a route for the reverse direction; thus allowing you to specify differing forward and reverse routes if so desired. These statements are converted into appropriate route(8) commands on your experimental nodes when they boot. In the above examples, the first form says to set up a manual route between $client and $server , using $router as the nexthop; $client and $router should be directly connected, and the interface on $server should be unambiguous; either directly connected to the router, or an edge node that has just a single interface. If the destination has multiple interfaces configured, and it is not connected directly to the nexthop, the interface that you are intending to route to is ambiguous. In the topology shown to the right, $nodeD has two interfaces configured. If you attempted to set up a route like this: $nodeA add-route $nodeD $nodeB you would receive an error since DETERLab staff would not easily be able to determine which of the two links on $nodeD you are referring to. Fortunately, there is an easy solution. Instead of a node, specify the link directly: $nodeA add-route [$ns link $nodeD $nodeC] $nodeB This tells us exactly which link you mean, enabling us to convert that information into a proper route command on $nodeA . The last form of the add-route command is used when adding a route to an entire LAN. It would be tedious and error prone to specify a route to each node in a LAN by hand. Instead, just route to the entire network: set clientlan [$ns make-lan \"$nodeE $nodeF $nodeG\" 100Mb 0ms] $nodeA add-route $clientlan $nodeB In general, it is still best practice to use either Session or Static routing for all but small, simple topologies. Explicitly setting up all the routes in even a moderately-sized experiment is extremely error prone. Consider this: a recently created experiment with 17 nodes and 10 subnets required 140 hand-created routes in the NS file . Two final, cautionary notes on routing: * The default route must be set to use the control network interface. You might be tempted to set the default route on your nodes to reduce the number of explicit routes used. Please avoid this. That would prevent nodes from contacting the outside world, i.e., you. * If you use your own routing daemon, you must avoid using the control network interface in the configuration. Since every node in the testbed is directly connected to the control network LAN, a naive routing daemon configuration will discover that any node is just one hop away, via the control network, from any other node and all inter-node traffic will be routed via that interface.","title":" Setting up IP routing between nodes"},{"location":"core/core-quickstart/","text":"Core Quickstart \u00b6 This page describes basic information about DETERLab and its core functionality. What is DETERLab? \u00b6 The DETERLab testbed is a security and education-enhanced version of Emulab. Funded by the National Science Foundation and the Department of Homeland Security, DETERLab is hosted by USC/ISI and UC Berkeley. DETERLab (like Emulab) offers user accounts with assorted permissions associated with different experiment groups. Each group can have its own pre-configured experimental environments running on Linux, BSD, Windows, or other operating systems. Users running DETERLab experiments have full control of real hardware and networks running preconfigured software packages. How does it work? \u00b6 The software running DETERLab loads operating system images (low level disk copies) onto free nodes in the testbed, and then reconfigures programmable switches to create VLANs with the newly-imaged nodes connected according to the topology specified by the experiment creator. After the system is fully imaged and configured, DETERLab executes specified scripts, unpacks tarballs, and/or installs RPM files according to the experiment's configuration. The end result is a live network of real machines, accessible via the Internet. Work in DETERLab is based on projects that include individual experiments and is accomplished either via the browser-based web interface (isi.deterlab.net) or via commandline on the DETERLab nodes. To access DETERLab, you need to create an account, which provides credentials for accessing both the web interface and nodes. How do I get a DETERLab account? \u00b6 You may obtain a DETERLab account by either starting a new project (if you are a PI or instructor) or joining an existing project (if you are a project member or a student). If you are the project investigator or instructor, you must create a project and invite your team members or students to join. If you are the member of a team using DETERLab, your project leader will invite you to join the appropriate DETERLab project. If you are a student, you may not create a project. Your instructor must create the project and, once approved, will give you information for joining the project. See Getting Started for more information. How do I use DETERLab? \u00b6 In general, once you have a DETERLab account, you follow these steps. The DETERLab Core Guide will walk you through a basic tutorial of these steps. 1. Design the topology \u00b6 Every experiment in DETERLab is based on a network topology file written in NS format and saved on the users node. The following is a very basic example: # This is a simple ns script. Comments start with #. set ns [new Simulator] source tb_compat.tcl set nodeA [$ns node] set nodeB [$ns node] set nodeC [$ns node] set nodeD [$ns node] set link0 [$ns duplex-link $nodeB $nodeA 30Mb 50ms DropTail] tb-set-link-loss $link0 0.01 set lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms] # Set the OS on a couple. tb-set-node-os $nodeA FBSD-STD tb-set-node-os $nodeC Ubuntu-STD $ns rtproto Static # Go! $ns run 2. Create, start and swap in (allocate resources for) an experiment \u00b6 Using your topology file, you start a new experiment via menu options in the DETERLab web interface. 3. Generate traffic for your nodes \u00b6 Now you can experiment and start generating traffic for your nodes. We provide a flexible framework to pull together the software you'll need. 4. View results by accessing nodes, modify the experiment as needed. \u00b6 Once your experiment has started, you now can access nodes via SSH and conduct your desired experiments in your new environment. You may modify aspects of a running experiment through the \"Modify experiment\" page in the web interface or by making changes to the NS file. 5. Save your work and swap out your experiment (release the resources) \u00b6 When you are ready to stop working on an experiment but know you will want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users. This is just a high-level overview. Go to the Core Guide for a basic hands-on example of using DETERLab Core.","title":"Core Quickstart"},{"location":"core/core-quickstart/#core-quickstart","text":"This page describes basic information about DETERLab and its core functionality.","title":"Core Quickstart"},{"location":"core/core-quickstart/#what-is-deterlab","text":"The DETERLab testbed is a security and education-enhanced version of Emulab. Funded by the National Science Foundation and the Department of Homeland Security, DETERLab is hosted by USC/ISI and UC Berkeley. DETERLab (like Emulab) offers user accounts with assorted permissions associated with different experiment groups. Each group can have its own pre-configured experimental environments running on Linux, BSD, Windows, or other operating systems. Users running DETERLab experiments have full control of real hardware and networks running preconfigured software packages.","title":"What is DETERLab?"},{"location":"core/core-quickstart/#how-does-it-work","text":"The software running DETERLab loads operating system images (low level disk copies) onto free nodes in the testbed, and then reconfigures programmable switches to create VLANs with the newly-imaged nodes connected according to the topology specified by the experiment creator. After the system is fully imaged and configured, DETERLab executes specified scripts, unpacks tarballs, and/or installs RPM files according to the experiment's configuration. The end result is a live network of real machines, accessible via the Internet. Work in DETERLab is based on projects that include individual experiments and is accomplished either via the browser-based web interface (isi.deterlab.net) or via commandline on the DETERLab nodes. To access DETERLab, you need to create an account, which provides credentials for accessing both the web interface and nodes.","title":"How does it work?"},{"location":"core/core-quickstart/#how-do-i-get-a-deterlab-account","text":"You may obtain a DETERLab account by either starting a new project (if you are a PI or instructor) or joining an existing project (if you are a project member or a student). If you are the project investigator or instructor, you must create a project and invite your team members or students to join. If you are the member of a team using DETERLab, your project leader will invite you to join the appropriate DETERLab project. If you are a student, you may not create a project. Your instructor must create the project and, once approved, will give you information for joining the project. See Getting Started for more information.","title":"How do I get a DETERLab account?"},{"location":"core/core-quickstart/#how-do-i-use-deterlab","text":"In general, once you have a DETERLab account, you follow these steps. The DETERLab Core Guide will walk you through a basic tutorial of these steps.","title":"How do I use DETERLab?"},{"location":"core/core-quickstart/#1-design-the-topology","text":"Every experiment in DETERLab is based on a network topology file written in NS format and saved on the users node. The following is a very basic example: # This is a simple ns script. Comments start with #. set ns [new Simulator] source tb_compat.tcl set nodeA [$ns node] set nodeB [$ns node] set nodeC [$ns node] set nodeD [$ns node] set link0 [$ns duplex-link $nodeB $nodeA 30Mb 50ms DropTail] tb-set-link-loss $link0 0.01 set lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms] # Set the OS on a couple. tb-set-node-os $nodeA FBSD-STD tb-set-node-os $nodeC Ubuntu-STD $ns rtproto Static # Go! $ns run","title":"1. Design the topology"},{"location":"core/core-quickstart/#2-create-start-and-swap-in-allocate-resources-for-an-experiment","text":"Using your topology file, you start a new experiment via menu options in the DETERLab web interface.","title":"2. Create, start and swap in (allocate resources for) an experiment"},{"location":"core/core-quickstart/#3-generate-traffic-for-your-nodes","text":"Now you can experiment and start generating traffic for your nodes. We provide a flexible framework to pull together the software you'll need.","title":"3. Generate traffic for your nodes"},{"location":"core/core-quickstart/#4-view-results-by-accessing-nodes-modify-the-experiment-as-needed","text":"Once your experiment has started, you now can access nodes via SSH and conduct your desired experiments in your new environment. You may modify aspects of a running experiment through the \"Modify experiment\" page in the web interface or by making changes to the NS file.","title":"4. View results by accessing nodes, modify the experiment as needed."},{"location":"core/core-quickstart/#5-save-your-work-and-swap-out-your-experiment-release-the-resources","text":"When you are ready to stop working on an experiment but know you will want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users. This is just a high-level overview. Go to the Core Guide for a basic hands-on example of using DETERLab Core.","title":"5. Save your work and swap out your experiment (release the resources)"},{"location":"core/core-reference/","text":"TBD","title":"Core reference"},{"location":"core/custom-images/","text":"Creating Custom Operating System Images \u00b6 What is an Operating System ID (OSID) versus an Image ID? \u00b6 In order to make the best use of custom operating system images, it is important to understand the difference between these two concepts. An Image ID is a descriptor for a disk image. This can be an image of an entire disk, a partition of a disk, or a set of partitions of a disk. By supporting multiple partitions, we can technically support different operating systems within the same disk image. These are referred to as combo images . The Image ID points to a real file that is stored in the directory /proj/YourProjectName/images . Other critical information is associated with the Image ID, such as what node types are supported by the images and what operating systems are on each partition. You can view the Image IDs on the List ImageIDs page , which is in the Experimentation drop down menu on the testbed web interface. An OSID describes an operating system which resides on a partition of a disk image. Every Image ID will have at least one OSID associated with it. In typical testbed usage, the Image ID and OSID will be the same since we usually do not put multiple operating systems on a single image. You can view the OSIDs on the List OSIDs page , which is in the Experimentation drop down menu on the testbed web interface. Standard Testbed Images \u00b6 We provide a number of supported testbed images here at DETERLab. These images can be viewed by looking at the Operating System ID list . Most new operating system images that we support are whole disk images, which is different from the more traditional scheme of using partition 1 for FreeBSD and partition 2 for Linux. To view what nodes a particular operating system image runs on and what sort of partition scheme it uses, please refer to the Image ID list page . The supported testbed images are listed in the Operating System Images documentation page. Custom OS Images \u00b6 If your set of operating system customizations cannot be easily contained within an RPM/TAR (or multiple RPM/TARs), then you can create your own custom OS image. DETERLab allows you to create your own disk images and load them on your experimental nodes automatically when your experiment is created or swapped in. Once you have created a custom disk image (and the associated ImageID/OSID descriptor for it, you can use that OSID in your NS file. When your experiment is swapped in, the testbed system will arrange for your disks to be loaded in parallel using a locally written multicast disk loading protocol. Note Experience has shown that it is much faster to load a disk image on 10 nodes at once, then it is to load a bunch of RPMS or tarballs on each node as it boots. So while it may seem like overkill to create your own disk image, we can assure you it is not. The most common approach is to use the New Image Descriptor form to create a disk image that contains a customized version of a standard Linux or the FreeBSD image. All you need to do is enter the node name in the form, and the testbed system will create the image for you automatically, notifying you via email when it is finished. You can then use that image in subsequent experiments by specifying the descriptor name in your NS file with the tb-set-node-os command. When the experiment is configured, the proper image will be loaded on each node automatically by the system. Creating Your Custom Image \u00b6 A typical approach to creating your own disk image is using one of the default images as a base or template. To do this: Create a single-node Linux or FreeBSD experiment. In your NS file, use the appropriate tb-set-node-os command, as in the following example: tb-set-node-os $nodeA FBSD-STD tb-set-node-os $nodeA Ubuntu-STD After your experiment has swapped in (you have received the email saying it is running), log into the node and load all of the software packages that you wish to load. If you want to install the latest version of the Linux kernel on one of our standard disk images, or on your own custom Linux image, be sure to arrange for any programs that need to be started at boot time. It is a good idea to reboot the node and make sure that everything is running as expected when it comes up. If you are creating a Windows-based image, you must \"prepare\" the node. The final thing to do before grabbing the image is to login on the console , drop to single user mode, and run the prepare script. This is described in detail in the custom Windows images section of the Windows page. Note the physical ( pcXXX ) name of the machine used! Create an image descriptor and image using the New Image Descriptor form. Wait for the email saying the image creation is done. Now you can create a second single-node experiment to test your new image. In your NS file, use tb-set-node-os to select the OSID that you just created. Be sure to remove any RPM or tarball directives. Submit that NS file and wait for the email notification. Then log into the new node and check to make sure everything is running normally. If everything is going well, terminate both of these single-node experiments. If not, release the experiment created in the previous step, and then go back and fix the original node ( pcXXX above). Recreate the image as needed: create_image -p <proj> <imageid> <node> Once your image is working properly, you can use it in any NS file by using the tb-set-node-os command. If you ever want to reload a node in your experiment, either with one of your images or with one of the default images, you can use the os_load command. Log into users and run: os_load -p <proj> -i <imageid> <node> This program will run in the foreground, waiting until the image has been loaded. At that point you should log in and make sure everything is working okay. You might want to watch the console line as well (see the Node Console section ). If you want to load the default image, then simply run: os_load <node> Hints When Making New OS Images \u00b6 Please, never try to create an image from a node or type that begins with the letter b , e.g. bpc183 or bpc2133 . These nodes are located in Berkeley which is physically located 400 miles away from the boss and users servers. After you have created an image, load it back and watch what happens through the serial port. Consider creating a two node experiment, one to create the image and the other to load it back. There is a command called os_load available on the users server: users% which os_load /usr/testbed/bin/os_load users% os_load -h option -h not recognized os_load [options] node [node ...] os_load [options] -e pid,eid where: -i Specify image name; otherwise load default image -p Specify project for finding image name (-i) -s Do *not* wait for nodes to finish reloading -m Specify internal image id (instead of -i and -p) -r Do *not* reboot nodes; do that yourself -e Reboot all nodes in an experiment node Node to reboot (pcXXX) While the second node is reloading, watch its progress in real time using the console command from the users server, ie: users% console pc193 If you think you've got a good image, but it flounders while coming up, create another experiment with an NS directive that says \"Even if you think the node has not booted, let my experiment swap in anyway.\" This may allow you to log in through the console and figure out what went wrong. An example of such a directive is: tb-set-node-failure-action $nodeA \"nonfatal\" Create whole disk images on a smaller machine rather than a single partition image. Updating your Custom Image \u00b6 Once you have your image, it is easy to update it later by taking a new snapshot from a node running your image. Assuming you have swapped in an experiment with a node running your image and you have made changes to that node, use the DETERLab web interface to navigate to the descriptor page for your image: Use the Experimentation drop down menu, and choose List ImageIDs to see the entire list of Images you may access. Find your custom image and click on it. In the More Options menu, click on Snapshot Node ... Fill in the name of the node that is running your image, and click on Go . As in the above instructions, wait for the email saying your image has been updated before you try and use the image.","title":"Creating Custom Images"},{"location":"core/custom-images/#creating-custom-operating-system-images","text":"","title":"Creating Custom Operating System Images"},{"location":"core/custom-images/#what-is-an-operating-system-id-osid-versus-an-image-id","text":"In order to make the best use of custom operating system images, it is important to understand the difference between these two concepts. An Image ID is a descriptor for a disk image. This can be an image of an entire disk, a partition of a disk, or a set of partitions of a disk. By supporting multiple partitions, we can technically support different operating systems within the same disk image. These are referred to as combo images . The Image ID points to a real file that is stored in the directory /proj/YourProjectName/images . Other critical information is associated with the Image ID, such as what node types are supported by the images and what operating systems are on each partition. You can view the Image IDs on the List ImageIDs page , which is in the Experimentation drop down menu on the testbed web interface. An OSID describes an operating system which resides on a partition of a disk image. Every Image ID will have at least one OSID associated with it. In typical testbed usage, the Image ID and OSID will be the same since we usually do not put multiple operating systems on a single image. You can view the OSIDs on the List OSIDs page , which is in the Experimentation drop down menu on the testbed web interface.","title":"What is an Operating System ID (OSID) versus an Image ID?"},{"location":"core/custom-images/#standard-testbed-images","text":"We provide a number of supported testbed images here at DETERLab. These images can be viewed by looking at the Operating System ID list . Most new operating system images that we support are whole disk images, which is different from the more traditional scheme of using partition 1 for FreeBSD and partition 2 for Linux. To view what nodes a particular operating system image runs on and what sort of partition scheme it uses, please refer to the Image ID list page . The supported testbed images are listed in the Operating System Images documentation page.","title":"Standard Testbed Images"},{"location":"core/custom-images/#custom-os-images","text":"If your set of operating system customizations cannot be easily contained within an RPM/TAR (or multiple RPM/TARs), then you can create your own custom OS image. DETERLab allows you to create your own disk images and load them on your experimental nodes automatically when your experiment is created or swapped in. Once you have created a custom disk image (and the associated ImageID/OSID descriptor for it, you can use that OSID in your NS file. When your experiment is swapped in, the testbed system will arrange for your disks to be loaded in parallel using a locally written multicast disk loading protocol. Note Experience has shown that it is much faster to load a disk image on 10 nodes at once, then it is to load a bunch of RPMS or tarballs on each node as it boots. So while it may seem like overkill to create your own disk image, we can assure you it is not. The most common approach is to use the New Image Descriptor form to create a disk image that contains a customized version of a standard Linux or the FreeBSD image. All you need to do is enter the node name in the form, and the testbed system will create the image for you automatically, notifying you via email when it is finished. You can then use that image in subsequent experiments by specifying the descriptor name in your NS file with the tb-set-node-os command. When the experiment is configured, the proper image will be loaded on each node automatically by the system.","title":"Custom OS Images"},{"location":"core/custom-images/#creating-your-custom-image","text":"A typical approach to creating your own disk image is using one of the default images as a base or template. To do this: Create a single-node Linux or FreeBSD experiment. In your NS file, use the appropriate tb-set-node-os command, as in the following example: tb-set-node-os $nodeA FBSD-STD tb-set-node-os $nodeA Ubuntu-STD After your experiment has swapped in (you have received the email saying it is running), log into the node and load all of the software packages that you wish to load. If you want to install the latest version of the Linux kernel on one of our standard disk images, or on your own custom Linux image, be sure to arrange for any programs that need to be started at boot time. It is a good idea to reboot the node and make sure that everything is running as expected when it comes up. If you are creating a Windows-based image, you must \"prepare\" the node. The final thing to do before grabbing the image is to login on the console , drop to single user mode, and run the prepare script. This is described in detail in the custom Windows images section of the Windows page. Note the physical ( pcXXX ) name of the machine used! Create an image descriptor and image using the New Image Descriptor form. Wait for the email saying the image creation is done. Now you can create a second single-node experiment to test your new image. In your NS file, use tb-set-node-os to select the OSID that you just created. Be sure to remove any RPM or tarball directives. Submit that NS file and wait for the email notification. Then log into the new node and check to make sure everything is running normally. If everything is going well, terminate both of these single-node experiments. If not, release the experiment created in the previous step, and then go back and fix the original node ( pcXXX above). Recreate the image as needed: create_image -p <proj> <imageid> <node> Once your image is working properly, you can use it in any NS file by using the tb-set-node-os command. If you ever want to reload a node in your experiment, either with one of your images or with one of the default images, you can use the os_load command. Log into users and run: os_load -p <proj> -i <imageid> <node> This program will run in the foreground, waiting until the image has been loaded. At that point you should log in and make sure everything is working okay. You might want to watch the console line as well (see the Node Console section ). If you want to load the default image, then simply run: os_load <node>","title":"Creating Your Custom Image"},{"location":"core/custom-images/#hints-when-making-new-os-images","text":"Please, never try to create an image from a node or type that begins with the letter b , e.g. bpc183 or bpc2133 . These nodes are located in Berkeley which is physically located 400 miles away from the boss and users servers. After you have created an image, load it back and watch what happens through the serial port. Consider creating a two node experiment, one to create the image and the other to load it back. There is a command called os_load available on the users server: users% which os_load /usr/testbed/bin/os_load users% os_load -h option -h not recognized os_load [options] node [node ...] os_load [options] -e pid,eid where: -i Specify image name; otherwise load default image -p Specify project for finding image name (-i) -s Do *not* wait for nodes to finish reloading -m Specify internal image id (instead of -i and -p) -r Do *not* reboot nodes; do that yourself -e Reboot all nodes in an experiment node Node to reboot (pcXXX) While the second node is reloading, watch its progress in real time using the console command from the users server, ie: users% console pc193 If you think you've got a good image, but it flounders while coming up, create another experiment with an NS directive that says \"Even if you think the node has not booted, let my experiment swap in anyway.\" This may allow you to log in through the console and figure out what went wrong. An example of such a directive is: tb-set-node-failure-action $nodeA \"nonfatal\" Create whole disk images on a smaller machine rather than a single partition image.","title":"Hints When Making New OS Images"},{"location":"core/custom-images/#updating-your-custom-image","text":"Once you have your image, it is easy to update it later by taking a new snapshot from a node running your image. Assuming you have swapped in an experiment with a node running your image and you have made changes to that node, use the DETERLab web interface to navigate to the descriptor page for your image: Use the Experimentation drop down menu, and choose List ImageIDs to see the entire list of Images you may access. Find your custom image and click on it. In the More Options menu, click on Snapshot Node ... Fill in the name of the node that is running your image, and click on Go . As in the above instructions, wait for the email saying your image has been updated before you try and use the image.","title":"Updating your Custom Image"},{"location":"core/dell-serial-console/","text":"Dell Serial Console \u00b6 Most of the machines at DETERLab are Dell servers. When connected via the serial console, some special key sequences are available. Please refer to the node types page for more information on the types of machines available through DETERLab. KEY MAPPING FOR CONSOLE REDIRECTION: Use the <ESC><0> key sequence for <F10> Use the <ESC><@> key sequence for <F12> Use the <ESC><Ctrl><M> key sequence for <Ctrl><M> Use the <ESC><Ctrl><H> key sequence for <Ctrl><H> Use the <ESC><Ctrl><I> key sequence for <Ctrl><I> Use the <ESC><Ctrl><J> key sequence for <Ctrl><J> Use the <ESC><X><X> key sequence for <Alt><x>, where x is any letter key, and X is the upper case of that key Use the <ESC><R><ESC><r><ESC><R> key sequence for <Ctrl><Alt><Del>","title":"Dell Serial Console"},{"location":"core/dell-serial-console/#dell-serial-console","text":"Most of the machines at DETERLab are Dell servers. When connected via the serial console, some special key sequences are available. Please refer to the node types page for more information on the types of machines available through DETERLab. KEY MAPPING FOR CONSOLE REDIRECTION: Use the <ESC><0> key sequence for <F10> Use the <ESC><@> key sequence for <F12> Use the <ESC><Ctrl><M> key sequence for <Ctrl><M> Use the <ESC><Ctrl><H> key sequence for <Ctrl><H> Use the <ESC><Ctrl><I> key sequence for <Ctrl><I> Use the <ESC><Ctrl><J> key sequence for <Ctrl><J> Use the <ESC><X><X> key sequence for <Alt><x>, where x is any letter key, and X is the upper case of that key Use the <ESC><R><ESC><r><ESC><R> key sequence for <Ctrl><Alt><Del>","title":"Dell Serial Console"},{"location":"core/deterlab-commands/","text":"DETERLab Commands \u00b6 The following commands are available from the commandline on users.deterlab.net . Note Commands should be pre-pended with the path: /usr/testbed/bin . For example, to start an experiment, you would use: /usr/testbed/bin/startexp [options] Transport Layers \u00b6 The DETERLab XMLRPC server can be accessed via two different transport layers: SSH and SSL. How to use SSH keys Follow these directions if you are unfamiliar with using SSH. How to use SSL You need to request a certificate from the DETERLab website in order to use the SSL based server. Click the ''My DETERLab'' menu item in the navbar, click the ''Profile'' tab on the page and then click on the ''Generate SSL Certificate'' link. Enter a passphrase to use to encrypt the private key. Once the key has been created, you will be given a link to download a text version (in PEM format). Simply provide this certificate as an input to your SSL client. Operational Commands \u00b6 startexp : Start an DETERLab experiment \u00b6 startexp [-q] [-i [-w]] [-f] [-N] [-E description] [-g gid] [-S reason] [-L reason] [-a <time>] [-l <time>] -p <pid> -e <eid> <nsfile> Options: -i Swapin immediately; by default, the experiment is batched. -w Wait for non-batchmode experiment to preload or swapin. -f Preload experiment (do not swapin or queue yet). -q Be less verbose. -S Experiment cannot be swapped; must provide reason. -L Experiment cannot be IDLE swapped; must provide reason. -a Auto swapout NN minutes after experiment is swapped in. -l Auto swapout NN minutes after experiment goes idle. -E A concise sentence describing your experiment. -g The subgroup in which to create the experiment. -p The project in which to create the experiment. -e The experiment name (unique, alphanumeric, no blanks). -N Suppress most email to the user and testbed-ops. nsfile NS file to parse for experiment batchexp : Synonym for startexp \u00b6 This is a legacy command. See command description for startexp. endexp : Terminate an experiment \u00b6 endexp [-w] [-N] -e pid,eid endexp [-w] [-N] pid eid Options: -w Wait for experiment to finish terminating. -e Project and Experiment ID. -N Suppress most email to the user and testbed-ops. Note Use with caution! This will tear down your experiment and you will not be able to swap it back in. By default, endexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent. The experiment may be terminated when it is currently swapped in ''or'' swapped out. delay_config : Change the link shaping characteristics for a link or LAN \u00b6 delay_config [options] -e pid,eid link PARAM#value ... delay_config [options] pid eid link PARAM#value ... Options: -m Modify virtual experiment as well as current state. -s Select the source of the link to change. -e Project and Experiment ID to operate on. link Name of link from your NS file (ie: link1 ). Parameters: BANDWIDTH#NNN N#bandwidth (10-100000 Kbits per second) PLR#NNN N#lossrate (0 <# plr < 1) DELAY#NNN N#delay (one-way delay in milliseconds > 0) LIMIT#NNN The queue size in bytes or packets QUEUE-IN-BYTES#N 0 means in packets, 1 means in bytes RED/GRED Options: (only if link was specified as RED/GRED) MAXTHRESH#NNN Maximum threshold for the average Q size THRESH#NNN Minimum threshold for the average Q size LINTERM#NNN Packet dropping probability Q_WEIGHT#NNN For calculating the average queue size modexp : Modify experiment \u00b6 modexp [-r] [-s] [-w] [-N] -e pid,eid nsfile modexp [-r] [-s] [-w] [-N] pid eid nsfile Options: -w Wait for experiment to finish swapping. -e Project and Experiment ID. -r Reboot nodes (when experiment is active). -s Restart event scheduler (when experiment is active). -N Suppress most email to the user and testbed-ops Note By default, modexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent. The experiment can be either swapped in ''or'' swapped out. If the experiment is swapped out, the new NS file replaces the existing NS file (the virtual topology is updated). If the experiment is swapped in (active), the physical topology is also updated, subject to the -r and -s options above. swapexp : Swap experiment in or out \u00b6 swapexp -e pid,eid in|out swapexp pid eid in|out Options: -w Wait for experiment to finish swapping. -e Project and Experiment ID. -N Suppress most email to the user and testbed-ops. in Swap experiment in (must currently be swapped out). out Swap experiment out (must currently be swapped in) Note By default, swapexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent. create_image : Create a disk image from a node \u00b6 create_image [options] imageid node Options: -w Wait for image to be created. -p Project ID of imageid. imageid Name of the image. node Node to create image from (pcXXX). Example: The following command creates or re-creates an image for a particular project: create_image -p <proj> <imageid> <node> eventsys_control : Start/Stop/Restart the event system \u00b6 eventsys_control -e pid,eid start|stop|replay eventsys_control pid eid start|stop|replay Options: -e Project and Experiment ID. stop Stop the event scheduler. start Start the event stream from time index 0. replay Replay the event stream from time index 0 loghole : Downloads and manages an experiment's log files \u00b6 This utility downloads log files from certain directories on the experimental nodes (e.g. /local/logs ) to the DETERLab users machine. After downloading, it can also be used to produce and manage archives of the log files. Using this utility to manage an experiment's log files is encouraged because it will transfer the logs in a network-friendly manner and is already integrated with the rest of DETERLab. For example, any programs executed using the DETERLab event-system will have their standard output/error automatically placed in the /local/logs directory. The tool can also be used to preserve multiple trials of an experiment by producing and managing zip archives of the logs. loghole [-hVdqva] [-e pid/eid] [-s server] [-P port] action [...] loghole sync [-nPs] [-r remotedir] [-l localdir] [node1 node2 ...] loghole validate loghole archive [-k (i-delete|space-is-needed)] [-a days] [-c comment] [-d] [archive-name] loghole change [-k (i-delete|space-is-needed)] [-a days] [-c comment] archive-name1 [archive-name2 ...] loghole list [-O1!Xo] [-m atmost] [-s megabytes] loghole show [archive-name] loghole clean [-fne] [node1 node2 ...] loghole gc [-n] [-m atmost] [-s megabytes] Options: -h, --help Print the usage message for the loghole utility as a whole or, if an action is given, the usage message for that action. -V, --version Print out version information and exit. -d, --debug Output debugging messages. -q, --quiet Decrease the level of verbosity, this is subtractive, so multiple uses of this option will make the utility quieter and quieter. The default level of verbosity is human-readable, below that is machine-readable, and below that is silent. For example, the default output from the \"list\" action looks like: [ ] foobar.1.zip 10/15 [!] foobar.0.zip 10/13 Using a single -q option changes the output to look like: foobar.1.zip foobar.0.zip -e, --experiment#PID/EID Specify the experiment(s) to operate on using the Project ID (or project name) and Experiment ID (or experiment name). If multiple -e options are given, the action will apply to all of them. This option overrides the default behavior of inferring the experiment from (Note: this sentence was cut off in the Emulab documentation). Examples: To synchronize the log directory for experiment neptune/myexp with the log holes on the experimental nodes: [vmars@users ~] loghole -e neptune/myexp sync To archive the newly recovered logs and print out just the name of the new log file: [vmars@users ~] loghole -e neptune/myexp -q archive More information: To see the detailed documentation of loghole , view the man page on users : loghole man os_load : Reload disks on selected nodes or all nodes in an experiment \u00b6 os_load [options] node [node ...] os_load [options] -e pid,eid Options: -i Specify image name; otherwise load default image. -p Specify project for finding image name ( -i ). -s Do not wait for nodes to finish reloading. -m Specify internal image id (instead of -i and -p ). -r Do not reboot nodes; do that yourself. -e Reboot all nodes in an experiment. node Node to reboot (pcXXX). portstats : Get portstats from the switches \u00b6 portstats <-p | pid eid> [vname ...] [vname:port ...] Options: -e Show only error counters. -a Show all stats. -z Zero out counts for selected counters after printing. -q Quiet: don't actually print counts - useful with -z . -c Print absolute, rather than relative, counts. -p The machines given are physical, not virtual, node IDs. No pid and eid should be given with this option. Warning If only the pid and eid are given, this command prints out information about all ports in the experiment. Otherwise, output is limited to the nodes and/or ports given. Note Statistics are reported from the switch's perspective. This means that ''In'' packets are those sent FROM the node, and ''Out'' packets are those sent TO the node. In the output, packets described as 'NUnicast' or 'NUcast' are non-unicast (broadcast or multicast) packets. node_reboot : Reboot selected nodes or all nodes in an experiment \u00b6 Use this if you need to powercycle a node. node_reboot [options] node [node ...] node_reboot [options] -e pid,eid Options: -w Wait for nodes is come back up. -c Reconfigure nodes instead of rebooting. -f Power cycle nodes (skip reboot!) -e Reboot all nodes in an experiment. node Node to reboot (pcXXX). Note You may provide more than one node on the command line. Be aware that you may power cycle only nodes in projects that you are member of. node_reboot does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason, node_reboot may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above). You may reboot all the nodes in an experiment by using the -e option to specify the project and experiment names. This option is provided as a shorthand method for rebooting large groups of nodes. Example: The following command will reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project. node_reboot -e testbed,multicast expwait : Wait for experiment to reach a state \u00b6 expwait [-t timeout] -e pid,eid state expwait [-t timeout] pid eid state Options: -e Project and Experiment ID in format <projectID>/<experimentID> . -t Maximum time to wait (in seconds). Informational Commands \u00b6 node_list : Print physical mapping of nodes in an experiment \u00b6 node_list [options] -e pid,eid Options: -e Project and Experiment ID to list. -p Print physical (DETER database) names (default). -P Like -p , but include node type. -v Print virtual (experiment assigned) names. -h Print physical name of host for virtual nodes. -c Print container VMs and physical nodes. Note This command now queries the XMLRPC interface as it used to do. Users who had been using script_wrapper.py node_list to access this function should use /usr/testbed/bin/node_list instead. The -c flag that outputs containerized node names has been modified in two ways. Names are produced without the DNS qualifiers as node names provided by other options of this command are. A node in a VM container named a will be reported as a not a.exp.proj as earlier versions of this feature did. This option now reports embedded_pnode containers as well (physical machines). If no containers VMs are present in an experiment, node_list -c and node_list -v produce identical output. The node_list command is now available as node_summary . expinfo : Get information about an experiment \u00b6 expinfo [-n] [-m] [-l] [-d] [-a] -e pid,eid expinfo [-n] [-m] [-l] [-d] [-a] pid eid Options: -e Project and Experiment ID. -n Show node information. -m Show node mapping. -l Show link information. -a Show all of the above. node_avail : Print free node counts \u00b6 node_avail [-p project] [-c class] [-t type] Options: -p project Specify project credentials for node types that are restricted. -c class The node class (Default: pc). -t type The node type. Example: The following command will print free nodes on pc850 nodes: $ node_avail -t pc850 node_avail_list : Print physical node_ids of available nodes \u00b6 node_avail_list [-p project] [-c class] [-t type] [-n nodes] Options: -p project Specify project credentials for node types that are restricted. -c class The node class (Default: pc). -t type The node type. -n pcX,pcY,...,pcZ A list of physical node_ids. Example: The following command will print the physical node_ids for available pc850 nodes: $ node_avail_list -t pc850 nscheck : Check and NS file for parser errors \u00b6 nscheck nsfile Option: nsfile Path to NS file you to wish check for parse errors.","title":"DETERLab Commands"},{"location":"core/deterlab-commands/#deterlab-commands","text":"The following commands are available from the commandline on users.deterlab.net . Note Commands should be pre-pended with the path: /usr/testbed/bin . For example, to start an experiment, you would use: /usr/testbed/bin/startexp [options]","title":"DETERLab Commands"},{"location":"core/deterlab-commands/#transport-layers","text":"The DETERLab XMLRPC server can be accessed via two different transport layers: SSH and SSL. How to use SSH keys Follow these directions if you are unfamiliar with using SSH. How to use SSL You need to request a certificate from the DETERLab website in order to use the SSL based server. Click the ''My DETERLab'' menu item in the navbar, click the ''Profile'' tab on the page and then click on the ''Generate SSL Certificate'' link. Enter a passphrase to use to encrypt the private key. Once the key has been created, you will be given a link to download a text version (in PEM format). Simply provide this certificate as an input to your SSL client.","title":"Transport Layers"},{"location":"core/deterlab-commands/#operational-commands","text":"","title":"Operational Commands"},{"location":"core/deterlab-commands/#startexp-start-an-deterlab-experiment","text":"startexp [-q] [-i [-w]] [-f] [-N] [-E description] [-g gid] [-S reason] [-L reason] [-a <time>] [-l <time>] -p <pid> -e <eid> <nsfile> Options: -i Swapin immediately; by default, the experiment is batched. -w Wait for non-batchmode experiment to preload or swapin. -f Preload experiment (do not swapin or queue yet). -q Be less verbose. -S Experiment cannot be swapped; must provide reason. -L Experiment cannot be IDLE swapped; must provide reason. -a Auto swapout NN minutes after experiment is swapped in. -l Auto swapout NN minutes after experiment goes idle. -E A concise sentence describing your experiment. -g The subgroup in which to create the experiment. -p The project in which to create the experiment. -e The experiment name (unique, alphanumeric, no blanks). -N Suppress most email to the user and testbed-ops. nsfile NS file to parse for experiment","title":"startexp: Start an DETERLab experiment "},{"location":"core/deterlab-commands/#batchexp-synonym-for-startexp","text":"This is a legacy command. See command description for startexp.","title":"batchexp: Synonym for startexp "},{"location":"core/deterlab-commands/#endexp-terminate-an-experiment","text":"endexp [-w] [-N] -e pid,eid endexp [-w] [-N] pid eid Options: -w Wait for experiment to finish terminating. -e Project and Experiment ID. -N Suppress most email to the user and testbed-ops. Note Use with caution! This will tear down your experiment and you will not be able to swap it back in. By default, endexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent. The experiment may be terminated when it is currently swapped in ''or'' swapped out.","title":"endexp: Terminate an experiment "},{"location":"core/deterlab-commands/#delay_config-change-the-link-shaping-characteristics-for-a-link-or-lan","text":"delay_config [options] -e pid,eid link PARAM#value ... delay_config [options] pid eid link PARAM#value ... Options: -m Modify virtual experiment as well as current state. -s Select the source of the link to change. -e Project and Experiment ID to operate on. link Name of link from your NS file (ie: link1 ). Parameters: BANDWIDTH#NNN N#bandwidth (10-100000 Kbits per second) PLR#NNN N#lossrate (0 <# plr < 1) DELAY#NNN N#delay (one-way delay in milliseconds > 0) LIMIT#NNN The queue size in bytes or packets QUEUE-IN-BYTES#N 0 means in packets, 1 means in bytes RED/GRED Options: (only if link was specified as RED/GRED) MAXTHRESH#NNN Maximum threshold for the average Q size THRESH#NNN Minimum threshold for the average Q size LINTERM#NNN Packet dropping probability Q_WEIGHT#NNN For calculating the average queue size","title":"delay_config: Change the link shaping characteristics for a link or LAN "},{"location":"core/deterlab-commands/#modexp-modify-experiment","text":"modexp [-r] [-s] [-w] [-N] -e pid,eid nsfile modexp [-r] [-s] [-w] [-N] pid eid nsfile Options: -w Wait for experiment to finish swapping. -e Project and Experiment ID. -r Reboot nodes (when experiment is active). -s Restart event scheduler (when experiment is active). -N Suppress most email to the user and testbed-ops Note By default, modexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent. The experiment can be either swapped in ''or'' swapped out. If the experiment is swapped out, the new NS file replaces the existing NS file (the virtual topology is updated). If the experiment is swapped in (active), the physical topology is also updated, subject to the -r and -s options above.","title":"modexp: Modify experiment "},{"location":"core/deterlab-commands/#swapexp-swap-experiment-in-or-out","text":"swapexp -e pid,eid in|out swapexp pid eid in|out Options: -w Wait for experiment to finish swapping. -e Project and Experiment ID. -N Suppress most email to the user and testbed-ops. in Swap experiment in (must currently be swapped out). out Swap experiment out (must currently be swapped in) Note By default, swapexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent.","title":"swapexp: Swap experiment in or out "},{"location":"core/deterlab-commands/#create_image-create-a-disk-image-from-a-node","text":"create_image [options] imageid node Options: -w Wait for image to be created. -p Project ID of imageid. imageid Name of the image. node Node to create image from (pcXXX). Example: The following command creates or re-creates an image for a particular project: create_image -p <proj> <imageid> <node>","title":"create_image: Create a disk image from a node "},{"location":"core/deterlab-commands/#eventsys_control-startstoprestart-the-event-system","text":"eventsys_control -e pid,eid start|stop|replay eventsys_control pid eid start|stop|replay Options: -e Project and Experiment ID. stop Stop the event scheduler. start Start the event stream from time index 0. replay Replay the event stream from time index 0","title":"eventsys_control: Start/Stop/Restart the event system "},{"location":"core/deterlab-commands/#loghole-downloads-and-manages-an-experiments-log-files","text":"This utility downloads log files from certain directories on the experimental nodes (e.g. /local/logs ) to the DETERLab users machine. After downloading, it can also be used to produce and manage archives of the log files. Using this utility to manage an experiment's log files is encouraged because it will transfer the logs in a network-friendly manner and is already integrated with the rest of DETERLab. For example, any programs executed using the DETERLab event-system will have their standard output/error automatically placed in the /local/logs directory. The tool can also be used to preserve multiple trials of an experiment by producing and managing zip archives of the logs. loghole [-hVdqva] [-e pid/eid] [-s server] [-P port] action [...] loghole sync [-nPs] [-r remotedir] [-l localdir] [node1 node2 ...] loghole validate loghole archive [-k (i-delete|space-is-needed)] [-a days] [-c comment] [-d] [archive-name] loghole change [-k (i-delete|space-is-needed)] [-a days] [-c comment] archive-name1 [archive-name2 ...] loghole list [-O1!Xo] [-m atmost] [-s megabytes] loghole show [archive-name] loghole clean [-fne] [node1 node2 ...] loghole gc [-n] [-m atmost] [-s megabytes] Options: -h, --help Print the usage message for the loghole utility as a whole or, if an action is given, the usage message for that action. -V, --version Print out version information and exit. -d, --debug Output debugging messages. -q, --quiet Decrease the level of verbosity, this is subtractive, so multiple uses of this option will make the utility quieter and quieter. The default level of verbosity is human-readable, below that is machine-readable, and below that is silent. For example, the default output from the \"list\" action looks like: [ ] foobar.1.zip 10/15 [!] foobar.0.zip 10/13 Using a single -q option changes the output to look like: foobar.1.zip foobar.0.zip -e, --experiment#PID/EID Specify the experiment(s) to operate on using the Project ID (or project name) and Experiment ID (or experiment name). If multiple -e options are given, the action will apply to all of them. This option overrides the default behavior of inferring the experiment from (Note: this sentence was cut off in the Emulab documentation). Examples: To synchronize the log directory for experiment neptune/myexp with the log holes on the experimental nodes: [vmars@users ~] loghole -e neptune/myexp sync To archive the newly recovered logs and print out just the name of the new log file: [vmars@users ~] loghole -e neptune/myexp -q archive More information: To see the detailed documentation of loghole , view the man page on users : loghole man","title":"loghole: Downloads and manages an experiment's log files "},{"location":"core/deterlab-commands/#os_load-reload-disks-on-selected-nodes-or-all-nodes-in-an-experiment","text":"os_load [options] node [node ...] os_load [options] -e pid,eid Options: -i Specify image name; otherwise load default image. -p Specify project for finding image name ( -i ). -s Do not wait for nodes to finish reloading. -m Specify internal image id (instead of -i and -p ). -r Do not reboot nodes; do that yourself. -e Reboot all nodes in an experiment. node Node to reboot (pcXXX).","title":"os_load: Reload disks on selected nodes or all nodes in an experiment "},{"location":"core/deterlab-commands/#portstats-get-portstats-from-the-switches","text":"portstats <-p | pid eid> [vname ...] [vname:port ...] Options: -e Show only error counters. -a Show all stats. -z Zero out counts for selected counters after printing. -q Quiet: don't actually print counts - useful with -z . -c Print absolute, rather than relative, counts. -p The machines given are physical, not virtual, node IDs. No pid and eid should be given with this option. Warning If only the pid and eid are given, this command prints out information about all ports in the experiment. Otherwise, output is limited to the nodes and/or ports given. Note Statistics are reported from the switch's perspective. This means that ''In'' packets are those sent FROM the node, and ''Out'' packets are those sent TO the node. In the output, packets described as 'NUnicast' or 'NUcast' are non-unicast (broadcast or multicast) packets.","title":"portstats: Get portstats from the switches "},{"location":"core/deterlab-commands/#node_reboot-reboot-selected-nodes-or-all-nodes-in-an-experiment","text":"Use this if you need to powercycle a node. node_reboot [options] node [node ...] node_reboot [options] -e pid,eid Options: -w Wait for nodes is come back up. -c Reconfigure nodes instead of rebooting. -f Power cycle nodes (skip reboot!) -e Reboot all nodes in an experiment. node Node to reboot (pcXXX). Note You may provide more than one node on the command line. Be aware that you may power cycle only nodes in projects that you are member of. node_reboot does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason, node_reboot may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above). You may reboot all the nodes in an experiment by using the -e option to specify the project and experiment names. This option is provided as a shorthand method for rebooting large groups of nodes. Example: The following command will reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project. node_reboot -e testbed,multicast","title":"node_reboot: Reboot selected nodes or all nodes in an experiment "},{"location":"core/deterlab-commands/#expwait-wait-for-experiment-to-reach-a-state","text":"expwait [-t timeout] -e pid,eid state expwait [-t timeout] pid eid state Options: -e Project and Experiment ID in format <projectID>/<experimentID> . -t Maximum time to wait (in seconds).","title":"expwait: Wait for experiment to reach a state "},{"location":"core/deterlab-commands/#informational-commands","text":"","title":"Informational Commands"},{"location":"core/deterlab-commands/#node_list-print-physical-mapping-of-nodes-in-an-experiment","text":"node_list [options] -e pid,eid Options: -e Project and Experiment ID to list. -p Print physical (DETER database) names (default). -P Like -p , but include node type. -v Print virtual (experiment assigned) names. -h Print physical name of host for virtual nodes. -c Print container VMs and physical nodes. Note This command now queries the XMLRPC interface as it used to do. Users who had been using script_wrapper.py node_list to access this function should use /usr/testbed/bin/node_list instead. The -c flag that outputs containerized node names has been modified in two ways. Names are produced without the DNS qualifiers as node names provided by other options of this command are. A node in a VM container named a will be reported as a not a.exp.proj as earlier versions of this feature did. This option now reports embedded_pnode containers as well (physical machines). If no containers VMs are present in an experiment, node_list -c and node_list -v produce identical output. The node_list command is now available as node_summary .","title":"node_list: Print physical mapping of nodes in an experiment "},{"location":"core/deterlab-commands/#expinfo-get-information-about-an-experiment","text":"expinfo [-n] [-m] [-l] [-d] [-a] -e pid,eid expinfo [-n] [-m] [-l] [-d] [-a] pid eid Options: -e Project and Experiment ID. -n Show node information. -m Show node mapping. -l Show link information. -a Show all of the above.","title":"expinfo: Get information about an experiment "},{"location":"core/deterlab-commands/#node_avail-print-free-node-counts","text":"node_avail [-p project] [-c class] [-t type] Options: -p project Specify project credentials for node types that are restricted. -c class The node class (Default: pc). -t type The node type. Example: The following command will print free nodes on pc850 nodes: $ node_avail -t pc850","title":"node_avail: Print free node counts "},{"location":"core/deterlab-commands/#node_avail_list-print-physical-node_ids-of-available-nodes","text":"node_avail_list [-p project] [-c class] [-t type] [-n nodes] Options: -p project Specify project credentials for node types that are restricted. -c class The node class (Default: pc). -t type The node type. -n pcX,pcY,...,pcZ A list of physical node_ids. Example: The following command will print the physical node_ids for available pc850 nodes: $ node_avail_list -t pc850","title":"node_avail_list: Print physical node_ids of available nodes "},{"location":"core/deterlab-commands/#nscheck-check-and-ns-file-for-parser-errors","text":"nscheck nsfile Option: nsfile Path to NS file you to wish check for parse errors.","title":"nscheck: Check and NS file for parser errors "},{"location":"core/faq/","text":"Frequently Asked Questions \u00b6 How do I install software onto my node? \u00b6 Each supported operating system has packages mirrored on a host called scratch and each operating system is configured to use this system to fetch packages from. Information for specific operating systems is documented there. How do I copy files onto my node? \u00b6 Your home directory on users is automatically mounted via NFS on every node in your experiment. As are your project directory in /proj and a special filesystem called /share . I need root access! \u00b6 If you need to customize the configuration, or perhaps reboot nodes, you can use the \"sudo\" command, located in /usr/local/bin on FreeBSD and /usr/bin Linux. All users are added to the Administrators group on Windows XP nodes. Our policy is very liberal; you can customize the configuration in any way you like, provided it does not interfere with the operation of the testbed. As as example, to reboot a node that is running FreeBSD: /usr/local/bin/sudo reboot Also, every testbed node has an automatically generated root password. Simply click on a node in the \"Reserved Nodes\" for your experiment and look at the root_password attribute. Can I access the nodes console? \u00b6 Yes. Each of the PCs has its own serial console line connected to a serial server . You can connect to a nodes serial console through the users machine, using our console program located in '/usr/testbed/bin'. For example, to connect over serial line to \"pc001\" in your experiment, SSH into users.deterlab.net , and then type console pc001 at the Unix prompt. You may then interact with the serial console (hit \"enter\" to elicit output from the target machine). To exit the console program, type Ctrl-] ; it's just a telnet session. In any case, all console output from each node is saved so that you may look at it it later. For each node, the console log is stored as /var/log/tiplogs/pcXXX.run . This run file is created when nodes are first allocated to an experiment, and the Unix permissions of the run files permit only members of the project to view them. When the nodes are deallocated, the run files are cleared, so if you want to save them, you must do so before terminating the experiment. In addition, you can view the console logs from the web interface, on the Show Experiment page. Of course, you may not interact with the console, but you can at least view the current log. Escape codes for Dell serial consoles are documented here . My node is wedged! \u00b6 Power cycling a node is easy since every node on the testbed is connected to a power controller. If you need to power cycle a node, log on to users.deterlab.net and use the \"node_reboot\" command: node_reboot <node> [node ... ] where node is the physical name, as listed in the node mapping table. You may provide more than one node on the command line. Be aware that you may power cycle only nodes in projects that you are member of. Also, node_reboot does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason, node_reboot may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above). You may also reboot all the nodes in an experiment by using the -e option to specify the project and experiment names. For example: node_reboot -e testbed,multicast will reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project. This option is provided as a shorthand method for rebooting large groups of nodes. I want to load a fresh operating system on my node \u00b6 Scrogging your disk is certainly not as common, but it does happen. You can either swap your experiment out and then back in (which will allocate another group of nodes), or if you prefer you can reload the disk image yourself. You will of course lose anything you have stored on that disk; it is a good idea to store only data that can be easily recreated, or else store it in your project directory in /proj . Reloading your disk with a fresh copy of an image is easy, and requires no intervention by DETER staff: os_load [-i ImageName] [-p Project] <node> [node ... ] If you do not specify an image name, the default image for that node type will be loaded (typically Ubuntu1604-STD). For testbed wide images, you do not have to specify a project. The os_load command will wait (not exit) until the nodes have been reloaded, so that you do not need to check the console lines of each node to determine when the load is done. For example, to load the image 'testpc167' which is in the project 'DeterTest' onto pc167, we type: users > os_load -i testpc167 -p DeterTest pc167 osload (pc167): Changing default OS to [OS 998: DeterTest,testpc167] osload: Updating image signature. Setting up reload for pc167 (mode: Frisbee) osload: Issuing reboot for pc167 and then waiting ... reboot (pc167): Attempting to reboot ... reboot (pc167): Successful! reboot: Done. There were 0 failures. reboot (pc167): child returned 0 status. osload (pc167): still waiting; it has been 1 minute(s) osload (pc167): still waiting; it has been 2 minute(s) osload (pc167): still waiting; it has been 3 minute(s) osload (pc167): still waiting; it has been 4 minute(s) osload: Done! There were 0 failures. users > I only want certain types of nodes for my experiment \u00b6 The NS command tb-set-hardware only lets you pick one type of hardware. If you are fine with a couple of different kinds of hardware, say you just want nodes that are at ISI part of the testbed, you can define a virtual node type in your NS file. For more information on virtual types, please refer to the Virtual Type Commands section of the NS command reference. Here is a quick example: tb-make-soft-vtype ISI {pc2133 pc3000 pc3060 pc3100} tb-make-soft-vtype UCB {bpc2133 bpc3000 bpc3060} tb-set-hardware $atISI ISI tb-set-hardware $atUCB UCB","title":"Frequently Asked Questions"},{"location":"core/faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"core/faq/#how-do-i-install-software-onto-my-node","text":"Each supported operating system has packages mirrored on a host called scratch and each operating system is configured to use this system to fetch packages from. Information for specific operating systems is documented there.","title":"How do I install software onto my node?"},{"location":"core/faq/#how-do-i-copy-files-onto-my-node","text":"Your home directory on users is automatically mounted via NFS on every node in your experiment. As are your project directory in /proj and a special filesystem called /share .","title":"How do I copy files onto my node?"},{"location":"core/faq/#i-need-root-access","text":"If you need to customize the configuration, or perhaps reboot nodes, you can use the \"sudo\" command, located in /usr/local/bin on FreeBSD and /usr/bin Linux. All users are added to the Administrators group on Windows XP nodes. Our policy is very liberal; you can customize the configuration in any way you like, provided it does not interfere with the operation of the testbed. As as example, to reboot a node that is running FreeBSD: /usr/local/bin/sudo reboot Also, every testbed node has an automatically generated root password. Simply click on a node in the \"Reserved Nodes\" for your experiment and look at the root_password attribute.","title":"I need root access!"},{"location":"core/faq/#can-i-access-the-nodes-console","text":"Yes. Each of the PCs has its own serial console line connected to a serial server . You can connect to a nodes serial console through the users machine, using our console program located in '/usr/testbed/bin'. For example, to connect over serial line to \"pc001\" in your experiment, SSH into users.deterlab.net , and then type console pc001 at the Unix prompt. You may then interact with the serial console (hit \"enter\" to elicit output from the target machine). To exit the console program, type Ctrl-] ; it's just a telnet session. In any case, all console output from each node is saved so that you may look at it it later. For each node, the console log is stored as /var/log/tiplogs/pcXXX.run . This run file is created when nodes are first allocated to an experiment, and the Unix permissions of the run files permit only members of the project to view them. When the nodes are deallocated, the run files are cleared, so if you want to save them, you must do so before terminating the experiment. In addition, you can view the console logs from the web interface, on the Show Experiment page. Of course, you may not interact with the console, but you can at least view the current log. Escape codes for Dell serial consoles are documented here .","title":"Can I access the nodes console?"},{"location":"core/faq/#my-node-is-wedged","text":"Power cycling a node is easy since every node on the testbed is connected to a power controller. If you need to power cycle a node, log on to users.deterlab.net and use the \"node_reboot\" command: node_reboot <node> [node ... ] where node is the physical name, as listed in the node mapping table. You may provide more than one node on the command line. Be aware that you may power cycle only nodes in projects that you are member of. Also, node_reboot does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason, node_reboot may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above). You may also reboot all the nodes in an experiment by using the -e option to specify the project and experiment names. For example: node_reboot -e testbed,multicast will reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project. This option is provided as a shorthand method for rebooting large groups of nodes.","title":"My node is wedged!"},{"location":"core/faq/#i-want-to-load-a-fresh-operating-system-on-my-node","text":"Scrogging your disk is certainly not as common, but it does happen. You can either swap your experiment out and then back in (which will allocate another group of nodes), or if you prefer you can reload the disk image yourself. You will of course lose anything you have stored on that disk; it is a good idea to store only data that can be easily recreated, or else store it in your project directory in /proj . Reloading your disk with a fresh copy of an image is easy, and requires no intervention by DETER staff: os_load [-i ImageName] [-p Project] <node> [node ... ] If you do not specify an image name, the default image for that node type will be loaded (typically Ubuntu1604-STD). For testbed wide images, you do not have to specify a project. The os_load command will wait (not exit) until the nodes have been reloaded, so that you do not need to check the console lines of each node to determine when the load is done. For example, to load the image 'testpc167' which is in the project 'DeterTest' onto pc167, we type: users > os_load -i testpc167 -p DeterTest pc167 osload (pc167): Changing default OS to [OS 998: DeterTest,testpc167] osload: Updating image signature. Setting up reload for pc167 (mode: Frisbee) osload: Issuing reboot for pc167 and then waiting ... reboot (pc167): Attempting to reboot ... reboot (pc167): Successful! reboot: Done. There were 0 failures. reboot (pc167): child returned 0 status. osload (pc167): still waiting; it has been 1 minute(s) osload (pc167): still waiting; it has been 2 minute(s) osload (pc167): still waiting; it has been 3 minute(s) osload (pc167): still waiting; it has been 4 minute(s) osload: Done! There were 0 failures. users >","title":"I want to load a fresh operating system on my node"},{"location":"core/faq/#i-only-want-certain-types-of-nodes-for-my-experiment","text":"The NS command tb-set-hardware only lets you pick one type of hardware. If you are fine with a couple of different kinds of hardware, say you just want nodes that are at ISI part of the testbed, you can define a virtual node type in your NS file. For more information on virtual types, please refer to the Virtual Type Commands section of the NS command reference. Here is a quick example: tb-make-soft-vtype ISI {pc2133 pc3000 pc3060 pc3100} tb-make-soft-vtype UCB {bpc2133 bpc3000 bpc3060} tb-set-hardware $atISI ISI tb-set-hardware $atUCB UCB","title":"I only want certain types of nodes for my experiment"},{"location":"core/generating-traffic/","text":"Generating Traffic with LegoTG \u00b6 LegoTG is a flexible framework for pulling together the appropriate software for the traffic generation process. The key insight of LegoTG is that the definition of \"realism\" in traffic generation is entirely dependent on the experiment/scenario. LegoTG itself does not determine what \"realistic\" traffic means for a particular experiment or scenario. Rather, the definition of realism must come from the LegoTG user. LegoTG enables a modular and composable approach to the traffic generation process. LegoTG's Orchestrator handles tying together various modules which handle the different aspects of generation and allows a user to create a plug-and-play traffic generator. We are developing software to perform data extraction as well as working on a generator which will be generic enough to handle a variety of modeled dimensions. In the LegoTG framework, each traffic generation functionality is realized through a separate piece of code, called a TGblock. The framework works like a child\u2019s building block set: TGblocks combine in different ways to achieve customizable and composable traffic generation. This combination and customization is achieved through LegoTG\u2019s Orchestrator, which sets up, configures, deploys, runs, synchronizes and stops TGblocks in distributed experiments. The entire specification of the traffic generation process for an experiment is in an experiment configuration file\u2014called an ExFile, which is an input for the Orchestrator. The ExFile offers a convenient capture of all the details of an experiment\u2019s background traffic set up, which promotes sharing and reproducibility of experiments. For more information and to download software, please see the LegoTG project page.","title":"Generating Traffic"},{"location":"core/generating-traffic/#generating-traffic-with-legotg","text":"LegoTG is a flexible framework for pulling together the appropriate software for the traffic generation process. The key insight of LegoTG is that the definition of \"realism\" in traffic generation is entirely dependent on the experiment/scenario. LegoTG itself does not determine what \"realistic\" traffic means for a particular experiment or scenario. Rather, the definition of realism must come from the LegoTG user. LegoTG enables a modular and composable approach to the traffic generation process. LegoTG's Orchestrator handles tying together various modules which handle the different aspects of generation and allows a user to create a plug-and-play traffic generator. We are developing software to perform data extraction as well as working on a generator which will be generic enough to handle a variety of modeled dimensions. In the LegoTG framework, each traffic generation functionality is realized through a separate piece of code, called a TGblock. The framework works like a child\u2019s building block set: TGblocks combine in different ways to achieve customizable and composable traffic generation. This combination and customization is achieved through LegoTG\u2019s Orchestrator, which sets up, configures, deploys, runs, synchronizes and stops TGblocks in distributed experiments. The entire specification of the traffic generation process for an experiment is in an experiment configuration file\u2014called an ExFile, which is an input for the Orchestrator. The ExFile offers a convenient capture of all the details of an experiment\u2019s background traffic set up, which promotes sharing and reproducibility of experiments. For more information and to download software, please see the LegoTG project page.","title":"Generating Traffic with LegoTG"},{"location":"core/hostnames/","text":"Hostnames for your nodes \u00b6 We set up names for your nodes in DNS and /etc/hosts files for use on the nodes in the experiment. Since our nodes have multiple interfaces (the control network, and, depending on the experiment, possibly several experimental interfaces) determining which name refers to which interface can be somewhat confusing. The rules below should help you figure this out. From users.deterlab.net - We set up names in the form of node.expt.proj in DNS. This name always refers to the node's control network interface, which is the only one reachable from users.deterlab.net . On the nodes themselves - There are three basic ways to refer to the interfaces of a node. The first is stored in DNS, and the second two are stored on the node in the /etc/hosts file. Short form - Within your experiment you should use just the node name (e..g, nodeA ) to refer to the nodes. This ensures that traffic between nodes in your experiment goes over the links you created Node-link form - You can refer to an individual experimental interface by suffixing it with the name of the link or LAN (as defined in your NS file) that it is a member of. For example, nodeA-link0 or server-serverLAN . This is the preferred way to refer to experimental interfaces, since it uniquely and unambiguously identifies an interface. Fully-qualified hostnames - These names are the same ones that use use on users.deterlab.net and look like node.expt.proj . This name always refers to the control network interface. Note It is a bad idea to pick virtual node names in your topology that clash with the physical node names in the testbed, such as \"pc45\".","title":"Hostnames for your nodes"},{"location":"core/hostnames/#hostnames-for-your-nodes","text":"We set up names for your nodes in DNS and /etc/hosts files for use on the nodes in the experiment. Since our nodes have multiple interfaces (the control network, and, depending on the experiment, possibly several experimental interfaces) determining which name refers to which interface can be somewhat confusing. The rules below should help you figure this out. From users.deterlab.net - We set up names in the form of node.expt.proj in DNS. This name always refers to the node's control network interface, which is the only one reachable from users.deterlab.net . On the nodes themselves - There are three basic ways to refer to the interfaces of a node. The first is stored in DNS, and the second two are stored on the node in the /etc/hosts file. Short form - Within your experiment you should use just the node name (e..g, nodeA ) to refer to the nodes. This ensures that traffic between nodes in your experiment goes over the links you created Node-link form - You can refer to an individual experimental interface by suffixing it with the name of the link or LAN (as defined in your NS file) that it is a member of. For example, nodeA-link0 or server-serverLAN . This is the preferred way to refer to experimental interfaces, since it uniquely and unambiguously identifies an interface. Fully-qualified hostnames - These names are the same ones that use use on users.deterlab.net and look like node.expt.proj . This name always refers to the control network interface. Note It is a bad idea to pick virtual node names in your topology that clash with the physical node names in the testbed, such as \"pc45\".","title":"Hostnames for your nodes"},{"location":"core/legacy-tools/","text":"Legacy Tools \u00b6 This page includes links to tools that have been useful to DETER users in the past. There is no guarantee that they will perform with current DETER software and they are listed for legacy purposes. Benchmarks \u00b6 DDoS Defense Benchmarks \u00b6 Developed and maintained by University of Delaware, this tool contains: A benchmark suite with a set of scenarios to be used for defense evaluation, integrated with SEER, A set of performance metrics that characterize an attack's impact and a defense's performance, and A set of tools used for benchmark development, integration of benchmarks with the DETER testbed and calculation of performance metrics from tcpdump traces collected during DDoS experimentation. Website : http://www.isi.edu/~mirkovic/bench Runs on : Any platform Best for : Testing DDoS defenses For questions, contact : Jelena Mirkovic at ISI Legitimate Traffic Generators \u00b6 SEER \u00b6 The Security Experimentation EnviRonment (SEER), developed by SPARTA, Inc., is a GUI-based user interface to DETERLab, helping an experimenter to set up, script, and perform experiments in the DETER environment. The SEER back-end includes tools to generate legitimate traffic using Harpoon or custom-made Web, DNS, Ping, IRC, FTP and VoIP agents. Note that this tool is no longer supported and is offered as-is. Website : http://seer.deterlab.net/trac Runs on : All platforms, written in Java Best for : Legitimate traffic generation, DoS traffic generation, visualization of traffic levels in topology Tcpreplay \u00b6 Tcpreplay is a suite of BSD licensed tools, which gives you the ability to inject previously captured traffic in libpcap format to test a variety of network devices. It allows you to classify traffic as client or server, rewrite Layer 2, 3 and 4 headers and finally replay the traffic back onto the network and through other devices such as switches, routers, firewalls, NIDS and IPS's. Tcpreplay supports both single and dual NIC modes for testing both sniffing and inline devices. Website : http://tcpreplay.synfin.net/trac/ Runs on : UNIX-flavored OSes and Win32 with Cygwin Best for : Replaying traces to regenerate same or similar traffic For questions, contact : Tcpreplay support Webstone \u00b6 Webstone, a benchmark owned by Mindcraft Inc., measures performance of web server software and hardware products. Webstone consists of a program called the webmaster which can be installed on a client in the network or on a separate computer. The webmaster distributes web client software as well as configuration files for testing to the client computers, that contact the web server to retrieve web pages or files in order to test web server performance. Webstone also tests operating system software, CPU and network speeds. While it was developed with the idea of measuring the performance of web servers, it can be used to generate background traffic in a network as the multiple clients keep contacting the server over a period of time thereby simulating web traffic in the network. Website : http://www.mindcraft.com/webstone/ Runs on : UNIX-flavored OSes and Windows NT Best for : Web traffic generation Harpoon \u00b6 Harpoon, developed at University of Wisconsin, is a flow-level traffic generator. It uses a set of distributional parameters that can be automatically extracted from Netflow traces to generate flows that exhibit the same statistical qualities present in measured Internet traces, including temporal and spatial characteristics. Harpoon can be used to generate representative background traffic for application or protocol testing, or for testing network switching hardware. Note, however, that while traffic dynamics will resemble the one found in traces, Harpoon traffic runs over HTTP and application behavior may be different from the real one. Website : https://github.com/jsommers/harpoon Runs on : UNIX-flavored OSes Best for : Generating traffic from traces or from high-level specifications. DoS and DDoS Attack Traffic Generators \u00b6 SEER \u00b6 ( See above ) SEER generates attack traffic using the Flooder tool, developed by SPARTA, and the Cleo tool developed by UCLA. Look at SEER's Web page for a more detailed description of these tools. The following collection of real DDoS tools has little new to offer with regard to attack traffic generation, when compared to SEER's capabilities. In general, SEER can generate same traffic variations as this tools, and is easier to control and customize. If, however, you are testing a defense that looks at control traffic of DoS networks these tools may be useful to you. They are all downloadable from third-party Web sites and are not maintained. Stacheldraht \u00b6 Stacheldraht combines features of Trinoo and TFN tools and adds encrypted communication between the attacker and the masters. Stacheldraht uses TCP for encrypted communication between the attacker and the masters, and TCP or ICMP for communication between master and agents. Another added feature is the ability to perform automatic updates of agent code. Available attacks are UDP flood, TCP SYN flood, ICMP ECHO flood and Smurf attacks. Website : http://packetstormsecurity.org/distributed/stachel.tgz Mstream \u00b6 Mstream generates a flood of TCP packets with the ACK bit set. Masters can be controlled remotely by one or more attackers using a password- protected interactive login. The communications between attacker and masters, and a master and agents, are configurable at compile time and have varied signif- icantly from incident to incident. Source addresses in attack packets are spoofed at random. The TCP ACK attack exhausts network resources and will likely cause a TCP RST to be sent to the spoofed source address (potentially also creating outgoing bandwidth consumption at the victim). Website : http://packetstormsecurity.org/distributed/mstream.txt Topology Generators and Convertors \u00b6 Rocketfuel-to-ns \u00b6 Rocketfuel-to-ns, developed by Purdue University, is a utility to convert RocketFuel-format data files into a set of configuration files runnable on am emulation testbed like the DETER testbed. Experiment configurations generated with this tool have the advantage of not being totally synthetic representations of the Internet; they provide a router-level topology based off real measurement data. This distribution also contains many sample NS files that represent real AS topologies. Website : http://www.cs.purdue.edu/homes/fahmy/software/rf2ns/index.html Runs on : UNIX Best for : Collecting real AS topologies and importing them into DETERLab. Inet \u00b6 Inet, developed by University of Michigan, is a generator of representative Autonomous System (AS) level Internet topologies. Website : http://topology.eecs.umich.edu/inet/ Runs on : FreeBSD, Linux, Mac OS and Solaris Best for : Synthetic topology generation, following a power law. Brite \u00b6 Brite, developed by Boston University, is a generator of flat AS, flat Router and hierarchical topologies, interoperable with various topology generators and simulators. Website : http://www.cs.bu.edu/brite/ Best for : Synthetic topology generation using different models and a GUI. GT-ITM \u00b6 GT-ITM: Georgia Tech Internetwork Topology Models, developed by Georgia Tech, generates graphs that model the topological structure of internetworks. Website : http://www.cc.gatech.edu/projects/gtitm/ Runs on : SunOS and Linux Best for : Synthetic topology generation for small size topologies.","title":"Legacy Tools"},{"location":"core/legacy-tools/#legacy-tools","text":"This page includes links to tools that have been useful to DETER users in the past. There is no guarantee that they will perform with current DETER software and they are listed for legacy purposes.","title":"Legacy Tools"},{"location":"core/legacy-tools/#benchmarks","text":"","title":"Benchmarks"},{"location":"core/legacy-tools/#ddos-defense-benchmarks","text":"Developed and maintained by University of Delaware, this tool contains: A benchmark suite with a set of scenarios to be used for defense evaluation, integrated with SEER, A set of performance metrics that characterize an attack's impact and a defense's performance, and A set of tools used for benchmark development, integration of benchmarks with the DETER testbed and calculation of performance metrics from tcpdump traces collected during DDoS experimentation. Website : http://www.isi.edu/~mirkovic/bench Runs on : Any platform Best for : Testing DDoS defenses For questions, contact : Jelena Mirkovic at ISI","title":"DDoS Defense Benchmarks"},{"location":"core/legacy-tools/#legitimate-traffic-generators","text":"","title":"Legitimate Traffic Generators"},{"location":"core/legacy-tools/#seer","text":"The Security Experimentation EnviRonment (SEER), developed by SPARTA, Inc., is a GUI-based user interface to DETERLab, helping an experimenter to set up, script, and perform experiments in the DETER environment. The SEER back-end includes tools to generate legitimate traffic using Harpoon or custom-made Web, DNS, Ping, IRC, FTP and VoIP agents. Note that this tool is no longer supported and is offered as-is. Website : http://seer.deterlab.net/trac Runs on : All platforms, written in Java Best for : Legitimate traffic generation, DoS traffic generation, visualization of traffic levels in topology","title":"SEER"},{"location":"core/legacy-tools/#tcpreplay","text":"Tcpreplay is a suite of BSD licensed tools, which gives you the ability to inject previously captured traffic in libpcap format to test a variety of network devices. It allows you to classify traffic as client or server, rewrite Layer 2, 3 and 4 headers and finally replay the traffic back onto the network and through other devices such as switches, routers, firewalls, NIDS and IPS's. Tcpreplay supports both single and dual NIC modes for testing both sniffing and inline devices. Website : http://tcpreplay.synfin.net/trac/ Runs on : UNIX-flavored OSes and Win32 with Cygwin Best for : Replaying traces to regenerate same or similar traffic For questions, contact : Tcpreplay support","title":"Tcpreplay"},{"location":"core/legacy-tools/#webstone","text":"Webstone, a benchmark owned by Mindcraft Inc., measures performance of web server software and hardware products. Webstone consists of a program called the webmaster which can be installed on a client in the network or on a separate computer. The webmaster distributes web client software as well as configuration files for testing to the client computers, that contact the web server to retrieve web pages or files in order to test web server performance. Webstone also tests operating system software, CPU and network speeds. While it was developed with the idea of measuring the performance of web servers, it can be used to generate background traffic in a network as the multiple clients keep contacting the server over a period of time thereby simulating web traffic in the network. Website : http://www.mindcraft.com/webstone/ Runs on : UNIX-flavored OSes and Windows NT Best for : Web traffic generation","title":"Webstone"},{"location":"core/legacy-tools/#harpoon","text":"Harpoon, developed at University of Wisconsin, is a flow-level traffic generator. It uses a set of distributional parameters that can be automatically extracted from Netflow traces to generate flows that exhibit the same statistical qualities present in measured Internet traces, including temporal and spatial characteristics. Harpoon can be used to generate representative background traffic for application or protocol testing, or for testing network switching hardware. Note, however, that while traffic dynamics will resemble the one found in traces, Harpoon traffic runs over HTTP and application behavior may be different from the real one. Website : https://github.com/jsommers/harpoon Runs on : UNIX-flavored OSes Best for : Generating traffic from traces or from high-level specifications.","title":"Harpoon"},{"location":"core/legacy-tools/#dos-and-ddos-attack-traffic-generators","text":"","title":"DoS and DDoS Attack Traffic Generators"},{"location":"core/legacy-tools/#seer_1","text":"( See above ) SEER generates attack traffic using the Flooder tool, developed by SPARTA, and the Cleo tool developed by UCLA. Look at SEER's Web page for a more detailed description of these tools. The following collection of real DDoS tools has little new to offer with regard to attack traffic generation, when compared to SEER's capabilities. In general, SEER can generate same traffic variations as this tools, and is easier to control and customize. If, however, you are testing a defense that looks at control traffic of DoS networks these tools may be useful to you. They are all downloadable from third-party Web sites and are not maintained.","title":"SEER"},{"location":"core/legacy-tools/#stacheldraht","text":"Stacheldraht combines features of Trinoo and TFN tools and adds encrypted communication between the attacker and the masters. Stacheldraht uses TCP for encrypted communication between the attacker and the masters, and TCP or ICMP for communication between master and agents. Another added feature is the ability to perform automatic updates of agent code. Available attacks are UDP flood, TCP SYN flood, ICMP ECHO flood and Smurf attacks. Website : http://packetstormsecurity.org/distributed/stachel.tgz","title":"Stacheldraht"},{"location":"core/legacy-tools/#mstream","text":"Mstream generates a flood of TCP packets with the ACK bit set. Masters can be controlled remotely by one or more attackers using a password- protected interactive login. The communications between attacker and masters, and a master and agents, are configurable at compile time and have varied signif- icantly from incident to incident. Source addresses in attack packets are spoofed at random. The TCP ACK attack exhausts network resources and will likely cause a TCP RST to be sent to the spoofed source address (potentially also creating outgoing bandwidth consumption at the victim). Website : http://packetstormsecurity.org/distributed/mstream.txt","title":"Mstream"},{"location":"core/legacy-tools/#topology-generators-and-convertors","text":"","title":"Topology Generators and Convertors"},{"location":"core/legacy-tools/#rocketfuel-to-ns","text":"Rocketfuel-to-ns, developed by Purdue University, is a utility to convert RocketFuel-format data files into a set of configuration files runnable on am emulation testbed like the DETER testbed. Experiment configurations generated with this tool have the advantage of not being totally synthetic representations of the Internet; they provide a router-level topology based off real measurement data. This distribution also contains many sample NS files that represent real AS topologies. Website : http://www.cs.purdue.edu/homes/fahmy/software/rf2ns/index.html Runs on : UNIX Best for : Collecting real AS topologies and importing them into DETERLab.","title":"Rocketfuel-to-ns"},{"location":"core/legacy-tools/#inet","text":"Inet, developed by University of Michigan, is a generator of representative Autonomous System (AS) level Internet topologies. Website : http://topology.eecs.umich.edu/inet/ Runs on : FreeBSD, Linux, Mac OS and Solaris Best for : Synthetic topology generation, following a power law.","title":"Inet"},{"location":"core/legacy-tools/#brite","text":"Brite, developed by Boston University, is a generator of flat AS, flat Router and hierarchical topologies, interoperable with various topology generators and simulators. Website : http://www.cs.bu.edu/brite/ Best for : Synthetic topology generation using different models and a GUI.","title":"Brite"},{"location":"core/legacy-tools/#gt-itm","text":"GT-ITM: Georgia Tech Internetwork Topology Models, developed by Georgia Tech, generates graphs that model the topological structure of internetworks. Website : http://www.cc.gatech.edu/projects/gtitm/ Runs on : SunOS and Linux Best for : Synthetic topology generation for small size topologies.","title":"GT-ITM"},{"location":"core/link-delays/","text":"Per-Link Traffic Shaping (\"linkdelays\") \u00b6 In order to conserve nodes, it is possible to specify that instead of doing traffic shaping on separate delay nodes (which eats up a node for every two shaped links), it be done on the nodes that are actually generating the traffic. Under FreeBSD, just like normal delay nodes, end node (sometimes called \"per-link\") traffic shaping uses IPFW to direct traffic into the proper Dummynet pipe. On each node in a duplex link or LAN, a set of IPFW rules and Dummynet pipes is set up. As traffic enters or leaves your node, IPFW looks at the packet and stuffs it into the proper Dummynet pipe. At the proper time, Dummynet takes the packet and sends it on its way. Under Linux, end node traffic shaping is performed by the packet scheduler modules, part of the kernel NET3 implementation. Each packet is added to the appropriate scheduler queue tree and shaped as specified in your NS file. Note that Linux traffic shaping currently only supports the drop-tail queueing discipline; gred and red are not available yet. To specify end node shaping in your NS file, simply set up a normal link or LAN, and then mark it as wanting to use end node traffic shaping. For example: set link0 [$ns duplex-link $nodeA $nodeD 50Mb 0ms DropTail] set lan0 [$ns make-lan \"nodeA nodeB nodeC\" 1Mb 100ms] tb-set-endnodeshaping $link0 1 tb-set-endnodeshaping $lan0 1 Please be aware though, that the kernels are different than the standard ones in a couple of ways: The kernel runs at a 1000HZ (1024HZ in Linux) clockrate instead of 100HZ. That is, the timer interrupts 1000 (1024) times per second instead of 100. This finer granularity allows the traffic shapers to do a better job of scheduling packets. Under FreeBSD, IPFW and Dummynet are compiled into the kernel, which affects the network stack; all incoming and outgoing packets are sent into IPFW to be matched on. Under Linux, packet scheduling exists implicitly, but uses lightweight modules by default. The packet timing mechanism in the linkdelay Linux kernel uses a slightly heavier (but more precise) method. Flow-based IP forwarding is turned off. This is also known as IP ''fast forwarding'' in the FreeBSD kernel. Note that regular IP packet forwarding is still enabled. To use end node traffic shaping globally, without having to specify per link or LAN, use the following in your NS file: tb-use-endnodeshaping 1 To specify non-shaped links, but perhaps control the shaping parameters later (increase delay, decrease bandwidth, etc.) after the experiment is swapped in, use the following in your NS file: tb-force-endnodeshaping 1 Multiplexed Links \u00b6 Another feature we have added (FreeBSD only) is ''multiplexed'' (sometimes called ''emulated'') links. An emulated link is one that can be multiplexed over a physical link along with other links. Say your experimental nodes have just one physical interface (call it \"fxp0\"), but you want to create two duplex links on it: set link0 [$ns duplex-link $nodeA $nodeB 50Mb 0ms DropTail] set link1 [$ns duplex-link $nodeA $nodeC 50Mb 0ms DropTail] tb-set-multiplexed $link0 1 tb-set-multiplexed $link1 1 Without multiplexed links, your experiment would not be mappable since there are no nodes that can support the two duplex links that NodeA requires; there is only one physical interface. Using multiplexed links however, the testbed software will assign both links on NodeA to one physical interface. That is because each duplex link is only 50Mbs, while the physical link (fxp0) is 100Mbs. Of course, if your application actually tried to use more than 50Mbs on each multiplexed link, there would be a problem; a flow using more than its share on link0 would cause packets on link1 to be dropped when they otherwise would not be. ('''At this time, you cannot specify that a LAN use multiplexed links''') To prevent this problem, a multiplexed link is automatically setup to use [#LINKDELAYS per-link traffic shaping]. Each of the links in the above example would get a set of DummyNet pipes restricting their bandwidth to 50Mbs. Each link is forced to behave just as it would if the actual link bandwidth were 50Mbs. This allows the underlying physical link to support the aggregate bandwidth. Of course, the same caveats listed for per-link delays apply when using multiplexed links. As a concrete example, consider the following NS file which creates a router and attaches it to 12 other nodes: set maxnodes 12 set router [$ns node] for {set i 1} {$i <= $maxnodes} {incr i} { set node($i) [$ns node] set link($i) [$ns duplex-link $node($i) $router 30Mb 10ms DropTail] tb-set-multiplexed $link($i) 1 } tb-set-vlink-emulation vlan # Turn on routing. $ns rtproto Static Since each node has four 100Mbs interfaces, the above mapping would not be possible without the use of multiplexed links. However, since each link is defined to use 30Mbs, by using multiplexed links, the 12 links can be shared over the four physical interfaces, without oversubscribing the 400Mbs aggregate bandwidth available to the node that is assigned to the router. '' Note: while it may sound a little like channel bonding, it is not!'' FreeBSD Technical Discussion \u00b6 First, let's just look at what happens with per-link delays on a duplex link. In this case, an IPFW pipe is set up on each node. The rule for the pipe looks like: ipfw add pipe 10 ip from any to any out xmit fxp0 which says that any packet going out on fxp0 should be stuffed into pipe 10. Consider the case of a ping packet that traverses a duplex link from NodeA to NodeB: * Once the proper interface is chosen (based on routing or the fact that the destination is directly connected), the packet is handed off to IPFW, which determines that the interface (fxp0) matches the rule specified above. * The packet is then stuffed into the corresponding Dummynet pipe, to emerge sometime later (based on the traffic shaping parameters) and be placed on the wire. * The packet then arrives at NodeB. * A ping reply packet is created and addressed to NodeA, placed into the proper Dummynet pipe, and arrives at NodeA. As you can see, each packet traversed exactly one Dummynet pipe (or put another way, the entire ping/reply sequence traversed two pipes). Constructing delayed LANs is more complicated than duplex links because of the desire to allow each node in a LAN to see different delays when talking to any other node in the LAN. That is, the delay when traversing from NodeA to NodeB is different than when traversing from NodeA to NodeC. Further, the return delays might be specified completely differently so that the return trips take a different amount of time. To support this, it is necessary to insert two delay pipes for each node. One pipe is for traffic leaving the node for the LAN, and the other pipe is for traffic entering the node from the LAN. Why not create ''N'' pipes on each node for each possible destination address in the LAN, so that each packet traverses only one pipe? The reason is that a node on a LAN has only one connection to it, and multiple pipes would not respect the aggregate bandwidth cap specified. The rule for the second pipe looks like: ipfw add pipe 15 ip from any to any in recv fxp0 which says that any packet received on fxp0 should be stuffed into pipe 15. The packet is later handed up to the application, or forwarded on to the next hop, if appropriate. The addition of multiplexed links complicates things further. To multiplex several different links on a physical interface, one must use either encapsulation (ipinip, VLAN, etc) or IP interface aliases. We chose IP aliases because it does not affect the MTU size. The downside of IP aliases is that it is difficult (if not impossible) to determine what flow a packet is part of, and thus which IPFW pipe to stuff the packet into. In other words, the rules used above: ipfw add ... out xmit fxp0 ipfw add ... in recv fxp0 do not work because there are now multiple flows multiplexed onto the interface (multiple IPs) and so there is no way to distinguish which flow. Consider a duplex link in which we use the first rule above. If the packet is not addressed to a direct neighbor, the routing code lookup will return a nexthop, which '''does''' indicate the flow, but because the rule is based simply on the interface (fxp0), all flows match! Unfortunately, IPFW does not provide an interface for matching on the nexthop address, but seeing as we are kernel hackers, this is easy to deal with by adding new syntax to IPFW to allow matching on nexthop: ipfw add ... out xmit fxp0 nexthop 192.168.2.3:255.255.255.0 Now, no matter how the user alters the routing table, packets will be stuffed into the proper pipe since the nexthop indicates which directly connected virtual link the packet was sent over. The use of a mask allows for matching when directly connected to a LAN (a simplification). Multiplexed LANs present even worse problems because of the need to figure out which flow an incoming packet is part of. When a packet arrives at an interface, there is nothing in the packet to indicate which IP alias the packet was intended for (or which it came from) when the packet is not destined for the local node (is being forwarded). Linux Technical Discussion \u00b6 Traffic shaping under Linux uses the NET3 packet scheduling modules, a hierarchically composable set of disciplines providing facilities such as bandwidth limiting, packet loss, and packet delay. As in the FreeBSD case, simplex (outgoing) link shaping is used on point-to-point links, while duplex shaping (going out, and coming in an interface) is used with LANs. See the previous section to understand why this is done. Unlike FreeBSD, Linux traffic shaping modules must be connected directly to a network device, and hence don't require a firewall directive to place packets into them. This means that all packets must pass through the shaping tree connected to a particular interface. Note that filters may be used on the shapers themselves to discriminate traffic flows, so it's not strictly the case that all traffic must be shaped if modules are attached. However, all traffic to an interface, at the least, is queued and de-queued through the root module of the shaping hierarchy. And all interfaces have at least a root module, but it is normally just a fast FIFO. Also of note is the fact that Linux traffic shaping normally only happens on the outgoing side of an interface, and requires a special virtual network device (known as an intermediate queuing device or IMQ) to capture incoming packets for shaping. This also requires the aid of the Linux firewalling facility, iptables, to divert the packets to the IMQs prior to routing. Here is an example duplex-link configuration with 50Mbps of bandwidth, a 0.05 PLR, and 20ms of delay in both directions: Outgoing side setup commands: # implicitly sets up class 1:1 tc qdisc add dev eth0 root handle 1 plr 0.05 # attach to class 1:1 and tell the module the default place to send # traffic is to class 2:1 (could attach filters to discriminate) tc qdisc add dev eth0 parent 1:1 handle 2 htb default 1 # class 2:1 does the actual limiting tc class add dev eth0 parent 2 classid 2:1 htb rate 50Mbit ceil 50Mbit # attach to class 2:1, also implicitly creates class 3:1, and attaches # a FIFO queue to it. tc qdisc add dev eth0 parent 2:1 handle 3 delay usecs 20000 The incoming side setup commands will look the same, but with eth0 replaced by imq0. Also, we have to tell the kernel to send packets coming into eth0 to imq0 (where they will be shaped): iptables -t mangle -A PREROUTING -i eth0 -j IMQ --todev 0 A flood ping sequence utilizing eth0 (echo->echo-reply) would experience a round trip delay of 40 ms, be restricted to 50Mbit, and have a 10% chance of losing packets. The doubling of numbers is due to shaping as packets go out, and come back in the interface. At the time of writing, we don't support multiplexed links under Linux, so no explicit matching against nexthop is necessary.","title":"End Node Traffic Shaping"},{"location":"core/link-delays/#per-link-traffic-shaping-linkdelays","text":"In order to conserve nodes, it is possible to specify that instead of doing traffic shaping on separate delay nodes (which eats up a node for every two shaped links), it be done on the nodes that are actually generating the traffic. Under FreeBSD, just like normal delay nodes, end node (sometimes called \"per-link\") traffic shaping uses IPFW to direct traffic into the proper Dummynet pipe. On each node in a duplex link or LAN, a set of IPFW rules and Dummynet pipes is set up. As traffic enters or leaves your node, IPFW looks at the packet and stuffs it into the proper Dummynet pipe. At the proper time, Dummynet takes the packet and sends it on its way. Under Linux, end node traffic shaping is performed by the packet scheduler modules, part of the kernel NET3 implementation. Each packet is added to the appropriate scheduler queue tree and shaped as specified in your NS file. Note that Linux traffic shaping currently only supports the drop-tail queueing discipline; gred and red are not available yet. To specify end node shaping in your NS file, simply set up a normal link or LAN, and then mark it as wanting to use end node traffic shaping. For example: set link0 [$ns duplex-link $nodeA $nodeD 50Mb 0ms DropTail] set lan0 [$ns make-lan \"nodeA nodeB nodeC\" 1Mb 100ms] tb-set-endnodeshaping $link0 1 tb-set-endnodeshaping $lan0 1 Please be aware though, that the kernels are different than the standard ones in a couple of ways: The kernel runs at a 1000HZ (1024HZ in Linux) clockrate instead of 100HZ. That is, the timer interrupts 1000 (1024) times per second instead of 100. This finer granularity allows the traffic shapers to do a better job of scheduling packets. Under FreeBSD, IPFW and Dummynet are compiled into the kernel, which affects the network stack; all incoming and outgoing packets are sent into IPFW to be matched on. Under Linux, packet scheduling exists implicitly, but uses lightweight modules by default. The packet timing mechanism in the linkdelay Linux kernel uses a slightly heavier (but more precise) method. Flow-based IP forwarding is turned off. This is also known as IP ''fast forwarding'' in the FreeBSD kernel. Note that regular IP packet forwarding is still enabled. To use end node traffic shaping globally, without having to specify per link or LAN, use the following in your NS file: tb-use-endnodeshaping 1 To specify non-shaped links, but perhaps control the shaping parameters later (increase delay, decrease bandwidth, etc.) after the experiment is swapped in, use the following in your NS file: tb-force-endnodeshaping 1","title":"Per-Link Traffic Shaping (\"linkdelays\")"},{"location":"core/link-delays/#multiplexed-links","text":"Another feature we have added (FreeBSD only) is ''multiplexed'' (sometimes called ''emulated'') links. An emulated link is one that can be multiplexed over a physical link along with other links. Say your experimental nodes have just one physical interface (call it \"fxp0\"), but you want to create two duplex links on it: set link0 [$ns duplex-link $nodeA $nodeB 50Mb 0ms DropTail] set link1 [$ns duplex-link $nodeA $nodeC 50Mb 0ms DropTail] tb-set-multiplexed $link0 1 tb-set-multiplexed $link1 1 Without multiplexed links, your experiment would not be mappable since there are no nodes that can support the two duplex links that NodeA requires; there is only one physical interface. Using multiplexed links however, the testbed software will assign both links on NodeA to one physical interface. That is because each duplex link is only 50Mbs, while the physical link (fxp0) is 100Mbs. Of course, if your application actually tried to use more than 50Mbs on each multiplexed link, there would be a problem; a flow using more than its share on link0 would cause packets on link1 to be dropped when they otherwise would not be. ('''At this time, you cannot specify that a LAN use multiplexed links''') To prevent this problem, a multiplexed link is automatically setup to use [#LINKDELAYS per-link traffic shaping]. Each of the links in the above example would get a set of DummyNet pipes restricting their bandwidth to 50Mbs. Each link is forced to behave just as it would if the actual link bandwidth were 50Mbs. This allows the underlying physical link to support the aggregate bandwidth. Of course, the same caveats listed for per-link delays apply when using multiplexed links. As a concrete example, consider the following NS file which creates a router and attaches it to 12 other nodes: set maxnodes 12 set router [$ns node] for {set i 1} {$i <= $maxnodes} {incr i} { set node($i) [$ns node] set link($i) [$ns duplex-link $node($i) $router 30Mb 10ms DropTail] tb-set-multiplexed $link($i) 1 } tb-set-vlink-emulation vlan # Turn on routing. $ns rtproto Static Since each node has four 100Mbs interfaces, the above mapping would not be possible without the use of multiplexed links. However, since each link is defined to use 30Mbs, by using multiplexed links, the 12 links can be shared over the four physical interfaces, without oversubscribing the 400Mbs aggregate bandwidth available to the node that is assigned to the router. '' Note: while it may sound a little like channel bonding, it is not!''","title":"Multiplexed Links"},{"location":"core/link-delays/#freebsd-technical-discussion","text":"First, let's just look at what happens with per-link delays on a duplex link. In this case, an IPFW pipe is set up on each node. The rule for the pipe looks like: ipfw add pipe 10 ip from any to any out xmit fxp0 which says that any packet going out on fxp0 should be stuffed into pipe 10. Consider the case of a ping packet that traverses a duplex link from NodeA to NodeB: * Once the proper interface is chosen (based on routing or the fact that the destination is directly connected), the packet is handed off to IPFW, which determines that the interface (fxp0) matches the rule specified above. * The packet is then stuffed into the corresponding Dummynet pipe, to emerge sometime later (based on the traffic shaping parameters) and be placed on the wire. * The packet then arrives at NodeB. * A ping reply packet is created and addressed to NodeA, placed into the proper Dummynet pipe, and arrives at NodeA. As you can see, each packet traversed exactly one Dummynet pipe (or put another way, the entire ping/reply sequence traversed two pipes). Constructing delayed LANs is more complicated than duplex links because of the desire to allow each node in a LAN to see different delays when talking to any other node in the LAN. That is, the delay when traversing from NodeA to NodeB is different than when traversing from NodeA to NodeC. Further, the return delays might be specified completely differently so that the return trips take a different amount of time. To support this, it is necessary to insert two delay pipes for each node. One pipe is for traffic leaving the node for the LAN, and the other pipe is for traffic entering the node from the LAN. Why not create ''N'' pipes on each node for each possible destination address in the LAN, so that each packet traverses only one pipe? The reason is that a node on a LAN has only one connection to it, and multiple pipes would not respect the aggregate bandwidth cap specified. The rule for the second pipe looks like: ipfw add pipe 15 ip from any to any in recv fxp0 which says that any packet received on fxp0 should be stuffed into pipe 15. The packet is later handed up to the application, or forwarded on to the next hop, if appropriate. The addition of multiplexed links complicates things further. To multiplex several different links on a physical interface, one must use either encapsulation (ipinip, VLAN, etc) or IP interface aliases. We chose IP aliases because it does not affect the MTU size. The downside of IP aliases is that it is difficult (if not impossible) to determine what flow a packet is part of, and thus which IPFW pipe to stuff the packet into. In other words, the rules used above: ipfw add ... out xmit fxp0 ipfw add ... in recv fxp0 do not work because there are now multiple flows multiplexed onto the interface (multiple IPs) and so there is no way to distinguish which flow. Consider a duplex link in which we use the first rule above. If the packet is not addressed to a direct neighbor, the routing code lookup will return a nexthop, which '''does''' indicate the flow, but because the rule is based simply on the interface (fxp0), all flows match! Unfortunately, IPFW does not provide an interface for matching on the nexthop address, but seeing as we are kernel hackers, this is easy to deal with by adding new syntax to IPFW to allow matching on nexthop: ipfw add ... out xmit fxp0 nexthop 192.168.2.3:255.255.255.0 Now, no matter how the user alters the routing table, packets will be stuffed into the proper pipe since the nexthop indicates which directly connected virtual link the packet was sent over. The use of a mask allows for matching when directly connected to a LAN (a simplification). Multiplexed LANs present even worse problems because of the need to figure out which flow an incoming packet is part of. When a packet arrives at an interface, there is nothing in the packet to indicate which IP alias the packet was intended for (or which it came from) when the packet is not destined for the local node (is being forwarded).","title":"FreeBSD Technical Discussion"},{"location":"core/link-delays/#linux-technical-discussion","text":"Traffic shaping under Linux uses the NET3 packet scheduling modules, a hierarchically composable set of disciplines providing facilities such as bandwidth limiting, packet loss, and packet delay. As in the FreeBSD case, simplex (outgoing) link shaping is used on point-to-point links, while duplex shaping (going out, and coming in an interface) is used with LANs. See the previous section to understand why this is done. Unlike FreeBSD, Linux traffic shaping modules must be connected directly to a network device, and hence don't require a firewall directive to place packets into them. This means that all packets must pass through the shaping tree connected to a particular interface. Note that filters may be used on the shapers themselves to discriminate traffic flows, so it's not strictly the case that all traffic must be shaped if modules are attached. However, all traffic to an interface, at the least, is queued and de-queued through the root module of the shaping hierarchy. And all interfaces have at least a root module, but it is normally just a fast FIFO. Also of note is the fact that Linux traffic shaping normally only happens on the outgoing side of an interface, and requires a special virtual network device (known as an intermediate queuing device or IMQ) to capture incoming packets for shaping. This also requires the aid of the Linux firewalling facility, iptables, to divert the packets to the IMQs prior to routing. Here is an example duplex-link configuration with 50Mbps of bandwidth, a 0.05 PLR, and 20ms of delay in both directions: Outgoing side setup commands: # implicitly sets up class 1:1 tc qdisc add dev eth0 root handle 1 plr 0.05 # attach to class 1:1 and tell the module the default place to send # traffic is to class 2:1 (could attach filters to discriminate) tc qdisc add dev eth0 parent 1:1 handle 2 htb default 1 # class 2:1 does the actual limiting tc class add dev eth0 parent 2 classid 2:1 htb rate 50Mbit ceil 50Mbit # attach to class 2:1, also implicitly creates class 3:1, and attaches # a FIFO queue to it. tc qdisc add dev eth0 parent 2:1 handle 3 delay usecs 20000 The incoming side setup commands will look the same, but with eth0 replaced by imq0. Also, we have to tell the kernel to send packets coming into eth0 to imq0 (where they will be shaped): iptables -t mangle -A PREROUTING -i eth0 -j IMQ --todev 0 A flood ping sequence utilizing eth0 (echo->echo-reply) would experience a round trip delay of 40 ms, be restricted to 50Mbit, and have a 10% chance of losing packets. The doubling of numbers is due to shaping as packets go out, and come back in the interface. At the time of writing, we don't support multiplexed links under Linux, so no explicit matching against nexthop is necessary.","title":"Linux Technical Discussion"},{"location":"core/logging-in/","text":"Logging into your Node \u00b6 By the time you receive the email message listing your nodes, the DETER configuration system will have ensured that your nodes are fully configured and ready to use. If you have selected one of the DETER-supported operating system images see supported images ), this configuration process includes: * loading fresh disk images so that each node is in a known clean state; * rebooting each node so that it is running the OS specified in the NS script; * configuring each of the network interfaces so that each one is \"up\" and talking to its virtual LAN (VLAN); * creating user accounts for each of the project members; * mounting the projects NFS directory in /proj so that project files are easily shared amongst all the nodes in the experiment; * creating a /etc/hosts file on each node so that you may refer to the experimental interfaces of other nodes by name instead of IP number; * configuring all of the delay parameters; * configuring the serial console lines so that project members may access the console ports from users.deterlab.net. As this point you may log into any of the nodes in your experiment. You will need to use Secure Shell (ssh) to log into users.deterlab.net Your login name and password will be the same as your Web Interface login and password. Note Although you can log into the web interface using your email address instead of your login name, you must use your login name when logging into users.deterlab.net . Once logged into users you can then SSH to your nodes. You should use the `qualified name' from the nodes mapping table so that you do not form dependencies on any particular physical node. For more information on using SSH with DETER, please take a look at the DETER SSH wiki page.","title":"Logging into your Node"},{"location":"core/logging-in/#logging-into-your-node","text":"By the time you receive the email message listing your nodes, the DETER configuration system will have ensured that your nodes are fully configured and ready to use. If you have selected one of the DETER-supported operating system images see supported images ), this configuration process includes: * loading fresh disk images so that each node is in a known clean state; * rebooting each node so that it is running the OS specified in the NS script; * configuring each of the network interfaces so that each one is \"up\" and talking to its virtual LAN (VLAN); * creating user accounts for each of the project members; * mounting the projects NFS directory in /proj so that project files are easily shared amongst all the nodes in the experiment; * creating a /etc/hosts file on each node so that you may refer to the experimental interfaces of other nodes by name instead of IP number; * configuring all of the delay parameters; * configuring the serial console lines so that project members may access the console ports from users.deterlab.net. As this point you may log into any of the nodes in your experiment. You will need to use Secure Shell (ssh) to log into users.deterlab.net Your login name and password will be the same as your Web Interface login and password. Note Although you can log into the web interface using your email address instead of your login name, you must use your login name when logging into users.deterlab.net . Once logged into users you can then SSH to your nodes. You should use the `qualified name' from the nodes mapping table so that you do not form dependencies on any particular physical node. For more information on using SSH with DETER, please take a look at the DETER SSH wiki page.","title":"Logging into your Node"},{"location":"core/node-types/","text":"Node Types \u00b6 This is not a complete list of all node types available at DETERLab, but below are the primary types. dl380g3 MicroCloud pc2133 (including pc2133 and bpc2133) pc3000 (including pc3000, bpc3000, pc3060, bpc3060, and pc3100) bvx2200 bpc2800 netfpga2 dl380g3 \u00b6 There are 120 dl380g3 class nodes available at ISI . Machine: HP Proliant DL360 G8 Server Each node has: Dual Intel(R) Xeon(R) hexa-core processors running at 2.2 Ghz with 15MB cache Intel VT-x support 24GB of RAM One 1Tb SATA HP Proliant Disk Drive 7.2k rpm G8 (boot priority) One 240Gb SATA HP Proliant Solid State Drive G8 Two experimental interfaces: One Dual port PCIe Intel Ten Gigabit Ethernet card for experimental ports One Quad port PCIe Intel Gigabit Ethernet card, presently with one port wired to the control network MicroCloud \u00b6 There are 128 MicroCloud nodes at ISI . Machine: High Density SuperMicro MicroCloud Chassis that fits 8 nodes in 3u of rack space. Each node has: One Intel(R) Xeon(R) E3-1260L quad-core processor running at 2.4 Ghz Intel VT-x and VT-d support 16GB of RAM One 250Gb SATA Western Digital RE4 Disk Drive 5 experimental interfaces One Dual port PCIe Intel Gigabit Ethernet card for the control network and an experimental port One Quad port PCIe Intel Gigabit Ethernet card for experimental network pc2133 \u00b6 This node type includes pc2133 and bpc2133: There are 63 pc2133 nodes at ISI . There are 64 bpc2133 nodes at UCB . The pc2133 and bpc2133 machines have the following features: Dell PowerEdge 860 Chasis One Intel(R) Xeon(R) CPU X3210 quad core processor running at 2.13 Ghz 4GB of RAM One 250Gb SATA Disk Drive One Dual port PCI-X Intel Gigabit Ethernet card for the control network (only one port is used). One Quad port PCIe Intel Gigabit Ethernet card for experimental network. CPU flags: \u00b6 fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx lm constant_tsc pni monitor ds_cpl '''vmx''' est tm2 ssse3 cx16 xtpr lahf_lm pc3000 \u00b6 This node type includes pc3000, bpc3000, pc3060, bpc3060, and pc3100. There are: 0 pc3000 nodes at ISI 32 bpc3000 nodes at UCB 17 pc3060 nodes at ISI 32 bpc3060 nodes at UCB 4 pc3100 nodes at ISI pc3000 and bpc3000 have the following features: Dell PowerEdge 1850 Chassis. Dual 3Ghz Intel Xeon processors. 2 GB of RAM One 36Gb 15k RPM SCSI drive (bpc machines may be configured with two). 4 Intel Gigabit experimental network ports. 1 Intel Gigabit experimental network port. pc3060 and bpc3060 machines are the same as the pc3000/bpc3000 machines except that they have one more experimental network interface. pc3100 machines have a total of 9 experimental interfaces and 1 control network interface. There are only 4 of these type of machine. bvx2200 \u00b6 There are 31 bvx2200 nodes at UCB . bvx2200 has the following features: Sun Microsystems Sun Fire X2100 M2 Chassis. Dual-Core 1.8 Ghz AMD Opteron(tm) Processor 1210. One 250Gb 7200 RPM SATA drive. 1 Broadcom NetXtreme Gigabit experimental network port. 2 Nvidia nForce MCP55 experimental network ports. 2 Intel Gigabit experimental network ports. bpc2800 \u00b6 There are 30 bpc2800 at UCB . The bpc2800 machines have the following features: Sun Microsystems Sun Fire V60 Chassis One Intel(R) Xeon(R) CPU dual core processor running at 2.8 GHz 2 GB of RAM One 36 GB SCSI Disk Drive Two Dual port PCI-X Intel Gigabit Ethernet cards, 1 port for control network and 3 ports for experimental network One Single port PCI-X Intel Gigabit Ethernet card for experimental network CPU flags \u00b6 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe pebs bts cid xtpr netfpga2 \u00b6 There are 10 netfpga2 class nodes in the testbed. These are pc2133 class machines. Each of these nodes has a single NetFPGA card installed.","title":"DETERLab Node Types"},{"location":"core/node-types/#node-types","text":"This is not a complete list of all node types available at DETERLab, but below are the primary types. dl380g3 MicroCloud pc2133 (including pc2133 and bpc2133) pc3000 (including pc3000, bpc3000, pc3060, bpc3060, and pc3100) bvx2200 bpc2800 netfpga2","title":"Node Types"},{"location":"core/node-types/#dl380g3","text":"There are 120 dl380g3 class nodes available at ISI . Machine: HP Proliant DL360 G8 Server Each node has: Dual Intel(R) Xeon(R) hexa-core processors running at 2.2 Ghz with 15MB cache Intel VT-x support 24GB of RAM One 1Tb SATA HP Proliant Disk Drive 7.2k rpm G8 (boot priority) One 240Gb SATA HP Proliant Solid State Drive G8 Two experimental interfaces: One Dual port PCIe Intel Ten Gigabit Ethernet card for experimental ports One Quad port PCIe Intel Gigabit Ethernet card, presently with one port wired to the control network","title":"dl380g3"},{"location":"core/node-types/#microcloud","text":"There are 128 MicroCloud nodes at ISI . Machine: High Density SuperMicro MicroCloud Chassis that fits 8 nodes in 3u of rack space. Each node has: One Intel(R) Xeon(R) E3-1260L quad-core processor running at 2.4 Ghz Intel VT-x and VT-d support 16GB of RAM One 250Gb SATA Western Digital RE4 Disk Drive 5 experimental interfaces One Dual port PCIe Intel Gigabit Ethernet card for the control network and an experimental port One Quad port PCIe Intel Gigabit Ethernet card for experimental network","title":"MicroCloud"},{"location":"core/node-types/#pc2133","text":"This node type includes pc2133 and bpc2133: There are 63 pc2133 nodes at ISI . There are 64 bpc2133 nodes at UCB . The pc2133 and bpc2133 machines have the following features: Dell PowerEdge 860 Chasis One Intel(R) Xeon(R) CPU X3210 quad core processor running at 2.13 Ghz 4GB of RAM One 250Gb SATA Disk Drive One Dual port PCI-X Intel Gigabit Ethernet card for the control network (only one port is used). One Quad port PCIe Intel Gigabit Ethernet card for experimental network.","title":"pc2133"},{"location":"core/node-types/#cpu-flags","text":"fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx lm constant_tsc pni monitor ds_cpl '''vmx''' est tm2 ssse3 cx16 xtpr lahf_lm","title":"CPU flags:"},{"location":"core/node-types/#pc3000","text":"This node type includes pc3000, bpc3000, pc3060, bpc3060, and pc3100. There are: 0 pc3000 nodes at ISI 32 bpc3000 nodes at UCB 17 pc3060 nodes at ISI 32 bpc3060 nodes at UCB 4 pc3100 nodes at ISI pc3000 and bpc3000 have the following features: Dell PowerEdge 1850 Chassis. Dual 3Ghz Intel Xeon processors. 2 GB of RAM One 36Gb 15k RPM SCSI drive (bpc machines may be configured with two). 4 Intel Gigabit experimental network ports. 1 Intel Gigabit experimental network port. pc3060 and bpc3060 machines are the same as the pc3000/bpc3000 machines except that they have one more experimental network interface. pc3100 machines have a total of 9 experimental interfaces and 1 control network interface. There are only 4 of these type of machine.","title":"pc3000"},{"location":"core/node-types/#bvx2200","text":"There are 31 bvx2200 nodes at UCB . bvx2200 has the following features: Sun Microsystems Sun Fire X2100 M2 Chassis. Dual-Core 1.8 Ghz AMD Opteron(tm) Processor 1210. One 250Gb 7200 RPM SATA drive. 1 Broadcom NetXtreme Gigabit experimental network port. 2 Nvidia nForce MCP55 experimental network ports. 2 Intel Gigabit experimental network ports.","title":"bvx2200"},{"location":"core/node-types/#bpc2800","text":"There are 30 bpc2800 at UCB . The bpc2800 machines have the following features: Sun Microsystems Sun Fire V60 Chassis One Intel(R) Xeon(R) CPU dual core processor running at 2.8 GHz 2 GB of RAM One 36 GB SCSI Disk Drive Two Dual port PCI-X Intel Gigabit Ethernet cards, 1 port for control network and 3 ports for experimental network One Single port PCI-X Intel Gigabit Ethernet card for experimental network","title":"bpc2800"},{"location":"core/node-types/#cpu-flags_1","text":"fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe pebs bts cid xtpr","title":"CPU flags"},{"location":"core/node-types/#netfpga2","text":"There are 10 netfpga2 class nodes in the testbed. These are pc2133 class machines. Each of these nodes has a single NetFPGA card installed.","title":"netfpga2"},{"location":"core/ns-commands/","text":"NS Commands \u00b6 In order to use the testbed specific commands, you must include the following line near the top of your NS topology file (before any testbed commands are used): source tb_compat.tcl If you wish to use your file under NS, download tb_compat.tcl and place it in the same directory as your NS file. When run in this way under NS, the testbed commands will have no effect, but NS will be able to parse your file. TCL, NS, and node names \u00b6 In your file, you will be creating nodes with something like the following line: set node1 [$ns node] With this command, the simulator, represented by $ns is creating a new node involving many internal data changes and returning a reference to it which is stored in the variable node1 . In almost all cases when you need to refer to a node, you will do it as $node1 , the $ indicating that you want the value of the variable node1 , i.e. the reference to the node. Thus you will be issuing commands like: $ns duplex-link $node1 $node2 100Mb 150ms DropTail tb-set-ip $node1 10.1.0.2 Note the instances of $ . You will notice that when your experiment is set up, the node names and such will be node1 , node2 , node3 , etc. This happens because the parser detects what variable you are using to store the node reference and uses that as the node name. In the case that you do something like: set node1 [$ns node2] set A $node1 The node will still be called node1 as that was the first variable to contain the reference. If you are dealing with many nodes you may store them in an array, using a command similar to the following: for {set i 0} {$i < 4} {incr i} { set nodes($i) [$ns node] } In this case, the names of the node will be nodes-0 , nodes-1 , nodes-2 , nodes-3 . In other words, the \"(\" character is replaced with \"-\", and \")\" is removed. This slightly different syntax is used to avoid any problems that \"()\" may cause later in the process. For example, the \"()\" characters may not appear in DNS entries. As a final note, everything said above for nodes applies equally to LANs, i.e.: set lan0 [$ns make-lan \"$node0 $node1\" 100Mb 0ms] tb-set-lan-loss $lan0 .02 Again, note the instances of $ . Links may also be named just like nodes and LANs. The names may then be used to set loss rates or IP addresses. This technique is the only way to set such attributes when there are multiple links between two nodes. set link1 [$ns duplex-link $node0 $node1 100Mb 0ms DropTail] tb-set-link-loss $link1 0.05 tb-set-ip-link $node0 $link1 10.1.0.128 Captured NS file parameters \u00b6 A common convention when writing NS files is to place any parameters in an array named opt at the beginning of the file. For example: set opt(CLIENT_COUNT) 5 set opt(BW) 10mb; Link bandwidth set opt(LAT) 10ms; Link latency ... $ns duplex-link $server $router $opt(BW) $opt(LAT) DropTail for {set i 0} {$i < $opt(CLIENT_COUNT)} {incr i} { set nodes($i) [$ns node] ... } set serverprog [$server program-agent -command \"starter.sh\"] Normally, this convention is only used to help organize the parameters. In DETERLab, however, the contents of the opt array are captured and made available to the emulated environment. For instance, the parameters are added as environment variables to any commands run by program-agents. So in the above example of NS code, the starter.sh script will be able to reference parameters by name, like so: #! /bin/sh echo \"Testing with $CLIENT_COUNT clients.\" ... Note that the contents of the opt array are not ordered, so you should not reference other parameters and expect the shell to expand them appropriately: set opt(prefix) \"/foo/bar\" set opt(BINDIR) '$prefix/bin'; # BAD set opt(prefix) \"/foo/bar\" set opt(BINDIR) \"$opt(prefix)/bin\"; # Good Ordering Issues \u00b6 tb- commands have the same status as all other Tcl and NS commands. Thus the order matters not only relative to each other but also relative to other commands. One common example of this is that IP commands must be issued after the links or LANs are created. Hardware Commands \u00b6 tb-set-hardware \u00b6 tb-set-hardware node type [args] tb-set-hardware $node3 pc tb-set-hardware $node4 shark where: node = The name of the node. type = The type of the node. Note Please see the Node Status page for a list of available types. pc is the default type. No current types have any additional arguments. IP Address Commands \u00b6 Each node will be assigned an IP address for each interface that is in use. The following commands will allow you to explicitly set those IP addresses. IP addresses will be automatically generated for all nodes for which you do not explicitly set IP addresses. In most cases, the IP addresses on either side of a link must be in the same subnet. Likewise, all IP addresses on a LAN should be in the same subnet. Generally the same subnet should not be used for more than one link or LAN in a given experiment, nor should one node have multiple interfaces in the same subnet. Automatically generated IP addresses will conform to these requirements. If part of a link or LAN is explicitly specified with the commands below then the remainder will be automatically generated under the same subnet. IP address assignment is deterministic and tries to fill lower IP's first, starting at 2. Except in the partial specification case (see above), all automatic IP addresses are in the network 10 . tb-set-ip \u00b6 tb-set-ip node ip tb-set-ip $node1 142.3.4.5 where: node = The node to assign the IP address to ip = The IP address. Note This command should only be used for nodes that have a single link. For nodes with multiple links the following commands should be used. Mixing tb-set-ip and any other IP command on the same node will result in an error. tb-set-ip-link \u00b6 tb-set-ip-link node link ip tb-set-ip-link $node0 $link0 142.3.4.6 where: node = The node to set the IP for. link = The link to set the IP for. ip = The IP address. Note One way to think of the arguments is a link with the node specifying which side of the link to set the IP for. This command cannot be mixed with tb-set-ip on the same node. tb-set-ip-lan \u00b6 tb-set-ip-lan node lan ip tb-set-ip-lan $node1 $lan0 142.3.4.6 where: node = The node to set the IP for. lan = The lan the IP is on. ip = The IP address. Note One way to think of the arguments is a node with the LAN specifying which port to set the IP address for. This command cannot be mixed with tb-set-ip on the same node. tb-set-ip-interface \u00b6 tb-set-ip-interface node dst ip tb-set-ip-interface $node2 $node1 142.3.4.6 where: node = The node to set the IP for. dst = The destination of the link to set the IP for. IP = The IP address. Note This command cannot be mixed on the same node with tb-set-ip . (See above) In the case of multiple links between the same pair of nodes, there is no way to distinguish which link to the set the IP for. This should be fixed soon. This command is converted internally to either tb-set-ip-link or tb-set-ip-lan . It is possible that error messages will report either of those commands instead of tb-set-ip-interface . tb-set-netmask \u00b6 tb-set-netmask lanlink netmask tb-set-netmask $link0 \"255.255.255.248\" where: lanlink = The lan or link to set the netmask for. netmask = The netmask in dotted notation. Note This command sets the netmask for a LAN or link. The mask must be big enough to support all of the nodes on the LAN or link! You may play with the bottom three octets (0xFFFFFXXX) of the mask; attempts to change the upper octets will cause an error. OS Commands \u00b6 tb-set-node-os \u00b6 tb-set-node-os node os tb-set-node-os $node1 FBSD-STD tb-set-node-os $node1 MY_OS where: node = The node to set the OS for. os = The id of the OS for that node. Note The OSID may either by one of the standard OS's we provide or a custom OSID, created via the web interface. If no OS is specified for a node, a default OS is chosen based on the nodes type. This is currently 'Ubuntu1604-STD ' for PCs. The currently available standard OS types are listed in the OS Images page. tb-set-node-rpms \u00b6 tb-set-node-rpms node rpms... tb-set-node-rpms $node0 rpm1 rpm2 rpm3 Note This command sets which RPMs are to be installed on the node when it first boots after being assigned to an experiment. Each RPM can be either a path to a file or a URL. Paths must be to files that reside in the /proj or /groups directory. You are not allowed to place your RPMs in your home directory. http(s):// and ftp:// URLs will be fetched into the experiment's directory, and re-distributed from there. See the Core Guide for more information. tb-set-node-startcmd \u00b6 tb-set-node-startcmd node startupcmd tb-set-node-startcmd $node0 \"mystart.sh -a >& /tmp/node0.log\" Note Specify a script or program to be run when the node is booted. See the Core Guide for more information. tb-set-node-cmdline \u00b6 tb-set-node-cmdline node cmdline tb-set-node-cmdline $node0 {???} Note Set the commandline to be passed to the kernel when it is booted. Currently, this is supported on OSKit kernels only. tb-set-node-tarfiles \u00b6 tb-set-node-tarfiles node install-dir1 tarfile1 ... The tb-set-node-tarfiles command is used to install one or more tar files onto a node's local disk. This command is useful for installing files that are used frequently, but will change very little during the course of your experiments. For example, if your software depends on a third-party library not provided in the standard disk images, you can produce a tarball and have the library ready for use on all the experimental nodes. Another example would be the data sets for your software. The benefit of installing files using this method is that they will reside on the node's local disk, so your experimental runs will not be disrupted by NFS traffic. Note Avoid using this command if the files are changing frequently because the tars are only (re)installed when the nodes boot. Installing individual tar files or RPMs is a midpoint in the spectrum of getting software onto the experimental nodes. At one extreme, you can read everything over NFS, which works well if the files are changing constantly, but can generate a great deal of strain on the control network and disrupt your experiment. The tar files and RPMs are also read over NFS when the nodes initially boot; however, there won't be any extra NFS traffic while you are running your experiment. Finally, if you need a lot of software installed on a large number of nodes, say greater than 20, it might be best to create a custom disk image . Using a disk image is easier on the control network since it is transferred using multicast, thus greatly reducing the amount of NFS traffic when the experiment is swapped in. Required Parameters: node - The node where the files should be installed. Each node has its own tar file list, which may or may not be different from the others. One or more install-dir and tarfile pairs are then listed in the order you wish them to be installed: install-dir - An existing directory on the node where the tar file should be unarchived (e.g. / , /usr , /usr/local ). The tar command will be run as \"root\" [#tb-set-node-tarfiles Note1], so all of the node's directories will be accessible to you. If the directory does not exist on the image or was not created by the unarchiving of a previous tar file, the installation will fail [#tb-set-node-tarfiles Note2]. tarfile - An existing tar file located in a project directory (e.g. /proj or /groups ) or an http , https , or ftp URL. In the case of URLs, they are downloaded when the experiment is swapped in and cached in the experiment's directory for future use. In either case, the tar file name is required to have one of the following extensions: .tar, .tar.Z, .tar.gz, or .tgz. Note that the tar file could have been created anywhere; however, if you want the unarchived files to have valid DETERLab user and group id's, you should create the tar file on ops or an experimental node. Example usage: # Overwrite files in /bin and /sbin. tb-set-node-tarfiles $node0 /bin /proj/foo/mybinmods.tar /sbin /proj/foo/mysbinmods.tar # Programmatically generate the list of tarballs. set tb [list] # Add a tarball located on a web site. lappend tb / http://foo.bar/bazzer.tgz # Add a tarball located in the DETER NFS space. lappend tb /usr/local /proj/foo/tarfiles/bar.tar.gz # Use 'eval' to expand the 'tb' list into individual # arguments to the tb-set-node-tarfiles command. eval tb-set-node-tarfiles $node1 $tb See also: tb-set-node-rpms Custom disk images Note Because the files are installed as root, care must be taken to protect the tar file so it cannot be replaced with a trojan that allowed less privileged users to become root. Currently, you can only tell how/why an installation failed by examining the node's console log on bootup. Link Loss Commands \u00b6 This is the NS syntax for creating a link: $ns duplex-link $node1 $node2 100Mb 150ms DropTail Note This does not allow for specifying link loss rates. DETERLab does, however, support link loss. The following commands can be used to specify link loss rates. tb-set-link-loss \u00b6 tb-set-link-loss src dst loss tb-set-link-loss link loss tb-set-link-loss $node1 $node2 0.05 tb-set-link-loss $link1 0.02 where: src , dst = Two nodes to describe the link. link = The link to set the rate for. loss = The loss rate (between 0 and 1). Note There are two syntaxes available. The first specifies a link by a source/destination pair. The second explicitly specifies the link. The source/destination pair is incapable of describing an individual link in the case of multiple links between two nodes. Use the second syntax for this case. tb-set-lan-loss \u00b6 tb-set-lan-loss lan loss tb-set-lan-loss $lan1 0.3 Where: lan = The lan to set the loss rate for. loss = The loss rate (between 0 and 1). Note This command sets the loss rate for the entire LAN. tb-set-node-lan-delay \u00b6 tb-set-node-lan-delay node lan delay tb-set-node-lan-delay $node0 $lan0 40ms Where: node = The node we are modifying the delay for. lan = Which LAN the node is in that we are affecting. delay = The new node to switch delay (see below). Note This command changes the delay between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node, possibly incurring additional latency from any delay parameters there. If this command is not used to overwrite the delay, then the delay for a given node to switch link is taken as one half of the delay passed to make-lan . Thus in a LAN where no tb-set-node-delay calls are made, the node-to-node latency will be the latency passed to make-lan . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times. Delays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. As a convenience, a delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other. tb-set-node-lan-bandwidth \u00b6 tb-set-node-lan-bandwidth node lan bandwidth tb-set-node-lan-bandwidth $node0 $lan0 20Mb Where: node = The node we are modifying the bandwidth for. lan = Which LAN the node is in that we are affecting. bandwidth = The new node to switch bandwidth (see below). Note This command changes the bandwidth between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may have a lower bandwidth. If this command is not used to overwrite the bandwidth, then the bandwidth for a given node to switch link is taken directly from the bandwidth passed to make-lan . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times. tb-set-node-lan-loss \u00b6 tb-set-node-lan-loss node lan loss tb-set-node-lan-loss $node0 $lan0 0.05 Where: node = The node we are modifying the loss for. lan = Which LAN the node is in that we are affecting. loss = The new node to switch loss (see below). Note This command changes the loss probability between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may also have a loss chance. Thus for packet going to switch with loss chance A and then going on the destination with loss chance B , the node-to-node loss chance is (1-(1-A)(1-B)) . If this command is not used to overwrite the loss, then the loss for a given node to switch link is taken from the loss rate passed to the make-lan command. If a loss rate of L is passed to make-lan then the node to switch loss rate for each node is set to (1-sqrt(1-L)) . Because each packet will have two such chances to be lost, the node-to-loss rate comes out as the desired L . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times. tb-set-node-lan-params \u00b6 tb-set-node-lan-params node lan delay bandwidth loss tb-set-node-lan-params $node0 $lan0 40ms 20Mb 0.05 Where: node = The node we are modifying the loss for. lan = Which LAN the node is in that we are affecting. delay = The new node to switch delay. bandwidth = The new node to switch bandwidth. loss = The new node to switch loss. Note This command is exactly equivalent to calling each of the above three commands appropriately. See above for more information. tb-set-link-simplex-params \u00b6 tb-set-link-simplex-params link src delay bw loss tb-set-link-simplex-params $link1 $srcnode 100ms 50Mb 0.2 Where: link = The link we are modifying. src = The source, defining which direction we are modifying. delay = The source to destination delay. bw = The source to destination bandwidth. loss = The source to destination loss. Note This commands modifies the delay characteristics of a link in a single direction. The other direction is unchanged. This command only applies to links. Use tb-set-lan-simplex-params below for LANs. tb-set-lan-simplex-params \u00b6 tb-set-lan-simplex-params lan node todelay tobw toloss fromdelay frombw fromloss tb-set-lan-simplex-params $lan1 $node1 100ms 10Mb 0.1 5ms 100Mb 0 Where: lan = The lan we are modifying. node = The member of the lan we are modifying. todelay = Node to lan delay. tobw = Node to lan bandwidth. toloss = Node to lan loss. fromdelay = Lan to node delay. frombw = Lan to node bandwidth. fromloss = Lan to node loss. Note This command is exactly like tb-set-node-lan-params except that it allows the characteristics in each direction to be chosen separately. See all the notes for tb-set-node-lan-params . tb-set-endnodeshaping \u00b6 tb-set-endnodeshaping link-or-lan enable tb-set-endnodeshaping $link1 1 tb-set-endnodeshaping $lan1 1 Where: link-or-lan = The link or LAN we are modifying. enable = Set to 1 to enable, 0 to disable. Note This command specifies whether end node shaping is used on the specified link or LAN (instead of a delay node). Disabled by default for all links and LANs. Only available when running the standard DETERLab FreeBSD or Linux kernels. See End Node Traffic Shaping and Multiplexed Links for more details. tb-set-noshaping \u00b6 tb-set-noshaping link-or-lan enable tb-set-noshaping $link1 1 tb-set-noshaping $lan1 1 Where: link-or-lan = The link or LAN we are modifying. enable = Set to 1 to disable bandwidth shaping, 0 to enable. Note This command specifies whether link bandwidth shaping should be enforced on the specified link or LAN. When enabled, bandwidth limits indicated for a link or LAN will not be enforced. Disabled by default for all links and LANs. That is, link bandwidth shaping is enforced on all links and LANs by default. If the delay and loss values for a tb-set-noshaping link are zero (the default), then no delay node or end-node delay pipe will be associated with the link or LAN. This command is a hack. The primary purpose for this command is to subvert the topology mapper ( assign ). Assign always observes the physical bandwidth constraints of the testbed. By using tb-set-noshaping , you can convince assign that links are low-bandwidth and thus get your topology mapped, but then not actually have the links shaped. tb-use-endnodeshaping \u00b6 tb-use-endnodeshaping enable tb-use-endnodeshaping 1 Where: enable = Set to 1 to enable end-node traffic shaping on all links and LANs. Note This command allows you to use end-node traffic shaping globally, without having to specify per link or LAN with tb-set-endnodeshaping . See End Node Traffic Shaping and Multiplexed Links for more details. tb-force-endnodeshaping \u00b6 tb-force-endnodeshaping enable tb-force-endnodeshaping 1 Where: enable = Set to 1 to force end-node traffic shaping on all links and LANs. Note This command allows you to specify non-shaped links and LANs at creation time, but still control the shaping parameters later (e.g., increase delay, decrease bandwidth) after the experiment is swapped in. This command forces allocation of end-node shaping infrastructure for all links. There is no equivalent to force delay node allocation. See End Node Traffic Shaping and Multiplexed Links for more details. tb-set-multiplexed \u00b6 tb-set-multiplexed link allow tb-set-multiplexed $link1 1 Where: link = The link we are modifying. 'allow` = Set to 1 to allow multiplexing of the link, 0 to disallow. Note This command allows a link to be multiplexed over a physical link along with other links. Disabled by default for all links. Only available when running the standard DETER FreeBSD (not Linux) and only for links (not LANs). See End Node Traffic Shaping and Multiplexed Links for more details. tb-set-vlink-emulation \u00b6 tb-set-vlink-emulation style tb-set-vlink-emulation $link1 vlan Where: style = One of \"vlan\" or \"veth-ne\" Note It seems to be necessary to set the virtual link emulation style to vlan for multiplexed links to work under linux. Virtual Type Commands \u00b6 Virtual Types are a method of defining fuzzy types, i.e. types that can be fulfilled by multiple different physical types. The advantage of virtual types, also known as 'vtypes ', is that all nodes of the same vtype will usually be the same physical type of node. In this way, vtypes allows logical grouping of nodes. As an example, imagine we have a network with internal routers connecting leaf nodes. We want the routers to all have the same hardware, and the leaf nodes to all have the same hardware, but the specifics do not matter. We have the following fragment in our NS file: ... tb-make-soft-vtype router {pc600 pc850} tb-make-soft-vtype leaf {pc600 pc850} tb-set-hardware $router1 router tb-set-hardware $router2 router tb-set-hardware $leaf1 leaf tb-set-hardware $leaf2 leaf Here we have set up two soft (see below) vtypes: router and leaf. Our router nodes are then specified to be of type router , and the leaf nodes of type leaf . When the experiment is swapped in, the testbed will attempt to make router1 and router2 be of the same type, and similarly, leaf1 and leaf2 of the same type. However, the routers/leafs may be pc600s or they may be pc850s, whichever is easier to fit in to the available resources. As a basic use, vtypes can be used to request nodes that are all the same type, but can be of any available type: ... tb-make-soft-vtype N {pc600 pc850} tb-set-hardware $node1 N tb-set-hardware $node2 N Vtypes come in two varieties: hard and soft. * 'Soft ' - With soft vtypes, the testbed will try to make all nodes of that vtype the same physical type, but may do otherwise if resources are tight. * 'Hard ' - Hard vtypes behave just like soft vtypes except that the testbed will give higher priority to vtype consistency and swapping in will fail if the vtypes cannot be satisfied. Therefore, if you use soft vtypes you are more likely to swap in but there is a chance your node of a specific vtype will not all be the same. If you use hard vtypes, all nodes of a given vtype will be the same, but swapping in may fail. Further, you can have weighted soft vtypes. Here you assign a weight from 0 to 1 exclusive to your vtype. The testbed will give higher priority to consistency in the higher weighted vtypes. The primary use of this is to rank multiple vtypes by importance of consistency. Soft vtypes have a weight of 0.5 by default. As a final note, when specifying the types of a vtype, use the most specific type possible. For example, the following command is not very useful: tb-make-soft-vtype router {pc pc600} This is because pc600 is a sub type of pc. You may very well end up with two routers as type pc with different hardware, as pc covers multiple types of hardware. tb-make-soft-vtype \u00b6 tb-make-soft-vtype vtype {types} tb-make-hard-vtype vtype {types} tb-make-weighted-vtype vtype weight {types} tb-make-soft-vtype router {pc600 pc850} tb-make-hard-vtype leaf {pc600 pc850} tb-make-weighted-vtype A 0.1 {pc600 pc850} Where: vtype = The name of the vtype to create. types = One or more physical types. weight = The weight of the vtype, 0 < weight < 1. Note These commands create vtypes. See notes above for a description of vtypes and the difference between soft and hard. tb-make-soft-vtype creates vtypes with weight 0.5. vtype commands must appear before tb-set-hardware commands that use them. Do not use tb-fix-node with nodes that have a vtype. Misc. Commands \u00b6 tb-fix-node \u00b6 tb-fix-node vnode pnode tb-fix-node $node0 pc42 Where: vnode = The node we are fixing. pnode = The physical node we want used. Note This command forces the virtual node to be mapped to the specified physical node. Swap in will fail if this cannot be done. Do not use this command on nodes that are a virtual type. tb-fix-interface \u00b6 tb-fix-interface vnode vlink iface tb-fix-interface $node0 $link0 \"eth0\" Where: vnode = The node we are fixing. vlink = The link connecting to that node that we want to set. iface = The DETERLab name for the interface that is to be used. Note The interface names used are the ones in the DETERLab database - we can make no guarantee that the OS image that boots on the node assigns the same name. Different types of nodes have different sets of interfaces, so this command is most useful if you are also using tb-fix-node and/or tb-set-hardware on the vnode . tb-set-uselatestwadata \u00b6 tb-set-uselatestwadata 0 tb-set-uselatestwadata 1 Note This command indicates which widearea data to use when mapping widearea nodes to links. The default is 0, which says to use the aged data. Setting it to 1 says to use the most recent data. tb-set-wasolver-weights \u00b6 tb-set-wasolver-weights delay bw plr tb-set-wasolver-weights 1 10 500 Where: delay = The weight to give delay when solving. bw = The weight to give bandwidth when solving. plr = The weight to give lossrate when solving. Note This command sets the relative weights to use when assigning widearea nodes to links. Specifying a zero says to ignore that particular metric when doing the assignment. Setting all three to zero results in an essentially random selection. tb-set-node-failure-action \u00b6 tb-set-node-failure-action node action tb-set-node-failure-action $nodeA \"fatal\" tb-set-node-failure-action $nodeB \"nonfatal\" Where: node = The node name. action = One of \"fatal\" or \"nonfatal\". Note This command sets the failure mode for a node. When an experiment is swapped in, the default action is to abort the swapin if any nodes fail to come up normally. This is the \"fatal\" mode. You may also set a node to \"nonfatal\" which will cause node bootup failures to be reported, but otherwise ignored during swapin. Note that this can result in your experiment not working properly if a dependent node fails, but typically you can arrange your software to deal with this.","title":"NS Commands Extensions"},{"location":"core/ns-commands/#ns-commands","text":"In order to use the testbed specific commands, you must include the following line near the top of your NS topology file (before any testbed commands are used): source tb_compat.tcl If you wish to use your file under NS, download tb_compat.tcl and place it in the same directory as your NS file. When run in this way under NS, the testbed commands will have no effect, but NS will be able to parse your file.","title":"NS Commands"},{"location":"core/ns-commands/#tcl-ns-and-node-names","text":"In your file, you will be creating nodes with something like the following line: set node1 [$ns node] With this command, the simulator, represented by $ns is creating a new node involving many internal data changes and returning a reference to it which is stored in the variable node1 . In almost all cases when you need to refer to a node, you will do it as $node1 , the $ indicating that you want the value of the variable node1 , i.e. the reference to the node. Thus you will be issuing commands like: $ns duplex-link $node1 $node2 100Mb 150ms DropTail tb-set-ip $node1 10.1.0.2 Note the instances of $ . You will notice that when your experiment is set up, the node names and such will be node1 , node2 , node3 , etc. This happens because the parser detects what variable you are using to store the node reference and uses that as the node name. In the case that you do something like: set node1 [$ns node2] set A $node1 The node will still be called node1 as that was the first variable to contain the reference. If you are dealing with many nodes you may store them in an array, using a command similar to the following: for {set i 0} {$i < 4} {incr i} { set nodes($i) [$ns node] } In this case, the names of the node will be nodes-0 , nodes-1 , nodes-2 , nodes-3 . In other words, the \"(\" character is replaced with \"-\", and \")\" is removed. This slightly different syntax is used to avoid any problems that \"()\" may cause later in the process. For example, the \"()\" characters may not appear in DNS entries. As a final note, everything said above for nodes applies equally to LANs, i.e.: set lan0 [$ns make-lan \"$node0 $node1\" 100Mb 0ms] tb-set-lan-loss $lan0 .02 Again, note the instances of $ . Links may also be named just like nodes and LANs. The names may then be used to set loss rates or IP addresses. This technique is the only way to set such attributes when there are multiple links between two nodes. set link1 [$ns duplex-link $node0 $node1 100Mb 0ms DropTail] tb-set-link-loss $link1 0.05 tb-set-ip-link $node0 $link1 10.1.0.128","title":"TCL, NS, and node names"},{"location":"core/ns-commands/#captured-ns-file-parameters","text":"A common convention when writing NS files is to place any parameters in an array named opt at the beginning of the file. For example: set opt(CLIENT_COUNT) 5 set opt(BW) 10mb; Link bandwidth set opt(LAT) 10ms; Link latency ... $ns duplex-link $server $router $opt(BW) $opt(LAT) DropTail for {set i 0} {$i < $opt(CLIENT_COUNT)} {incr i} { set nodes($i) [$ns node] ... } set serverprog [$server program-agent -command \"starter.sh\"] Normally, this convention is only used to help organize the parameters. In DETERLab, however, the contents of the opt array are captured and made available to the emulated environment. For instance, the parameters are added as environment variables to any commands run by program-agents. So in the above example of NS code, the starter.sh script will be able to reference parameters by name, like so: #! /bin/sh echo \"Testing with $CLIENT_COUNT clients.\" ... Note that the contents of the opt array are not ordered, so you should not reference other parameters and expect the shell to expand them appropriately: set opt(prefix) \"/foo/bar\" set opt(BINDIR) '$prefix/bin'; # BAD set opt(prefix) \"/foo/bar\" set opt(BINDIR) \"$opt(prefix)/bin\"; # Good","title":"Captured NS file parameters"},{"location":"core/ns-commands/#ordering-issues","text":"tb- commands have the same status as all other Tcl and NS commands. Thus the order matters not only relative to each other but also relative to other commands. One common example of this is that IP commands must be issued after the links or LANs are created.","title":"Ordering Issues"},{"location":"core/ns-commands/#hardware-commands","text":"","title":"Hardware Commands"},{"location":"core/ns-commands/#tb-set-hardware","text":"tb-set-hardware node type [args] tb-set-hardware $node3 pc tb-set-hardware $node4 shark where: node = The name of the node. type = The type of the node. Note Please see the Node Status page for a list of available types. pc is the default type. No current types have any additional arguments.","title":"tb-set-hardware"},{"location":"core/ns-commands/#ip-address-commands","text":"Each node will be assigned an IP address for each interface that is in use. The following commands will allow you to explicitly set those IP addresses. IP addresses will be automatically generated for all nodes for which you do not explicitly set IP addresses. In most cases, the IP addresses on either side of a link must be in the same subnet. Likewise, all IP addresses on a LAN should be in the same subnet. Generally the same subnet should not be used for more than one link or LAN in a given experiment, nor should one node have multiple interfaces in the same subnet. Automatically generated IP addresses will conform to these requirements. If part of a link or LAN is explicitly specified with the commands below then the remainder will be automatically generated under the same subnet. IP address assignment is deterministic and tries to fill lower IP's first, starting at 2. Except in the partial specification case (see above), all automatic IP addresses are in the network 10 .","title":"IP Address Commands"},{"location":"core/ns-commands/#tb-set-ip","text":"tb-set-ip node ip tb-set-ip $node1 142.3.4.5 where: node = The node to assign the IP address to ip = The IP address. Note This command should only be used for nodes that have a single link. For nodes with multiple links the following commands should be used. Mixing tb-set-ip and any other IP command on the same node will result in an error.","title":"tb-set-ip"},{"location":"core/ns-commands/#tb-set-ip-link","text":"tb-set-ip-link node link ip tb-set-ip-link $node0 $link0 142.3.4.6 where: node = The node to set the IP for. link = The link to set the IP for. ip = The IP address. Note One way to think of the arguments is a link with the node specifying which side of the link to set the IP for. This command cannot be mixed with tb-set-ip on the same node.","title":"tb-set-ip-link"},{"location":"core/ns-commands/#tb-set-ip-lan","text":"tb-set-ip-lan node lan ip tb-set-ip-lan $node1 $lan0 142.3.4.6 where: node = The node to set the IP for. lan = The lan the IP is on. ip = The IP address. Note One way to think of the arguments is a node with the LAN specifying which port to set the IP address for. This command cannot be mixed with tb-set-ip on the same node.","title":"tb-set-ip-lan"},{"location":"core/ns-commands/#tb-set-ip-interface","text":"tb-set-ip-interface node dst ip tb-set-ip-interface $node2 $node1 142.3.4.6 where: node = The node to set the IP for. dst = The destination of the link to set the IP for. IP = The IP address. Note This command cannot be mixed on the same node with tb-set-ip . (See above) In the case of multiple links between the same pair of nodes, there is no way to distinguish which link to the set the IP for. This should be fixed soon. This command is converted internally to either tb-set-ip-link or tb-set-ip-lan . It is possible that error messages will report either of those commands instead of tb-set-ip-interface .","title":"tb-set-ip-interface"},{"location":"core/ns-commands/#tb-set-netmask","text":"tb-set-netmask lanlink netmask tb-set-netmask $link0 \"255.255.255.248\" where: lanlink = The lan or link to set the netmask for. netmask = The netmask in dotted notation. Note This command sets the netmask for a LAN or link. The mask must be big enough to support all of the nodes on the LAN or link! You may play with the bottom three octets (0xFFFFFXXX) of the mask; attempts to change the upper octets will cause an error.","title":"tb-set-netmask"},{"location":"core/ns-commands/#os-commands","text":"","title":"OS Commands "},{"location":"core/ns-commands/#tb-set-node-os","text":"tb-set-node-os node os tb-set-node-os $node1 FBSD-STD tb-set-node-os $node1 MY_OS where: node = The node to set the OS for. os = The id of the OS for that node. Note The OSID may either by one of the standard OS's we provide or a custom OSID, created via the web interface. If no OS is specified for a node, a default OS is chosen based on the nodes type. This is currently 'Ubuntu1604-STD ' for PCs. The currently available standard OS types are listed in the OS Images page.","title":"tb-set-node-os"},{"location":"core/ns-commands/#tb-set-node-rpms","text":"tb-set-node-rpms node rpms... tb-set-node-rpms $node0 rpm1 rpm2 rpm3 Note This command sets which RPMs are to be installed on the node when it first boots after being assigned to an experiment. Each RPM can be either a path to a file or a URL. Paths must be to files that reside in the /proj or /groups directory. You are not allowed to place your RPMs in your home directory. http(s):// and ftp:// URLs will be fetched into the experiment's directory, and re-distributed from there. See the Core Guide for more information.","title":"tb-set-node-rpms"},{"location":"core/ns-commands/#tb-set-node-startcmd","text":"tb-set-node-startcmd node startupcmd tb-set-node-startcmd $node0 \"mystart.sh -a >& /tmp/node0.log\" Note Specify a script or program to be run when the node is booted. See the Core Guide for more information.","title":"tb-set-node-startcmd"},{"location":"core/ns-commands/#tb-set-node-cmdline","text":"tb-set-node-cmdline node cmdline tb-set-node-cmdline $node0 {???} Note Set the commandline to be passed to the kernel when it is booted. Currently, this is supported on OSKit kernels only.","title":"tb-set-node-cmdline"},{"location":"core/ns-commands/#tb-set-node-tarfiles","text":"tb-set-node-tarfiles node install-dir1 tarfile1 ... The tb-set-node-tarfiles command is used to install one or more tar files onto a node's local disk. This command is useful for installing files that are used frequently, but will change very little during the course of your experiments. For example, if your software depends on a third-party library not provided in the standard disk images, you can produce a tarball and have the library ready for use on all the experimental nodes. Another example would be the data sets for your software. The benefit of installing files using this method is that they will reside on the node's local disk, so your experimental runs will not be disrupted by NFS traffic. Note Avoid using this command if the files are changing frequently because the tars are only (re)installed when the nodes boot. Installing individual tar files or RPMs is a midpoint in the spectrum of getting software onto the experimental nodes. At one extreme, you can read everything over NFS, which works well if the files are changing constantly, but can generate a great deal of strain on the control network and disrupt your experiment. The tar files and RPMs are also read over NFS when the nodes initially boot; however, there won't be any extra NFS traffic while you are running your experiment. Finally, if you need a lot of software installed on a large number of nodes, say greater than 20, it might be best to create a custom disk image . Using a disk image is easier on the control network since it is transferred using multicast, thus greatly reducing the amount of NFS traffic when the experiment is swapped in. Required Parameters: node - The node where the files should be installed. Each node has its own tar file list, which may or may not be different from the others. One or more install-dir and tarfile pairs are then listed in the order you wish them to be installed: install-dir - An existing directory on the node where the tar file should be unarchived (e.g. / , /usr , /usr/local ). The tar command will be run as \"root\" [#tb-set-node-tarfiles Note1], so all of the node's directories will be accessible to you. If the directory does not exist on the image or was not created by the unarchiving of a previous tar file, the installation will fail [#tb-set-node-tarfiles Note2]. tarfile - An existing tar file located in a project directory (e.g. /proj or /groups ) or an http , https , or ftp URL. In the case of URLs, they are downloaded when the experiment is swapped in and cached in the experiment's directory for future use. In either case, the tar file name is required to have one of the following extensions: .tar, .tar.Z, .tar.gz, or .tgz. Note that the tar file could have been created anywhere; however, if you want the unarchived files to have valid DETERLab user and group id's, you should create the tar file on ops or an experimental node. Example usage: # Overwrite files in /bin and /sbin. tb-set-node-tarfiles $node0 /bin /proj/foo/mybinmods.tar /sbin /proj/foo/mysbinmods.tar # Programmatically generate the list of tarballs. set tb [list] # Add a tarball located on a web site. lappend tb / http://foo.bar/bazzer.tgz # Add a tarball located in the DETER NFS space. lappend tb /usr/local /proj/foo/tarfiles/bar.tar.gz # Use 'eval' to expand the 'tb' list into individual # arguments to the tb-set-node-tarfiles command. eval tb-set-node-tarfiles $node1 $tb See also: tb-set-node-rpms Custom disk images Note Because the files are installed as root, care must be taken to protect the tar file so it cannot be replaced with a trojan that allowed less privileged users to become root. Currently, you can only tell how/why an installation failed by examining the node's console log on bootup.","title":"tb-set-node-tarfiles"},{"location":"core/ns-commands/#link-loss-commands","text":"This is the NS syntax for creating a link: $ns duplex-link $node1 $node2 100Mb 150ms DropTail Note This does not allow for specifying link loss rates. DETERLab does, however, support link loss. The following commands can be used to specify link loss rates.","title":"Link Loss Commands"},{"location":"core/ns-commands/#tb-set-link-loss","text":"tb-set-link-loss src dst loss tb-set-link-loss link loss tb-set-link-loss $node1 $node2 0.05 tb-set-link-loss $link1 0.02 where: src , dst = Two nodes to describe the link. link = The link to set the rate for. loss = The loss rate (between 0 and 1). Note There are two syntaxes available. The first specifies a link by a source/destination pair. The second explicitly specifies the link. The source/destination pair is incapable of describing an individual link in the case of multiple links between two nodes. Use the second syntax for this case.","title":"tb-set-link-loss"},{"location":"core/ns-commands/#tb-set-lan-loss","text":"tb-set-lan-loss lan loss tb-set-lan-loss $lan1 0.3 Where: lan = The lan to set the loss rate for. loss = The loss rate (between 0 and 1). Note This command sets the loss rate for the entire LAN.","title":"tb-set-lan-loss"},{"location":"core/ns-commands/#tb-set-node-lan-delay","text":"tb-set-node-lan-delay node lan delay tb-set-node-lan-delay $node0 $lan0 40ms Where: node = The node we are modifying the delay for. lan = Which LAN the node is in that we are affecting. delay = The new node to switch delay (see below). Note This command changes the delay between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node, possibly incurring additional latency from any delay parameters there. If this command is not used to overwrite the delay, then the delay for a given node to switch link is taken as one half of the delay passed to make-lan . Thus in a LAN where no tb-set-node-delay calls are made, the node-to-node latency will be the latency passed to make-lan . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times. Delays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. As a convenience, a delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other.","title":"tb-set-node-lan-delay"},{"location":"core/ns-commands/#tb-set-node-lan-bandwidth","text":"tb-set-node-lan-bandwidth node lan bandwidth tb-set-node-lan-bandwidth $node0 $lan0 20Mb Where: node = The node we are modifying the bandwidth for. lan = Which LAN the node is in that we are affecting. bandwidth = The new node to switch bandwidth (see below). Note This command changes the bandwidth between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may have a lower bandwidth. If this command is not used to overwrite the bandwidth, then the bandwidth for a given node to switch link is taken directly from the bandwidth passed to make-lan . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times.","title":"tb-set-node-lan-bandwidth"},{"location":"core/ns-commands/#tb-set-node-lan-loss","text":"tb-set-node-lan-loss node lan loss tb-set-node-lan-loss $node0 $lan0 0.05 Where: node = The node we are modifying the loss for. lan = Which LAN the node is in that we are affecting. loss = The new node to switch loss (see below). Note This command changes the loss probability between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may also have a loss chance. Thus for packet going to switch with loss chance A and then going on the destination with loss chance B , the node-to-node loss chance is (1-(1-A)(1-B)) . If this command is not used to overwrite the loss, then the loss for a given node to switch link is taken from the loss rate passed to the make-lan command. If a loss rate of L is passed to make-lan then the node to switch loss rate for each node is set to (1-sqrt(1-L)) . Because each packet will have two such chances to be lost, the node-to-loss rate comes out as the desired L . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times.","title":"tb-set-node-lan-loss"},{"location":"core/ns-commands/#tb-set-node-lan-params","text":"tb-set-node-lan-params node lan delay bandwidth loss tb-set-node-lan-params $node0 $lan0 40ms 20Mb 0.05 Where: node = The node we are modifying the loss for. lan = Which LAN the node is in that we are affecting. delay = The new node to switch delay. bandwidth = The new node to switch bandwidth. loss = The new node to switch loss. Note This command is exactly equivalent to calling each of the above three commands appropriately. See above for more information.","title":"tb-set-node-lan-params"},{"location":"core/ns-commands/#tb-set-link-simplex-params","text":"tb-set-link-simplex-params link src delay bw loss tb-set-link-simplex-params $link1 $srcnode 100ms 50Mb 0.2 Where: link = The link we are modifying. src = The source, defining which direction we are modifying. delay = The source to destination delay. bw = The source to destination bandwidth. loss = The source to destination loss. Note This commands modifies the delay characteristics of a link in a single direction. The other direction is unchanged. This command only applies to links. Use tb-set-lan-simplex-params below for LANs.","title":"tb-set-link-simplex-params"},{"location":"core/ns-commands/#tb-set-lan-simplex-params","text":"tb-set-lan-simplex-params lan node todelay tobw toloss fromdelay frombw fromloss tb-set-lan-simplex-params $lan1 $node1 100ms 10Mb 0.1 5ms 100Mb 0 Where: lan = The lan we are modifying. node = The member of the lan we are modifying. todelay = Node to lan delay. tobw = Node to lan bandwidth. toloss = Node to lan loss. fromdelay = Lan to node delay. frombw = Lan to node bandwidth. fromloss = Lan to node loss. Note This command is exactly like tb-set-node-lan-params except that it allows the characteristics in each direction to be chosen separately. See all the notes for tb-set-node-lan-params .","title":"tb-set-lan-simplex-params"},{"location":"core/ns-commands/#tb-set-endnodeshaping","text":"tb-set-endnodeshaping link-or-lan enable tb-set-endnodeshaping $link1 1 tb-set-endnodeshaping $lan1 1 Where: link-or-lan = The link or LAN we are modifying. enable = Set to 1 to enable, 0 to disable. Note This command specifies whether end node shaping is used on the specified link or LAN (instead of a delay node). Disabled by default for all links and LANs. Only available when running the standard DETERLab FreeBSD or Linux kernels. See End Node Traffic Shaping and Multiplexed Links for more details.","title":"tb-set-endnodeshaping"},{"location":"core/ns-commands/#tb-set-noshaping","text":"tb-set-noshaping link-or-lan enable tb-set-noshaping $link1 1 tb-set-noshaping $lan1 1 Where: link-or-lan = The link or LAN we are modifying. enable = Set to 1 to disable bandwidth shaping, 0 to enable. Note This command specifies whether link bandwidth shaping should be enforced on the specified link or LAN. When enabled, bandwidth limits indicated for a link or LAN will not be enforced. Disabled by default for all links and LANs. That is, link bandwidth shaping is enforced on all links and LANs by default. If the delay and loss values for a tb-set-noshaping link are zero (the default), then no delay node or end-node delay pipe will be associated with the link or LAN. This command is a hack. The primary purpose for this command is to subvert the topology mapper ( assign ). Assign always observes the physical bandwidth constraints of the testbed. By using tb-set-noshaping , you can convince assign that links are low-bandwidth and thus get your topology mapped, but then not actually have the links shaped.","title":"tb-set-noshaping"},{"location":"core/ns-commands/#tb-use-endnodeshaping","text":"tb-use-endnodeshaping enable tb-use-endnodeshaping 1 Where: enable = Set to 1 to enable end-node traffic shaping on all links and LANs. Note This command allows you to use end-node traffic shaping globally, without having to specify per link or LAN with tb-set-endnodeshaping . See End Node Traffic Shaping and Multiplexed Links for more details.","title":"tb-use-endnodeshaping"},{"location":"core/ns-commands/#tb-force-endnodeshaping","text":"tb-force-endnodeshaping enable tb-force-endnodeshaping 1 Where: enable = Set to 1 to force end-node traffic shaping on all links and LANs. Note This command allows you to specify non-shaped links and LANs at creation time, but still control the shaping parameters later (e.g., increase delay, decrease bandwidth) after the experiment is swapped in. This command forces allocation of end-node shaping infrastructure for all links. There is no equivalent to force delay node allocation. See End Node Traffic Shaping and Multiplexed Links for more details.","title":"tb-force-endnodeshaping"},{"location":"core/ns-commands/#tb-set-multiplexed","text":"tb-set-multiplexed link allow tb-set-multiplexed $link1 1 Where: link = The link we are modifying. 'allow` = Set to 1 to allow multiplexing of the link, 0 to disallow. Note This command allows a link to be multiplexed over a physical link along with other links. Disabled by default for all links. Only available when running the standard DETER FreeBSD (not Linux) and only for links (not LANs). See End Node Traffic Shaping and Multiplexed Links for more details.","title":"tb-set-multiplexed"},{"location":"core/ns-commands/#tb-set-vlink-emulation","text":"tb-set-vlink-emulation style tb-set-vlink-emulation $link1 vlan Where: style = One of \"vlan\" or \"veth-ne\" Note It seems to be necessary to set the virtual link emulation style to vlan for multiplexed links to work under linux.","title":"tb-set-vlink-emulation"},{"location":"core/ns-commands/#virtual-type-commands","text":"Virtual Types are a method of defining fuzzy types, i.e. types that can be fulfilled by multiple different physical types. The advantage of virtual types, also known as 'vtypes ', is that all nodes of the same vtype will usually be the same physical type of node. In this way, vtypes allows logical grouping of nodes. As an example, imagine we have a network with internal routers connecting leaf nodes. We want the routers to all have the same hardware, and the leaf nodes to all have the same hardware, but the specifics do not matter. We have the following fragment in our NS file: ... tb-make-soft-vtype router {pc600 pc850} tb-make-soft-vtype leaf {pc600 pc850} tb-set-hardware $router1 router tb-set-hardware $router2 router tb-set-hardware $leaf1 leaf tb-set-hardware $leaf2 leaf Here we have set up two soft (see below) vtypes: router and leaf. Our router nodes are then specified to be of type router , and the leaf nodes of type leaf . When the experiment is swapped in, the testbed will attempt to make router1 and router2 be of the same type, and similarly, leaf1 and leaf2 of the same type. However, the routers/leafs may be pc600s or they may be pc850s, whichever is easier to fit in to the available resources. As a basic use, vtypes can be used to request nodes that are all the same type, but can be of any available type: ... tb-make-soft-vtype N {pc600 pc850} tb-set-hardware $node1 N tb-set-hardware $node2 N Vtypes come in two varieties: hard and soft. * 'Soft ' - With soft vtypes, the testbed will try to make all nodes of that vtype the same physical type, but may do otherwise if resources are tight. * 'Hard ' - Hard vtypes behave just like soft vtypes except that the testbed will give higher priority to vtype consistency and swapping in will fail if the vtypes cannot be satisfied. Therefore, if you use soft vtypes you are more likely to swap in but there is a chance your node of a specific vtype will not all be the same. If you use hard vtypes, all nodes of a given vtype will be the same, but swapping in may fail. Further, you can have weighted soft vtypes. Here you assign a weight from 0 to 1 exclusive to your vtype. The testbed will give higher priority to consistency in the higher weighted vtypes. The primary use of this is to rank multiple vtypes by importance of consistency. Soft vtypes have a weight of 0.5 by default. As a final note, when specifying the types of a vtype, use the most specific type possible. For example, the following command is not very useful: tb-make-soft-vtype router {pc pc600} This is because pc600 is a sub type of pc. You may very well end up with two routers as type pc with different hardware, as pc covers multiple types of hardware.","title":"Virtual Type Commands"},{"location":"core/ns-commands/#tb-make-soft-vtype","text":"tb-make-soft-vtype vtype {types} tb-make-hard-vtype vtype {types} tb-make-weighted-vtype vtype weight {types} tb-make-soft-vtype router {pc600 pc850} tb-make-hard-vtype leaf {pc600 pc850} tb-make-weighted-vtype A 0.1 {pc600 pc850} Where: vtype = The name of the vtype to create. types = One or more physical types. weight = The weight of the vtype, 0 < weight < 1. Note These commands create vtypes. See notes above for a description of vtypes and the difference between soft and hard. tb-make-soft-vtype creates vtypes with weight 0.5. vtype commands must appear before tb-set-hardware commands that use them. Do not use tb-fix-node with nodes that have a vtype.","title":"tb-make-soft-vtype"},{"location":"core/ns-commands/#misc-commands","text":"","title":"Misc. Commands"},{"location":"core/ns-commands/#tb-fix-node","text":"tb-fix-node vnode pnode tb-fix-node $node0 pc42 Where: vnode = The node we are fixing. pnode = The physical node we want used. Note This command forces the virtual node to be mapped to the specified physical node. Swap in will fail if this cannot be done. Do not use this command on nodes that are a virtual type.","title":"tb-fix-node"},{"location":"core/ns-commands/#tb-fix-interface","text":"tb-fix-interface vnode vlink iface tb-fix-interface $node0 $link0 \"eth0\" Where: vnode = The node we are fixing. vlink = The link connecting to that node that we want to set. iface = The DETERLab name for the interface that is to be used. Note The interface names used are the ones in the DETERLab database - we can make no guarantee that the OS image that boots on the node assigns the same name. Different types of nodes have different sets of interfaces, so this command is most useful if you are also using tb-fix-node and/or tb-set-hardware on the vnode .","title":"tb-fix-interface"},{"location":"core/ns-commands/#tb-set-uselatestwadata","text":"tb-set-uselatestwadata 0 tb-set-uselatestwadata 1 Note This command indicates which widearea data to use when mapping widearea nodes to links. The default is 0, which says to use the aged data. Setting it to 1 says to use the most recent data.","title":"tb-set-uselatestwadata"},{"location":"core/ns-commands/#tb-set-wasolver-weights","text":"tb-set-wasolver-weights delay bw plr tb-set-wasolver-weights 1 10 500 Where: delay = The weight to give delay when solving. bw = The weight to give bandwidth when solving. plr = The weight to give lossrate when solving. Note This command sets the relative weights to use when assigning widearea nodes to links. Specifying a zero says to ignore that particular metric when doing the assignment. Setting all three to zero results in an essentially random selection.","title":"tb-set-wasolver-weights"},{"location":"core/ns-commands/#tb-set-node-failure-action","text":"tb-set-node-failure-action node action tb-set-node-failure-action $nodeA \"fatal\" tb-set-node-failure-action $nodeB \"nonfatal\" Where: node = The node name. action = One of \"fatal\" or \"nonfatal\". Note This command sets the failure mode for a node. When an experiment is swapped in, the default action is to abort the swapin if any nodes fail to come up normally. This is the \"fatal\" mode. You may also set a node to \"nonfatal\" which will cause node bootup failures to be reported, but otherwise ignored during swapin. Note that this can result in your experiment not working properly if a dependent node fails, but typically you can arrange your software to deal with this.","title":"tb-set-node-failure-action"},{"location":"core/os-images/","text":"Operating System Images \u00b6 Here is the list of currently supported DETERLab operating system images. If you have a DETERLab account, you can view the most updated information as well as statistics on each machine on the OSID page on the testbed. Supported OS Images as of 01/19/2017 \u00b6 Name OS Description FBSD10-STD FreeBSD FreeBSD 10.x Standard CentOS6-64-STD Linux CentOS6 64-Bit image CentOS7-STD Linux CentOS7 64 bit KALI-RLG Linux Kali 2016.1 Penetration Testing Metasploitable2 Linux An intentionally vulnerable system Ubuntu1404-32-STD Linux Ubuntu 14.04 LTS 32 bit Standard Image Ubuntu1404-64-STD Linux Ubuntu 14.04 LTS 64 bit Standard Image Ubuntu1604-STD Linux Ubuntu 16.04 LTS 64 bit Standard Image Updates for Custom Images \u00b6 Updating Linux images made before Jan 25, 2013 \u00b6 We made a change to make mounting NFS home directories more robust. You may update your custom images by running: sudo curl --output /usr/local/etc/emulab/liblocsetup.pm boss.isi.deterlab.net/downloads/client-update/linux-liblocsetup.pm sudo chmod a+rx /usr/local/etc/emulab/liblocsetup.pm and taking a snapshot. On CentOS-6-64-STD you must to install Time::HiRes by running: sudo yum install perl-Time-HiRes","title":"OS Images"},{"location":"core/os-images/#operating-system-images","text":"Here is the list of currently supported DETERLab operating system images. If you have a DETERLab account, you can view the most updated information as well as statistics on each machine on the OSID page on the testbed.","title":"Operating System Images"},{"location":"core/os-images/#supported-os-images-as-of-01192017","text":"Name OS Description FBSD10-STD FreeBSD FreeBSD 10.x Standard CentOS6-64-STD Linux CentOS6 64-Bit image CentOS7-STD Linux CentOS7 64 bit KALI-RLG Linux Kali 2016.1 Penetration Testing Metasploitable2 Linux An intentionally vulnerable system Ubuntu1404-32-STD Linux Ubuntu 14.04 LTS 32 bit Standard Image Ubuntu1404-64-STD Linux Ubuntu 14.04 LTS 64 bit Standard Image Ubuntu1604-STD Linux Ubuntu 16.04 LTS 64 bit Standard Image","title":"Supported OS Images as of 01/19/2017"},{"location":"core/os-images/#updates-for-custom-images","text":"","title":"Updates for Custom Images"},{"location":"core/os-images/#updating-linux-images-made-before-jan-25-2013","text":"We made a change to make mounting NFS home directories more robust. You may update your custom images by running: sudo curl --output /usr/local/etc/emulab/liblocsetup.pm boss.isi.deterlab.net/downloads/client-update/linux-liblocsetup.pm sudo chmod a+rx /usr/local/etc/emulab/liblocsetup.pm and taking a snapshot. On CentOS-6-64-STD you must to install Time::HiRes by running: sudo yum install perl-Time-HiRes","title":"Updating Linux images made before Jan 25, 2013"},{"location":"core/risky-experiment/","text":"Risky Experiment Support \u00b6 We define as a risky experiment each experiment that either uses some type of malware such as DoS tool, a worm, an exploit, etc, even when the malware code is written by the experimenter and/or requires connectivity to the outside directly from experimental nodes. We recognize all these experiment types are interesting and important to facilitate new research in security. At the same time there is potential risk to the testbed and the Internet that must be contained. This risk includes: Malware interfering with other experiments on DETER Malware escaping to the outside world Malware overwhelming critical infrastructure in DETER, such as users and boss machines and control network Malware from the outside world infecting experimental machines and spreading in DETER or propagating back to the outside (with implications of DETER unwittingly participating in attacks). We have developed strategies to contain experiment risk while allowing users to observe phenomena of interest to them. This means that containment is customized for each experiment. This customization is performed automatically depending on information a user specifies on the \"Begin Experiment Web\" page. Important A \"risky experiment\" in DETERLab is a heavyweight mechanism. We request that you only implement it if other approaches, such as SSH tunnels, do not work. Please file a ticket (you must log in to Trac with your DETERLab username and password) if you run into any problems. This code is still being developed and we welcome your feedback. Who Is Eligible to Run Risky Experiments? \u00b6 To be able to create and run risky experiments you must first request permission from DETERLab. Please file a ticket with your project name, a description of your experiment and its risk. We may need to exchange a few emails with you before granting permission to use this feature. Attempts to create risky experiments without approval of DETER staff will fail. Specifying Risk and Connectivity Options \u00b6 This section explains how to specify risk and connectivity options and what effect this will have on your experiment. Note Many containment strategies require automated modification of the experiment's NS file. We can currently only parse \"simple\" NS files, i.e. those that specify each node's features separately rather than using \"foreach\" option. We expect to extend our parsing ability in the near future. parsable.ns\u200b is a sample file we can parse, and unparsable.ns\u200b is the same file in a version we cannot parse yet. Further, it is unclear yet how containment works with NS options such as traffic generation via NS file, use of templates, use of Planetlab nodes, etc. Configuring for Malware \u00b6 This section describes which options to check for malware. Experiment uses live malware If your experiment uses any type of malware such as DoS, worm code, exploit code, scanning code, etc. even if it is written by you, please check this box. You are not likely to experience any adverse effects if this box is checked. If you need no outside connectivity we may not take any special containment action, in addition to those already in place for all experiments. If you need outside connectivity we may log your traffic or filter it based on malware signatures to ensure no malware traffic escapes DETER. The type of containment will depend on your connectivity selections, and on type of malware you are using. Again, you are unlikely to experience any adverse effects from our actions. Malware self-propagates? Check this box if you use malware such as worms that can spread by itself, once launched, through a set of vulnerable nodes without human action. Also check this box if you use email or other types of viruses that spread when a human accesses them through a benign application (e.g. human opens an email, human opens an IM attachment with the virus, etc). Checking this box will increase DETER's monitoring and filtering of traffic generated by your experiment to the control net and the outside. Type of malware. This selection helps us customize our monitoring and filtering to a specific malware you are using. We expect this list to grow in the future. If no selection fits malware you are using, please select Unknown. Connectivity \u00b6 Experiment needs outside connectivity. Check this box if you require the ability to initiate conversations with the Internet from your experiment. Connectivity will be provided by adding a special type of node, called tunnel to your topology. We will add this node automatically. Its name - tunnel - becomes a keyword so no other nodes in your topology can be named tunnel if this box is selected. The tunnel node will be linked to a random, well-connected node in your topology. We will also automatically set up routing on your nodes so they know they reach outside via the tunnel node. You will be able to see the added code in the NS file. Do not change it, otherwise you are likely to lose connectivity. You can change other parts of your NS file at will, including changing a link that connects the tunnel node with the topology to reconnect it to another experimental node. Do not create additional links to the tunnel node, this likely won't work. If at some point you do not need connectivity anymore, visit the Modify Experiment page and uncheck this box. Note You will never get an unlimited outside connectivity. Instead we install a firewall on a tunnel node and open communication between your experiment and remote IPs you have specified on fine-grain basis. The rest of the form allows you to specify allowed communication patterns. If you don't know which nodes you will need to talk to at experiment creation time please file a ticket and explain to us your experiment design. We'll work together with you to find a safe way to support your experiment. Select type of communication with the outside. If you plan to communicate with the Internet for benign purposes (e.g., to get rpms, to contact public Web servers in a benign way, to let others talk to a Web server you have set up in DETER) you would select the benign option. If you plan to let malware generate traffic to the outside or you plan to attract malware from the outside select Malware-generated. If you use multiple traffic generators that fall into different categories, select Mixed. Type of communication you select will have direct influence on the level of our monitoring and filtering on tunnel nodes. Names and ports of experiment nodes that receive outside connections. If you plan to set up a server on some of your experimental nodes that will receive communication from the outside specify the nodes, destination ports and protocols here. Names of the nodes are names from your NS file. Only single protocols, not protocol ranges, can be specified. Supported protocols are tcp and udp. This communication is NAT-ed via the tunnel node, thus remote clients will need to contact you on a specific, high-numbered port. You will be notified of NAT specifics in an email once you swap your experiment in. This information can also be viewed on your experiment's Web page. For example, let a node n1 in your topology have IP address 1.2.3.4 and wants to run a public Web server. You would specify n1/80/tcp in the textbox for experiment nodes that receive outside connections. NAT may map this into 5.6.7.8 port 10001. Remote clients would then type \u200bhttp://5.6.7.8:10001 to access your Web server. If this is confusing please file a ticket for more information. IPs and ports of outside nodes that receive connections from the experiment. If you need to contact remote servers from your experimental nodes (e.g., to send some statistics to a server in your institution) specify the IPs, destination ports and protocols here. Same restrictions for ports (single numbers, not ranges) and protocols (tcp and udp) apply. Note that a lot of public servers have multiple IP addresses. If you want to contact such servers (e.g. Google) you will need to list all IP addresses to ensure that when you try to access them via their URL, the corresponding IP is on our list of allowed remote hosts. Otherwise you could list only one IP for such public servers and contact them using this IP address and not the URL (e.g., you could type 74.125.19.103 instead of www.google.com in your browser's address bar and this would guarantee that you always go to this one specific Google server). Logging on Tunnel Nodes \u00b6 For security reasons, all prohibited traffic on tunnel nodes is logged. This means that traffic you specifically allowed via our forms for experiment creation and modification will not be logged but any other traffic to/from tunnel nodes and any other transit traffic will be logged. Accessing Tunnel Nodes \u00b6 For security reasons, users do not have sudoer access to tunnel nodes. Rebooting and Modifying Experiments with Tunnel Nodes \u00b6 You can safely reboot tunnel nodes if you want - they should come back up with all the correct settings. If you reboot any other nodes in your experiment you will need to set up routing to the outside. You can do this by logging on the rebooted nodes and running /share/t1t2/set_route if you have sudo privileges on that node. If you run into problems please contact us and we will help restore the connectivity. The safest way to change something in your experiment is via Modify Experiment Web page. This should properly set up all the routes, firewall rules, etc. in your experiment. You will not be allowed to modify tunnel node's settings such as OS image and name but you can modify where this node links with your topology. If you need to add some start commands on experiment nodes (except the tunnel node) the best way is to copy the commands we have automatically inserted into your startup script, add commands you want, then replace the start-cmd options in the NS file with the name of your startup script. If you mess up startcmd options in the NS file routing on experiment nodes will not be set up properly. To amend the problem you can delete the entire \"tunnelcode\" section of the NS file, make sure that the connectivity box is checked and run Modify Experiment and the proper code for connectivity will be automatically inserted.","title":"Risky Experiments"},{"location":"core/risky-experiment/#risky-experiment-support","text":"We define as a risky experiment each experiment that either uses some type of malware such as DoS tool, a worm, an exploit, etc, even when the malware code is written by the experimenter and/or requires connectivity to the outside directly from experimental nodes. We recognize all these experiment types are interesting and important to facilitate new research in security. At the same time there is potential risk to the testbed and the Internet that must be contained. This risk includes: Malware interfering with other experiments on DETER Malware escaping to the outside world Malware overwhelming critical infrastructure in DETER, such as users and boss machines and control network Malware from the outside world infecting experimental machines and spreading in DETER or propagating back to the outside (with implications of DETER unwittingly participating in attacks). We have developed strategies to contain experiment risk while allowing users to observe phenomena of interest to them. This means that containment is customized for each experiment. This customization is performed automatically depending on information a user specifies on the \"Begin Experiment Web\" page. Important A \"risky experiment\" in DETERLab is a heavyweight mechanism. We request that you only implement it if other approaches, such as SSH tunnels, do not work. Please file a ticket (you must log in to Trac with your DETERLab username and password) if you run into any problems. This code is still being developed and we welcome your feedback.","title":"Risky Experiment Support"},{"location":"core/risky-experiment/#who-is-eligible-to-run-risky-experiments","text":"To be able to create and run risky experiments you must first request permission from DETERLab. Please file a ticket with your project name, a description of your experiment and its risk. We may need to exchange a few emails with you before granting permission to use this feature. Attempts to create risky experiments without approval of DETER staff will fail.","title":"Who Is Eligible to Run Risky Experiments?"},{"location":"core/risky-experiment/#specifying-risk-and-connectivity-options","text":"This section explains how to specify risk and connectivity options and what effect this will have on your experiment. Note Many containment strategies require automated modification of the experiment's NS file. We can currently only parse \"simple\" NS files, i.e. those that specify each node's features separately rather than using \"foreach\" option. We expect to extend our parsing ability in the near future. parsable.ns\u200b is a sample file we can parse, and unparsable.ns\u200b is the same file in a version we cannot parse yet. Further, it is unclear yet how containment works with NS options such as traffic generation via NS file, use of templates, use of Planetlab nodes, etc.","title":"Specifying Risk and Connectivity Options"},{"location":"core/risky-experiment/#configuring-for-malware","text":"This section describes which options to check for malware. Experiment uses live malware If your experiment uses any type of malware such as DoS, worm code, exploit code, scanning code, etc. even if it is written by you, please check this box. You are not likely to experience any adverse effects if this box is checked. If you need no outside connectivity we may not take any special containment action, in addition to those already in place for all experiments. If you need outside connectivity we may log your traffic or filter it based on malware signatures to ensure no malware traffic escapes DETER. The type of containment will depend on your connectivity selections, and on type of malware you are using. Again, you are unlikely to experience any adverse effects from our actions. Malware self-propagates? Check this box if you use malware such as worms that can spread by itself, once launched, through a set of vulnerable nodes without human action. Also check this box if you use email or other types of viruses that spread when a human accesses them through a benign application (e.g. human opens an email, human opens an IM attachment with the virus, etc). Checking this box will increase DETER's monitoring and filtering of traffic generated by your experiment to the control net and the outside. Type of malware. This selection helps us customize our monitoring and filtering to a specific malware you are using. We expect this list to grow in the future. If no selection fits malware you are using, please select Unknown.","title":"Configuring for Malware"},{"location":"core/risky-experiment/#connectivity","text":"Experiment needs outside connectivity. Check this box if you require the ability to initiate conversations with the Internet from your experiment. Connectivity will be provided by adding a special type of node, called tunnel to your topology. We will add this node automatically. Its name - tunnel - becomes a keyword so no other nodes in your topology can be named tunnel if this box is selected. The tunnel node will be linked to a random, well-connected node in your topology. We will also automatically set up routing on your nodes so they know they reach outside via the tunnel node. You will be able to see the added code in the NS file. Do not change it, otherwise you are likely to lose connectivity. You can change other parts of your NS file at will, including changing a link that connects the tunnel node with the topology to reconnect it to another experimental node. Do not create additional links to the tunnel node, this likely won't work. If at some point you do not need connectivity anymore, visit the Modify Experiment page and uncheck this box. Note You will never get an unlimited outside connectivity. Instead we install a firewall on a tunnel node and open communication between your experiment and remote IPs you have specified on fine-grain basis. The rest of the form allows you to specify allowed communication patterns. If you don't know which nodes you will need to talk to at experiment creation time please file a ticket and explain to us your experiment design. We'll work together with you to find a safe way to support your experiment. Select type of communication with the outside. If you plan to communicate with the Internet for benign purposes (e.g., to get rpms, to contact public Web servers in a benign way, to let others talk to a Web server you have set up in DETER) you would select the benign option. If you plan to let malware generate traffic to the outside or you plan to attract malware from the outside select Malware-generated. If you use multiple traffic generators that fall into different categories, select Mixed. Type of communication you select will have direct influence on the level of our monitoring and filtering on tunnel nodes. Names and ports of experiment nodes that receive outside connections. If you plan to set up a server on some of your experimental nodes that will receive communication from the outside specify the nodes, destination ports and protocols here. Names of the nodes are names from your NS file. Only single protocols, not protocol ranges, can be specified. Supported protocols are tcp and udp. This communication is NAT-ed via the tunnel node, thus remote clients will need to contact you on a specific, high-numbered port. You will be notified of NAT specifics in an email once you swap your experiment in. This information can also be viewed on your experiment's Web page. For example, let a node n1 in your topology have IP address 1.2.3.4 and wants to run a public Web server. You would specify n1/80/tcp in the textbox for experiment nodes that receive outside connections. NAT may map this into 5.6.7.8 port 10001. Remote clients would then type \u200bhttp://5.6.7.8:10001 to access your Web server. If this is confusing please file a ticket for more information. IPs and ports of outside nodes that receive connections from the experiment. If you need to contact remote servers from your experimental nodes (e.g., to send some statistics to a server in your institution) specify the IPs, destination ports and protocols here. Same restrictions for ports (single numbers, not ranges) and protocols (tcp and udp) apply. Note that a lot of public servers have multiple IP addresses. If you want to contact such servers (e.g. Google) you will need to list all IP addresses to ensure that when you try to access them via their URL, the corresponding IP is on our list of allowed remote hosts. Otherwise you could list only one IP for such public servers and contact them using this IP address and not the URL (e.g., you could type 74.125.19.103 instead of www.google.com in your browser's address bar and this would guarantee that you always go to this one specific Google server).","title":"Connectivity"},{"location":"core/risky-experiment/#logging-on-tunnel-nodes","text":"For security reasons, all prohibited traffic on tunnel nodes is logged. This means that traffic you specifically allowed via our forms for experiment creation and modification will not be logged but any other traffic to/from tunnel nodes and any other transit traffic will be logged.","title":"Logging on Tunnel Nodes"},{"location":"core/risky-experiment/#accessing-tunnel-nodes","text":"For security reasons, users do not have sudoer access to tunnel nodes.","title":"Accessing Tunnel Nodes"},{"location":"core/risky-experiment/#rebooting-and-modifying-experiments-with-tunnel-nodes","text":"You can safely reboot tunnel nodes if you want - they should come back up with all the correct settings. If you reboot any other nodes in your experiment you will need to set up routing to the outside. You can do this by logging on the rebooted nodes and running /share/t1t2/set_route if you have sudo privileges on that node. If you run into problems please contact us and we will help restore the connectivity. The safest way to change something in your experiment is via Modify Experiment Web page. This should properly set up all the routes, firewall rules, etc. in your experiment. You will not be allowed to modify tunnel node's settings such as OS image and name but you can modify where this node links with your topology. If you need to add some start commands on experiment nodes (except the tunnel node) the best way is to copy the commands we have automatically inserted into your startup script, add commands you want, then replace the start-cmd options in the NS file with the name of your startup script. If you mess up startcmd options in the NS file routing on experiment nodes will not be set up properly. To amend the problem you can delete the entire \"tunnelcode\" section of the NS file, make sure that the connectivity box is checked and run Modify Experiment and the proper code for connectivity will be automatically inserted.","title":"Rebooting and Modifying Experiments with Tunnel Nodes"},{"location":"core/sample-topologies/","text":"Sample Topologies \u00b6 The following are various topologies you can use to experiment with DETERLab Core. Toy topologies \u00b6 LAN \u00b6 set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 5 set lanstr \"\" for {set i 0} {$i < $NODES} {incr i} { set node($i) [$ns node] append lanstr \"$node($i) \" } # Change the BW and delay if you want set lan0 [$ns make-lan \"$lanstr\" 100Mb 0ms] $ns rtproto Static $ns run Ring \u00b6 set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 5 set node(0) [$ns node] for {set i 1} {$i < $NODES} {incr i} { set node($i) [$ns node] set lastindex [expr $i-1] # Change BW and delay if you want set Link$i [$ns duplex-link $node($i) $node($lastindex) 100Mb 0ms DropTail] } set lastindex [expr $i-1] # Change BW and delay if you want set Link$i [$ns duplex-link $node(0) $node($lastindex) 100Mb 0ms DropTail] $ns rtproto Static $ns run Dumbbell \u00b6 set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 10 set rem 0 set l 0 for {set i 0} {$i < 2} {incr i} { set node($rem) [$ns node] for {set j 1} {$j < $NODES/2} {incr j} { set index [expr $rem+$j] set node($index) [$ns node] # Change BW and delay if you want set Link$l [$ns duplex-link $node($rem) $node($index) 100Mb 0ms DropTail] set l [expr $l+1] } set rem [expr $rem+$NODES/2] } set rem [expr $NODES/2] # Change BW and delay if you want set Link$l [$ns duplex-link $node($rem) $node(0) 100Mb 0ms DropTail] $ns rtproto Static $ns run Tree \u00b6 set ns [new Simulator] source tb_compat.tcl # Change fanout if you want but bear in mind that some of # our nodes have only 5 interfaces, so max # of experimental # interfaces (and fan out) is 4 set FANOUT 3 # Change depth if you want set DEPTH 3 set node(0) [$ns node] set lastj 0 set f $FANOUT set lastl 0 for {set i 0} {$i < $DEPTH} {incr i} { for {set j 1} {$j <# $f} {incr j} { set index [expr $lastj+$j] set node($index) [$ns node] set lastindex [expr ($index-1)/$FANOUT] # Change BW and delay if you want set Link$lastl [$ns duplex-link $node($index) $node($lastindex) 100Mb 0ms DropTail] set lastl [expr $lastl+1] } set f [expr $f*$FANOUT] set lastj [expr $lastj+$j-1] } $ns rtproto Static $ns run Real AS topologies \u00b6 Because most of our PCs have up to 4 experimental interfaces, note that some of these topologies may have to be modified to have a fan out of at most 4. http://www.cs.purdue.edu/homes/fahmy/software/rf2ns/topo/","title":"Sample Topologies"},{"location":"core/sample-topologies/#sample-topologies","text":"The following are various topologies you can use to experiment with DETERLab Core.","title":"Sample Topologies"},{"location":"core/sample-topologies/#toy-topologies","text":"","title":"Toy topologies"},{"location":"core/sample-topologies/#lan","text":"set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 5 set lanstr \"\" for {set i 0} {$i < $NODES} {incr i} { set node($i) [$ns node] append lanstr \"$node($i) \" } # Change the BW and delay if you want set lan0 [$ns make-lan \"$lanstr\" 100Mb 0ms] $ns rtproto Static $ns run","title":"LAN"},{"location":"core/sample-topologies/#ring","text":"set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 5 set node(0) [$ns node] for {set i 1} {$i < $NODES} {incr i} { set node($i) [$ns node] set lastindex [expr $i-1] # Change BW and delay if you want set Link$i [$ns duplex-link $node($i) $node($lastindex) 100Mb 0ms DropTail] } set lastindex [expr $i-1] # Change BW and delay if you want set Link$i [$ns duplex-link $node(0) $node($lastindex) 100Mb 0ms DropTail] $ns rtproto Static $ns run","title":"Ring"},{"location":"core/sample-topologies/#dumbbell","text":"set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 10 set rem 0 set l 0 for {set i 0} {$i < 2} {incr i} { set node($rem) [$ns node] for {set j 1} {$j < $NODES/2} {incr j} { set index [expr $rem+$j] set node($index) [$ns node] # Change BW and delay if you want set Link$l [$ns duplex-link $node($rem) $node($index) 100Mb 0ms DropTail] set l [expr $l+1] } set rem [expr $rem+$NODES/2] } set rem [expr $NODES/2] # Change BW and delay if you want set Link$l [$ns duplex-link $node($rem) $node(0) 100Mb 0ms DropTail] $ns rtproto Static $ns run","title":"Dumbbell"},{"location":"core/sample-topologies/#tree","text":"set ns [new Simulator] source tb_compat.tcl # Change fanout if you want but bear in mind that some of # our nodes have only 5 interfaces, so max # of experimental # interfaces (and fan out) is 4 set FANOUT 3 # Change depth if you want set DEPTH 3 set node(0) [$ns node] set lastj 0 set f $FANOUT set lastl 0 for {set i 0} {$i < $DEPTH} {incr i} { for {set j 1} {$j <# $f} {incr j} { set index [expr $lastj+$j] set node($index) [$ns node] set lastindex [expr ($index-1)/$FANOUT] # Change BW and delay if you want set Link$lastl [$ns duplex-link $node($index) $node($lastindex) 100Mb 0ms DropTail] set lastl [expr $lastl+1] } set f [expr $f*$FANOUT] set lastj [expr $lastj+$j-1] } $ns rtproto Static $ns run","title":"Tree"},{"location":"core/sample-topologies/#real-as-topologies","text":"Because most of our PCs have up to 4 experimental interfaces, note that some of these topologies may have to be modified to have a fan out of at most 4. http://www.cs.purdue.edu/homes/fahmy/software/rf2ns/topo/","title":"Real AS topologies"},{"location":"core/serial-console/","text":"Using the Serial Console \u00b6 Determining which nodes to connect to \u00b6 You can determine the nodes allocated to your experiment by looking at the Reserved Nodes table on the Show Experiment page on the web interface. DETERLab nodes are generally named 'pcXXX' for nodes at ISI and 'bpcXXX' for node at UCB . Connecting to the Serial Console \u00b6 Every node on the testbed has serial console access enabled. To connect to a node's serial console, you must first log into users.deterlab.net and use the console command located in /usr/testbed/bin (which should be in every user's PATH by default). To connect to a particular node, type console pcXXX where pcXXX is a node allocated to your experiment. To disconnect from the console session, type Ctrl and then exit . The console command is actually a wrapper for telnet. Serial Console Logs \u00b6 All console output from each node is saved in /var/log/tiplogs/pcXXX.run , where pcXXX is a node allocated to your experiment. This run file is created when nodes are first allocated to an experiment, and the Unix permissions of the run files permit only members of the project to view them. Warning When the experiment is swapped out, the run logs are removed. In order to preserve them, you must make a copy before swapping out your experiment. Console logs may be viewed through the web interface on the Show Experiment page by clicking on the icon in the Console column of the Reserved Nodes table. Additional information \u00b6 Dell Serial Console Information","title":"Using the Serial Console"},{"location":"core/serial-console/#using-the-serial-console","text":"","title":"Using the Serial Console"},{"location":"core/serial-console/#determining-which-nodes-to-connect-to","text":"You can determine the nodes allocated to your experiment by looking at the Reserved Nodes table on the Show Experiment page on the web interface. DETERLab nodes are generally named 'pcXXX' for nodes at ISI and 'bpcXXX' for node at UCB .","title":"Determining which nodes to connect to"},{"location":"core/serial-console/#connecting-to-the-serial-console","text":"Every node on the testbed has serial console access enabled. To connect to a node's serial console, you must first log into users.deterlab.net and use the console command located in /usr/testbed/bin (which should be in every user's PATH by default). To connect to a particular node, type console pcXXX where pcXXX is a node allocated to your experiment. To disconnect from the console session, type Ctrl and then exit . The console command is actually a wrapper for telnet.","title":"Connecting to the Serial Console"},{"location":"core/serial-console/#serial-console-logs","text":"All console output from each node is saved in /var/log/tiplogs/pcXXX.run , where pcXXX is a node allocated to your experiment. This run file is created when nodes are first allocated to an experiment, and the Unix permissions of the run files permit only members of the project to view them. Warning When the experiment is swapped out, the run logs are removed. In order to preserve them, you must make a copy before swapping out your experiment. Console logs may be viewed through the web interface on the Show Experiment page by clicking on the icon in the Console column of the Reserved Nodes table.","title":"Serial Console Logs"},{"location":"core/serial-console/#additional-information","text":"Dell Serial Console Information","title":"Additional information"},{"location":"core/sharing/","text":"Using Shared Materials \u00b6 Sharing and Using Shared Materials \u00b6 These functionalities are available by choosing My Deterlab and then the Sharing tab. Any user can share materials they think would be helpful to others, and any user can find shared materials. What can be shared \u00b6 Currently, you can share teaching materials (lectures, homeworks, teacher manuals or class capture-the-flag exercises) and research materials (experiments, tools, datasets and HOWTOs). Any material can be shared either by uploading a ZIP file or by specifying a URL. ZIP files will be unzipped prior to moving them to our shared space. Requirements for Sharing \u00b6 A material is uniquely identified by its title, type and username of the user who shared it. Multiple users can share versions of a material with the same title and type, or a single user can share multiple materials with the same title but different types. We assume that ZIP files are created by having a folder with materials you want to share and zipping it. This folder must have an index.html file inside, which contains at least 50 characters. When sharing something you need to specify its type (see here ), a few keywords that people can use to search for your material and an E-mail address of the person responsible for maintenance. Material Types \u00b6 Here are some semi-formal definitions of the types of materials that can be shared. Lectures \u00b6 A lecture can be a Word document, a set of slides, or a URL to an online content. You could share a whole lecture or a set of smaller modules covering some topic. Homework \u00b6 A homework is a specification of an assignment that a student will see. Ideally it should follow the format specified here but we will accept other formats. Teacher Manuals \u00b6 A teacher manual accompanies a homework or a CCTF. Ideally it should follow the format specified here but we will accept other formats. Note Teacher manuals are only visible to heads of class projects on DETERLab and they can only be downloaded as ZIP files. CCTFs \u00b6 CCTFs or Class Capture-the-Flag exercises are targeted exercises that pit two student teams against each other in attack/defense scenarios. These are ideal to assign to classes after they have completed a few homeworks with DETERLab. For more information about CCTFs see this paper . Experiments \u00b6 An experiment is a set of files (e.g., NS file, input data, output data, setup scripts, etc.) that enables someone else to recreate an experiment done by a user. This definition is intentionally open-ended. Share any files you believe are useful to others that seek to repeat or build upon your work. Datasets \u00b6 A data set is a collection of data you want to share with others. Such data should be either related to your DETERLab use (e.g., it was used in an experiment by you that later produced results for a publication) or should be generated by your DETERLab experiment (e.g., traces of traffic collected in your experiment as you performed some attack). Warning Only share data that is not private! If in doubt, ask us. Tools \u00b6 A tool is some application that is useful for experimentation, such as a traffic generator, an attack generator, etc. Ideally you would share both the source and the binary of your tool, and some test data. HOWTOs \u00b6 A HOWTO is a small building block for an experiment. For example, it could be a recipe how to set up a DNS server, or how to perform a SYN flood attack. Finding Materials \u00b6 You can find materials by searching for them under the Sharing tab. You can search by keyword (or leave empty to search for all materials), by type or both. Adopting Materials \u00b6 Once you [#find find] materials you are interested in you can either download them as a ZIP file, or if you teach a class, you can adopt them directly into your class (visibility of adopted materials is set to all students). If you want to modify a material before adopting it to your class, download it as ZIP, apply changes locally and then upload it to your class. Modifying Materials Shared by You \u00b6 If you want to modify a material you have shared previously simply re-share it using the same title and type. This will overwrite the previously-shared content.","title":"Using Shared Materials"},{"location":"core/sharing/#using-shared-materials","text":"","title":"Using Shared Materials"},{"location":"core/sharing/#sharing-and-using-shared-materials","text":"These functionalities are available by choosing My Deterlab and then the Sharing tab. Any user can share materials they think would be helpful to others, and any user can find shared materials.","title":"Sharing and Using Shared Materials"},{"location":"core/sharing/#what-can-be-shared","text":"Currently, you can share teaching materials (lectures, homeworks, teacher manuals or class capture-the-flag exercises) and research materials (experiments, tools, datasets and HOWTOs). Any material can be shared either by uploading a ZIP file or by specifying a URL. ZIP files will be unzipped prior to moving them to our shared space.","title":"What can be shared "},{"location":"core/sharing/#requirements-for-sharing","text":"A material is uniquely identified by its title, type and username of the user who shared it. Multiple users can share versions of a material with the same title and type, or a single user can share multiple materials with the same title but different types. We assume that ZIP files are created by having a folder with materials you want to share and zipping it. This folder must have an index.html file inside, which contains at least 50 characters. When sharing something you need to specify its type (see here ), a few keywords that people can use to search for your material and an E-mail address of the person responsible for maintenance.","title":"Requirements for Sharing"},{"location":"core/sharing/#material-types","text":"Here are some semi-formal definitions of the types of materials that can be shared.","title":"Material Types "},{"location":"core/sharing/#lectures","text":"A lecture can be a Word document, a set of slides, or a URL to an online content. You could share a whole lecture or a set of smaller modules covering some topic.","title":"Lectures"},{"location":"core/sharing/#homework","text":"A homework is a specification of an assignment that a student will see. Ideally it should follow the format specified here but we will accept other formats.","title":"Homework"},{"location":"core/sharing/#teacher-manuals","text":"A teacher manual accompanies a homework or a CCTF. Ideally it should follow the format specified here but we will accept other formats. Note Teacher manuals are only visible to heads of class projects on DETERLab and they can only be downloaded as ZIP files.","title":"Teacher Manuals"},{"location":"core/sharing/#cctfs","text":"CCTFs or Class Capture-the-Flag exercises are targeted exercises that pit two student teams against each other in attack/defense scenarios. These are ideal to assign to classes after they have completed a few homeworks with DETERLab. For more information about CCTFs see this paper .","title":"CCTFs"},{"location":"core/sharing/#experiments","text":"An experiment is a set of files (e.g., NS file, input data, output data, setup scripts, etc.) that enables someone else to recreate an experiment done by a user. This definition is intentionally open-ended. Share any files you believe are useful to others that seek to repeat or build upon your work.","title":"Experiments"},{"location":"core/sharing/#datasets","text":"A data set is a collection of data you want to share with others. Such data should be either related to your DETERLab use (e.g., it was used in an experiment by you that later produced results for a publication) or should be generated by your DETERLab experiment (e.g., traces of traffic collected in your experiment as you performed some attack). Warning Only share data that is not private! If in doubt, ask us.","title":"Datasets"},{"location":"core/sharing/#tools","text":"A tool is some application that is useful for experimentation, such as a traffic generator, an attack generator, etc. Ideally you would share both the source and the binary of your tool, and some test data.","title":"Tools"},{"location":"core/sharing/#howtos","text":"A HOWTO is a small building block for an experiment. For example, it could be a recipe how to set up a DNS server, or how to perform a SYN flood attack.","title":"HOWTOs"},{"location":"core/sharing/#finding-materials","text":"You can find materials by searching for them under the Sharing tab. You can search by keyword (or leave empty to search for all materials), by type or both.","title":"Finding Materials "},{"location":"core/sharing/#adopting-materials","text":"Once you [#find find] materials you are interested in you can either download them as a ZIP file, or if you teach a class, you can adopt them directly into your class (visibility of adopted materials is set to all students). If you want to modify a material before adopting it to your class, download it as ZIP, apply changes locally and then upload it to your class.","title":"Adopting Materials "},{"location":"core/sharing/#modifying-materials-shared-by-you","text":"If you want to modify a material you have shared previously simply re-share it using the same title and type. This will overwrite the previously-shared content.","title":"Modifying Materials Shared by You "},{"location":"core/swapping/","text":"Understanding Swapping (Node Use Policies) \u00b6 What are DETERLab's use policies? \u00b6 As a courtesy to other experimenters, we ask that experiments be swapped out or terminated when they are no longer in active use. There are a limited number of nodes available, and node reservations are exclusive, so it is important to free nodes that will be idle so that others may use them. In summary, our policy is that experiments should be swapped out when they are not in use. We encourage you to do that yourself. In general, if experiments are idle for several hours, the system will automatically swap them out, or send you mail about it, and/or an operator may manually swap them out. The actual grace period will differ depending on the size of the experiment, the current demand for resources, and other factors (such as whether you've been a good DETERLab citizen in the past!). If you mark your experiment \"non-idle-swappable\" at creation time or before swapin, and testbed-ops approves your justification, we will make every effort to contact you before swapping it, since local node state could be lost on swapout. Please see full details below. What is \"active use\"? \u00b6 A node or experiment that is being actively used will be doing something related to your experiment. In almost all cases, someone will either be logged into it using it interactively, or some program will be running, sending and receiving packets, and performing the operations necessary to carry out the experiment. When is an experiment considered idle? \u00b6 Your experiment will be considered idle if it has no measurable activity for a significant period of time (a few hours; the exact time is typically set at swapin time). We detect the following types of activity: Any network activity on the experimental network Substantial activity on the control network TTY/console activity on nodes High CPU activity on nodes Certain external events, such as rebooting a node with node_reboot If your experiment's activity falls outside these measured types of activity, or it seems that DETERLab is not assessing your idle time correctly, please be sure to let us know when you create your experiment, or you may be swapped out unexpectedly. ''It is considered abuse to generate artificial activity in order to prevent your experiment from being marked idle. Abusers' access to DETERLab will be revoked, and their actions will be reported to their project leader. Please do not do this. If you think you need special assistance for a deadline, demo or other reason, please contact us .'' What is \"swapping\"? \u00b6 Swapping is the process of instantiating your experiment, i.e., allocating nodes, configuring links, etc. It also refers to the reverse process, in which nodes are released. These processes are called \"swapping in\" and \"swapping out\" respectively. What is an \"Idle-Swap\"? \u00b6 An \"Idle-Swap\" is when DETERLab or its operators swap out your experiment because it was idle for too long. There are two ways that your experiment may be idle-swapped: automatic and manual. The most common is automatic, which happens when Idle-Swap is enabled for your experiment and the experiment has been continuously idle for the idle-swap time that was set at creation/swapin time (usually a few hours). DETERLab will then automatically swap it out. The other way to get idle-swapped is manually, by a DETERLab operator. This typically happens when there is very high resource demand and the experiment has been idle a substantial time, usually a few hours. In this case we will typically make every effort to contact you, since it may cause you to lose data stored on the nodes. ''Note that operators (and you) may swap your excessively idle experiment whether or not it is marked idle-swappable.'' When you create your experiment, you may uncheck the \"Idle-Swap\" box, disabling the automatic idle-swapping of your experiment. If you do so, you must specify the reason, which will be reviewed by testbed-ops. If your reason is judged unacceptable or insufficient, we will explain why, and your experiment will be marked idle-swappable. Valid reasons might be things such as: ''Your idle-detection system fails to detect my experimental activity.'' ''I have node-local state that is impractical to copy off in a timely or reliable manner, because .....'' ''My experiment takes a huge number of nodes, I have several runs to make with intervening think time, and if someone grabs some of these nodes if I'm swapped while thinking, I'll miss my deadline 2 days from now.'' If an experiment is non-idle-swappable, our system will not automatically swap it out, and testbed administrators will attempt to contact you in the event a swapout becomes necessary. However, we expect you to be responsible for managing your experiment in a responsible way, a way that uses DETERLab's hardware resources efficiently. When you create your experiment, you may decrease the idle-swap time from the displayed default, but you may not raise it. If lowering it is compatible with your planned use, doing so helps you be a good DETERLab citizen. If you want it raised, for example for reasons similar to those given above, send mail to testbed-ops AT deterlab.net. You may edit the swap settings (Idle-Swap, Max-Duration, and corresponding reasons and timeouts) using the \"Modify Settings\" menu item on the ''Experiment'' page for your experiment. How long is too long for a node to be idle? \u00b6 Ideally, an experiment should be used nearly continuously from start to finish of the experiment, then swapped out or terminated. However, this isn't always possible. In general, if your experiment is idle for 2 hours or more, it should be swapped out. This is especially true at night (in U.S. timezones) and on weekends. Many experimenters take advantage of lower demand during evenings and weekends to run their large-scale (50-150 node) tests. If your experiment uses 10 nodes or more, it is even more important to release your nodes as soon as possible. Swapin and swapout only take a few minutes (typically 3-5 for swapin, and less than 1 for swapout), so you won't lose much time by doing it. Sometimes an experiment will run long enough that you cannot be online to terminate it, for example, if the experiment completes in the middle of the night. We provide three mechanisms to assist you in terminating your experiment and releasing nodes in a timely manner. The first is the Idle Swap, explained above, the second is Scheduled Termination , and the third is the \"Max Duration\" option, explained below . What is \"node state\"? \u00b6 Some experiments have state that is stored exclusively on the nodes themselves, on their local hard drives. This is state that is not in your NS file or files or disk images that it references, and therefore is not preserved in our database across swapin/swapout. This is state you add to your machines \"by hand\" after DETERLab sets up your experiment, like files you add or modify on filesystems local to test nodes. Local node state does not include any data you store in /users , /proj , or /groups , since those are saved on a fileserver, and not on the local nodes. Most experiments don't have any local node state, and can be swapped out and in without losing any information. This is highly recommended, since it is more courteous to other experimenters. It allows you or DETERLab to easily free up your nodes at any time without losing any of your work. '''Please make your experiments adhere to this guideline whenever possible.''' An experiment that needs local state that inherently cannot be saved (for some reason) or that you will not be able to copy off before your experiment hits the \"idle-swap time,\" should not be marked \"idle-swap\" when you create it. In the ''Begin Experiment'' form you must explain the reason. If you must have node state, you can save it before you swap out by copying it by hand (e.g., into a tar or RPM file), or creating a disk image of the node in question, and later reloading it to a new node after you swap in again. Disk images in effect create a \"custom OS\" that may be loaded automatically based on your NS file. More information about disk images can be found on our Disk Image page (you must be logged in to use it). We will be developing a system that will allow the swapping system automatically to save and restore the local node state of an entire experiment. I just received an email asking me to swap or terminate my experiment. \u00b6 DETERLab has a system for detecting node use, to help achieve more efficient and fair use of DETERLab's limited resources. This system sends email messages to experiment leaders whose experiments have been idle for several hours. If you get a message like this, your experiment has been inactive for too long and you should free up its nodes. If the experiment continues to be idle, more reminders may be sent, and soon your project leader will be one of the recipients. After you have been notified, your experiment may be swapped at any time, depending on current demand for nodes, and other factors. If you feel you received the message in error, please respond to Testbed Operations (testbed-ops@isi.deterlab.net) as soon as possible, describing how you have used your node in the last few hours. There are some types of activity that are difficult to accurately detect, so we'd like to know how we can improve our activity detection system. '''Above all, do not ignore these messages.''' If you get several reminders and don't respond, your experiment will be swapped out, potentially causing loss of some of your work (see \"node state\" above). If there is a reason you need to keep your experiment running, tell us so we don't inadvertently cause problems for you. Someone swapped my experiment! \u00b6 As described above, the system automatically swaps out your experiment after it reaches its idle time limit, or sometimes an DETERLab operator does it earlier when resources are in especially high demand. In the latter case, we will typically try to contact you by email before we swap it out. However, especially if the experiment has been idle for several hours, we may swap it out for you without waiting very long to hear from you. Because of this, it is critical that you keep in close contact with us about an experiment that we may perceive as idle if you want to avoid any loss of your work. What is \"Max duration\"? \u00b6 Each experiment may have a Maximum Duration, where an experimenter specifies the maximum amount of time that the experiment should stay swapped in. When that time is exceeded, the experiment is unconditionally swapped out. The timer is reset every time the experiment swaps in. A reminder message is sent about an hour before the experiment is swapped. This swapout happens regardless of any activity on the nodes, and can be averted by using the \"Edit Metadata\" menu item on the experiment's page to turn off the Maximum Duration feature or to lengthen the duration. This feature allows users to schedule experiment swapouts, helping them to release nodes in a timely manner. For instance, if you plan to use your experiment throughout an 8 hour work day, you can schedule a swapout for 8 hours after it is swapped in. That way, if you forget to swap out before leaving for the day, it will automatically free up the nodes for other users, without leaving the nodes idle for several hours before being idle-swapped, and will work even if you leave your test programs running, making the experiment look non-idle. For automated experiments, it lets you schedule a swapout for slightly after the maximum amount of time your experiment should last. It can also help catch \"runaway\" experiments (typically batch). \"Max duration\" has a similar effect as scheduled termination/swapout , which is specified in the NS file. The differences are that the former lets you adjust the duration while the experiment is running, you get a warning email, and you're always swapped, never terminated. (It's also implemented differently, with a 5 minute scheduling granularity.)","title":"Understanding Swapping (Node Use Policies)"},{"location":"core/swapping/#understanding-swapping-node-use-policies","text":"","title":"Understanding Swapping (Node Use Policies)"},{"location":"core/swapping/#what-are-deterlabs-use-policies","text":"As a courtesy to other experimenters, we ask that experiments be swapped out or terminated when they are no longer in active use. There are a limited number of nodes available, and node reservations are exclusive, so it is important to free nodes that will be idle so that others may use them. In summary, our policy is that experiments should be swapped out when they are not in use. We encourage you to do that yourself. In general, if experiments are idle for several hours, the system will automatically swap them out, or send you mail about it, and/or an operator may manually swap them out. The actual grace period will differ depending on the size of the experiment, the current demand for resources, and other factors (such as whether you've been a good DETERLab citizen in the past!). If you mark your experiment \"non-idle-swappable\" at creation time or before swapin, and testbed-ops approves your justification, we will make every effort to contact you before swapping it, since local node state could be lost on swapout. Please see full details below.","title":"What are DETERLab's use policies?"},{"location":"core/swapping/#what-is-active-use","text":"A node or experiment that is being actively used will be doing something related to your experiment. In almost all cases, someone will either be logged into it using it interactively, or some program will be running, sending and receiving packets, and performing the operations necessary to carry out the experiment.","title":"What is \"active use\"?"},{"location":"core/swapping/#when-is-an-experiment-considered-idle","text":"Your experiment will be considered idle if it has no measurable activity for a significant period of time (a few hours; the exact time is typically set at swapin time). We detect the following types of activity: Any network activity on the experimental network Substantial activity on the control network TTY/console activity on nodes High CPU activity on nodes Certain external events, such as rebooting a node with node_reboot If your experiment's activity falls outside these measured types of activity, or it seems that DETERLab is not assessing your idle time correctly, please be sure to let us know when you create your experiment, or you may be swapped out unexpectedly. ''It is considered abuse to generate artificial activity in order to prevent your experiment from being marked idle. Abusers' access to DETERLab will be revoked, and their actions will be reported to their project leader. Please do not do this. If you think you need special assistance for a deadline, demo or other reason, please contact us .''","title":"When is an experiment considered idle?"},{"location":"core/swapping/#what-is-swapping","text":"Swapping is the process of instantiating your experiment, i.e., allocating nodes, configuring links, etc. It also refers to the reverse process, in which nodes are released. These processes are called \"swapping in\" and \"swapping out\" respectively.","title":"What is \"swapping\"? "},{"location":"core/swapping/#what-is-an-idle-swap","text":"An \"Idle-Swap\" is when DETERLab or its operators swap out your experiment because it was idle for too long. There are two ways that your experiment may be idle-swapped: automatic and manual. The most common is automatic, which happens when Idle-Swap is enabled for your experiment and the experiment has been continuously idle for the idle-swap time that was set at creation/swapin time (usually a few hours). DETERLab will then automatically swap it out. The other way to get idle-swapped is manually, by a DETERLab operator. This typically happens when there is very high resource demand and the experiment has been idle a substantial time, usually a few hours. In this case we will typically make every effort to contact you, since it may cause you to lose data stored on the nodes. ''Note that operators (and you) may swap your excessively idle experiment whether or not it is marked idle-swappable.'' When you create your experiment, you may uncheck the \"Idle-Swap\" box, disabling the automatic idle-swapping of your experiment. If you do so, you must specify the reason, which will be reviewed by testbed-ops. If your reason is judged unacceptable or insufficient, we will explain why, and your experiment will be marked idle-swappable. Valid reasons might be things such as: ''Your idle-detection system fails to detect my experimental activity.'' ''I have node-local state that is impractical to copy off in a timely or reliable manner, because .....'' ''My experiment takes a huge number of nodes, I have several runs to make with intervening think time, and if someone grabs some of these nodes if I'm swapped while thinking, I'll miss my deadline 2 days from now.'' If an experiment is non-idle-swappable, our system will not automatically swap it out, and testbed administrators will attempt to contact you in the event a swapout becomes necessary. However, we expect you to be responsible for managing your experiment in a responsible way, a way that uses DETERLab's hardware resources efficiently. When you create your experiment, you may decrease the idle-swap time from the displayed default, but you may not raise it. If lowering it is compatible with your planned use, doing so helps you be a good DETERLab citizen. If you want it raised, for example for reasons similar to those given above, send mail to testbed-ops AT deterlab.net. You may edit the swap settings (Idle-Swap, Max-Duration, and corresponding reasons and timeouts) using the \"Modify Settings\" menu item on the ''Experiment'' page for your experiment.","title":"What is an \"Idle-Swap\"? "},{"location":"core/swapping/#how-long-is-too-long-for-a-node-to-be-idle","text":"Ideally, an experiment should be used nearly continuously from start to finish of the experiment, then swapped out or terminated. However, this isn't always possible. In general, if your experiment is idle for 2 hours or more, it should be swapped out. This is especially true at night (in U.S. timezones) and on weekends. Many experimenters take advantage of lower demand during evenings and weekends to run their large-scale (50-150 node) tests. If your experiment uses 10 nodes or more, it is even more important to release your nodes as soon as possible. Swapin and swapout only take a few minutes (typically 3-5 for swapin, and less than 1 for swapout), so you won't lose much time by doing it. Sometimes an experiment will run long enough that you cannot be online to terminate it, for example, if the experiment completes in the middle of the night. We provide three mechanisms to assist you in terminating your experiment and releasing nodes in a timely manner. The first is the Idle Swap, explained above, the second is Scheduled Termination , and the third is the \"Max Duration\" option, explained below .","title":"How long is too long for a node to be idle?"},{"location":"core/swapping/#what-is-node-state","text":"Some experiments have state that is stored exclusively on the nodes themselves, on their local hard drives. This is state that is not in your NS file or files or disk images that it references, and therefore is not preserved in our database across swapin/swapout. This is state you add to your machines \"by hand\" after DETERLab sets up your experiment, like files you add or modify on filesystems local to test nodes. Local node state does not include any data you store in /users , /proj , or /groups , since those are saved on a fileserver, and not on the local nodes. Most experiments don't have any local node state, and can be swapped out and in without losing any information. This is highly recommended, since it is more courteous to other experimenters. It allows you or DETERLab to easily free up your nodes at any time without losing any of your work. '''Please make your experiments adhere to this guideline whenever possible.''' An experiment that needs local state that inherently cannot be saved (for some reason) or that you will not be able to copy off before your experiment hits the \"idle-swap time,\" should not be marked \"idle-swap\" when you create it. In the ''Begin Experiment'' form you must explain the reason. If you must have node state, you can save it before you swap out by copying it by hand (e.g., into a tar or RPM file), or creating a disk image of the node in question, and later reloading it to a new node after you swap in again. Disk images in effect create a \"custom OS\" that may be loaded automatically based on your NS file. More information about disk images can be found on our Disk Image page (you must be logged in to use it). We will be developing a system that will allow the swapping system automatically to save and restore the local node state of an entire experiment.","title":"What is \"node state\"? "},{"location":"core/swapping/#i-just-received-an-email-asking-me-to-swap-or-terminate-my-experiment","text":"DETERLab has a system for detecting node use, to help achieve more efficient and fair use of DETERLab's limited resources. This system sends email messages to experiment leaders whose experiments have been idle for several hours. If you get a message like this, your experiment has been inactive for too long and you should free up its nodes. If the experiment continues to be idle, more reminders may be sent, and soon your project leader will be one of the recipients. After you have been notified, your experiment may be swapped at any time, depending on current demand for nodes, and other factors. If you feel you received the message in error, please respond to Testbed Operations (testbed-ops@isi.deterlab.net) as soon as possible, describing how you have used your node in the last few hours. There are some types of activity that are difficult to accurately detect, so we'd like to know how we can improve our activity detection system. '''Above all, do not ignore these messages.''' If you get several reminders and don't respond, your experiment will be swapped out, potentially causing loss of some of your work (see \"node state\" above). If there is a reason you need to keep your experiment running, tell us so we don't inadvertently cause problems for you.","title":"I just received an email asking me to swap or terminate my experiment. "},{"location":"core/swapping/#someone-swapped-my-experiment","text":"As described above, the system automatically swaps out your experiment after it reaches its idle time limit, or sometimes an DETERLab operator does it earlier when resources are in especially high demand. In the latter case, we will typically try to contact you by email before we swap it out. However, especially if the experiment has been idle for several hours, we may swap it out for you without waiting very long to hear from you. Because of this, it is critical that you keep in close contact with us about an experiment that we may perceive as idle if you want to avoid any loss of your work.","title":"Someone swapped my experiment!"},{"location":"core/swapping/#what-is-max-duration","text":"Each experiment may have a Maximum Duration, where an experimenter specifies the maximum amount of time that the experiment should stay swapped in. When that time is exceeded, the experiment is unconditionally swapped out. The timer is reset every time the experiment swaps in. A reminder message is sent about an hour before the experiment is swapped. This swapout happens regardless of any activity on the nodes, and can be averted by using the \"Edit Metadata\" menu item on the experiment's page to turn off the Maximum Duration feature or to lengthen the duration. This feature allows users to schedule experiment swapouts, helping them to release nodes in a timely manner. For instance, if you plan to use your experiment throughout an 8 hour work day, you can schedule a swapout for 8 hours after it is swapped in. That way, if you forget to swap out before leaving for the day, it will automatically free up the nodes for other users, without leaving the nodes idle for several hours before being idle-swapped, and will work even if you leave your test programs running, making the experiment look non-idle. For automated experiments, it lets you schedule a swapout for slightly after the maximum amount of time your experiment should last. It can also help catch \"runaway\" experiments (typically batch). \"Max duration\" has a similar effect as scheduled termination/swapout , which is specified in the NS file. The differences are that the former lets you adjust the duration while the experiment is running, you get a warning email, and you're always swapped, never terminated. (It's also implemented differently, with a 5 minute scheduling granularity.)","title":"What is \"Max duration\"?"},{"location":"core/user-guidelines/","text":"User Do's and Don'ts \u00b6 If you are a student: DO read the Student Introduction to DETERLab . Preserving our Control Network \u00b6 DON'T use control network unless absolutely necessary. This means: DON'T use full node names such as ping node1.YourExperiment.YourProject DO use short names such as ping node1 . This ensures that traffic goes over experimental network. DON'T generate traffic to 192.168.x.x network. DO use addresses of experimental interfaces. These can be from any IPv4 address range, depending on your NS file, but are often from the 10.10.x.x address range. Preserving our File System \u00b6 DON'T store large files (e.g. uncompressed kernel source) in your home or project directory unless you need them in multiple experiment instances. DO store these files locally on a node, e.g., in /tmp folder. If you need more disk space on a Linux or FreeBSD node you can mount more to /mnt/local by doing sudo /usr/local/etc/emulab/mkextrafs.pl /mnt user='whoami' sudo chown $user /mnt/local or on an XP node, SSH in and set a root password: % passwd root then through rdesktop log in as root, and use start -> control panel -> performance and maintenance -> administrative tools -> computer management -> storage -> disk management (local) to create a new partition. After the partition formats, log out of the root account and the storage will be available to your usual identity. Transfer the files to your home directory before you swap out to save them. DON'T transfer large (>500 MB) files frequently between your home or project directories and a local directory on your experimental machine. If you need to regularly save and read large files that persist between experiment instances create a ticket and we will help you use our ZFS storage. DON'T perform large (> 500 MB) or frequent (< 10 s) reads/writes on your experimental nodes into your home or project directory DO perform these reads/writes on a local disk ( /tmp or /mnt/local on experimental machines) DON'T compile software or kernels in your home directory DO compile them on a local disk ( /tmp or /mnt/local on experimental machines) Preserving CPU Cycles on users \u00b6 DON'T compile large files or run CPU intensive jobs on users.deterlab.net . DO allocate experimental nodes, store files locally and compile/run jobs there.","title":"User Do's and Don'ts"},{"location":"core/user-guidelines/#user-dos-and-donts","text":"If you are a student: DO read the Student Introduction to DETERLab .","title":"User Do's and Don'ts"},{"location":"core/user-guidelines/#preserving-our-control-network","text":"DON'T use control network unless absolutely necessary. This means: DON'T use full node names such as ping node1.YourExperiment.YourProject DO use short names such as ping node1 . This ensures that traffic goes over experimental network. DON'T generate traffic to 192.168.x.x network. DO use addresses of experimental interfaces. These can be from any IPv4 address range, depending on your NS file, but are often from the 10.10.x.x address range.","title":"Preserving our Control Network"},{"location":"core/user-guidelines/#preserving-our-file-system","text":"DON'T store large files (e.g. uncompressed kernel source) in your home or project directory unless you need them in multiple experiment instances. DO store these files locally on a node, e.g., in /tmp folder. If you need more disk space on a Linux or FreeBSD node you can mount more to /mnt/local by doing sudo /usr/local/etc/emulab/mkextrafs.pl /mnt user='whoami' sudo chown $user /mnt/local or on an XP node, SSH in and set a root password: % passwd root then through rdesktop log in as root, and use start -> control panel -> performance and maintenance -> administrative tools -> computer management -> storage -> disk management (local) to create a new partition. After the partition formats, log out of the root account and the storage will be available to your usual identity. Transfer the files to your home directory before you swap out to save them. DON'T transfer large (>500 MB) files frequently between your home or project directories and a local directory on your experimental machine. If you need to regularly save and read large files that persist between experiment instances create a ticket and we will help you use our ZFS storage. DON'T perform large (> 500 MB) or frequent (< 10 s) reads/writes on your experimental nodes into your home or project directory DO perform these reads/writes on a local disk ( /tmp or /mnt/local on experimental machines) DON'T compile software or kernels in your home directory DO compile them on a local disk ( /tmp or /mnt/local on experimental machines)","title":"Preserving our File System"},{"location":"core/user-guidelines/#preserving-cpu-cycles-on-users","text":"DON'T compile large files or run CPU intensive jobs on users.deterlab.net . DO allocate experimental nodes, store files locally and compile/run jobs there.","title":"Preserving CPU Cycles on users"},{"location":"core/using-nodes/","text":"Using Your Nodes \u00b6 Know your DETER servers \u00b6 Here are the most important things to know. www.isi.deterlab.net is the primary web interface for the testbed. users.deterlab.net is the host through which the testbed nodes are accessed and it is primary file server. scratch is the local package mirror for CentOS, Ubuntu, and FreeBSD. Accessing your nodes \u00b6 To access your nodes you will need to: Swap in your experiment SSH into users.deterlab.net SSH into your experimental nodes using node.exprt.proj . For example to access node client in experiment test and project CS444 one could type ssh client.test.CS444 on the users.deterlab.net. Please see more detailed documentation about hostnames and logging in . Modes of Use \u00b6 There are several ways in which you can use your nodes: As you start developing your experiment you may want to design your NS topology, swap the experiment in, and then SSH to your nodes and manually execute commands. As your work progresses you may want to develop scripts (e.g., using Bash or Python or MAGI or Ansible) to automate running of your experiments We have developed three toolkits to help you with experiment design and automation. First, you can use DEW - distributed experiment workflows to design your experiment in a human-readable format and generate NS file and bash scripts. We provide more guidance on this direction in DEW YouTube channel as well as in documentation on DEW Web site. Second, if you use image Ubuntu-DEW on your nodes, all the commands you type and snippets of their outputs will be saved in your project directory. You can use the tool flight_log , which is automatically installed in that image, to remind yourself of the commands you ran in the past and to select those you want to include in a Bash script. The script will be automatically generated for you. More information about this direction is in DEW YouTube channel . Third, you can use our MAGI orchestrator to create scripts that will be more robust and readable than Bash scripts. FAQ \u00b6 We have compiled a list of frequenty asked questions about node access here","title":"Using Your Nodes"},{"location":"core/using-nodes/#using-your-nodes","text":"","title":"Using Your Nodes"},{"location":"core/using-nodes/#know-your-deter-servers","text":"Here are the most important things to know. www.isi.deterlab.net is the primary web interface for the testbed. users.deterlab.net is the host through which the testbed nodes are accessed and it is primary file server. scratch is the local package mirror for CentOS, Ubuntu, and FreeBSD.","title":"Know your DETER servers"},{"location":"core/using-nodes/#accessing-your-nodes","text":"To access your nodes you will need to: Swap in your experiment SSH into users.deterlab.net SSH into your experimental nodes using node.exprt.proj . For example to access node client in experiment test and project CS444 one could type ssh client.test.CS444 on the users.deterlab.net. Please see more detailed documentation about hostnames and logging in .","title":"Accessing your nodes"},{"location":"core/using-nodes/#modes-of-use","text":"There are several ways in which you can use your nodes: As you start developing your experiment you may want to design your NS topology, swap the experiment in, and then SSH to your nodes and manually execute commands. As your work progresses you may want to develop scripts (e.g., using Bash or Python or MAGI or Ansible) to automate running of your experiments We have developed three toolkits to help you with experiment design and automation. First, you can use DEW - distributed experiment workflows to design your experiment in a human-readable format and generate NS file and bash scripts. We provide more guidance on this direction in DEW YouTube channel as well as in documentation on DEW Web site. Second, if you use image Ubuntu-DEW on your nodes, all the commands you type and snippets of their outputs will be saved in your project directory. You can use the tool flight_log , which is automatically installed in that image, to remind yourself of the commands you ran in the past and to select those you want to include in a Bash script. The script will be automatically generated for you. More information about this direction is in DEW YouTube channel . Third, you can use our MAGI orchestrator to create scripts that will be more robust and readable than Bash scripts.","title":"Modes of Use"},{"location":"core/using-nodes/#faq","text":"We have compiled a list of frequenty asked questions about node access here","title":"FAQ"},{"location":"core/windows/","text":"Windows XP \u00b6 Microsoft Windows XP is supported as one of the operating system types for experiment nodes in DETER. As much as possible, we have left Windows XP \"stock\". Some Windows services are shut down: Messenger, SSDP Discovery Service, Universal Plug and Play Device Host, and Remote Registry. Other setting changes are described under Network config and Routing below. Before booting the node at swap-in time, DETER loads a fresh image of Windows XP onto the experiment nodes in parallel, using our frisbee service. DETER software automatically configures each Windows XP node, providing the expected experiment user environment including: user accounts and DETER SSH keys; remote home, project, and shared directories; and network connections. The Cygwin GNU environment is provided, including Bash and TCSH shells, the C/C++, Perl and Python programming languages, and several editors including Emacs, vim, nano and ed. Cygwin handles both Unix and Windows-style command paths, as described below . The DETERLab web interface manages a separate Windows password in the user profile, as well as making login connections to the experiment nodes. Remote Desktop Protocol service supports Windows Desktop logins from the user's workstation screen to the experiment node. SSH and Serial Console command-line connections are also supported. Windows XP installations are more hardware dependent than Linux or FreeBSD. At present, this Windows XP image runs on only pc3000 class machines. Differences from FreeBSD and Linux \u00b6 The biggest difference of course, is that this is Windows , with Cygwin layered on top, and DETERLab management services added. In particular, this is Windows XP (NT 5.1), with various levels of service packs and updates (see below .) File Sharing \u00b6 The second-biggest difference is that shared directories are provided not by the NFS (Network File System) protocol, but instead by the SMB (Server Message Block) protocol, otherwise known as Windows File Sharing. The \"Client for Microsoft Networks\" software contacts the SMB server, in this case Samba running on the file server known as Fs (an alias for Users .) The SMB protocol authenticates using a plain-text user name and password, encrypted as they go across the network. (These Windows Shares are then accessed by UNC paths under Cygwin mounts, [#SMB_mounts described below].) In Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with backslashes , e.g. \\\\fs\\share or \\\\fs\\<username> . User and project shares are marked \"not browsable\", so just \\\\fs shows only share . If you want to serve files from one of your experiment nodes to others, see the section on [#netbt_command The netbt command ]. Windows Passwords \u00b6 A separate Windows password is kept for use only with experiment nodes running Windows. It is presented behind-the-scenes to rdesktop for RDP logins by our Web interface under Unix, and for the Samba mount of shared directories like your home directory under an SSH login, so you don't have to type it in those cases. You will have to type it each time if you use the Microsoft RDC (Remote Desktop Connector) client program from a Windows machine. The default Windows password is randomly generated. It's easy to change it to something easier to remember. To see or edit your Windows password, log in to DETERLab, and click Manage User Profile and then Edit Profile under User Options . You will see Windows Password fields in addition to the regular DETERLab Password fields. When you change your Windows password, you will also have to re-type it as a check. The new Windows password should propagate to the Samba server on Fs instantly, so you can swap in an experiment and log in to its Windows nodes with the new password. If you have already swapped-in experiment nodes and changed your Windows password, the account information including passwords will be updated at the next DETERLab watchdog daemon isalive interval. This should be in 3 to 6 minutes. Experiment setup for Windows nodes \u00b6 All you have to do is put a line specifying a WINXP OS image in your experiment NS file, like this: tb-set-node-os $node WINXP-UPDATE The Windows XP images are not specific to a particular hardware type. (See the Change Log for more information.) You may explicitly specify the hardware type to run on if you wish, for example: tb-set-hardware $node pc3000 Since the bandwidth of the connection between the ISI and Berkeley portions of the testbed is constrained, it is best to run Windows nodes only on the ISI side of the testbed. This can be accomplished by creating a custom hardware type with tb-make-soft-vtype my_custom_node_type {pc3060 pc3000 pc2133 pc2133x} and then specifying that your Windows nodes must only swap-in on this custom type using tb-set-hardware $node my_custom_node_type See the note below on using the tb-set-node-failure-action command for experiments with a large number of Windows nodes. This can save a swap-in with a large number of Windows nodes, or prevent a single node boot failure on a swapmod from swapping-out the whole experiment. If you use these commands: tb-set-node-startcmd , tb-set-node-tarfiles , or tb-set-node-rpms you should read the sections on Permissions and Windows GUI programs below. The only available Windows image currently is: WINXP-UPDATE - The most recent Windows XP-SP3+. It is updated periodically from Windows Update, typically after a Microsoft \"Patch Tuesday\", the second Tuesday of each month. All critical and security fixes are installed, up through the date we pull the image file. (See the date created field on the individual WINXP Image IDs ). Note The Windows Firewall is disabled by default (as it will inform you repeatedly!) Network config \u00b6 Some default Windows networking features are disabled. NetBT (!NetBios over TCP) ( NetbiosOptions=2 ) and DNS auto-registration ( DisableDynamicUpdate=1 ) are disabled to allow network idle detection by the slothd service. TCP/IP address autoconfiguration is disabled ( IPAutoconfigurationEnabled=0 ) so that un-switched interfaces like the sixth NICs on the pc3000's don't get bogus Microsoft class B network 169.254.0.0 addresses assigned. The Windows ipconfig /all command only shows the configuration information for the enabled network interfaces. There will always be one enabled control net interface on the 192.168.0.0/22 network. The others are disabled if not used in your experiment. (See file /var/emulab/boot/ipconfig-cache for a full listing from boot time, including the interfaces that were later disabled.) If you specified links or LANs in your experiment network topology, other interfaces will be enabled, with an IP address, subnet mask, and gateway that you can specify in the NS file. Notice that the Windows names of the interfaces start with Local Area Connection and have a number appended. You can't count on what this number is, since it depends on the order the NIC's are probed as Windows boots. Note Often, we have seen ipconfig report an IP address and mask of 0.0.0.0 , while the TCP/IP properties dialog boxes and the netsh command show the proper values. Our startup scripts disable and re-enable the network interface in an attempt to reset this. Sometimes it doesn't work, and another reboot is done in an attempt to get the network up. Routing \u00b6 Full-blown router nodes cannot run Windows, i.e. rtproto Session is not supported. However, basic routing between connected network components of your experiment topology works. The Windows command to see the routing tables is route print . The IPEnableRouter=1 registry key is set on multi-homed hosts in the experiment network, before they are rebooted to change the hostname. rtproto Static is supported in all recent WINXP images, but not in WINXP-02-16 (2005) or before. rtproto Static-old or rtproto Manual will work in any image. There is more information on routing in the Routing section of the Core Guide . Windows nodes boot twice \u00b6 Notice that Windows reboots an extra time after being loaded onto a node during swap-in. It must reboot after changing the node name to set up the network stack properly. Be patient, Windows XP doesn't boot quickly. With hardware-independent , ( sysprep'ed ) images, the first boot is actually running Mini-Setup as well, setting up device drivers and so on. It's best not to log in to the nodes until the experiment is fully swapped-in. (You may be able to log in briefly between the first two reboots; if you see the wrong pcXXX name, you'll know that a reboot is imminent.) You can know that the swap-in process is finished by any of these methods: Waiting until you get the \"experiment swapped in\" email from DETERLab. Checking the node status on the experiment status page in DETERLab. (You must refresh the page to see node status change.) Watching the realtime swap-in log to monitor its progress. Note Sometimes Windows XP fails to do the second reboot. One reason is transient race conditions in the Windows startup, for example in the network stack when there are multiple network interface devices being initialized at the same time. We make a strong effort to recover from this, but if the recovery code fails, by default it results in a swap-in or swapmod failure. At boot time, the startup service on Windows XP runs the /usr/local/etc/emulab/rc/rc.bootsetup script, logging output to /var/log/bootsetup.log . If you're having swap-in problems and rc.bootsetup doesn't finish sending ISUP to DETERLab within 10 minutes, the node will be rebooted. After a couple of reboot cycles without a ISUP , DETERLab gives up on the node. You can cause these boot-time problems to be nonfatal by adding this line to your ns file for each Windows node : tb-set-node-failure-action $node \"nonfatal\" (where $node is replaced with the node variable, of course.) DETERLab will still complain if it doesn't get the ISUP signal at the end of rc.bootsetup, but the swap-in or swapmod will proceed and allow you to figure out what's happening. Then you will probably have to manually reboot the failed Windows node to make it available to your experiment. If you try to login to a node after swap-in to diagnose the problem and your Windows password isn't honored, use this command on Ops to remotely reboot the node: node_reboot pcxxx If you are able to log in but your remote home directory isn't mounted, this is another symptom of a partial set-up. You have the additional option of executing this command on the node itself: /sbin/reboot This gives Windows another chance to get it right. Login connections to Windows \u00b6 You can manually start up the SSH or RDP client programs to connect and log in to nodes in your experiment, or use the console command on Ops. You will have to type your Windows Password when logging in, except for SSH when you have ssh-agent keys loaded. Or you can set up your browser to automatically connect in one click from the DETERLab web interface and pop up a connection window. Once you start swapping in an experiment, the Experiment Information page contains a table of the physical node ID and logical node name, status, and connection buttons. The captions at the top of the button columns link to pages explaining how to set up up mime-types in your browser to make the buttons work, from FreeBSD, Linux, and Windows workstations: SSH (setup) - The SSH connection button gives a Bash or TCSH shell, as usual. Your DETERLab SSH keys are installed on the node in a /sshkeys subdirectory. Console - The serial console is supported for Cygwin shell logins using the agetty and sysvinit packages. This is the only way in when network connections are closed down! You can also monitor the Frisbee loading and booting of the Windows image on the console. RDP - The RDP button starts up a Remote Desktop Protocol connection, giving a Windows Desktop login from the user's workstation screen to the experiment node. The rdesktop client software is used from Linux and Unix client workstations. A Microsoft RDC (Remote Desktop Connector) client program is included in Windows XP, and may be installed onto other versions of Windows as well. It has the feature that you can make it full-screen without (too much) confusion, since it hangs a little tab at the top of the screen to switch back. Unfortunately, we have no way to present your DETERLab Windows password to RDC, so you'll have to type it on each login. Note If you import dot-files into DETERLab that replace the system execution search path rather than add to it, you will have a problem running Windows system commands in shells. Fix this by adding /cygdrive/c/WINDOWS/system32 and /cygdrive/c/WINDOWS to your $PATH in ~/.cshrc and either ~/.bash_profile or ~/.profile . Don't worry about your home directory dot-files being shared among Windows, FreeBSD, and Linux nodes; non-existent directories in the $PATH are ignored by shells. When new DETERLab user accounts are created, the default CSH and Bash dotfiles are copied from the FreeBSD /usr/share/skel . They replace the whole $PATH rather than add to it. Then we append a DETERLab-specific part that takes care of the path, conditionally adding the Windows directories on Cygwin. Note The Windows ping program has completely different option arguments from the Linux and FreeBSD ones, and they differ widely from each other. There is a ping package in Cygwin that is a port of the 4.3bsd ping. Its options are close to a common subset of the Linux and FreeBSD options, so it will be included in future WINXP images: ping [ -dfqrv ] host [ packetsize [count [ preload]]] You can load it yourself now using Cygwin Setup . Note There are no Cygwin ports of some other useful networking commands, such as traceroute and ifconfig -a . The Windows system equivalents are tracert and ipconfig /all . RDP details \u00b6 Here are some fine points and hints for RDP logins to remote Windows desktops: Microsoft allows only one desktop login at a time to Windows XP , although this is the same Citrix Hydra technology that supports many concurrent logins to Terminal Server or Server 2003. The Fast User Switching option to XP is turned on, so a second RDP connection disconnects a previous one rather than killing it. Similarly, just closing your RDP client window disconnects your Windows Login session rather than killing it. You can reconnect later on without losing anything. SSH doesn't count as a desktop, so you can SSH in and use this command: qwinsta (Query WINdows STAtion) to show existing winstation sessions and their session ID's, and this one to reset (kill) a session by ID: rwinsta . We rename My Computer to show the PCxxx physical node name, but it doesn't appear on the Windows XP desktop by default. The XP user interface incorporates \"My Computer\" into the upper-right quadrant of the \"Start\" menu by default, and removes it from the desktop. You can go back to the \"classic\" user interface of Windows 2000, including showing \"My Computer\". Right-click on the background of the Taskbar which contains the \"Start\" button at the left, and choose \"Properties\". Select the \"Start Menu\" tab, click the \"Classic Start menu\" radio-button, and click \"OK\". Alternatively, you can force \"My Computer\" to appear on your XP desktop by right-clicking on the desktop background and choosing \"Properties\". Select the \"Desktop\" tab and click \"Customize Desktop...\" to get the \"Desktop Items\" dialog. Turn on the \"My Computer\" checkbox, then click \"OK\" twice. There are several Desktop icons (i.e. \"shortcuts\") installed by default in the XP images: Computer Management, Bash and TCSH shells, and NtEmacs . You will notice two flavors of Bash and TCSH icons on the desktop, labeled rxvt and Cygwin . The rxvt shells run in windows with X -like cut-and-paste mouse clicks: Left-click starts a selection, Right-click extends it, and middle-click pastes. These are the ones to use if you're connecting from an X workstation. Note The default colors used in Bash and rxvt don't work well in 4-bit color mode under RDP. Make sure you update your rdp-mime.pl to get the rdesktop -a 16 argument for 16-bit color. Or, you can over-ride the rxvt defaults by putting lines in your ~/.Xdefaults file like this: rxvt*background: steelblue The Cygwin shells run in a Windows Terminal window, just as the Windows cmd.exe does. These are the ones to use if you're connecting from a Windows workstation. Quick-edit mode is on by default, so you can cut-and-paste freely between your local workstation desktop and your remote RDP desktops. In a Windows Terminal window on your RDP remote desktop, the quick-edit cut-and-paste mouse clicks are: Left-drag the mouse to mark a rectangle of text, highlighting it. Type Enter or right-click the mouse when text is highlighted , to copy* the selected text to the clipboard. ( Escape* cancels the selection without copying it.) Right-click the mouse with nothing selected to paste the contents of the clipboard. On the first login by a user , Windows creates the user's Windows profile directory under C:\\Documents and Settings , and creates the registry key (folder) for persistent settings for that user. We arrange that early in the user's login process, a user HOME environment variable value is set in the user's registry. Otherwise Emacs wouldn't know how to find your .emacs setup file in your remotely mounted home directory. User \"root\" is special, and has a local home directory under /home . /home is a Cygwin symbolic link to C:\\Documents and Settings . The Windows XP Start menu has no Shutdown button under RDP. Instead, it is labeled Disconnect and only closes the RDP client window, leaving the login session and the node running. If you simply close the window, or the RDP client network connection is lost, you are also disconnected rather than logged out. When you reconnect, it comes right back, just as it was. To restart the computer, run /sbin/reboot , or use the \"Shut Down\" menu of Task Manager . One way to start Task Manager is to right-click on the background of the Taskbar at the bottom of the screen and select \"Task Manager\". The netbt command \u00b6 The NetBT (Netbios over TCP) protocol is used to announce shared directories (folders) from one Windows machine to others. (See the Name and Session services in http://en.wikipedia.org/wiki/Netbios .) The SMB (Server Message Block) protocol is used to actually serve files. (See http://en.wikipedia.org/wiki/Server_Message_Block .) In DETERLab, we normally disable NetBT on experiment nodes, because it chatters and messes up slothd network idle detection, and is not needed for the usual SMB mounts of /users , /proj , and /share dirs, which are served from a Samba service on fs . However, NetBT does have to be enabled on the experiment nodes if you want to make Windows file shares between them. The netbt script sets the registry keys on the Windows network interface objects. Run it on the server nodes (the ones containing directories which you want to share) and reboot them afterwards to activate. There is an optional -r argument to reboot the node. Usage: netbt [-r] off|on If you use netbt to turn on NetBT, it persists across reboots. No reboot is necessary if you use Network Connections in the Control Panel to turn on NetBT. It takes effect immediately, but is turned off at reboot unless you do netbt on afterward as well. Right-click Local Area Connection (or the name of another connection, if appropriate), click Properties, click Internet Protocol (TCP/IP), and then click the Properties button. On the Internet Protocol (TCP/IP) Properties page, click the Advanced button, and click the WINS tab. Select Enable or Disable NetBIOS over TCP/IP. ipconfig /all reports \"NetBIOS over Tcpip . . . : Disabled\" on interfaces where NetBT is disabled, and says nothing where NetBT is enabled. To start sharing a directory, on the node, use the net share command, or turn on network sharing on the Sharing tab of the Properties of a directory (folder.) On XP-SP2 or above, when you first do this, the \"Network sharing and security\" subdialog says: As a security measure, Windows has disabled remote access to this computer. However, you can enable remote access and safely share files by running the _Network_Setup_Wizard_. _If_you_understand_the_security_risks_but_want_to_share_ _files_without_running_the_wizard,_click_here._\" Skip the wizard and click the latter (\"I understand\") link. Then click \"Just enable file sharing\", and \"OK\". Then you finally get the click-box to \"Share this folder on the network\". The machine names for UNC paths sharing are the same as in shell prompts: pcXXX , where XXX is the machine number. These will show up in My Network Places / Entire Network / Microsoft Windows Network / DETER once you have used them. IP numbers can also be used in UNC paths, giving you a way to share files across experiment network links rather than the control network. There is an DETER-generated LMHOSTS file, to provide the usual node aliases within an experiment, but it is currently ignored even though \"Enable LMHOSTS lookup\" is turned on in the TCP/IP WINS settings. Try nbtstat -c and nbtstat -R to experiment with this. (See the Microsoft doc for nbtstat . Making Custom Windows OS Images \u00b6 Making custom Windows images is similar to doing it on the other DETER operating systems , except that you must do a little more work to run the prepare script as user root since there are no su or sudo commands on Windows. This is optional on the other OS types, but on Windows, proper TCP/IP network setup depends on prepare being run. Log in to the node where you want to save a custom image. Give the shell command to change the root password. Pick a password string you can remember, typing it twice as prompted: % passwd root Enter the new password (minimum of 5, maximum of 8 characters). Please use a combination of upper and lower case letters and numbers. New password: Re-enter new password: This works because you are part of the Windows Administrators group . Otherwise you would have to already know the root password to change it. Note If you change the root password and reboot Windows before running prepare below, the root password will not match the definitions of the DETER Windows services (daemons) that run as root, so they will not start up. Log out all sessions by users other than root , because prepare will be unable to remove their login profile directories if they are logged in. (See QWINSTA .) Log in to the node as user root through the Console or SSH, using the password you set above, then run the prepare command. (It will print \"Must be root to run this script!\" and do nothing if not run as root.) /usr/local/etc/emulab/prepare If run without option arguments, prepare will ask for the root password you want to use in your new image, prompting twice as the passwd command did above. It needs this to redefine the DETER Windows services (daemons) that run as root. It doesn't need to be the same as the root password you logged in with, since it sets the root password to be sure. The Administrator password is changed as well, since the Sysprep option needs that (below.) You can give the -p option to specify the root password on the command line: /usr/local/etc/emulab/prepare -p myRootPwd The -n option says not to change the passwords at all, and the DETER Windows services are not redefined. /usr/local/etc/emulab/prepare -n The -s option is used to make hardware-independent images using the Windows Sysprep deploy tool. If you use it with the -n option instead of giving a password, it assumes that you separately blank the Administrator password, or edit your Administrator password into the [GuiUnattended]AdminPassword entry of the sysprep.inf file. /usr/local/etc/emulab/prepare -s -p myRootPwd Note This must be done from a login on the serial console , because Sysprep shuts down the network. prepare -s refuses to run from an SSH or RDP login. Note Currently, hardware-independent images must be made on a pc850, and will then run on the pc600, pc3000, and pc3000w as well. There is an unresolved boot-time problem going the other direction, from the pc3000 to a pc850 or pc600. Windows normally casts some aspects of the NT image into concrete at the first boot after installation, including the specific boot disk driver to be used by the NT loader (IDE, SCSI, or SATA.) Sysprep is used by PC hardware manufacturers as they make XP installation disks with their own drivers installed. The Sysprep option to run an unattended Mini-Setup at first boot instead of the normal \"Out Of the Box Experience\" is used in some large corporate roll-outs. We do both. The DETER /share/windows/sysprep directory contains several versions of the XP deploy tools matched to the XP service pack level, appropriate device driver directories, and a draft sysprep.inf file to direct the automated install process. Mini-setup needs to reboot after setting up device drivers. XP also needs to [#Boots_twice reboot] after changing the host name. We combine the two by using a Cmdlines.txt script to run rc.firstboot -mini to set the host name at the end of Mini-Setup . Thus we only pay the extra time to set up device drivers and so on from scratch, about two minutes, rather than adding a third hardware and XP reboot cycle. Note As you create your Image Descriptor, set the reboot wait-time to 360 rather than 240 so that swap-ins don't time out. Then log out and create your custom image . Note Windows XP is too big to fit in the partitioning scheme used by FreeBSD and Linux, so it's necessary when making a Windows custom image to specify Partition 1 , and click Whole Disk Image. When you're testing your custom image, it's a good idea to set the tb-set-node-failure-action to \"nonfatal\" in the ns file so you get a chance to examine an image that hasn't completed the set-up process. See the note below for other useful ideas. Cygwin \u00b6 Cygwin is GNU + Cygnus + Windows , providing Linux-like functionality at the API, command-line, and package installation levels. Cygwin documentation \u00b6 Cygwin is well documented. Here are some links to get you started: Users guide Cygwin highlights Cygwin-added utilities FAQ API compatibility and Cygwin functions Cygwin packages \u00b6 A number of optional Cygwin packages are installed in the image due to our building and running the DETER client software, plus some editors for convenience. These packages are currently agetty, bison, cvs, cygrunsrv, ed, file, flex, gcc, gdb, inetutils, make, minires-devel, more, nano, openssh, openssl-devel, patch, perl, perl-libwin32, psmisc, python, rpm, rsync, shutdown, sysvinit, tcsh, vim, wget, and zip. The Cygwin command cygcheck -c lists the packages that are installed, and their current version number and status. Package-specific notes and/or documentation for installed packages are in /usr{,/share}/doc/Cygwin/*.README and /usr/share/doc/*/README files. The Cygwin package site lists the available pre-compiled packages and provides a search engine. If you want to install more Cygwin pre-compiled packages, run the graphical installer: C:/Software/Cygwin/setup.exe The Cygwin command cygcheck -l package-name lists the contents of an installed package, which may help you to make a tarfile or rpm from a package you have installed. You can then cause it to be installed automatically by DETER into all of the nodes of your experiment. See the [Tutorial#TARBALLS Tutorial] for more information about installing RPM's and tarballs . Watch out for post-install scripts in: /etc/postinstall/package-name.sh{,.done} Many packages not in the Cygwin package site have also been ported to Cygwin already. Download the sources to an experiment node and try ./configure make make install as usual. SMB mounts and Samba \u00b6 User home directories and other shared directories are served by fs , another alias for Ops/Users, via the SMB protocol (Server Message Block, also known as Windows File Sharing) with the Windows Client connecting to the Samba server. UNC paths with leading double-slashes and a server name, e.g. //fs , are used to access the SMB Shares under Cygwin. DETER then uses the Cygwin mount command to make them appear on the usual Unix paths for the DETER shared directories: /users/<username> , /proj/<pid> , /group/<pid>/<gid> , and /share . The Cygwin mount command lists what you could access on the Samba server, with the UNC path in the first column. Unix file permissions may further limit your access on the Samba server. Log in to Ops to investigate. /share/windows contains Windows software. See /share/windows/README.bin for descriptions of binary packages available for installation. In Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with backslashes , e.g. \\\\fs\\share or \\\\fs\\<username> . User and project shares are marked \"not browsable\", so just \\\\fs shows only share . Windows limitation: There is only one protection mask for everything in a whole mount/share under SMB. It's set in the \"share properties\" on the server (Samba config file in this case) so chmod will do you no good across SMB. Cygwin limitation: There is a hard-coded limit of 30 mount points in Cygwin. Cygwin uses 4 of them, and DETER uses another 3 or 4. So some of your /users mounts will fail on Windows startup if you have more than 23 or 24 members in your project, unless they are grouped into smaller subgroups. Cygwin arcana \u00b6 File paths Cygwin accepts either flavor of slashes in paths, Unix/POSIX-style forward-slashes, or Windows-style back-slashes. In Unix shell commands, backslashes need to be quoted. Single-quotes work best. Doubling each backslash also works. This must also be done inside double-quotes. Examples: '\\single\\quoted' , \"\\\\double\\\\quoted\" , \\\\un\\\\quoted . (The difference between double and single quotes is that $variable references and back-quoted command execution are expanded in double-quotes.) When you invoke Windows (as opposed to Cygwin) commands, for example net use , they will know nothing about Unix-style paths in their arguments. The cygpath utility is an aid to converting paths between the Unix and Windows conventions. cygpath -w converts its arguments to Windows format, and cygpath -u converts its arguments to Unix format, e.g. $ cygpath -w /cygdrive/c/WINDOWS c:\\WINDOWS $ cygpath -u 'c:\\WINDOWS' /cygdrive/c/WINDOWS Mount points Cygwin mount points are shown by the mount and df commands. Note that there is a hard-coded limit of 30 mount points in Cygwin. Attempts to use the Cygwin mount command after that will fail. See the discussion of mount points and UNC //machine paths to SMB shares above . Another special case is the Unix root , \" / \". It's mounted to C:\\cygwin in the Windows filesystem. Drive letter mounts Cygwin knows about drive letter prefixes like C: \u00c2 , which are equivalent to /cygdrive/<drive-letter> \u00c2 . However, /cygdrive , like /dev , isn't a real directory, so you can't ls it. Some Windows software requires drive-letter mounts to be created for its use. You can use the Windows net use command to associate drive letters with UNC paths to SMB shares, e.g. net use W: '\\\\fs\\share\\windows' You can use the Windows subst command to associate drive letters with local paths, e.g. subst T: 'C:\\Temp' Filename completion in Cygwin shells with <Tab> doesn't work following a drive-letter prefix, but it works normally after a /cygdrive/ prefix. Also, filename completion is case-sensitive, although the underlying Windows is case-insensitive, so a filename in the wrong case is still opened properly. NTSEC Cygwin is running in NTSEC (NT Security) mode, so /etc/passwd and /etc/group contain Windows SID's as user and group ID's. Your Windows UID is the computer SID with a user number appended, something like S-1-5-21-2000478354-436374069-1060284298-1334 . Cygwin commands, such as id , ls -ln , and chown/chgrp , use the numeric suffix as the uid, e.g. 1334 . This is different from your normal DETER Unix user ID number, and the Samba server takes care of the difference. The id command reports your user id and group memberships. Note that all users are in group None on XP. Contrary to the name, this is a group that contains all users . It was named Everybody on Windows 2000, which was a better name. setuid There is no direct equivalent of the Unix setuid programs under Windows, and hence no su or sudo commands. The Windows equivalent to running a Unix command as root is membership in the Windows Administrators group. DETER project members who have either local_root or group_root privileges are put in group wheel , another alias for Administrators . Project members with user privileges are not members of the wheel group. You can ssh a command to the node as the target user, as long as you arrange for the proper authentication. For C/C++ code, there is a setuid() function in the Cygwin library, which \"impersonates\" the user if proper setup is done first. root There is not normally a Windows account named root . root on XP is just another user who is a member of the Administrators group, see below. We create a root account as part of the DETER setup to own installed software, and to run services and Unix scripts that check that they're running with root privileges. You can log in as root via RDP, ssh , or the serial console if you change the root password as described in the custom Windows OS images section. The root user does not have any Samba privileges to access Samba shared mounts, including the /proj , /groups , and /users . Administrators group All users are members of the Windows Administrators group. (The DETER non-local-root user property is not implemented on Windows.) Membership in the Windows Administrators group is very different from being root on Unix, and is also different from being logged in as Administrator. Administrators group membership on Windows only means you can set the ownership, group, and permissions on any file using the Cygwin chown , chgrp , chmod , or their Windows equivalents. Until you have done that, you can be completely locked out by read, write, or execute/open permissions of the directory or files. Another subtlety is that the group called None on XP is what used to be named Everybody on Windows 2000. All users are automatically in group None , so in practice setting group None permissions is no different from setting public access permissions. Permissions Cygwin does a pretty good job of mapping Unix user-group-other file permissions to Windows NT security ACLs. On Windows, unlike Unix, file and directory permissions can lock out root, Administrator, or SYSTEM user access. Many Unix scripts don't bother with permissions if they're running as root, and hence need modification to run on Cygwin. This creates a potential problem with the tb-set-node-tarfiles and tb-set-node-rpms commands. The tb-set-node-tarfiles page says \"Notes: 1. ... the files are installed as root\". So you can easily install files that your login doesn't have permission to access. The solution is to chmod the files before making the tarball or rpm file to grant appropriate access permissions. Executables Cygwin tries to treat .exe files the same as executable files without the .exe suffix, but with execute permissions turned on. (See the Cygwin Users Guide .) This breaks down in Makefile actions and scripts, where rm , ls -l , and install commands may need an explicit .exe added. Windows GUI programs You cannot run Windows GUI (Graphical User Interface) programs under ssh, on the serial console, or by tb-set-node-startcmd. There is no user login graphics context until you log in via RDP. However, you can use a startcmd to set a Windows registry key that causes a GUI program to be run automatically for all users when they log in to the node via RDP , if that's what you want. The program can be one that is installed by tb-set-node-tarfiles. You can pick any regkey name you want and put it in the Run registry folder. It's good not to step on the ones already there, so choose a name specific to your program. Put the following in your startcmd script: regtool -s set /HKLM/SOFTWARE/Microsoft/Windows/CurrentVersion/Run/mypgm 'C:\\mypgm\\mypgm.exe' (where mypgm is the name of your program, of course.) Notice that the value string is single-quoted with C: and backslashes. Windows interprets this regkey, and wants its flavor of file path. NtEmacs \u00b6 We don't include the Cygwin X server in our XP images to keep the bulk and complexity down. So NtEmacs 21.3 is provided instead of the Cygwin X Emacs. NtEmacs \"frames\" are windows on the Windows Desktop, e.g. ^X-5-2 makes another one. The /usr/local/bin/emacs executable is a symlink to /cygdrive/c/emacs-21.3/bin/runemacs.exe , which starts up an Emacs on the desktop. This only works under RDP, since SSH logins have a null desktop. There is also a /usr/local/bin/emacs-exe executable, a symlink to /cygdrive/c/emacs-21.3/bin/emacs.exe , which is only useful as an Emacs compiler. It could be used to run Emacs in an SSH or Serial Console login window with the -nw (no windows) flag, except that it exits with emacs: standard input is not a tty . Another thing not to try is running emacs-exe -nw in a Bash or TCSH shell on the RDP desktop. It crashed Windows XP when I tried it. Can drag-and-drop files from Windows Explorer to !NtEmacs windows. cygwin-mount.el in c:/emacs-21.3/site-lisp/ makes Cygwin mounts visible within !NtEmacs . It doesn't do Cygwin symlinks yet. Options - See ~root/.emacs mouse-wheel-mode CUA-mode option ( ^C copy / ^X cut on selection, ^V paste, ^Z undo). Ctrl and Alt key mappings, etc.","title":"Windows XP"},{"location":"core/windows/#windows-xp","text":"Microsoft Windows XP is supported as one of the operating system types for experiment nodes in DETER. As much as possible, we have left Windows XP \"stock\". Some Windows services are shut down: Messenger, SSDP Discovery Service, Universal Plug and Play Device Host, and Remote Registry. Other setting changes are described under Network config and Routing below. Before booting the node at swap-in time, DETER loads a fresh image of Windows XP onto the experiment nodes in parallel, using our frisbee service. DETER software automatically configures each Windows XP node, providing the expected experiment user environment including: user accounts and DETER SSH keys; remote home, project, and shared directories; and network connections. The Cygwin GNU environment is provided, including Bash and TCSH shells, the C/C++, Perl and Python programming languages, and several editors including Emacs, vim, nano and ed. Cygwin handles both Unix and Windows-style command paths, as described below . The DETERLab web interface manages a separate Windows password in the user profile, as well as making login connections to the experiment nodes. Remote Desktop Protocol service supports Windows Desktop logins from the user's workstation screen to the experiment node. SSH and Serial Console command-line connections are also supported. Windows XP installations are more hardware dependent than Linux or FreeBSD. At present, this Windows XP image runs on only pc3000 class machines.","title":"Windows XP"},{"location":"core/windows/#differences-from-freebsd-and-linux","text":"The biggest difference of course, is that this is Windows , with Cygwin layered on top, and DETERLab management services added. In particular, this is Windows XP (NT 5.1), with various levels of service packs and updates (see below .)","title":"Differences from FreeBSD and Linux"},{"location":"core/windows/#file-sharing","text":"The second-biggest difference is that shared directories are provided not by the NFS (Network File System) protocol, but instead by the SMB (Server Message Block) protocol, otherwise known as Windows File Sharing. The \"Client for Microsoft Networks\" software contacts the SMB server, in this case Samba running on the file server known as Fs (an alias for Users .) The SMB protocol authenticates using a plain-text user name and password, encrypted as they go across the network. (These Windows Shares are then accessed by UNC paths under Cygwin mounts, [#SMB_mounts described below].) In Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with backslashes , e.g. \\\\fs\\share or \\\\fs\\<username> . User and project shares are marked \"not browsable\", so just \\\\fs shows only share . If you want to serve files from one of your experiment nodes to others, see the section on [#netbt_command The netbt command ].","title":"File Sharing"},{"location":"core/windows/#windows-passwords","text":"A separate Windows password is kept for use only with experiment nodes running Windows. It is presented behind-the-scenes to rdesktop for RDP logins by our Web interface under Unix, and for the Samba mount of shared directories like your home directory under an SSH login, so you don't have to type it in those cases. You will have to type it each time if you use the Microsoft RDC (Remote Desktop Connector) client program from a Windows machine. The default Windows password is randomly generated. It's easy to change it to something easier to remember. To see or edit your Windows password, log in to DETERLab, and click Manage User Profile and then Edit Profile under User Options . You will see Windows Password fields in addition to the regular DETERLab Password fields. When you change your Windows password, you will also have to re-type it as a check. The new Windows password should propagate to the Samba server on Fs instantly, so you can swap in an experiment and log in to its Windows nodes with the new password. If you have already swapped-in experiment nodes and changed your Windows password, the account information including passwords will be updated at the next DETERLab watchdog daemon isalive interval. This should be in 3 to 6 minutes.","title":"Windows Passwords"},{"location":"core/windows/#experiment-setup-for-windows-nodes","text":"All you have to do is put a line specifying a WINXP OS image in your experiment NS file, like this: tb-set-node-os $node WINXP-UPDATE The Windows XP images are not specific to a particular hardware type. (See the Change Log for more information.) You may explicitly specify the hardware type to run on if you wish, for example: tb-set-hardware $node pc3000 Since the bandwidth of the connection between the ISI and Berkeley portions of the testbed is constrained, it is best to run Windows nodes only on the ISI side of the testbed. This can be accomplished by creating a custom hardware type with tb-make-soft-vtype my_custom_node_type {pc3060 pc3000 pc2133 pc2133x} and then specifying that your Windows nodes must only swap-in on this custom type using tb-set-hardware $node my_custom_node_type See the note below on using the tb-set-node-failure-action command for experiments with a large number of Windows nodes. This can save a swap-in with a large number of Windows nodes, or prevent a single node boot failure on a swapmod from swapping-out the whole experiment. If you use these commands: tb-set-node-startcmd , tb-set-node-tarfiles , or tb-set-node-rpms you should read the sections on Permissions and Windows GUI programs below. The only available Windows image currently is: WINXP-UPDATE - The most recent Windows XP-SP3+. It is updated periodically from Windows Update, typically after a Microsoft \"Patch Tuesday\", the second Tuesday of each month. All critical and security fixes are installed, up through the date we pull the image file. (See the date created field on the individual WINXP Image IDs ). Note The Windows Firewall is disabled by default (as it will inform you repeatedly!)","title":"Experiment setup for Windows nodes"},{"location":"core/windows/#network-config","text":"Some default Windows networking features are disabled. NetBT (!NetBios over TCP) ( NetbiosOptions=2 ) and DNS auto-registration ( DisableDynamicUpdate=1 ) are disabled to allow network idle detection by the slothd service. TCP/IP address autoconfiguration is disabled ( IPAutoconfigurationEnabled=0 ) so that un-switched interfaces like the sixth NICs on the pc3000's don't get bogus Microsoft class B network 169.254.0.0 addresses assigned. The Windows ipconfig /all command only shows the configuration information for the enabled network interfaces. There will always be one enabled control net interface on the 192.168.0.0/22 network. The others are disabled if not used in your experiment. (See file /var/emulab/boot/ipconfig-cache for a full listing from boot time, including the interfaces that were later disabled.) If you specified links or LANs in your experiment network topology, other interfaces will be enabled, with an IP address, subnet mask, and gateway that you can specify in the NS file. Notice that the Windows names of the interfaces start with Local Area Connection and have a number appended. You can't count on what this number is, since it depends on the order the NIC's are probed as Windows boots. Note Often, we have seen ipconfig report an IP address and mask of 0.0.0.0 , while the TCP/IP properties dialog boxes and the netsh command show the proper values. Our startup scripts disable and re-enable the network interface in an attempt to reset this. Sometimes it doesn't work, and another reboot is done in an attempt to get the network up.","title":"Network config"},{"location":"core/windows/#routing","text":"Full-blown router nodes cannot run Windows, i.e. rtproto Session is not supported. However, basic routing between connected network components of your experiment topology works. The Windows command to see the routing tables is route print . The IPEnableRouter=1 registry key is set on multi-homed hosts in the experiment network, before they are rebooted to change the hostname. rtproto Static is supported in all recent WINXP images, but not in WINXP-02-16 (2005) or before. rtproto Static-old or rtproto Manual will work in any image. There is more information on routing in the Routing section of the Core Guide .","title":"Routing"},{"location":"core/windows/#windows-nodes-boot-twice","text":"Notice that Windows reboots an extra time after being loaded onto a node during swap-in. It must reboot after changing the node name to set up the network stack properly. Be patient, Windows XP doesn't boot quickly. With hardware-independent , ( sysprep'ed ) images, the first boot is actually running Mini-Setup as well, setting up device drivers and so on. It's best not to log in to the nodes until the experiment is fully swapped-in. (You may be able to log in briefly between the first two reboots; if you see the wrong pcXXX name, you'll know that a reboot is imminent.) You can know that the swap-in process is finished by any of these methods: Waiting until you get the \"experiment swapped in\" email from DETERLab. Checking the node status on the experiment status page in DETERLab. (You must refresh the page to see node status change.) Watching the realtime swap-in log to monitor its progress. Note Sometimes Windows XP fails to do the second reboot. One reason is transient race conditions in the Windows startup, for example in the network stack when there are multiple network interface devices being initialized at the same time. We make a strong effort to recover from this, but if the recovery code fails, by default it results in a swap-in or swapmod failure. At boot time, the startup service on Windows XP runs the /usr/local/etc/emulab/rc/rc.bootsetup script, logging output to /var/log/bootsetup.log . If you're having swap-in problems and rc.bootsetup doesn't finish sending ISUP to DETERLab within 10 minutes, the node will be rebooted. After a couple of reboot cycles without a ISUP , DETERLab gives up on the node. You can cause these boot-time problems to be nonfatal by adding this line to your ns file for each Windows node : tb-set-node-failure-action $node \"nonfatal\" (where $node is replaced with the node variable, of course.) DETERLab will still complain if it doesn't get the ISUP signal at the end of rc.bootsetup, but the swap-in or swapmod will proceed and allow you to figure out what's happening. Then you will probably have to manually reboot the failed Windows node to make it available to your experiment. If you try to login to a node after swap-in to diagnose the problem and your Windows password isn't honored, use this command on Ops to remotely reboot the node: node_reboot pcxxx If you are able to log in but your remote home directory isn't mounted, this is another symptom of a partial set-up. You have the additional option of executing this command on the node itself: /sbin/reboot This gives Windows another chance to get it right.","title":"Windows nodes boot twice"},{"location":"core/windows/#login-connections-to-windows","text":"You can manually start up the SSH or RDP client programs to connect and log in to nodes in your experiment, or use the console command on Ops. You will have to type your Windows Password when logging in, except for SSH when you have ssh-agent keys loaded. Or you can set up your browser to automatically connect in one click from the DETERLab web interface and pop up a connection window. Once you start swapping in an experiment, the Experiment Information page contains a table of the physical node ID and logical node name, status, and connection buttons. The captions at the top of the button columns link to pages explaining how to set up up mime-types in your browser to make the buttons work, from FreeBSD, Linux, and Windows workstations: SSH (setup) - The SSH connection button gives a Bash or TCSH shell, as usual. Your DETERLab SSH keys are installed on the node in a /sshkeys subdirectory. Console - The serial console is supported for Cygwin shell logins using the agetty and sysvinit packages. This is the only way in when network connections are closed down! You can also monitor the Frisbee loading and booting of the Windows image on the console. RDP - The RDP button starts up a Remote Desktop Protocol connection, giving a Windows Desktop login from the user's workstation screen to the experiment node. The rdesktop client software is used from Linux and Unix client workstations. A Microsoft RDC (Remote Desktop Connector) client program is included in Windows XP, and may be installed onto other versions of Windows as well. It has the feature that you can make it full-screen without (too much) confusion, since it hangs a little tab at the top of the screen to switch back. Unfortunately, we have no way to present your DETERLab Windows password to RDC, so you'll have to type it on each login. Note If you import dot-files into DETERLab that replace the system execution search path rather than add to it, you will have a problem running Windows system commands in shells. Fix this by adding /cygdrive/c/WINDOWS/system32 and /cygdrive/c/WINDOWS to your $PATH in ~/.cshrc and either ~/.bash_profile or ~/.profile . Don't worry about your home directory dot-files being shared among Windows, FreeBSD, and Linux nodes; non-existent directories in the $PATH are ignored by shells. When new DETERLab user accounts are created, the default CSH and Bash dotfiles are copied from the FreeBSD /usr/share/skel . They replace the whole $PATH rather than add to it. Then we append a DETERLab-specific part that takes care of the path, conditionally adding the Windows directories on Cygwin. Note The Windows ping program has completely different option arguments from the Linux and FreeBSD ones, and they differ widely from each other. There is a ping package in Cygwin that is a port of the 4.3bsd ping. Its options are close to a common subset of the Linux and FreeBSD options, so it will be included in future WINXP images: ping [ -dfqrv ] host [ packetsize [count [ preload]]] You can load it yourself now using Cygwin Setup . Note There are no Cygwin ports of some other useful networking commands, such as traceroute and ifconfig -a . The Windows system equivalents are tracert and ipconfig /all .","title":"Login connections to Windows"},{"location":"core/windows/#rdp-details","text":"Here are some fine points and hints for RDP logins to remote Windows desktops: Microsoft allows only one desktop login at a time to Windows XP , although this is the same Citrix Hydra technology that supports many concurrent logins to Terminal Server or Server 2003. The Fast User Switching option to XP is turned on, so a second RDP connection disconnects a previous one rather than killing it. Similarly, just closing your RDP client window disconnects your Windows Login session rather than killing it. You can reconnect later on without losing anything. SSH doesn't count as a desktop, so you can SSH in and use this command: qwinsta (Query WINdows STAtion) to show existing winstation sessions and their session ID's, and this one to reset (kill) a session by ID: rwinsta . We rename My Computer to show the PCxxx physical node name, but it doesn't appear on the Windows XP desktop by default. The XP user interface incorporates \"My Computer\" into the upper-right quadrant of the \"Start\" menu by default, and removes it from the desktop. You can go back to the \"classic\" user interface of Windows 2000, including showing \"My Computer\". Right-click on the background of the Taskbar which contains the \"Start\" button at the left, and choose \"Properties\". Select the \"Start Menu\" tab, click the \"Classic Start menu\" radio-button, and click \"OK\". Alternatively, you can force \"My Computer\" to appear on your XP desktop by right-clicking on the desktop background and choosing \"Properties\". Select the \"Desktop\" tab and click \"Customize Desktop...\" to get the \"Desktop Items\" dialog. Turn on the \"My Computer\" checkbox, then click \"OK\" twice. There are several Desktop icons (i.e. \"shortcuts\") installed by default in the XP images: Computer Management, Bash and TCSH shells, and NtEmacs . You will notice two flavors of Bash and TCSH icons on the desktop, labeled rxvt and Cygwin . The rxvt shells run in windows with X -like cut-and-paste mouse clicks: Left-click starts a selection, Right-click extends it, and middle-click pastes. These are the ones to use if you're connecting from an X workstation. Note The default colors used in Bash and rxvt don't work well in 4-bit color mode under RDP. Make sure you update your rdp-mime.pl to get the rdesktop -a 16 argument for 16-bit color. Or, you can over-ride the rxvt defaults by putting lines in your ~/.Xdefaults file like this: rxvt*background: steelblue The Cygwin shells run in a Windows Terminal window, just as the Windows cmd.exe does. These are the ones to use if you're connecting from a Windows workstation. Quick-edit mode is on by default, so you can cut-and-paste freely between your local workstation desktop and your remote RDP desktops. In a Windows Terminal window on your RDP remote desktop, the quick-edit cut-and-paste mouse clicks are: Left-drag the mouse to mark a rectangle of text, highlighting it. Type Enter or right-click the mouse when text is highlighted , to copy* the selected text to the clipboard. ( Escape* cancels the selection without copying it.) Right-click the mouse with nothing selected to paste the contents of the clipboard. On the first login by a user , Windows creates the user's Windows profile directory under C:\\Documents and Settings , and creates the registry key (folder) for persistent settings for that user. We arrange that early in the user's login process, a user HOME environment variable value is set in the user's registry. Otherwise Emacs wouldn't know how to find your .emacs setup file in your remotely mounted home directory. User \"root\" is special, and has a local home directory under /home . /home is a Cygwin symbolic link to C:\\Documents and Settings . The Windows XP Start menu has no Shutdown button under RDP. Instead, it is labeled Disconnect and only closes the RDP client window, leaving the login session and the node running. If you simply close the window, or the RDP client network connection is lost, you are also disconnected rather than logged out. When you reconnect, it comes right back, just as it was. To restart the computer, run /sbin/reboot , or use the \"Shut Down\" menu of Task Manager . One way to start Task Manager is to right-click on the background of the Taskbar at the bottom of the screen and select \"Task Manager\".","title":"RDP details"},{"location":"core/windows/#the-netbt-command","text":"The NetBT (Netbios over TCP) protocol is used to announce shared directories (folders) from one Windows machine to others. (See the Name and Session services in http://en.wikipedia.org/wiki/Netbios .) The SMB (Server Message Block) protocol is used to actually serve files. (See http://en.wikipedia.org/wiki/Server_Message_Block .) In DETERLab, we normally disable NetBT on experiment nodes, because it chatters and messes up slothd network idle detection, and is not needed for the usual SMB mounts of /users , /proj , and /share dirs, which are served from a Samba service on fs . However, NetBT does have to be enabled on the experiment nodes if you want to make Windows file shares between them. The netbt script sets the registry keys on the Windows network interface objects. Run it on the server nodes (the ones containing directories which you want to share) and reboot them afterwards to activate. There is an optional -r argument to reboot the node. Usage: netbt [-r] off|on If you use netbt to turn on NetBT, it persists across reboots. No reboot is necessary if you use Network Connections in the Control Panel to turn on NetBT. It takes effect immediately, but is turned off at reboot unless you do netbt on afterward as well. Right-click Local Area Connection (or the name of another connection, if appropriate), click Properties, click Internet Protocol (TCP/IP), and then click the Properties button. On the Internet Protocol (TCP/IP) Properties page, click the Advanced button, and click the WINS tab. Select Enable or Disable NetBIOS over TCP/IP. ipconfig /all reports \"NetBIOS over Tcpip . . . : Disabled\" on interfaces where NetBT is disabled, and says nothing where NetBT is enabled. To start sharing a directory, on the node, use the net share command, or turn on network sharing on the Sharing tab of the Properties of a directory (folder.) On XP-SP2 or above, when you first do this, the \"Network sharing and security\" subdialog says: As a security measure, Windows has disabled remote access to this computer. However, you can enable remote access and safely share files by running the _Network_Setup_Wizard_. _If_you_understand_the_security_risks_but_want_to_share_ _files_without_running_the_wizard,_click_here._\" Skip the wizard and click the latter (\"I understand\") link. Then click \"Just enable file sharing\", and \"OK\". Then you finally get the click-box to \"Share this folder on the network\". The machine names for UNC paths sharing are the same as in shell prompts: pcXXX , where XXX is the machine number. These will show up in My Network Places / Entire Network / Microsoft Windows Network / DETER once you have used them. IP numbers can also be used in UNC paths, giving you a way to share files across experiment network links rather than the control network. There is an DETER-generated LMHOSTS file, to provide the usual node aliases within an experiment, but it is currently ignored even though \"Enable LMHOSTS lookup\" is turned on in the TCP/IP WINS settings. Try nbtstat -c and nbtstat -R to experiment with this. (See the Microsoft doc for nbtstat .","title":"The netbt command"},{"location":"core/windows/#making-custom-windows-os-images","text":"Making custom Windows images is similar to doing it on the other DETER operating systems , except that you must do a little more work to run the prepare script as user root since there are no su or sudo commands on Windows. This is optional on the other OS types, but on Windows, proper TCP/IP network setup depends on prepare being run. Log in to the node where you want to save a custom image. Give the shell command to change the root password. Pick a password string you can remember, typing it twice as prompted: % passwd root Enter the new password (minimum of 5, maximum of 8 characters). Please use a combination of upper and lower case letters and numbers. New password: Re-enter new password: This works because you are part of the Windows Administrators group . Otherwise you would have to already know the root password to change it. Note If you change the root password and reboot Windows before running prepare below, the root password will not match the definitions of the DETER Windows services (daemons) that run as root, so they will not start up. Log out all sessions by users other than root , because prepare will be unable to remove their login profile directories if they are logged in. (See QWINSTA .) Log in to the node as user root through the Console or SSH, using the password you set above, then run the prepare command. (It will print \"Must be root to run this script!\" and do nothing if not run as root.) /usr/local/etc/emulab/prepare If run without option arguments, prepare will ask for the root password you want to use in your new image, prompting twice as the passwd command did above. It needs this to redefine the DETER Windows services (daemons) that run as root. It doesn't need to be the same as the root password you logged in with, since it sets the root password to be sure. The Administrator password is changed as well, since the Sysprep option needs that (below.) You can give the -p option to specify the root password on the command line: /usr/local/etc/emulab/prepare -p myRootPwd The -n option says not to change the passwords at all, and the DETER Windows services are not redefined. /usr/local/etc/emulab/prepare -n The -s option is used to make hardware-independent images using the Windows Sysprep deploy tool. If you use it with the -n option instead of giving a password, it assumes that you separately blank the Administrator password, or edit your Administrator password into the [GuiUnattended]AdminPassword entry of the sysprep.inf file. /usr/local/etc/emulab/prepare -s -p myRootPwd Note This must be done from a login on the serial console , because Sysprep shuts down the network. prepare -s refuses to run from an SSH or RDP login. Note Currently, hardware-independent images must be made on a pc850, and will then run on the pc600, pc3000, and pc3000w as well. There is an unresolved boot-time problem going the other direction, from the pc3000 to a pc850 or pc600. Windows normally casts some aspects of the NT image into concrete at the first boot after installation, including the specific boot disk driver to be used by the NT loader (IDE, SCSI, or SATA.) Sysprep is used by PC hardware manufacturers as they make XP installation disks with their own drivers installed. The Sysprep option to run an unattended Mini-Setup at first boot instead of the normal \"Out Of the Box Experience\" is used in some large corporate roll-outs. We do both. The DETER /share/windows/sysprep directory contains several versions of the XP deploy tools matched to the XP service pack level, appropriate device driver directories, and a draft sysprep.inf file to direct the automated install process. Mini-setup needs to reboot after setting up device drivers. XP also needs to [#Boots_twice reboot] after changing the host name. We combine the two by using a Cmdlines.txt script to run rc.firstboot -mini to set the host name at the end of Mini-Setup . Thus we only pay the extra time to set up device drivers and so on from scratch, about two minutes, rather than adding a third hardware and XP reboot cycle. Note As you create your Image Descriptor, set the reboot wait-time to 360 rather than 240 so that swap-ins don't time out. Then log out and create your custom image . Note Windows XP is too big to fit in the partitioning scheme used by FreeBSD and Linux, so it's necessary when making a Windows custom image to specify Partition 1 , and click Whole Disk Image. When you're testing your custom image, it's a good idea to set the tb-set-node-failure-action to \"nonfatal\" in the ns file so you get a chance to examine an image that hasn't completed the set-up process. See the note below for other useful ideas.","title":"Making Custom Windows OS Images"},{"location":"core/windows/#cygwin","text":"Cygwin is GNU + Cygnus + Windows , providing Linux-like functionality at the API, command-line, and package installation levels.","title":"Cygwin"},{"location":"core/windows/#cygwin-documentation","text":"Cygwin is well documented. Here are some links to get you started: Users guide Cygwin highlights Cygwin-added utilities FAQ API compatibility and Cygwin functions","title":"Cygwin documentation"},{"location":"core/windows/#cygwin-packages","text":"A number of optional Cygwin packages are installed in the image due to our building and running the DETER client software, plus some editors for convenience. These packages are currently agetty, bison, cvs, cygrunsrv, ed, file, flex, gcc, gdb, inetutils, make, minires-devel, more, nano, openssh, openssl-devel, patch, perl, perl-libwin32, psmisc, python, rpm, rsync, shutdown, sysvinit, tcsh, vim, wget, and zip. The Cygwin command cygcheck -c lists the packages that are installed, and their current version number and status. Package-specific notes and/or documentation for installed packages are in /usr{,/share}/doc/Cygwin/*.README and /usr/share/doc/*/README files. The Cygwin package site lists the available pre-compiled packages and provides a search engine. If you want to install more Cygwin pre-compiled packages, run the graphical installer: C:/Software/Cygwin/setup.exe The Cygwin command cygcheck -l package-name lists the contents of an installed package, which may help you to make a tarfile or rpm from a package you have installed. You can then cause it to be installed automatically by DETER into all of the nodes of your experiment. See the [Tutorial#TARBALLS Tutorial] for more information about installing RPM's and tarballs . Watch out for post-install scripts in: /etc/postinstall/package-name.sh{,.done} Many packages not in the Cygwin package site have also been ported to Cygwin already. Download the sources to an experiment node and try ./configure make make install as usual.","title":"Cygwin packages"},{"location":"core/windows/#smb-mounts-and-samba","text":"User home directories and other shared directories are served by fs , another alias for Ops/Users, via the SMB protocol (Server Message Block, also known as Windows File Sharing) with the Windows Client connecting to the Samba server. UNC paths with leading double-slashes and a server name, e.g. //fs , are used to access the SMB Shares under Cygwin. DETER then uses the Cygwin mount command to make them appear on the usual Unix paths for the DETER shared directories: /users/<username> , /proj/<pid> , /group/<pid>/<gid> , and /share . The Cygwin mount command lists what you could access on the Samba server, with the UNC path in the first column. Unix file permissions may further limit your access on the Samba server. Log in to Ops to investigate. /share/windows contains Windows software. See /share/windows/README.bin for descriptions of binary packages available for installation. In Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with backslashes , e.g. \\\\fs\\share or \\\\fs\\<username> . User and project shares are marked \"not browsable\", so just \\\\fs shows only share . Windows limitation: There is only one protection mask for everything in a whole mount/share under SMB. It's set in the \"share properties\" on the server (Samba config file in this case) so chmod will do you no good across SMB. Cygwin limitation: There is a hard-coded limit of 30 mount points in Cygwin. Cygwin uses 4 of them, and DETER uses another 3 or 4. So some of your /users mounts will fail on Windows startup if you have more than 23 or 24 members in your project, unless they are grouped into smaller subgroups.","title":"SMB mounts and Samba"},{"location":"core/windows/#cygwin-arcana","text":"File paths Cygwin accepts either flavor of slashes in paths, Unix/POSIX-style forward-slashes, or Windows-style back-slashes. In Unix shell commands, backslashes need to be quoted. Single-quotes work best. Doubling each backslash also works. This must also be done inside double-quotes. Examples: '\\single\\quoted' , \"\\\\double\\\\quoted\" , \\\\un\\\\quoted . (The difference between double and single quotes is that $variable references and back-quoted command execution are expanded in double-quotes.) When you invoke Windows (as opposed to Cygwin) commands, for example net use , they will know nothing about Unix-style paths in their arguments. The cygpath utility is an aid to converting paths between the Unix and Windows conventions. cygpath -w converts its arguments to Windows format, and cygpath -u converts its arguments to Unix format, e.g. $ cygpath -w /cygdrive/c/WINDOWS c:\\WINDOWS $ cygpath -u 'c:\\WINDOWS' /cygdrive/c/WINDOWS Mount points Cygwin mount points are shown by the mount and df commands. Note that there is a hard-coded limit of 30 mount points in Cygwin. Attempts to use the Cygwin mount command after that will fail. See the discussion of mount points and UNC //machine paths to SMB shares above . Another special case is the Unix root , \" / \". It's mounted to C:\\cygwin in the Windows filesystem. Drive letter mounts Cygwin knows about drive letter prefixes like C: \u00c2 , which are equivalent to /cygdrive/<drive-letter> \u00c2 . However, /cygdrive , like /dev , isn't a real directory, so you can't ls it. Some Windows software requires drive-letter mounts to be created for its use. You can use the Windows net use command to associate drive letters with UNC paths to SMB shares, e.g. net use W: '\\\\fs\\share\\windows' You can use the Windows subst command to associate drive letters with local paths, e.g. subst T: 'C:\\Temp' Filename completion in Cygwin shells with <Tab> doesn't work following a drive-letter prefix, but it works normally after a /cygdrive/ prefix. Also, filename completion is case-sensitive, although the underlying Windows is case-insensitive, so a filename in the wrong case is still opened properly. NTSEC Cygwin is running in NTSEC (NT Security) mode, so /etc/passwd and /etc/group contain Windows SID's as user and group ID's. Your Windows UID is the computer SID with a user number appended, something like S-1-5-21-2000478354-436374069-1060284298-1334 . Cygwin commands, such as id , ls -ln , and chown/chgrp , use the numeric suffix as the uid, e.g. 1334 . This is different from your normal DETER Unix user ID number, and the Samba server takes care of the difference. The id command reports your user id and group memberships. Note that all users are in group None on XP. Contrary to the name, this is a group that contains all users . It was named Everybody on Windows 2000, which was a better name. setuid There is no direct equivalent of the Unix setuid programs under Windows, and hence no su or sudo commands. The Windows equivalent to running a Unix command as root is membership in the Windows Administrators group. DETER project members who have either local_root or group_root privileges are put in group wheel , another alias for Administrators . Project members with user privileges are not members of the wheel group. You can ssh a command to the node as the target user, as long as you arrange for the proper authentication. For C/C++ code, there is a setuid() function in the Cygwin library, which \"impersonates\" the user if proper setup is done first. root There is not normally a Windows account named root . root on XP is just another user who is a member of the Administrators group, see below. We create a root account as part of the DETER setup to own installed software, and to run services and Unix scripts that check that they're running with root privileges. You can log in as root via RDP, ssh , or the serial console if you change the root password as described in the custom Windows OS images section. The root user does not have any Samba privileges to access Samba shared mounts, including the /proj , /groups , and /users . Administrators group All users are members of the Windows Administrators group. (The DETER non-local-root user property is not implemented on Windows.) Membership in the Windows Administrators group is very different from being root on Unix, and is also different from being logged in as Administrator. Administrators group membership on Windows only means you can set the ownership, group, and permissions on any file using the Cygwin chown , chgrp , chmod , or their Windows equivalents. Until you have done that, you can be completely locked out by read, write, or execute/open permissions of the directory or files. Another subtlety is that the group called None on XP is what used to be named Everybody on Windows 2000. All users are automatically in group None , so in practice setting group None permissions is no different from setting public access permissions. Permissions Cygwin does a pretty good job of mapping Unix user-group-other file permissions to Windows NT security ACLs. On Windows, unlike Unix, file and directory permissions can lock out root, Administrator, or SYSTEM user access. Many Unix scripts don't bother with permissions if they're running as root, and hence need modification to run on Cygwin. This creates a potential problem with the tb-set-node-tarfiles and tb-set-node-rpms commands. The tb-set-node-tarfiles page says \"Notes: 1. ... the files are installed as root\". So you can easily install files that your login doesn't have permission to access. The solution is to chmod the files before making the tarball or rpm file to grant appropriate access permissions. Executables Cygwin tries to treat .exe files the same as executable files without the .exe suffix, but with execute permissions turned on. (See the Cygwin Users Guide .) This breaks down in Makefile actions and scripts, where rm , ls -l , and install commands may need an explicit .exe added. Windows GUI programs You cannot run Windows GUI (Graphical User Interface) programs under ssh, on the serial console, or by tb-set-node-startcmd. There is no user login graphics context until you log in via RDP. However, you can use a startcmd to set a Windows registry key that causes a GUI program to be run automatically for all users when they log in to the node via RDP , if that's what you want. The program can be one that is installed by tb-set-node-tarfiles. You can pick any regkey name you want and put it in the Run registry folder. It's good not to step on the ones already there, so choose a name specific to your program. Put the following in your startcmd script: regtool -s set /HKLM/SOFTWARE/Microsoft/Windows/CurrentVersion/Run/mypgm 'C:\\mypgm\\mypgm.exe' (where mypgm is the name of your program, of course.) Notice that the value string is single-quoted with C: and backslashes. Windows interprets this regkey, and wants its flavor of file path.","title":"Cygwin arcana"},{"location":"core/windows/#ntemacs","text":"We don't include the Cygwin X server in our XP images to keep the bulk and complexity down. So NtEmacs 21.3 is provided instead of the Cygwin X Emacs. NtEmacs \"frames\" are windows on the Windows Desktop, e.g. ^X-5-2 makes another one. The /usr/local/bin/emacs executable is a symlink to /cygdrive/c/emacs-21.3/bin/runemacs.exe , which starts up an Emacs on the desktop. This only works under RDP, since SSH logins have a null desktop. There is also a /usr/local/bin/emacs-exe executable, a symlink to /cygdrive/c/emacs-21.3/bin/emacs.exe , which is only useful as an Emacs compiler. It could be used to run Emacs in an SSH or Serial Console login window with the -nw (no windows) flag, except that it exits with emacs: standard input is not a tty . Another thing not to try is running emacs-exe -nw in a Bash or TCSH shell on the RDP desktop. It crashed Windows XP when I tried it. Can drag-and-drop files from Windows Explorer to !NtEmacs windows. cygwin-mount.el in c:/emacs-21.3/site-lisp/ makes Cygwin mounts visible within !NtEmacs . It doesn't do Cygwin symlinks yet. Options - See ~root/.emacs mouse-wheel-mode CUA-mode option ( ^C copy / ^X cut on selection, ^V paste, ^Z undo). Ctrl and Alt key mappings, etc.","title":"NtEmacs"},{"location":"education/","text":"Education Materials Overview \u00b6 DETERLab is dedicated to supporting cyber security education. Since its inception, DETERLab has been used by 358 research projects, from 262 institutions and involving 918 researchers, from 203 locations and 46 countries. DETERLab offers excellent support for teaching. Instructors can: Benefit from a large collection of publicly available teaching materials Automatically create student accounts Upload class materials Assign homework/projects to students Track student progress on assignments Download assignments for grading Help students directly with many issues, without involving DETERLab staff Students benefit from using DETERLab, too. They develop practical skills in cybersecurity, networking, operating system administration, and coding. These skills make a big difference in job search!","title":"Education Materials Overview"},{"location":"education/#education-materials-overview","text":"DETERLab is dedicated to supporting cyber security education. Since its inception, DETERLab has been used by 358 research projects, from 262 institutions and involving 918 researchers, from 203 locations and 46 countries. DETERLab offers excellent support for teaching. Instructors can: Benefit from a large collection of publicly available teaching materials Automatically create student accounts Upload class materials Assign homework/projects to students Track student progress on assignments Download assignments for grading Help students directly with many issues, without involving DETERLab staff Students benefit from using DETERLab, too. They develop practical skills in cybersecurity, networking, operating system administration, and coding. These skills make a big difference in job search!","title":"Education Materials Overview"},{"location":"education/class-support/","text":"Class Support \u00b6 Classes use DETERLab differently than researchers do. Both groups can use DETERLab's full range of tools and resources, but we limit the amount of accidental sharing students do and assign them accounts that we can reuse. In order to do that we need your help in keeping track of who is using DETERLab for your classes and broadly what resources you will be using. The details are below. If you run into a problem when using DETERLab in your classes please let us know. If you come up with a better solution than the ones we described below we'd really like to hear about it! Course Setup - What we need from you to set up a DETERLab course. Course Wrap-Up - Actions to take at the end of your course Course Hand-Off - How to have a different instructor re-use the same course at your institution Managing Your Course - How add/remove students, unfreeze accounts, add assignments, etc. Access Control - Changes we made to the testbed to enforce students' individual work Resource Limits - Fair sharing of resources","title":"Class Support"},{"location":"education/class-support/#class-support","text":"Classes use DETERLab differently than researchers do. Both groups can use DETERLab's full range of tools and resources, but we limit the amount of accidental sharing students do and assign them accounts that we can reuse. In order to do that we need your help in keeping track of who is using DETERLab for your classes and broadly what resources you will be using. The details are below. If you run into a problem when using DETERLab in your classes please let us know. If you come up with a better solution than the ones we described below we'd really like to hear about it! Course Setup - What we need from you to set up a DETERLab course. Course Wrap-Up - Actions to take at the end of your course Course Hand-Off - How to have a different instructor re-use the same course at your institution Managing Your Course - How add/remove students, unfreeze accounts, add assignments, etc. Access Control - Changes we made to the testbed to enforce students' individual work Resource Limits - Fair sharing of resources","title":"Class Support"},{"location":"education/connect-with-teachers/","text":"Connect With Other Teachers \u00b6 If you are teaching a class with DETERLab, we will automatically subscribe you to our education mailing list . You may also ask to be subscribed. This list is for teachers and TAs only. Teachers must request that their TAs get subscribed. You can use this list to discuss issues related to your class and DETERLab. We use this list to facilitate scheduling and resolve conflicts between assignment deadlines and to plan resource usage.","title":"Connect With Other Teachers"},{"location":"education/connect-with-teachers/#connect-with-other-teachers","text":"If you are teaching a class with DETERLab, we will automatically subscribe you to our education mailing list . You may also ask to be subscribed. This list is for teachers and TAs only. Teachers must request that their TAs get subscribed. You can use this list to discuss issues related to your class and DETERLab. We use this list to facilitate scheduling and resolve conflicts between assignment deadlines and to plan resource usage.","title":"Connect With Other Teachers"},{"location":"education/course-setup/","text":"Course Setup \u00b6 Since we apply different access control ( see about this topic ) for educational projects, we really need to hear from you if you're planning to run a class on DETERLab. This is the procedure you should follow: Actions at the Start/End of a Class \u00b6 Start a class project \u00b6 If you don't already have a project for the given class, start a new project on DETERLab by selecting Experimentation->Start New Project once you log into DETERLab or click here . Tell us in the description that this is a class project. Note Only do this if you have never taught a given class. For each new semester that you teach for the same class, recycle your current class project (see here ). If you already have a research project on DETERLab, do not reuse your research project for your class. Start a new project and categorize it as \"class\" in the project application. Wait for your project to be approved. It should take a few days and you should receive an automated email message once it is approved. Set up your class \u00b6 If you don't already have a class project see above. Log in to DETERLab, click the My DETERLab link, find the Teaching tab and click on your class. Then select Setup Class from the left menu. You will not be able to enroll students until you complete this step. You will be able to populate your class with materials. Input the end date for your class, the estimated number of students and at least one anticipated assignment. For the assignment, make your best guess of the start and submission dates, and the number of machines you will need per student. You can change these values at any time. The system calculates the class limit automatically as per_student_num_students_0.25 . You can also assign a value that is more suitable for your needs. At least one of the \"number of machines per student\" or \"class limit\" values must be entered. Create accounts for students and TAs \u00b6 DO NOT ask students or TAs to open accounts themselves. Follow the steps below. Note If this is a repeat offering of the class, make sure to recycle all accounts first (see how ). Materials will remain in the class so you can reuse them in the current offering if you like. Remember to change the visibility of materials manually (see how in Manage materials ). To create accounts: Copy and paste your students' (or TA's) emails, one per line. Account creation takes up to a minute per student. When accounts are created, the system will automatically email the students/TAs so make sure to alert them to the fact that you are signing them up for a DETERLab account. Note If your students or TAs later desire to use DETERLab for research, they will need to join a research project and open an individual account on DETERLab. Class accounts are only for class use . If you later desire to use DETERLab in research please apply for a research project. Course Wrap-up \u00b6 Student account locking and removal \u00b6 On the day that the end-of class is reached: All student accounts will be locked and instructors will be notified by email. This means that your students will no longer be able to log into DETERLab. Two weeks after the class end date: We will email the instructor reminding them that student accounts are about to be recycled. If the instructor wishes to delay this, they can update the end-of-class date. Four weeks after the class end date: All student accounts will be wiped (files removed, email aliases removed, new SSL certificates generated). Incompletes \u00b6 The instructor can preserve accounts for students who were granted incomplete grades in the class by doing the following: In My DETERLab view, find the Teaching tab and click on Manage students or TAs from the left menu. Select the students you want to grant \"incompletes\" for. Select Grant Incomplete from the drop box below the student list. These accounts will not be wiped. Note If the incomplete is granted before the end of class, the account will not be locked. If the incomplete is granted after the class ends but before it is is wiped, the account will be unlocked, but the student will need to set the password again in the manner they did at the beginning of the semester. Once the student has completed the work the teacher should recycle the student's account ( see how ). Course hand-off to another instructor \u00b6 Some classes within the same institution may be taught by different instructors each time. To hand your class off to another instructor: Ask the new instructor to use this link to create a DETERLab account and join the project \"Share\". If they already have a DETERLab account, skip this step. Either you or the new instructor should file a ticket asking Testbed Ops to complete the hand-off. Please follow these steps even if a hand-off is temporary. Note These instructions do not apply to instructors from different institutions looking to adopt each other's material in a course they teach. For that, look at our guidelines for sharing . Managing a Class \u00b6 Students \u00b6 You and your TA both can manage your course with minimal involvement of DETERLab operations. You can create accounts for students and TAs, recycle accounts for the next semester, delete accounts for students who dropped the class, and follow student's usage of DETERLab. Additionally, you can do the following actions to help students that have problems during your class: Reset student passwords \u00b6 If a student forgets their password: From the left menu choose Manage students or TAs . Select the student and choose Reset password from the select box below the student list. The student will receive an automated email with instructions for choosing a new password. Unfreeze student accounts \u00b6 If a student makes too many failed login attempts, their account will be automatically frozen. To unfreeze it: From the left menu choose Manage students or TAs . Select the student and choose Unfreeze Web access from the select box below the student list. The students will receive an automated email with instructions for choosing a new password. Reset SSH keys \u00b6 If a student mangles or deletes their .ssh directory, any attempt to swap in an experiment will fail with the error message \"event system failed to start\". To reset their SSH keys: From the left menu choose Manage students or TAs . Select the student and choose Reset SSH keys from the select box below the student list. Students will receive an automated email with further instructions. Login as a student in the web interface \u00b6 From the left menu choose Manage students or TAs Click on the glasses icon next to the student. Log into a student's experiment \u00b6 SSH to users.deterlab.net and then SSH to the student's experiment as root , like this: ssh root@.... To get a student's view, type sudo su student_username . Recycle an account \u00b6 From the left menu choose Manage students or TAs . Select the appropriate students and choose Recycle from the select box below the student list. Warning This will wipe out the student's password and SSH keys and all the items in the student's home directory. Materials \u00b6 The following functions occur within DETERLab's web interface (go to My DETERLab -> Teaching ). Adding new materials \u00b6 A \"material\" is a useful link for your students, required reading, a set of lecture slides, a homework assignment, etc. There are two ways to add a new material to your class. 1. Adopt a shared material \u00b6 See instructions about sharing . 2. Upload a material or specify a URL \u00b6 From the left menu choose Add Materials to Class and follow the direction to upload a file or use a URL. For upload, we only support adding of ZIP files that are automatically unzipped after upload. You can zip and upload a single file (e.g., a Word document) or place many materials in a folder, create index.html to point to them and zip and upload the entire folder. For URLs, we only support those that start with http . If your URL starts with https , use http instead - servers automatically rewrite these URLs to use https . From the selection menu, choose the closest type of the material. The visibility setting determines who can see the materials: only the instructor/TA, a group of students or all students. Managing materials \u00b6 Click on Manage Materials from the left menu. You can change visibility of materials or delete them. Note Deleting a material also deletes all assignments based on this material and any submissions for these assignments. Assignments \u00b6 These functions occur within DETERLab's web interface (go to My DETERLab -> Teaching ). Assign to Students \u00b6 To assign something to students: Add it to your class via Add Materials . Choose Assign to Students from the left menu. Select materials you want to assign from the list and choose if you want to assign them to all students, a group of students, or individual students. You must set the due date for the assignment and fill either the anticipated number of nodes per student or the class limit ( see about class limits ). Once you create an assignment, the system will automatically enforce the limit for your class from the date of the assignment's creation until the due date. If you modify the due date (see how), the system will automatically modify the limits. Managing Assignments \u00b6 To make changes to assignments: From the left menu choose Manage Assignments . To delete an assignment, select those you want to delete and click Modify (see warning below). To change the due date, input a new date and click Modify To view student progress or download submissions for grading, TODO. You can download submissions many times. New submissions will be added to the folder with the old ones. Warning Deleting an assignment also deletes all submissions for it. Access Control \u00b6 When an experiment is created in a DETERLab research project by one member, all members have their home directories exported to the experiment's machines. All members can also log on to the experiment's machines. This creates a problem for the educational use of DETERLab since students can log on to experiments created by their classmates and can also access their home directories (if they change permissions on them first, running sudo from experimental machines). To address these problems we treat class projects differently than research projects. If a student creates an experiment in the default group of the project, only this student (and no other students, TAs or instructors) gets his home directory exported to the experiment's machines. Instructors and TA's will be able to log on to the machines with root privileges ( see how ). No other students will be able to log on this experiment's machines. This creates conditions for class use of DETERLab by individual students. If a student creates an experiment in a group other than the default group of the project, all members of that group (and no other students, TAs or instructors) get their home directories exported to the experiment's machines and can log on to them. Instructors and TA's will be able to log on to the machines with root privileges. This creates conditions for class use of DETERLab by groups of students. Content is protected between groups but collaboration is facilitated within each group. In the past, we have been asked by instructors to prevent topology display on the experiment's web page so students could learn how to use reconnaissance tools to infer it. We are working on automating this process. At the moment, if you need this service file a ticket . Resource Limits \u00b6 Once you set up your course and input a schedule of assignments, an automated program will enforce the limits you set on your class. This means that if your limit is L for a given day, your students will only be able to use up to L total machines on that day simultaneously . You can always change limits to accommodate more students by choosing Setup Class from the left menu. Please be mindful of other DETERLab users and set reasonable limits for your class. These are usually around 1/4 of the maximum anticipated usage for your class. If you move around your class assignments or change what you want to do and how many machines will be in use, the limits will be adjusted automatically. When you create the Nth assignment for your students, our system rewrites the Nth anticipated assignment in your class schedule with the actual assignment you created. If you end up moving a due date of your assignment, the limits will be extended to match the new due date. Note Use of resources by students that are granted \"incompletes\" in your class is not counted against the class limit. Also use of resources by the teacher or the TAs is not counted against the class limit. Warning If a student attempts to swap in an experiment in your class on a day for which limits have not been set, and if this student does not have an incomplete, this swap in will fail and the student will receive a message that he/she was not allowed to allocate resources.","title":"Course Setup"},{"location":"education/course-setup/#course-setup","text":"Since we apply different access control ( see about this topic ) for educational projects, we really need to hear from you if you're planning to run a class on DETERLab. This is the procedure you should follow:","title":"Course Setup"},{"location":"education/course-setup/#actions-at-the-startend-of-a-class","text":"","title":"Actions at the Start/End of a Class"},{"location":"education/course-setup/#start-a-class-project","text":"If you don't already have a project for the given class, start a new project on DETERLab by selecting Experimentation->Start New Project once you log into DETERLab or click here . Tell us in the description that this is a class project. Note Only do this if you have never taught a given class. For each new semester that you teach for the same class, recycle your current class project (see here ). If you already have a research project on DETERLab, do not reuse your research project for your class. Start a new project and categorize it as \"class\" in the project application. Wait for your project to be approved. It should take a few days and you should receive an automated email message once it is approved.","title":"Start a class project"},{"location":"education/course-setup/#set-up-your-class","text":"If you don't already have a class project see above. Log in to DETERLab, click the My DETERLab link, find the Teaching tab and click on your class. Then select Setup Class from the left menu. You will not be able to enroll students until you complete this step. You will be able to populate your class with materials. Input the end date for your class, the estimated number of students and at least one anticipated assignment. For the assignment, make your best guess of the start and submission dates, and the number of machines you will need per student. You can change these values at any time. The system calculates the class limit automatically as per_student_num_students_0.25 . You can also assign a value that is more suitable for your needs. At least one of the \"number of machines per student\" or \"class limit\" values must be entered.","title":"Set up your class"},{"location":"education/course-setup/#create-accounts-for-students-and-tas","text":"DO NOT ask students or TAs to open accounts themselves. Follow the steps below. Note If this is a repeat offering of the class, make sure to recycle all accounts first (see how ). Materials will remain in the class so you can reuse them in the current offering if you like. Remember to change the visibility of materials manually (see how in Manage materials ). To create accounts: Copy and paste your students' (or TA's) emails, one per line. Account creation takes up to a minute per student. When accounts are created, the system will automatically email the students/TAs so make sure to alert them to the fact that you are signing them up for a DETERLab account. Note If your students or TAs later desire to use DETERLab for research, they will need to join a research project and open an individual account on DETERLab. Class accounts are only for class use . If you later desire to use DETERLab in research please apply for a research project.","title":"Create accounts for students and TAs"},{"location":"education/course-setup/#course-wrap-up","text":"","title":"Course Wrap-up"},{"location":"education/course-setup/#student-account-locking-and-removal","text":"On the day that the end-of class is reached: All student accounts will be locked and instructors will be notified by email. This means that your students will no longer be able to log into DETERLab. Two weeks after the class end date: We will email the instructor reminding them that student accounts are about to be recycled. If the instructor wishes to delay this, they can update the end-of-class date. Four weeks after the class end date: All student accounts will be wiped (files removed, email aliases removed, new SSL certificates generated).","title":"Student account locking and removal"},{"location":"education/course-setup/#incompletes","text":"The instructor can preserve accounts for students who were granted incomplete grades in the class by doing the following: In My DETERLab view, find the Teaching tab and click on Manage students or TAs from the left menu. Select the students you want to grant \"incompletes\" for. Select Grant Incomplete from the drop box below the student list. These accounts will not be wiped. Note If the incomplete is granted before the end of class, the account will not be locked. If the incomplete is granted after the class ends but before it is is wiped, the account will be unlocked, but the student will need to set the password again in the manner they did at the beginning of the semester. Once the student has completed the work the teacher should recycle the student's account ( see how ).","title":"Incompletes"},{"location":"education/course-setup/#course-hand-off-to-another-instructor","text":"Some classes within the same institution may be taught by different instructors each time. To hand your class off to another instructor: Ask the new instructor to use this link to create a DETERLab account and join the project \"Share\". If they already have a DETERLab account, skip this step. Either you or the new instructor should file a ticket asking Testbed Ops to complete the hand-off. Please follow these steps even if a hand-off is temporary. Note These instructions do not apply to instructors from different institutions looking to adopt each other's material in a course they teach. For that, look at our guidelines for sharing .","title":"Course hand-off to another instructor"},{"location":"education/course-setup/#managing-a-class","text":"","title":"Managing a Class"},{"location":"education/course-setup/#students","text":"You and your TA both can manage your course with minimal involvement of DETERLab operations. You can create accounts for students and TAs, recycle accounts for the next semester, delete accounts for students who dropped the class, and follow student's usage of DETERLab. Additionally, you can do the following actions to help students that have problems during your class:","title":"Students"},{"location":"education/course-setup/#reset-student-passwords","text":"If a student forgets their password: From the left menu choose Manage students or TAs . Select the student and choose Reset password from the select box below the student list. The student will receive an automated email with instructions for choosing a new password.","title":"Reset student passwords"},{"location":"education/course-setup/#unfreeze-student-accounts","text":"If a student makes too many failed login attempts, their account will be automatically frozen. To unfreeze it: From the left menu choose Manage students or TAs . Select the student and choose Unfreeze Web access from the select box below the student list. The students will receive an automated email with instructions for choosing a new password.","title":"Unfreeze student accounts"},{"location":"education/course-setup/#reset-ssh-keys","text":"If a student mangles or deletes their .ssh directory, any attempt to swap in an experiment will fail with the error message \"event system failed to start\". To reset their SSH keys: From the left menu choose Manage students or TAs . Select the student and choose Reset SSH keys from the select box below the student list. Students will receive an automated email with further instructions.","title":"Reset SSH keys"},{"location":"education/course-setup/#login-as-a-student-in-the-web-interface","text":"From the left menu choose Manage students or TAs Click on the glasses icon next to the student.","title":"Login as a student in the web interface"},{"location":"education/course-setup/#log-into-a-students-experiment","text":"SSH to users.deterlab.net and then SSH to the student's experiment as root , like this: ssh root@.... To get a student's view, type sudo su student_username .","title":"Log into a student's experiment"},{"location":"education/course-setup/#recycle-an-account","text":"From the left menu choose Manage students or TAs . Select the appropriate students and choose Recycle from the select box below the student list. Warning This will wipe out the student's password and SSH keys and all the items in the student's home directory.","title":"Recycle an account"},{"location":"education/course-setup/#materials","text":"The following functions occur within DETERLab's web interface (go to My DETERLab -> Teaching ).","title":"Materials"},{"location":"education/course-setup/#adding-new-materials","text":"A \"material\" is a useful link for your students, required reading, a set of lecture slides, a homework assignment, etc. There are two ways to add a new material to your class.","title":"Adding new materials"},{"location":"education/course-setup/#1-adopt-a-shared-material","text":"See instructions about sharing .","title":"1. Adopt a shared material"},{"location":"education/course-setup/#2-upload-a-material-or-specify-a-url","text":"From the left menu choose Add Materials to Class and follow the direction to upload a file or use a URL. For upload, we only support adding of ZIP files that are automatically unzipped after upload. You can zip and upload a single file (e.g., a Word document) or place many materials in a folder, create index.html to point to them and zip and upload the entire folder. For URLs, we only support those that start with http . If your URL starts with https , use http instead - servers automatically rewrite these URLs to use https . From the selection menu, choose the closest type of the material. The visibility setting determines who can see the materials: only the instructor/TA, a group of students or all students.","title":"2. Upload a material or specify a URL"},{"location":"education/course-setup/#managing-materials","text":"Click on Manage Materials from the left menu. You can change visibility of materials or delete them. Note Deleting a material also deletes all assignments based on this material and any submissions for these assignments.","title":"Managing materials"},{"location":"education/course-setup/#assignments","text":"These functions occur within DETERLab's web interface (go to My DETERLab -> Teaching ).","title":"Assignments"},{"location":"education/course-setup/#assign-to-students","text":"To assign something to students: Add it to your class via Add Materials . Choose Assign to Students from the left menu. Select materials you want to assign from the list and choose if you want to assign them to all students, a group of students, or individual students. You must set the due date for the assignment and fill either the anticipated number of nodes per student or the class limit ( see about class limits ). Once you create an assignment, the system will automatically enforce the limit for your class from the date of the assignment's creation until the due date. If you modify the due date (see how), the system will automatically modify the limits.","title":"Assign to Students"},{"location":"education/course-setup/#managing-assignments","text":"To make changes to assignments: From the left menu choose Manage Assignments . To delete an assignment, select those you want to delete and click Modify (see warning below). To change the due date, input a new date and click Modify To view student progress or download submissions for grading, TODO. You can download submissions many times. New submissions will be added to the folder with the old ones. Warning Deleting an assignment also deletes all submissions for it.","title":"Managing Assignments"},{"location":"education/course-setup/#access-control","text":"When an experiment is created in a DETERLab research project by one member, all members have their home directories exported to the experiment's machines. All members can also log on to the experiment's machines. This creates a problem for the educational use of DETERLab since students can log on to experiments created by their classmates and can also access their home directories (if they change permissions on them first, running sudo from experimental machines). To address these problems we treat class projects differently than research projects. If a student creates an experiment in the default group of the project, only this student (and no other students, TAs or instructors) gets his home directory exported to the experiment's machines. Instructors and TA's will be able to log on to the machines with root privileges ( see how ). No other students will be able to log on this experiment's machines. This creates conditions for class use of DETERLab by individual students. If a student creates an experiment in a group other than the default group of the project, all members of that group (and no other students, TAs or instructors) get their home directories exported to the experiment's machines and can log on to them. Instructors and TA's will be able to log on to the machines with root privileges. This creates conditions for class use of DETERLab by groups of students. Content is protected between groups but collaboration is facilitated within each group. In the past, we have been asked by instructors to prevent topology display on the experiment's web page so students could learn how to use reconnaissance tools to infer it. We are working on automating this process. At the moment, if you need this service file a ticket .","title":"Access Control"},{"location":"education/course-setup/#resource-limits","text":"Once you set up your course and input a schedule of assignments, an automated program will enforce the limits you set on your class. This means that if your limit is L for a given day, your students will only be able to use up to L total machines on that day simultaneously . You can always change limits to accommodate more students by choosing Setup Class from the left menu. Please be mindful of other DETERLab users and set reasonable limits for your class. These are usually around 1/4 of the maximum anticipated usage for your class. If you move around your class assignments or change what you want to do and how many machines will be in use, the limits will be adjusted automatically. When you create the Nth assignment for your students, our system rewrites the Nth anticipated assignment in your class schedule with the actual assignment you created. If you end up moving a due date of your assignment, the limits will be extended to match the new due date. Note Use of resources by students that are granted \"incompletes\" in your class is not counted against the class limit. Also use of resources by the teacher or the TAs is not counted against the class limit. Warning If a student attempts to swap in an experiment in your class on a day for which limits have not been set, and if this student does not have an incomplete, this swap in will fail and the student will receive a message that he/she was not allowed to allocate resources.","title":"Resource Limits"},{"location":"education/good-teaching-practices/","text":"Good Teaching Practices \u00b6 Ask your students to contact a TA or you first with problems, then the TA or you can contact us with any new issues. Some teachers have even offered to take points off if a student contacts DETER Ops directly. Make it clear to students that they are using a shared testbed, and should not leave their work for the last day before the deadline. Ask students to promptly swap out experiments if they will not use them for at least an hour. They should read student guidelines to learn how to save their work and retrieve it on the next swap in. Ask students to set the idle swap out period to 1 hour . They should swap out manually whenever they are done with a chunk of work, but this setting will catch the cases when they forget to do so. Contact us promptly if you run into any issues with experiment creation, lack of resources, etc. Communicate to us any small issues that frustrate you so we can improve our handling of classes. Please make sure to explain to students the difference between using the control and the experimental network (see here ), and make sure your assignment instructions minimize use of control network whenever possible.","title":"Good Teaching Practices"},{"location":"education/good-teaching-practices/#good-teaching-practices","text":"Ask your students to contact a TA or you first with problems, then the TA or you can contact us with any new issues. Some teachers have even offered to take points off if a student contacts DETER Ops directly. Make it clear to students that they are using a shared testbed, and should not leave their work for the last day before the deadline. Ask students to promptly swap out experiments if they will not use them for at least an hour. They should read student guidelines to learn how to save their work and retrieve it on the next swap in. Ask students to set the idle swap out period to 1 hour . They should swap out manually whenever they are done with a chunk of work, but this setting will catch the cases when they forget to do so. Contact us promptly if you run into any issues with experiment creation, lack of resources, etc. Communicate to us any small issues that frustrate you so we can improve our handling of classes. Please make sure to explain to students the difference between using the control and the experimental network (see here ), and make sure your assignment instructions minimize use of control network whenever possible.","title":"Good Teaching Practices"},{"location":"education/guidelines-for-students/","text":"Guidelines for Students \u00b6 Read Student Introduction to DETERLab - especially the Things to keep in mind section. Read User DOs and DON'Ts . Contact your TA or instructor first with DETERLab problems. They can help you with password reset, inability to log on, inability to swap in and many other problems. They will pass any they cannot solve to the Testbed Ops team. Do NOT contact Ops directly. Pace yourself and do not leave work for the last day before the deadline. Many courses share the testbed, along with many researchers. There may not be enough resources for you if you ask for them at the last moment. Promptly swap out experiments if you will not use them for at least an hour. Read Student Introduction to DETERLab to learn how to save your work and retrieve it on the next swap in. Set the idle swap out period to 1 hour . While you should swap out manually whenever you are done with a chunk of work, this setting will catch the cases when you forget to do so. If you cannot swap in due to lack of free machines, keep trying for a day. Our load goes down during nights and weekends. If you still have problems after a day contact your instructor or TA who will request help from testbed ops.","title":"Guidelines for Students"},{"location":"education/guidelines-for-students/#guidelines-for-students","text":"Read Student Introduction to DETERLab - especially the Things to keep in mind section. Read User DOs and DON'Ts . Contact your TA or instructor first with DETERLab problems. They can help you with password reset, inability to log on, inability to swap in and many other problems. They will pass any they cannot solve to the Testbed Ops team. Do NOT contact Ops directly. Pace yourself and do not leave work for the last day before the deadline. Many courses share the testbed, along with many researchers. There may not be enough resources for you if you ask for them at the last moment. Promptly swap out experiments if you will not use them for at least an hour. Read Student Introduction to DETERLab to learn how to save your work and retrieve it on the next swap in. Set the idle swap out period to 1 hour . While you should swap out manually whenever you are done with a chunk of work, this setting will catch the cases when you forget to do so. If you cannot swap in due to lack of free machines, keep trying for a day. Our load goes down during nights and weekends. If you still have problems after a day contact your instructor or TA who will request help from testbed ops.","title":"Guidelines for Students"},{"location":"education/guidelines-for-teachers/","text":"Guidelines for Teachers \u00b6 Welcome to the DETERLab's site for teacher support in education. The DETERLab testbed has been used in many security classes to demonstrate and complement concepts taught in class. Such practices enhance student learning and promote interest in the material. This page provides guidelines to teachers on DETERLab's support for class activities that differs from our support to research users. It further provides guidelines on using our DETERLab's UI to find, adopt and contribute teaching materials. DETER's support for classes - Read this if you plan to teach a class with DETERLab testbed. Good practices - Learn how to maximize the benefits of using DETERLab testbed in class. Sharing teaching materials - Read this if you have materials to contribute, or would like to find materials to adopt. Connect with other teachers - Find ways to connect with other teachers that use the DETERLab testbed.","title":"Guidelines for Teachers"},{"location":"education/guidelines-for-teachers/#guidelines-for-teachers","text":"Welcome to the DETERLab's site for teacher support in education. The DETERLab testbed has been used in many security classes to demonstrate and complement concepts taught in class. Such practices enhance student learning and promote interest in the material. This page provides guidelines to teachers on DETERLab's support for class activities that differs from our support to research users. It further provides guidelines on using our DETERLab's UI to find, adopt and contribute teaching materials. DETER's support for classes - Read this if you plan to teach a class with DETERLab testbed. Good practices - Learn how to maximize the benefits of using DETERLab testbed in class. Sharing teaching materials - Read this if you have materials to contribute, or would like to find materials to adopt. Connect with other teachers - Find ways to connect with other teachers that use the DETERLab testbed.","title":"Guidelines for Teachers"},{"location":"education/public-materials/","text":"Public Materials \u00b6 Go to this section of the DETERLab testbed to search for educational materials that have been made available to the public: https://www.isi.deterlab.net/sharedpublic.php","title":"Public Materials"},{"location":"education/public-materials/#public-materials","text":"Go to this section of the DETERLab testbed to search for educational materials that have been made available to the public: https://www.isi.deterlab.net/sharedpublic.php","title":"Public Materials"},{"location":"education/student-intro/","text":"Student Introduction to DETERLab \u00b6 Contributors: Peter A. H. Peterson, UCLA. pahp@... David Morgan, USC. davidmor@... What is DETERLab? \u00b6 DETERLab is a security and education-enhanced version of Emulab. Funded by the National Science Foundation and the Department of Homeland Security, DETERLab is hosted by USC/ISI and UC Berkeley. \"USC/ISI's DETERLab (cyber DEfense Technology Experimental Research Laboratory) is a state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. DETERLab is a shared testbed providing a platform for research in cyber security and serving a broad user community, including academia, industry, and government. To date, DETERLab-based projects have included behavior analysis and defensive technologies including DDoS attacks, worm and botnet attacks, encryption, pattern detection, and intrusion-tolerant storage protocols. [ 1 ].\" DETERLab (like Emulab) offers user accounts with assorted permissions associated with different experiment groups. Each group can have its own preconfigured experimental environments running on Linux, BSD, Windows, or other operating systems. Users running DETERLab experiments have full control of real hardware and networks running preconfigured software packages. These features make it an ideal platform for computer science and especially computer security education. Many instructors have designed class exercises (homework assignments, project assignments, in-class demos, etc.) consisting of a lab manual, software, data, network configurations, and machines from DETER's pool. This allows each student to run her own experiments on dedicated hardware. How does it work? \u00b6 The software running DETERLab will load operating system images (low level disk copies) onto to free nodes in the testbed, and then reconfigure programmable switches to create VLANs with the newly-imaged nodes connected according to the topology specified by the experiment creator. After the system is fully imaged and configured, DETERLab will execute specified scripts, unpack tarballs, and/or install rpm files according to the experiment's configuration. The end result is a live network of real machines, accessible via the Internet. How do I get a DETERLab login? \u00b6 Your instructor will request an account for you. Simply send your preferred email address to your instructor. Once the testbed ops set up your account, you will receive an email with your username and password at the address you supplied. Within one week, use those credentials to log in. Edit your profile as follows: a. Choose \"Profile\" tab. b. Choose \"Edit profile\" menu option. c. Replace any default contents in the two fields shown with your actual name and working phone number d. Change your password! e. Click \"Submit\" Using DETERLab \u00b6 How do I start an exercise? \u00b6 Before you can perform the tasks described in your exercise assignment, you will, in many cases, need to create an experiment in DETERLab to work on. This will be your environment to use whenever you need it. To create a new experiment: Log into DETERLab with your account. Under the \"Experimentation\" menu at the top of the page, click \"Begin an Experiment\". Select your Class Project name from the \"Select Project\" dropdown. (Throughout this document, we'll assume your class project name is YourProject) Leave the \"Group\" dropdown set to Default unless otherwise instructed. In the \"Name\" field, enter a name of the format username-exercisename. (Example: jstudent-exploits). Enter a brief description in the \"Description\" field. In the \"Your NS File\" field, follow the instructions in the \"Setup\" section of your exercise manual. Set the \"Idle Swap\" field to 1 h. Leave the rest of the settings for \"Swapping,\" \"Linktest Option,\" and \"BatchMode\" alone (unless otherwise instructed). If you would like to start your lab now, check the \"Swap In Immediately\" box and move to the next section. Otherwise, do not check this box. Click \"Submit\"! How do I work on my exercise? \u00b6 Log into DETERLab with your DETERLab account (or contact your instructor if you need an account). Click on the \"My DETERLab\" link on the left hand menu. In the \"Current Experiments\" table, click on the name of the experiment you want. Under the \"Experiment Options\" menu on the left margin, click \"Swap Experiment In\", then click \"Confirm\". The swap in process will take 5 to 10 minutes. While you're waiting, you can watch the swap in process displayed in your web browser. Or, you can watch your email box for a message letting you know that it has finished swapping in. When the experiment has finished swapping in, you can perform the tasks in your exercise manual. How do I access my experiment? \u00b6 Your experiment is made up of one or more machines on the internal DETERLab network, which is behind a firewall. To access your experimental nodes, you'll need to first SSH to users.deterlab.net. If you don't know how to use SSH, see our tutorial. users.deterlab.net (or users for short) is the \"control server\" for DETERLab. From users you can contact all your nodes, reboot them, connect to their serial ports, etc. Once you log in to users, you'll need to SSH again to your actual experimental nodes. Since your nodes' addresses may change every time you swap them in, it's best to SSH to the permanent network names of the nodes. Here's how to figure out what their names are: Once your experiment has swapped in: Navigate to the experiment you just installed. If you just swapped in your experiment, the quickest way to find your node names is to click on the experiment name in the table under \"Swap Control.\" However, you can also get there by clicking \"My DETERLab\". Your experiment is listed as \"active\" in the \"State\" column. Click on the experiment's name in the \"EID\" column. Once you can see your experiment's page, click on the \"Details\" tab in the main content panel. Your nodes' network names are listed under the heading \"Qualified Name.\" For example, node1.YourExperiment.YourProject.isi.deterlab.net. You should familiarize yourself with the information available on this page, but for now we just need to know the long DNS qualified name(s) node(s) you just swapped in. If you are curious, you should also look at the \"Settings\" (generic info), \"Visualization,\" and \"NS File.\" (The topology mapplet may be disabled for some labs, so these last two may not be visible). Now that you are logged in to users.deterlab.net, your nodes are swapped in, and you know their network name(s), you can SSH from users to your experimental nodes by executing: ssh node1.YourExperiment.YourProject.isi.deterlab.net. You will not need to re-authenticate. You may need to wait a few more minutes. Once DETERLab is finished setting up the experiment, the nodes still need a minute or two to boot and complete their configuration. If you get a message about \"server configuration\" when you try to log in, wait a few minutes and try again. If a lab instructs you to create new users on your experimental nodes, you can log in as them by running ssh newuser@node1.YourExperiment.YourProject.isi.deterlab.net or ssh newuser@localhost from the experimental node. Congratulations! Your lab environment is now set up, and you can get to work at the tasks in your lab manual. Make sure you read the \"Things to keep in mind\" section below! Some labs benefit from Port Forwarding. Port Forwarding is a technique that can allow you to access your experimental nodes directly from your desktop computer. This is especially useful for accessing web applications running on your experimental nodes. See our ssh tutorial for more information. Finally, when you are done working with your nodes, you should save your work and swap out the experiment so that someone else can use the physical machines. Things to keep in mind \u00b6 Carefully read the evolving version of this documentXXX. Saving and securing your files on DETERLab \u00b6 Every user on DETERLab has a home directory on users.deterlab.net which is mounted via NFS (Network File System) to experimental nodes. This means that anything you place in your home directory on one experimental node (or the users machine) is visible in your home directory on your other experimental nodes. Your home directory is private, so you may save your work in that directory. However, everything else on experimental nodes is permanently lost when an experiment is swapped out. Make sure you save your work in your home directory before swapping out your experiment''' Another place to save your files would be /proj/YourProject. This directory is also NFS-mounted to all experimental nodes so same rules apply about writing to it a lot, as for your home directory. It is shared by all members of your project/class. Again, on DETERLab, files ARE NOT SAVED between swap-ins. Additionally, class experiments may be forcibly swapped out after a certain number of idle hours (or some maximum amount of time). You must manually save copies of any files you want to keep in your home directory. Any files left elsewhere on the experimental nodes will be erased and lost forever. This means that if you want to store progress for a lab and come back to it later, you will need to put it in your home directory before swapping out the experiment. Swap out -- DON'T \"terminate\"! \u00b6 When you are done with your experiment for the time being, please make sure you save your work into an appropriate location and then swap out your experiment. To do this, use the \"Swap Experiment Out\" link in the \"Experiment Options\" panel. (This is the same place that used to have a \"Swap Experiment In\" link.) This allows the resources to be deallocated so that someone else can use them. Do not use the potentially misleading \"Terminate Experiment\" link unless you are completely finished with your exercise. Termination will erase the experiment and you won't be able to swap it back in without recreating it. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you're taking a break from the work, but coming back later. Terminating says \"I won't need this experiment again, ever.\" This may be confusing, especially since \"Swap Out\" seems to imply that it saves your progress (it doesn't, as described above). Just remember to Swap In/Out, and never \"Terminate\" unless you're sure you're completely done with the experiment. If you do end up terminating an experiment, you can always go back and recreate it. Submitting your work to your instructor \u00b6 Each exercise manual has a section entitled \"Submission Instructions,\" and your instructor may have given you additional instructions for submission. Follow the instructions in that section, and submit your work to your instructor. Unless otherwise instructed, it's a good idea to include: Your name Your preferred email address Your student ID (if applicable) Your DETERLab username Your experiment's name (e.g., jstudent-exploits) Frequently Asked Questions \u00b6 Please check the following list of questions for answers. If you do not find an answer to your question here or elsewhere, please email your instructor or TA. Do not email testbed ops unless specifically instructed to do so by your instructor. Why can't I log in to DETERLab? \u00b6 DETERLab has an automatic blacklist mechanism. If you enter the wrong username and password combination too many times, your account will no longer be accessible from your current IP address. If you think that this has happened to you, you can try logging in from another address (if you know how), or you can email your instructor or TA and specify your IP address. They will relay the request to the testbed ops that this specific blacklist entry should be erased. If you have questions you think should be added to this FAQ, or other information you think should be added to this document, please contact usXXX.","title":"Student Introduction to DETERLab"},{"location":"education/student-intro/#student-introduction-to-deterlab","text":"Contributors: Peter A. H. Peterson, UCLA. pahp@... David Morgan, USC. davidmor@...","title":"Student Introduction to DETERLab"},{"location":"education/student-intro/#what-is-deterlab","text":"DETERLab is a security and education-enhanced version of Emulab. Funded by the National Science Foundation and the Department of Homeland Security, DETERLab is hosted by USC/ISI and UC Berkeley. \"USC/ISI's DETERLab (cyber DEfense Technology Experimental Research Laboratory) is a state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. DETERLab is a shared testbed providing a platform for research in cyber security and serving a broad user community, including academia, industry, and government. To date, DETERLab-based projects have included behavior analysis and defensive technologies including DDoS attacks, worm and botnet attacks, encryption, pattern detection, and intrusion-tolerant storage protocols. [ 1 ].\" DETERLab (like Emulab) offers user accounts with assorted permissions associated with different experiment groups. Each group can have its own preconfigured experimental environments running on Linux, BSD, Windows, or other operating systems. Users running DETERLab experiments have full control of real hardware and networks running preconfigured software packages. These features make it an ideal platform for computer science and especially computer security education. Many instructors have designed class exercises (homework assignments, project assignments, in-class demos, etc.) consisting of a lab manual, software, data, network configurations, and machines from DETER's pool. This allows each student to run her own experiments on dedicated hardware.","title":"What is DETERLab?"},{"location":"education/student-intro/#how-does-it-work","text":"The software running DETERLab will load operating system images (low level disk copies) onto to free nodes in the testbed, and then reconfigure programmable switches to create VLANs with the newly-imaged nodes connected according to the topology specified by the experiment creator. After the system is fully imaged and configured, DETERLab will execute specified scripts, unpack tarballs, and/or install rpm files according to the experiment's configuration. The end result is a live network of real machines, accessible via the Internet.","title":"How does it work?"},{"location":"education/student-intro/#how-do-i-get-a-deterlab-login","text":"Your instructor will request an account for you. Simply send your preferred email address to your instructor. Once the testbed ops set up your account, you will receive an email with your username and password at the address you supplied. Within one week, use those credentials to log in. Edit your profile as follows: a. Choose \"Profile\" tab. b. Choose \"Edit profile\" menu option. c. Replace any default contents in the two fields shown with your actual name and working phone number d. Change your password! e. Click \"Submit\"","title":"How do I get a DETERLab login?"},{"location":"education/student-intro/#using-deterlab","text":"","title":"Using DETERLab"},{"location":"education/student-intro/#how-do-i-start-an-exercise","text":"Before you can perform the tasks described in your exercise assignment, you will, in many cases, need to create an experiment in DETERLab to work on. This will be your environment to use whenever you need it. To create a new experiment: Log into DETERLab with your account. Under the \"Experimentation\" menu at the top of the page, click \"Begin an Experiment\". Select your Class Project name from the \"Select Project\" dropdown. (Throughout this document, we'll assume your class project name is YourProject) Leave the \"Group\" dropdown set to Default unless otherwise instructed. In the \"Name\" field, enter a name of the format username-exercisename. (Example: jstudent-exploits). Enter a brief description in the \"Description\" field. In the \"Your NS File\" field, follow the instructions in the \"Setup\" section of your exercise manual. Set the \"Idle Swap\" field to 1 h. Leave the rest of the settings for \"Swapping,\" \"Linktest Option,\" and \"BatchMode\" alone (unless otherwise instructed). If you would like to start your lab now, check the \"Swap In Immediately\" box and move to the next section. Otherwise, do not check this box. Click \"Submit\"!","title":"How do I start an exercise?"},{"location":"education/student-intro/#how-do-i-work-on-my-exercise","text":"Log into DETERLab with your DETERLab account (or contact your instructor if you need an account). Click on the \"My DETERLab\" link on the left hand menu. In the \"Current Experiments\" table, click on the name of the experiment you want. Under the \"Experiment Options\" menu on the left margin, click \"Swap Experiment In\", then click \"Confirm\". The swap in process will take 5 to 10 minutes. While you're waiting, you can watch the swap in process displayed in your web browser. Or, you can watch your email box for a message letting you know that it has finished swapping in. When the experiment has finished swapping in, you can perform the tasks in your exercise manual.","title":"How do I work on my exercise?"},{"location":"education/student-intro/#how-do-i-access-my-experiment","text":"Your experiment is made up of one or more machines on the internal DETERLab network, which is behind a firewall. To access your experimental nodes, you'll need to first SSH to users.deterlab.net. If you don't know how to use SSH, see our tutorial. users.deterlab.net (or users for short) is the \"control server\" for DETERLab. From users you can contact all your nodes, reboot them, connect to their serial ports, etc. Once you log in to users, you'll need to SSH again to your actual experimental nodes. Since your nodes' addresses may change every time you swap them in, it's best to SSH to the permanent network names of the nodes. Here's how to figure out what their names are: Once your experiment has swapped in: Navigate to the experiment you just installed. If you just swapped in your experiment, the quickest way to find your node names is to click on the experiment name in the table under \"Swap Control.\" However, you can also get there by clicking \"My DETERLab\". Your experiment is listed as \"active\" in the \"State\" column. Click on the experiment's name in the \"EID\" column. Once you can see your experiment's page, click on the \"Details\" tab in the main content panel. Your nodes' network names are listed under the heading \"Qualified Name.\" For example, node1.YourExperiment.YourProject.isi.deterlab.net. You should familiarize yourself with the information available on this page, but for now we just need to know the long DNS qualified name(s) node(s) you just swapped in. If you are curious, you should also look at the \"Settings\" (generic info), \"Visualization,\" and \"NS File.\" (The topology mapplet may be disabled for some labs, so these last two may not be visible). Now that you are logged in to users.deterlab.net, your nodes are swapped in, and you know their network name(s), you can SSH from users to your experimental nodes by executing: ssh node1.YourExperiment.YourProject.isi.deterlab.net. You will not need to re-authenticate. You may need to wait a few more minutes. Once DETERLab is finished setting up the experiment, the nodes still need a minute or two to boot and complete their configuration. If you get a message about \"server configuration\" when you try to log in, wait a few minutes and try again. If a lab instructs you to create new users on your experimental nodes, you can log in as them by running ssh newuser@node1.YourExperiment.YourProject.isi.deterlab.net or ssh newuser@localhost from the experimental node. Congratulations! Your lab environment is now set up, and you can get to work at the tasks in your lab manual. Make sure you read the \"Things to keep in mind\" section below! Some labs benefit from Port Forwarding. Port Forwarding is a technique that can allow you to access your experimental nodes directly from your desktop computer. This is especially useful for accessing web applications running on your experimental nodes. See our ssh tutorial for more information. Finally, when you are done working with your nodes, you should save your work and swap out the experiment so that someone else can use the physical machines.","title":"How do I access my experiment?"},{"location":"education/student-intro/#things-to-keep-in-mind","text":"Carefully read the evolving version of this documentXXX.","title":"Things to keep in mind"},{"location":"education/student-intro/#saving-and-securing-your-files-on-deterlab","text":"Every user on DETERLab has a home directory on users.deterlab.net which is mounted via NFS (Network File System) to experimental nodes. This means that anything you place in your home directory on one experimental node (or the users machine) is visible in your home directory on your other experimental nodes. Your home directory is private, so you may save your work in that directory. However, everything else on experimental nodes is permanently lost when an experiment is swapped out. Make sure you save your work in your home directory before swapping out your experiment''' Another place to save your files would be /proj/YourProject. This directory is also NFS-mounted to all experimental nodes so same rules apply about writing to it a lot, as for your home directory. It is shared by all members of your project/class. Again, on DETERLab, files ARE NOT SAVED between swap-ins. Additionally, class experiments may be forcibly swapped out after a certain number of idle hours (or some maximum amount of time). You must manually save copies of any files you want to keep in your home directory. Any files left elsewhere on the experimental nodes will be erased and lost forever. This means that if you want to store progress for a lab and come back to it later, you will need to put it in your home directory before swapping out the experiment.","title":"Saving and securing your files on DETERLab"},{"location":"education/student-intro/#swap-out-dont-terminate","text":"When you are done with your experiment for the time being, please make sure you save your work into an appropriate location and then swap out your experiment. To do this, use the \"Swap Experiment Out\" link in the \"Experiment Options\" panel. (This is the same place that used to have a \"Swap Experiment In\" link.) This allows the resources to be deallocated so that someone else can use them. Do not use the potentially misleading \"Terminate Experiment\" link unless you are completely finished with your exercise. Termination will erase the experiment and you won't be able to swap it back in without recreating it. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you're taking a break from the work, but coming back later. Terminating says \"I won't need this experiment again, ever.\" This may be confusing, especially since \"Swap Out\" seems to imply that it saves your progress (it doesn't, as described above). Just remember to Swap In/Out, and never \"Terminate\" unless you're sure you're completely done with the experiment. If you do end up terminating an experiment, you can always go back and recreate it.","title":"Swap out -- DON'T \"terminate\"!"},{"location":"education/student-intro/#submitting-your-work-to-your-instructor","text":"Each exercise manual has a section entitled \"Submission Instructions,\" and your instructor may have given you additional instructions for submission. Follow the instructions in that section, and submit your work to your instructor. Unless otherwise instructed, it's a good idea to include: Your name Your preferred email address Your student ID (if applicable) Your DETERLab username Your experiment's name (e.g., jstudent-exploits)","title":"Submitting your work to your instructor"},{"location":"education/student-intro/#frequently-asked-questions","text":"Please check the following list of questions for answers. If you do not find an answer to your question here or elsewhere, please email your instructor or TA. Do not email testbed ops unless specifically instructed to do so by your instructor.","title":"Frequently Asked Questions"},{"location":"education/student-intro/#why-cant-i-log-in-to-deterlab","text":"DETERLab has an automatic blacklist mechanism. If you enter the wrong username and password combination too many times, your account will no longer be accessible from your current IP address. If you think that this has happened to you, you can try logging in from another address (if you know how), or you can email your instructor or TA and specify your IP address. They will relay the request to the testbed ops that this specific blacklist entry should be erased. If you have questions you think should be added to this FAQ, or other information you think should be added to this document, please contact usXXX.","title":"Why can't I log in to DETERLab?"},{"location":"federation/","text":"Federation \u00b6 What is Federation? \u00b6 The DETER federation architecture is a model and mechanism for creating experiments that span multiple testbeds. We have written several papers about the architecture. This site is the home for our implementation, fedd. For more information, go to the Federation site. http://fedd.deterlab.net/","title":"Federation"},{"location":"federation/#federation","text":"","title":"Federation"},{"location":"federation/#what-is-federation","text":"The DETER federation architecture is a model and mechanism for creating experiments that span multiple testbeds. We have written several papers about the architecture. This site is the home for our implementation, fedd. For more information, go to the Federation site. http://fedd.deterlab.net/","title":"What is Federation?"},{"location":"orchestrator/","text":"MAGI Orchestrator \u00b6 The MAGI Orchestrator is a DETERLab capability that allows you to automate and manage the procedures of a DETERLab experiment. Docs in this section include: Orchestrator Quickstart Orchestrator Guide Orchestrator Case Studies Simple Client Server Case Study Scaled Client Server Case Study Feedback Case Study Writing Your Own MAGI Agents Configuring a MAGI Agent at Runtime Orchestrator Reference","title":"MAGI Orchestrator"},{"location":"orchestrator/#magi-orchestrator","text":"The MAGI Orchestrator is a DETERLab capability that allows you to automate and manage the procedures of a DETERLab experiment. Docs in this section include: Orchestrator Quickstart Orchestrator Guide Orchestrator Case Studies Simple Client Server Case Study Scaled Client Server Case Study Feedback Case Study Writing Your Own MAGI Agents Configuring a MAGI Agent at Runtime Orchestrator Reference","title":"MAGI Orchestrator"},{"location":"orchestrator/agent-configuration/","text":"Configuring a MAGI Agent at Runtime \u00b6 In Writing MAGI Agents , you saw how to create a basic agent. The sample agent created a single file on a test node. This document will explain how to use configuration in the AAL file to configure an agent at runtime. Setting Agent Configuration \u00b6 This document will expand the sample code of our FileCreator example. For reference, here is the agent code: from magi.util.agent import DispatchAgent, agentmethod from magi.util.processAgent import initializeProcessAgent # The FileCreator agent implementation, derived from DispatchAgent. class FileCreator(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) self.filename = '/tmp/newfile' # A single method which creates the file named by self.filename. # (The @agentmethod() decorator is not required, but is encouraged. # it does nothing of substance now, but may in the future.) @agentmethod() def createFile(self, msg): **Create a file on the host.** # open and immediately close the file to create it. open(self.filename, 'w').close() # the getAgent() method must be defined somewhere for all agents. # The Magi daemon invokes this mehod to get a reference to an # agent. It uses this reference to run and interact with an agent # instance. def getAgent(): agent = FileCreator() return agent # In case the agent is run as a separate process, we need to # create an instance of the agent, initialize the required # parameters based on the received arguments, and then call the # run method defined in DispatchAgent. if __name__ \"__main__\": agent = FileCreator() initializeProcessAgent(agent, sys.argv) agent.run() If we reset the self.filename variable in the agent before invoking createFile in the AAL, we can change the file that is created. The base class DispatchAgent itself is derived from a class that will let us do this. The Agent class implements two methods: setConfiguration - Sets the passed parameters as class instance variables. confirmConfiguration - This method is meant to be re-implemented in your agent if you need confirm the variables set are valid for your agent. To set the self.filename variable in the FileCreator Agents, we modify the AAL to include a call to the Agent method setConfiguration , passing in a list of key-value pairs. (In the following example, it is a single key-value pair.) - type: event agent: myFileCreators method: setConfiguration args: filename: /tmp/myCreatedFile Note that you do not specify self when referencing an Agent variable. We make sure to place this event in the AAL event stream prior to the createFile event. The complete AAL file is: streamstarts: [main] groups: myFileCreatorGroup: [NODES] agents: myFileCreators: group: myFileCreatorGroup # (note: the \"PATH\" argument is the agent directory. The # directory must contain an IDL and agent implementation. It must # also contain a *__init__.py* file, which is required # for it to be considered as a valid python package.) path: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /tmp/myCreatedFile - type: event agent: myFileCreators method: createFile args: {} Now when we run the Agent again (possibly using agentTool to restart the Magi daemons), we see the following events: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/tmp/myCreatedFile ... ) --> myFileCreatorGroup stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ ssh myNode.myExperiment.myGroup ls -l /tmp/myCreatedFile -rw-r--r-- 1 root root 0 Mar 5 13:55 /tmp/myCreatedFile $ And we see that our specified file, /tmp/myCreatedFile was created. Confirming Valid Configuration \u00b6 This works well, but the input to the Agent is free-form. What if the user gives invalid input, like the wrong type or data that is not in a valid range? This is where the Agent confirmConfiguration method comes into play. confirmConfiguration should be written for any Agent that wants to validate its state. It gets invoked in the AAL file after the user invokes setConfiguration . Note: The concept of an Agent confirming user input will change in future releases of MAGI. The Orchestrator (or other MAGI/Montage components) will use the interface specification in the Agent\u2019s IDL file to ensure the input to the agent is valid. Suppose our sample agent wanted to allow the user to create a file in only the /local directory on the host machine. The confirmConfiguration method that does this is: def confirmConfiguration(self): **Make sure the user input is a string value and starts with \"/local\".** if not isinstance(self.filename, (str, unicode)): return False if not self.filename.startswith('/local'): return False return True When we add this method to our sample Agent, and run the experiment with the existing AAL file, which contains configuration that does not start with /local , the Orchestrator gives us an error while executing the event stream: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/tmp/myCreatedFile ... ) --> myFileCreatorGroup stream unknown : exit : method setConfiguration returned False on agent unknown in group unknown and on node(s): moat. $ The Orchestrator exited with an error, as it should. If we now modify the AAL file to include a valid configuration, the Orchestrator succeeds. The updated AAL fragment is: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile When we run the Orchestrator with the modified AAL, it succeeds as the agent configuration is now valid: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/local/myGreatFile ... ) --> myFileCreatorGroup stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ And the \u201cvalid\u201d file has been created on the machine: $ ssh myNode.myExperiment.myGroup ls -l /local total 4 drwxrwxr-x 2 glawler Deter 4096 Mar 5 08:35 logs -rw-r--r-- 1 root root 0 Mar 5 14:33 myGreatFile $ Triggers and Event Stream Sequence Points \u00b6 If you run the AAL and Agent code above, you may see that it does not actually work. One small needed detail has been left out of the AAL file. Normally the Orchestrator will run through the events in the AAL as fast is it can. If we used the event streams in the AAL file as it now stands: eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile - type: event agent: myFileCreators method: createFile args: {} The Orchestrator will send two messages to the Agents in rapid succession: the setConfiguration and createFile event messages. If the setConfiguration call returns False , which it will given invalid input, the Orchestrator will not receive the message because would have sent the messages and exited. Therefore, we need a way to tell the Orchestrator to wait for a response from setConfiguration before continuing. This is done by inserting a small pause, using a trigger which times out after 3 seconds: # Wait 3 seconds for a response to setConfiguration # timeout value is in milliseconds. - type: trigger triggers: [{timeout: 3000}] If we insert this trigger between setConfiguration and createFile , the Orchestrator will receive the error message from the agent and exit on error. The full AAL file now is: streamstarts: [main] groups: myFileCreatorGroup: [witch, moat] agents: myFileCreators: group: myFileCreatorGroup path: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile - type: trigger triggers: [{timeout: 3000}] - type: event agent: myFileCreators method: createFile args: {} But how do we know that waiting for 3 seconds is a long enough time to wait? Wouldn\u2019t it be better if we could tell the Orchestrator to wait for a response from the agent before continuing? We can do this using a named trigger. We add a trigger statement to the setConfiguration event clause and modify the trigger to wait for that event before continuing to process the event stream: - type: event agent: myFileCreators trigger: configDone method: setConfiguration args: filename: /local/myGreatFile # Wait for the event \"configDone\" from all fileCreator agents. - type: trigger triggers: [{event: configDone, agent: myFileCreators}] Now when setConfiguration is called on the Agent, the daemon will send a trigger with the event configDone after the method has returned. With this modified trigger, the Orchestrator will wait for the trigger event configDone before processing the next event in the event stream. Here is the Orchestrator output now. Note that setConfiguration now \u201cfires\u201d a trigger (sends a trigger) and the Orchestrator waits until the trigger is resolved before moving on. $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/local/myGreatFile ... ) --> myFileCreatorGroup (fires trigger: configDone) stream main : done : trigger configDone complete. stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ For reference, the new agent implementation, AAL file, and IDL, file can be downloaded as a tar file here: FileCreator-withconfig.tbz .","title":"Configuring a MAGI Agent at Runtime"},{"location":"orchestrator/agent-configuration/#configuring-a-magi-agent-at-runtime","text":"In Writing MAGI Agents , you saw how to create a basic agent. The sample agent created a single file on a test node. This document will explain how to use configuration in the AAL file to configure an agent at runtime.","title":"Configuring a MAGI Agent at Runtime"},{"location":"orchestrator/agent-configuration/#setting-agent-configuration","text":"This document will expand the sample code of our FileCreator example. For reference, here is the agent code: from magi.util.agent import DispatchAgent, agentmethod from magi.util.processAgent import initializeProcessAgent # The FileCreator agent implementation, derived from DispatchAgent. class FileCreator(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) self.filename = '/tmp/newfile' # A single method which creates the file named by self.filename. # (The @agentmethod() decorator is not required, but is encouraged. # it does nothing of substance now, but may in the future.) @agentmethod() def createFile(self, msg): **Create a file on the host.** # open and immediately close the file to create it. open(self.filename, 'w').close() # the getAgent() method must be defined somewhere for all agents. # The Magi daemon invokes this mehod to get a reference to an # agent. It uses this reference to run and interact with an agent # instance. def getAgent(): agent = FileCreator() return agent # In case the agent is run as a separate process, we need to # create an instance of the agent, initialize the required # parameters based on the received arguments, and then call the # run method defined in DispatchAgent. if __name__ \"__main__\": agent = FileCreator() initializeProcessAgent(agent, sys.argv) agent.run() If we reset the self.filename variable in the agent before invoking createFile in the AAL, we can change the file that is created. The base class DispatchAgent itself is derived from a class that will let us do this. The Agent class implements two methods: setConfiguration - Sets the passed parameters as class instance variables. confirmConfiguration - This method is meant to be re-implemented in your agent if you need confirm the variables set are valid for your agent. To set the self.filename variable in the FileCreator Agents, we modify the AAL to include a call to the Agent method setConfiguration , passing in a list of key-value pairs. (In the following example, it is a single key-value pair.) - type: event agent: myFileCreators method: setConfiguration args: filename: /tmp/myCreatedFile Note that you do not specify self when referencing an Agent variable. We make sure to place this event in the AAL event stream prior to the createFile event. The complete AAL file is: streamstarts: [main] groups: myFileCreatorGroup: [NODES] agents: myFileCreators: group: myFileCreatorGroup # (note: the \"PATH\" argument is the agent directory. The # directory must contain an IDL and agent implementation. It must # also contain a *__init__.py* file, which is required # for it to be considered as a valid python package.) path: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /tmp/myCreatedFile - type: event agent: myFileCreators method: createFile args: {} Now when we run the Agent again (possibly using agentTool to restart the Magi daemons), we see the following events: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/tmp/myCreatedFile ... ) --> myFileCreatorGroup stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ ssh myNode.myExperiment.myGroup ls -l /tmp/myCreatedFile -rw-r--r-- 1 root root 0 Mar 5 13:55 /tmp/myCreatedFile $ And we see that our specified file, /tmp/myCreatedFile was created.","title":"Setting Agent Configuration"},{"location":"orchestrator/agent-configuration/#confirming-valid-configuration","text":"This works well, but the input to the Agent is free-form. What if the user gives invalid input, like the wrong type or data that is not in a valid range? This is where the Agent confirmConfiguration method comes into play. confirmConfiguration should be written for any Agent that wants to validate its state. It gets invoked in the AAL file after the user invokes setConfiguration . Note: The concept of an Agent confirming user input will change in future releases of MAGI. The Orchestrator (or other MAGI/Montage components) will use the interface specification in the Agent\u2019s IDL file to ensure the input to the agent is valid. Suppose our sample agent wanted to allow the user to create a file in only the /local directory on the host machine. The confirmConfiguration method that does this is: def confirmConfiguration(self): **Make sure the user input is a string value and starts with \"/local\".** if not isinstance(self.filename, (str, unicode)): return False if not self.filename.startswith('/local'): return False return True When we add this method to our sample Agent, and run the experiment with the existing AAL file, which contains configuration that does not start with /local , the Orchestrator gives us an error while executing the event stream: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/tmp/myCreatedFile ... ) --> myFileCreatorGroup stream unknown : exit : method setConfiguration returned False on agent unknown in group unknown and on node(s): moat. $ The Orchestrator exited with an error, as it should. If we now modify the AAL file to include a valid configuration, the Orchestrator succeeds. The updated AAL fragment is: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile When we run the Orchestrator with the modified AAL, it succeeds as the agent configuration is now valid: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/local/myGreatFile ... ) --> myFileCreatorGroup stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ And the \u201cvalid\u201d file has been created on the machine: $ ssh myNode.myExperiment.myGroup ls -l /local total 4 drwxrwxr-x 2 glawler Deter 4096 Mar 5 08:35 logs -rw-r--r-- 1 root root 0 Mar 5 14:33 myGreatFile $","title":"Confirming Valid Configuration"},{"location":"orchestrator/agent-configuration/#triggers-and-event-stream-sequence-points","text":"If you run the AAL and Agent code above, you may see that it does not actually work. One small needed detail has been left out of the AAL file. Normally the Orchestrator will run through the events in the AAL as fast is it can. If we used the event streams in the AAL file as it now stands: eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile - type: event agent: myFileCreators method: createFile args: {} The Orchestrator will send two messages to the Agents in rapid succession: the setConfiguration and createFile event messages. If the setConfiguration call returns False , which it will given invalid input, the Orchestrator will not receive the message because would have sent the messages and exited. Therefore, we need a way to tell the Orchestrator to wait for a response from setConfiguration before continuing. This is done by inserting a small pause, using a trigger which times out after 3 seconds: # Wait 3 seconds for a response to setConfiguration # timeout value is in milliseconds. - type: trigger triggers: [{timeout: 3000}] If we insert this trigger between setConfiguration and createFile , the Orchestrator will receive the error message from the agent and exit on error. The full AAL file now is: streamstarts: [main] groups: myFileCreatorGroup: [witch, moat] agents: myFileCreators: group: myFileCreatorGroup path: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile - type: trigger triggers: [{timeout: 3000}] - type: event agent: myFileCreators method: createFile args: {} But how do we know that waiting for 3 seconds is a long enough time to wait? Wouldn\u2019t it be better if we could tell the Orchestrator to wait for a response from the agent before continuing? We can do this using a named trigger. We add a trigger statement to the setConfiguration event clause and modify the trigger to wait for that event before continuing to process the event stream: - type: event agent: myFileCreators trigger: configDone method: setConfiguration args: filename: /local/myGreatFile # Wait for the event \"configDone\" from all fileCreator agents. - type: trigger triggers: [{event: configDone, agent: myFileCreators}] Now when setConfiguration is called on the Agent, the daemon will send a trigger with the event configDone after the method has returned. With this modified trigger, the Orchestrator will wait for the trigger event configDone before processing the next event in the event stream. Here is the Orchestrator output now. Note that setConfiguration now \u201cfires\u201d a trigger (sends a trigger) and the Orchestrator waits until the trigger is resolved before moving on. $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/local/myGreatFile ... ) --> myFileCreatorGroup (fires trigger: configDone) stream main : done : trigger configDone complete. stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ For reference, the new agent implementation, AAL file, and IDL, file can be downloaded as a tar file here: FileCreator-withconfig.tbz .","title":"Triggers and Event Stream Sequence Points"},{"location":"orchestrator/agent-library/","text":"MAGI Agent Library \u00b6 Every node has a daemon running with a series of Agents on it. These Agents are each executed in their own thread or process. They are provided with a defined interface with which to send and receive messages to other objects in the experiment. The MAGI daemon will route messages to the agent based on the routing information in the message. The daemon supports group-based, name-based, and \u201cdock\u201d-based routing. (A dock is like a port for a traditional daemon; an agent listens on a dock.) Once a message is delivered to an agent, the format of the message data is then up to the agent itself. Most agents will not need to parse messages directly, however, because the MAGI Agent Library supports a number of useful abstractions implemented in base classes from which Agent authors can derive. These are described in detail below. Agent Execution Models \u00b6 There are two execution models supported by the daemon for Agents: Thread-based - A thread-based Agent is loaded and runs in the process space of the daemon. The daemon communicates with a thread-based agent directly. Process-based - A process-based Agent is started as a separate process. The daemon communicates with it via standard interprocess communication techniques: a pipe or a socket. Here is a list outlining the differences between the execution models. Threads Pro : Lightweight Pro : Messages passed as objects without need for serialization Con : Must be written in Python Con : Must be aware of other threads when it comes to file descriptors or other shared memory Process (Pipe or Socket) Pro : Agents may be written in languages other than Python. Pro : May kill off agent individually from the shell Con : Heavier weight if invoking a new interpreter for each Agent for scripted languages Con : Message transceiver is more complex, in particular if a library for the language has not been written. Note As of now, only Python is supported. We are working on adding support for other languages. Interface Description Language (IDL) \u00b6 Agent authors must write an IDL that matches the interface exported by their agent. This IDL is used by MAGI to validate the interface of the agent (and in the future to generate GUIs for agent execution and configuration.) The IDL should specify: * agent execution model (thread or process); * any variables exposed by the agent and their types, ranges, or enumerated values; * any public methods and the method arguments and their types; * \u201chelp\u201d strings for each method and agent variable which explain their purpose; * any Agent library from which they derive. This may seem like a lot to specify, but the Agent Library supplies IDL for base Agents -- so in practice much of the IDL specification will be supplied to the Agent author. The IDL format and keywords are given in a table below. (TBD - Coming soon) Agent Library \u00b6 In this section we describe the Agent Library and give brief examples for usage. Classes are organized from the bottom up, that is, starting with the class from which the others derive. Note When using the Orchestrator to run your experiment, the Orchestrator will, by default, handle a return value of False from an Agent method as a reason to unload all Agents, break down communication groups and exit. Thus your Agent may stop an experiment by returning False . Agent \u00b6 This is the base Agent class. It implements a setConfiguration method. If derived from, the user may call setConfiguration to set any self variables in your class. Agent also implements an empty confirmConfiguration method that is called once the self variables are set. You may implement your own confirmConfiguration if you need to make sure the user has set your internal variables to match any constraints you may want to impose. Returning False from this method will signal to the Orchestrator that something is wrong and the Orchestrator should handle this as an error. The default implementation of confirmConfiguration simply returns True . The method signature for confirmConfiguration() is def confirmConfiguration(self): It takes no arguments. In your confirmConfiguration method, you should confirm that your agent internal variables are the correct type and in the expected range. In the following example, imagine an agent has a variable that is an integer and the range of the value must be between 1 and 10. An agent can use the Agent class to implement this as so: from magi.util.agent import Agent class myAgent(Agent): def __init__(self): self.value = None def confirmConfiguration(): if not isinstance(self.value, int): return False if not 1 <= self.value <= 10: return False return True If the variable self.value is not an integer or is not between 1 and 10, confirmConfiguration returns False . If running this agent with the Orchestrator, the False value will get returned to the Orchestrator which will unload all agents, destroy all group communications, then exit. Thus your agent may cause the experiment to stop and be reset when it is not given the correct inputs. Note In the future, this functionality of enforcing correct input will be handled outside of the agent code. The IDL associated with the agent already specifies correct input and the Orchestrator (or other Montage/MAGI front end tool) will enforce proper input. All classes in the AgentLibrary inherit from Agent . The Agent documentation can seen here . DispatchAgent \u00b6 The DispatchAgent implements the simple remote procedure call (RPC) mechanism used by the Orchestrator (and available through the MAGI python API). This allows any method in a class derived from DispatchAgent to be invoked in an AAL file (or by a MagiMessage if using the MAGI python interface directly). You almost always want to derive your agent from DispatchAgent . The DispatchAgent code simply loops, parsing incoming messages, looking for an event message. When it finds one, it attempts to call the method specified in the message with the arguments given in the message, thus implementing a basic RPC functionality in your agent. The first argument to your RPC-enabled method is the received message. It is accompanied by the optional named-parameters, sent as part of the MagiMessage . The Agent Library exports a function decorator for DispatchAgent -callable methods named agentmethod . It is not currently used for anything, but it is suggested that agent developers use it anyway. The DispatchAgent reads incoming messages and invokes the required method synchronously, i.e., it waits for a method call to return before reading the next message. Here is a simple example: from magi.util.agent import DispatchAgent, agentmethod def myAgent(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) @agentmethod() def doAction(self, msg): pass Given the agent myAgent above and the AAL fragment below, the method doAction will be called on all test nodes associated with myAgentGroup . eventstreams: myStream: - type: event agent: myAgentGroup method: doAction args: { } The DispatchAgent documentation may seen here . NonBlockingDispatchAgent \u00b6 The NonBlockingDispatchAgent is similar to DispatchAgent . The only difference is that NonBlockingDispatchAgent invokes the methods asynchronously , i.e., it forks a new thread for each method call and does not wait for the call to return. It invokes the required method and moves on to read the next message. ReportingDispatchAgent \u00b6 You will note that the DispatchAgent only allows an outside source to send commands to the agent. There is no communication backwards. The ReportingDispatchAgent base class has a slightly different run loop. Rather than blocking forever on incoming messages, it will also call its own method, periodic , to allow other operations to occur. The call to periodic will return the amount of time in seconds (as a float) that it will wait until calling periodic again. The periodic function therefore controls how often it is called. The first call will happen as soon as the run is called. The method signature of the periodic method is: def periodic(self, now): If periodic is not implemented in the subclass, an exception is raised. This example code writes the current time to a file once a second. Note the explicit use of the Agent class to set the file name. import os.path from magi.util.agent import ReportingDispatchAgent, agentmethod class myTimeTracker(ReportingDispatchAgent): def __init__(self): ReportingDispatchAgent.__init__(self) self.filename = None def confirmConfiguration(self): if not os.path.exists(self.filename): return False def periodic(self, now): with open(self.filename, 'a') as fd: fd.write('%f\\n' % now) # call again one second from now return 1.0 The ReportingDispatchAgent documentation may seen here. http://montage.deterlab.net/backend/python/util.html#magi.util.agent.ReportingDispatchAgent SharedServer \u00b6 The SharedServer class inherits from DispatchAgent and expects the subclass to implement the methods runserver and terminateserver to start or stop a local server process. The SharedServer class takes care of multiple agents requesting use of the server and only calls runserver or terminateserver when required. This ensures that there is ever only one instance of the server running at once on a given host. A canonical example of this would be a web server running a single instance of Apache. The methods runserver and stopserver take no arguments. Below is an example of a simple agent that starts and stops Apache on the local host. If there are other agents running on the machine that require Apache to be running, they may inherit from SharedServer as well, thus ensuring that there is only ever one instance of Apache running. from subprocess import check_call, CalledProcessError from magi.util.agent import SharedServer class ApacheServerAgent(SharedServer): def __init__(self): SharedServer.__init__(self) def runserver(self): try: check_call('apachectl start'.split()) except CalledProcessError: return False return True def stopserver(self): try: check_call('apachectl stop'.split()) except CalledProcessError: return False return True The SharedServer documentation may seen here . TrafficClientAgent \u00b6 TrafficClientAgent models an agent that periodically generates traffic. It must implement the getCmd method, returning a string to execute on the commandline to generate traffic. For example, the getCmd could return a curl or wget command to generate client-side HTML traffic. The signature of getCmd is: def getCmd(self, destination) Where destination is a server host name from which the agent should request traffic. The TrafficClientAgent class implements the following event-callable methods: startClient() and stopClient() . Neither method takes any arguments. These methods may be invoked from an AAL and start and stop the client respectively. The base class contains a number of variables which control how often getCmd is called and which servers should be contacted: * servers : A list of server hostnames * interval : A distribution variable Note A distribution variable is any valid python expression that returns a float. It may be as simple as an integer, \u201c1\u201d, or an actual distribution function. The Agent Library provides minmax , gamma , pareto , and expo in the distributions module. Thus a valid value for the TrafficClientAgent interval value could be minmax(1,10) , which returns a value between 1 and 10 inclusive. The signatures of these distributions are: minmax(min, max) gamma(alpha, rate, cap = None) pareto(alpha, scale = 1.0, cap = None) expo(lambd, scale = 1.0, cap = None) Below is a sample TrafficClientAgent which implements a simple HTTP client-side traffic agent. It assumes the destinations have been set correctly (via the Agent setConfiguration method) and there are web servers already running there. from magi.util.agent import TrafficClientAgent class mySimpleHTTPClient(TrafficClientAgent): def __init__(self): TrafficClientAgent.__init__(self) def getCmd(self, destination): cmd = 'curl -s -o /dev/null http://%s/index.html' % destination return cmd When this agent is used with the following AAL clauses, the servers server_1 and server_2 are used as HTTP traffic generation servers and traffic is generated once an interval where the interval ranges randomly between 5 and 10 seconds, inclusive. The first event sets the agent\u2019s internal configuration. The second event starts the traffic generation. eventstreams: myStream: - type: event agent: myHTTPClients method: setConfiguration args: interval: 'minmax(5, 10)' servers: ['server_1', 'server_2'] - type: event agent: myHTTPClients method: startClient args: { } The TrafficClientAgent documentation may seen here. http://montage.deterlab.net/backend/python/util.html#magi.util.agent.TrafficClientAgent ProbabilisticTrafficClientAgent \u00b6 ProbabilisticTrafficClientAgent provides the same service as TrafficAgent , but getCmd is called only when the configured probability function evaluates to a non-zero value. ConnectedTrafficClientAgent \u00b6 ConnectedTrafficClientAgent is a base for an agent that controls a set of agents that have standing connections to, and traffic between, a set of servers. connect() and disconnect() are called periodically when a given client should connect or disconnect to a given server. generateTraffic() is called when the given client should generate traffic between itself and the server it is connected to. The sequence of calls is: [period], connect(), [period], generateTraffic(), [period], generateTraffic(), ..., disconnect() This sequence may be repeated. Derived classes should implement connect() , disconnect() , and generateTraffic() . Agent Load and Execution Chain (for threaded agents) \u00b6 (TBD - Coming soon)","title":"MAGI Agent Library"},{"location":"orchestrator/agent-library/#magi-agent-library","text":"Every node has a daemon running with a series of Agents on it. These Agents are each executed in their own thread or process. They are provided with a defined interface with which to send and receive messages to other objects in the experiment. The MAGI daemon will route messages to the agent based on the routing information in the message. The daemon supports group-based, name-based, and \u201cdock\u201d-based routing. (A dock is like a port for a traditional daemon; an agent listens on a dock.) Once a message is delivered to an agent, the format of the message data is then up to the agent itself. Most agents will not need to parse messages directly, however, because the MAGI Agent Library supports a number of useful abstractions implemented in base classes from which Agent authors can derive. These are described in detail below.","title":"MAGI Agent Library"},{"location":"orchestrator/agent-library/#agent-execution-models","text":"There are two execution models supported by the daemon for Agents: Thread-based - A thread-based Agent is loaded and runs in the process space of the daemon. The daemon communicates with a thread-based agent directly. Process-based - A process-based Agent is started as a separate process. The daemon communicates with it via standard interprocess communication techniques: a pipe or a socket. Here is a list outlining the differences between the execution models. Threads Pro : Lightweight Pro : Messages passed as objects without need for serialization Con : Must be written in Python Con : Must be aware of other threads when it comes to file descriptors or other shared memory Process (Pipe or Socket) Pro : Agents may be written in languages other than Python. Pro : May kill off agent individually from the shell Con : Heavier weight if invoking a new interpreter for each Agent for scripted languages Con : Message transceiver is more complex, in particular if a library for the language has not been written. Note As of now, only Python is supported. We are working on adding support for other languages.","title":"Agent Execution Models"},{"location":"orchestrator/agent-library/#interface-description-language-idl","text":"Agent authors must write an IDL that matches the interface exported by their agent. This IDL is used by MAGI to validate the interface of the agent (and in the future to generate GUIs for agent execution and configuration.) The IDL should specify: * agent execution model (thread or process); * any variables exposed by the agent and their types, ranges, or enumerated values; * any public methods and the method arguments and their types; * \u201chelp\u201d strings for each method and agent variable which explain their purpose; * any Agent library from which they derive. This may seem like a lot to specify, but the Agent Library supplies IDL for base Agents -- so in practice much of the IDL specification will be supplied to the Agent author. The IDL format and keywords are given in a table below. (TBD - Coming soon)","title":"Interface Description Language (IDL)"},{"location":"orchestrator/agent-library/#agent-library","text":"In this section we describe the Agent Library and give brief examples for usage. Classes are organized from the bottom up, that is, starting with the class from which the others derive. Note When using the Orchestrator to run your experiment, the Orchestrator will, by default, handle a return value of False from an Agent method as a reason to unload all Agents, break down communication groups and exit. Thus your Agent may stop an experiment by returning False .","title":"Agent Library"},{"location":"orchestrator/agent-library/#agent","text":"This is the base Agent class. It implements a setConfiguration method. If derived from, the user may call setConfiguration to set any self variables in your class. Agent also implements an empty confirmConfiguration method that is called once the self variables are set. You may implement your own confirmConfiguration if you need to make sure the user has set your internal variables to match any constraints you may want to impose. Returning False from this method will signal to the Orchestrator that something is wrong and the Orchestrator should handle this as an error. The default implementation of confirmConfiguration simply returns True . The method signature for confirmConfiguration() is def confirmConfiguration(self): It takes no arguments. In your confirmConfiguration method, you should confirm that your agent internal variables are the correct type and in the expected range. In the following example, imagine an agent has a variable that is an integer and the range of the value must be between 1 and 10. An agent can use the Agent class to implement this as so: from magi.util.agent import Agent class myAgent(Agent): def __init__(self): self.value = None def confirmConfiguration(): if not isinstance(self.value, int): return False if not 1 <= self.value <= 10: return False return True If the variable self.value is not an integer or is not between 1 and 10, confirmConfiguration returns False . If running this agent with the Orchestrator, the False value will get returned to the Orchestrator which will unload all agents, destroy all group communications, then exit. Thus your agent may cause the experiment to stop and be reset when it is not given the correct inputs. Note In the future, this functionality of enforcing correct input will be handled outside of the agent code. The IDL associated with the agent already specifies correct input and the Orchestrator (or other Montage/MAGI front end tool) will enforce proper input. All classes in the AgentLibrary inherit from Agent . The Agent documentation can seen here .","title":"Agent"},{"location":"orchestrator/agent-library/#dispatchagent","text":"The DispatchAgent implements the simple remote procedure call (RPC) mechanism used by the Orchestrator (and available through the MAGI python API). This allows any method in a class derived from DispatchAgent to be invoked in an AAL file (or by a MagiMessage if using the MAGI python interface directly). You almost always want to derive your agent from DispatchAgent . The DispatchAgent code simply loops, parsing incoming messages, looking for an event message. When it finds one, it attempts to call the method specified in the message with the arguments given in the message, thus implementing a basic RPC functionality in your agent. The first argument to your RPC-enabled method is the received message. It is accompanied by the optional named-parameters, sent as part of the MagiMessage . The Agent Library exports a function decorator for DispatchAgent -callable methods named agentmethod . It is not currently used for anything, but it is suggested that agent developers use it anyway. The DispatchAgent reads incoming messages and invokes the required method synchronously, i.e., it waits for a method call to return before reading the next message. Here is a simple example: from magi.util.agent import DispatchAgent, agentmethod def myAgent(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) @agentmethod() def doAction(self, msg): pass Given the agent myAgent above and the AAL fragment below, the method doAction will be called on all test nodes associated with myAgentGroup . eventstreams: myStream: - type: event agent: myAgentGroup method: doAction args: { } The DispatchAgent documentation may seen here .","title":"DispatchAgent"},{"location":"orchestrator/agent-library/#nonblockingdispatchagent","text":"The NonBlockingDispatchAgent is similar to DispatchAgent . The only difference is that NonBlockingDispatchAgent invokes the methods asynchronously , i.e., it forks a new thread for each method call and does not wait for the call to return. It invokes the required method and moves on to read the next message.","title":"NonBlockingDispatchAgent"},{"location":"orchestrator/agent-library/#reportingdispatchagent","text":"You will note that the DispatchAgent only allows an outside source to send commands to the agent. There is no communication backwards. The ReportingDispatchAgent base class has a slightly different run loop. Rather than blocking forever on incoming messages, it will also call its own method, periodic , to allow other operations to occur. The call to periodic will return the amount of time in seconds (as a float) that it will wait until calling periodic again. The periodic function therefore controls how often it is called. The first call will happen as soon as the run is called. The method signature of the periodic method is: def periodic(self, now): If periodic is not implemented in the subclass, an exception is raised. This example code writes the current time to a file once a second. Note the explicit use of the Agent class to set the file name. import os.path from magi.util.agent import ReportingDispatchAgent, agentmethod class myTimeTracker(ReportingDispatchAgent): def __init__(self): ReportingDispatchAgent.__init__(self) self.filename = None def confirmConfiguration(self): if not os.path.exists(self.filename): return False def periodic(self, now): with open(self.filename, 'a') as fd: fd.write('%f\\n' % now) # call again one second from now return 1.0 The ReportingDispatchAgent documentation may seen here. http://montage.deterlab.net/backend/python/util.html#magi.util.agent.ReportingDispatchAgent","title":"ReportingDispatchAgent"},{"location":"orchestrator/agent-library/#sharedserver","text":"The SharedServer class inherits from DispatchAgent and expects the subclass to implement the methods runserver and terminateserver to start or stop a local server process. The SharedServer class takes care of multiple agents requesting use of the server and only calls runserver or terminateserver when required. This ensures that there is ever only one instance of the server running at once on a given host. A canonical example of this would be a web server running a single instance of Apache. The methods runserver and stopserver take no arguments. Below is an example of a simple agent that starts and stops Apache on the local host. If there are other agents running on the machine that require Apache to be running, they may inherit from SharedServer as well, thus ensuring that there is only ever one instance of Apache running. from subprocess import check_call, CalledProcessError from magi.util.agent import SharedServer class ApacheServerAgent(SharedServer): def __init__(self): SharedServer.__init__(self) def runserver(self): try: check_call('apachectl start'.split()) except CalledProcessError: return False return True def stopserver(self): try: check_call('apachectl stop'.split()) except CalledProcessError: return False return True The SharedServer documentation may seen here .","title":"SharedServer"},{"location":"orchestrator/agent-library/#trafficclientagent","text":"TrafficClientAgent models an agent that periodically generates traffic. It must implement the getCmd method, returning a string to execute on the commandline to generate traffic. For example, the getCmd could return a curl or wget command to generate client-side HTML traffic. The signature of getCmd is: def getCmd(self, destination) Where destination is a server host name from which the agent should request traffic. The TrafficClientAgent class implements the following event-callable methods: startClient() and stopClient() . Neither method takes any arguments. These methods may be invoked from an AAL and start and stop the client respectively. The base class contains a number of variables which control how often getCmd is called and which servers should be contacted: * servers : A list of server hostnames * interval : A distribution variable Note A distribution variable is any valid python expression that returns a float. It may be as simple as an integer, \u201c1\u201d, or an actual distribution function. The Agent Library provides minmax , gamma , pareto , and expo in the distributions module. Thus a valid value for the TrafficClientAgent interval value could be minmax(1,10) , which returns a value between 1 and 10 inclusive. The signatures of these distributions are: minmax(min, max) gamma(alpha, rate, cap = None) pareto(alpha, scale = 1.0, cap = None) expo(lambd, scale = 1.0, cap = None) Below is a sample TrafficClientAgent which implements a simple HTTP client-side traffic agent. It assumes the destinations have been set correctly (via the Agent setConfiguration method) and there are web servers already running there. from magi.util.agent import TrafficClientAgent class mySimpleHTTPClient(TrafficClientAgent): def __init__(self): TrafficClientAgent.__init__(self) def getCmd(self, destination): cmd = 'curl -s -o /dev/null http://%s/index.html' % destination return cmd When this agent is used with the following AAL clauses, the servers server_1 and server_2 are used as HTTP traffic generation servers and traffic is generated once an interval where the interval ranges randomly between 5 and 10 seconds, inclusive. The first event sets the agent\u2019s internal configuration. The second event starts the traffic generation. eventstreams: myStream: - type: event agent: myHTTPClients method: setConfiguration args: interval: 'minmax(5, 10)' servers: ['server_1', 'server_2'] - type: event agent: myHTTPClients method: startClient args: { } The TrafficClientAgent documentation may seen here. http://montage.deterlab.net/backend/python/util.html#magi.util.agent.TrafficClientAgent","title":"TrafficClientAgent"},{"location":"orchestrator/agent-library/#probabilistictrafficclientagent","text":"ProbabilisticTrafficClientAgent provides the same service as TrafficAgent , but getCmd is called only when the configured probability function evaluates to a non-zero value.","title":"ProbabilisticTrafficClientAgent"},{"location":"orchestrator/agent-library/#connectedtrafficclientagent","text":"ConnectedTrafficClientAgent is a base for an agent that controls a set of agents that have standing connections to, and traffic between, a set of servers. connect() and disconnect() are called periodically when a given client should connect or disconnect to a given server. generateTraffic() is called when the given client should generate traffic between itself and the server it is connected to. The sequence of calls is: [period], connect(), [period], generateTraffic(), [period], generateTraffic(), ..., disconnect() This sequence may be repeated. Derived classes should implement connect() , disconnect() , and generateTraffic() .","title":"ConnectedTrafficClientAgent"},{"location":"orchestrator/agent-library/#agent-load-and-execution-chain-for-threaded-agents","text":"(TBD - Coming soon)","title":"Agent Load and Execution Chain (for threaded agents)"},{"location":"orchestrator/data-management/","text":"Orchestrator Data Management \u00b6 Data Management is a very important aspect of experimentation, which is why the data management layer is a very important aspect of the Orchestrator's framework. The following are some of the important terms that are used in context of Orchestrator's data management layer. Sensor : Orchestrator agent that senses information and needs to store it. Collector : Database server that can be used to store data. Shard : In case of a distributed database setup, the data is partitioned and stored in multiple database servers. This concept of partitioning data is known as \"sharding\", and each of the database servers is known as a \"shard\". Orchestrator's data management layer is highly configurable, with experimenters having the ability to setup a centralized or a distributed database, and also configure, at the node level, where sensors collect data. In case of a distributed/shared database setup, Orchestrator sets up a global database server. This server gives a holistic view of the database. Orchestrator data management uses MongoDB at its base. Data Manager Configuration \u00b6 The data management layer configuration is part of the Orchestrator\u2019s experiment level and node level configuration files. As mentioned earlier, Orchestrator\u2019s data management layer is highly configurable. More information about the same in available at DBDL: Configure the Orchestrator Data Management Layer . Orchestrator\u2019s data management layer enables an experimenter to do the following. Sense and Collect \u00b6 The following are the steps an agent developer should follow to populate Orchestrator\u2019s database Import the database management utility from magi.util import database Initialize a database collection passing it a unique name. We suggest using the agent name. Each agent implementation that extends from one of the predefined agents, like the DispatchAgent, has a variable \u201cname\u201d that stores the agent name. self.collection = database.getCollection(self.name) Insert data. Each record can be inserted as a dictionary of key-value pairs. self.collection.insert({\u201ckey1\u201d : \u201cvalue1\u201d, \u201ckey2\u201d: \u201cvalue2\u201d}) Note The db management utility inserts three other entries per record: host: <node\u2019s hostname> created: <record creation time> agent: <agent name> Query and Analyze \u00b6 In case of a distributed database setup, a user can connect to the mongo db server running on the global server node to get an experiment-wide view. However, in case of an unsharded setup, a user would have to connect to the appropriate collector based on the sensor-collector mapping to fetch data stored by a particular sensor. Orchestrator, by default, sets up an non-distributed database, with all the sensors collecting at the same collector. > mongo node-1.myExperiment.myProject:27018 mongo> use magi switched to db magi mongo> db.experiment_data.find() { \"agent\" : \"user_agent\", \"host\" : \"node-1\", \"created\" : 1409075736.646182, \"key1\" : \"value1\", \"key2\" : \"value2\" } { \"agent\" : \"user_agent\", \"host\" : \"node-2\", \"created\" : 1409075737.514683, \"key3\" : \"value3\", \"key4\" : \"value4\" } In case of a distributed setup, the configuration file would have information about a global server host. An experimenter can connect to the global server to get an experiment wide view of the database, or connect to individual collectors to get their local view. And, for more advanced queries, you can refer the Mongo documentation available at http://docs.mongodb.org/manual/tutorial/query-documents/ .","title":"Orchestrator Data Management"},{"location":"orchestrator/data-management/#orchestrator-data-management","text":"Data Management is a very important aspect of experimentation, which is why the data management layer is a very important aspect of the Orchestrator's framework. The following are some of the important terms that are used in context of Orchestrator's data management layer. Sensor : Orchestrator agent that senses information and needs to store it. Collector : Database server that can be used to store data. Shard : In case of a distributed database setup, the data is partitioned and stored in multiple database servers. This concept of partitioning data is known as \"sharding\", and each of the database servers is known as a \"shard\". Orchestrator's data management layer is highly configurable, with experimenters having the ability to setup a centralized or a distributed database, and also configure, at the node level, where sensors collect data. In case of a distributed/shared database setup, Orchestrator sets up a global database server. This server gives a holistic view of the database. Orchestrator data management uses MongoDB at its base.","title":"Orchestrator Data Management"},{"location":"orchestrator/data-management/#data-manager-configuration","text":"The data management layer configuration is part of the Orchestrator\u2019s experiment level and node level configuration files. As mentioned earlier, Orchestrator\u2019s data management layer is highly configurable. More information about the same in available at DBDL: Configure the Orchestrator Data Management Layer . Orchestrator\u2019s data management layer enables an experimenter to do the following.","title":"Data Manager Configuration"},{"location":"orchestrator/data-management/#sense-and-collect","text":"The following are the steps an agent developer should follow to populate Orchestrator\u2019s database Import the database management utility from magi.util import database Initialize a database collection passing it a unique name. We suggest using the agent name. Each agent implementation that extends from one of the predefined agents, like the DispatchAgent, has a variable \u201cname\u201d that stores the agent name. self.collection = database.getCollection(self.name) Insert data. Each record can be inserted as a dictionary of key-value pairs. self.collection.insert({\u201ckey1\u201d : \u201cvalue1\u201d, \u201ckey2\u201d: \u201cvalue2\u201d}) Note The db management utility inserts three other entries per record: host: <node\u2019s hostname> created: <record creation time> agent: <agent name>","title":"Sense and Collect"},{"location":"orchestrator/data-management/#query-and-analyze","text":"In case of a distributed database setup, a user can connect to the mongo db server running on the global server node to get an experiment-wide view. However, in case of an unsharded setup, a user would have to connect to the appropriate collector based on the sensor-collector mapping to fetch data stored by a particular sensor. Orchestrator, by default, sets up an non-distributed database, with all the sensors collecting at the same collector. > mongo node-1.myExperiment.myProject:27018 mongo> use magi switched to db magi mongo> db.experiment_data.find() { \"agent\" : \"user_agent\", \"host\" : \"node-1\", \"created\" : 1409075736.646182, \"key1\" : \"value1\", \"key2\" : \"value2\" } { \"agent\" : \"user_agent\", \"host\" : \"node-2\", \"created\" : 1409075737.514683, \"key3\" : \"value3\", \"key4\" : \"value4\" } In case of a distributed setup, the configuration file would have information about a global server host. An experimenter can connect to the global server to get an experiment wide view of the database, or connect to individual collectors to get their local view. And, for more advanced queries, you can refer the Mongo documentation available at http://docs.mongodb.org/manual/tutorial/query-documents/ .","title":"Query and Analyze"},{"location":"orchestrator/feedback/","text":"Feedback Case Study \u00b6 Introduction \u00b6 This case study demonstrates our system\u2019s ability to do real-time feedback from the experiment using triggers. We show how information from the experiment may be used to extend the Orchestrator autonomy and deterministically control dynamic experiments. This is an example where the active state of an experiment is a driving input to the control. The data management layer too plays an important role in enabling the flow of information. In this case study we show how, in a semi-controllable environment, the amount of traffic on a given link may be controlled using feedback from the experiment itself . The traffic on one of the links in the experiment must be maintained within a certain range; for this example, the range was 100-105 MB. We assume that the uncontrollable traffic on the link would not exceed the required maximum. This experiment is set up with the following characteristics: the monitored link has some noise (uncontrollable traffic) flowing through it. The noise has been artificially generated. We set the noise generating clients to pull randomly changing amount of traffic from the servers, in order to enact the noise. The solution is not dependent on the experiment topology. We deploy a traffic monitor agent on one of the end nodes of the link to be monitored. The traffic monitoring agent continuously monitors the traffic on the link. We also deploy a traffic generator agent (control client) that coordinates with the traffic monitor agent in real time and generates exactly the amount of traffic that would help maintain the total traffic on the link within the required limits. To demonstrate this case study, we set up an experiment topology similar to the one seen in the following figure, with 50 noise generating clients and 10 servers. We also tested a scaled up version of the experiment with 300 noise generating agents and 100 servers. However, due to the resource constraints on the testbed, we recommend you try this case with the smaller topology first. The scaling up of the experiment may be achieved with very simple modifications. We use two kinds of agents: A server agent (located on the server nodes) that runs an apache server and serves random data of requested size. A client agent (located on the client nodes) that periodically requests data from a randomally chosen server agent. The size of data that a client agent requests is configurable. The initial configuration is set to ~1.2 MB for each of the 50 noise generating clients, and ~40 MB for the control client, adding up to a total of ~100 MB. Now, to synthetically generate uncontrollable traffic, the size of data requested by the noise generating clients is changed randomly. We set the probability of change as: 80% - no change, 10% - increment, and 10% - reduction. The amount by which the fetch size is changed is calculated randomly. Then, to add control to this experiment, specifically on the traffic on the monitored link, we add a traffic monitor and a traffic generator (control client). We deploy a traffic monitor agent on the client-side end node of the link to be monitored. This agent continuously monitors all the traffic flowing through the node. Further, we attach a node (control client) to the traffic generator, and deploy a client agent on it. The Orchestrator, based on the real time traffic feedback, sends change load messages to the control client agent, in order to maintain the total traffic on the monitored link within the set limits. Event Streams This example has six events streams; the server stream, the noise stream, the noise modify stream, the control client stream, the control stream and the duration stream. Mapping to the Topology \u00b6 The groups directive in the AAL file allows mapping a agent behavior to one or more nodes. #!php groups: server_group: &slist [s-0, s-1, s-2, s-3, s-4, s-5, s-6, s-7, s-8, s-9] noise_group: [uc-0, uc-1, uc-2, uc-3, uc-4, uc-5, uc-6, uc-7, uc-8, uc-9, uc-10, uc-11, uc-12, uc-13, uc-14, uc-15, uc-16, uc-17, uc-18, uc-19, uc-20, uc-21, uc-22, uc-23, uc-24, uc-25, uc-26, uc-27, uc-28, uc-29, uc-30, uc-31, uc-32, uc-33, uc-34, uc-35, uc-36, uc-37, uc-38, uc-39, uc-40, uc-41, uc-42, uc-43, uc-44, uc-45, uc-46, uc-47, uc-48, uc-49] sensor_group: [rc] client_group: [c-0] In this example, we observe that there are four groups server_group, noise_group, sensor_group and client_group. The server_group consists of 10 servers. The noise_group consists of 50 noise generating clients. The sensor_group consists of the lone sensor node and the client_group consists of the controlling client. Additionally,we use yaml pointers to annotate the server_group as \u201cslist\u201d. The slist annotation is used to refer to the list of servers for configuring the client agents in the section below. Configuring the Agents There are four types of agents, server_agent, noise_agent, monitor_agent, and client_agent. Each of the server agent is used to start a web server serving garbage data of requested size. The noise agents are used to create the noise of the network. Each of the noise agent periodically fetches data of a randomally changing size from a ramdonly chosen server. server_agent: group: server_group path: /share/magi/modules/apache/apache.tar.gz execargs: [] noise_agent: group: noise_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '2', sizes: '120000'} monitor_agent: group: sensor_group path: /proj/montage/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} client_agent: group: client_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '2', sizes: '4000000'} Server Stream \u00b6 The server event stream consists of three states. The start state which generates a trigger, called serverStarted, once all the server agents are activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the noise stream and the client event stream. Once the trigger is received, it enters the stop state, where the server is deactivated or terminated. Noise Stream and Control Client Stream The noise and client event streams consists of five states. First, the agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all the agent nodes. The streams then synchronize with the server stream by waiting for the serverStarted trigger from the server nodes. Once they receive the trigger the agents are activated in the start state. Each agent fetches web pages from one of the listed servers. Next, the streams wait for the monitorStopped trigger from the monitorstream. Once the trigger is received the clients are instructed to stop fetching data. On termination, the agents sends a noiseStopped/clientStopped trigger that allows the server stream to synchronize and terminate the servers, which is done only after all the http client agents have terminated. Noise Modify Stream The noise modify stream starts once the noise generating agents start. It continously instructs the noise generating agents to randomly modify the amount of noise being generated. This is done to create an uncontrolled noise generation behaviour. Control Stream The control stream starts once the control client agent has started. It configures the traffic monitoring agent and instucts it to start monitoring. Once the monitoring starts, this stream continously monitors the amount of traffic flowing on the monitored link, and based on the traffic information, instructs the control client to modify the amount of traffic it is pulling, in order to maintain the total traffic on the monitored link within the required limits. Duration Stream The duration stream manages the time duration for which the experiment needs to run. Its starts once the monitor agent has started. The stream then waits for \u2206t before instructing the monitor agent stop and terminating all of the agents. Running the Experiment \u00b6 Step 1: Set up your environment \u00b6 Set up your environment. Assuming your experiment is named myExp, your DETER project is myProj, and the AAL file is called procedure.aal. PROJ=myExp EXP=myProj AAL=procedure.aal Step 2: Set Up Containerized Experiment \u00b6 As this experiment requires a lot of nodes, we should try and use containerized nodes. Create a containerized version of the experiment using this network description file: casestudy_feedback.tcl NS=fb_topology.tcl > /share/containers/containerize.py $PROJ $EXP $NS --packing=8 --openvz-diskspace 15G --pnode-type MicroCloud,pc2133 Step 3: Swap in your Experiment \u00b6 Swap in the newly created experiment. Step 4: Run Orchestrator \u00b6 Once the experiment is swapped in, run the Orchestrator, giving it this AAL: casestudy_feedback.aal , the experiment name and project name. > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseFeedback\u201d: The Orchestrator enacts an internally defined stream called initialization that is responsible for establishing all the groups and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , the initialization stream is complete. Now all of the six above mentioned streams start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The noiseStream and the controlClientStream on receiving the trigger serverStarted from the server_group, send the startClient event to the noise_group and the control_group, respectively. All memebers of both the groups start http clients and fire noiseStarted and controlClientStarted triggers. The noiseModifyStream on receiving the noiseStarted trigger joins a loop that sends a changeTraffic event to the noise_group, every two seconds. The control stream on receiving the controlClientStarted trigger sends a startCollection event to the monitor_group. The lone member of the monitor_group starts monitoring the interfaces on the node, and fires a monitorStarted trigger. The control stream then, joins a loop that sends a sense event to the monitor_group, every two seconds, and based on the return value in the response trigger intfSensed, sends a increaseTraffic or a reduceTraffic event to the control_group, if required. The duration stream after receiving the monitorStarted trigger, waits for 5 minutes. On completion, it sends a stopCollection event to the monitor_group. The monitor agent stop monitoring and sends back a monitorStopped trigger. Once the noiseStream and the controlClientStream recieve the monitorStopped trigger, they send out the stopClient event to their respective members. The http clients are stopped on all the members, and the noiseStopped and the controlClientStopped triggers are sent back to the orchestrator. The serverStream, on receiving the noiseStopped and the controlClientStopped triggers, sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the durationStream. On receiving the serverStopped trigger, the durationStream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment. Procedure: casestudy_feedback.aal Topology: casestudy_feedback.tcl Archive Logs: casestudy_feedback.tar.gz Orchestration: casestudy_feedback.orch.log Visualizing Experiment Results \u00b6 Offline: A traffic plot may be generated using the MAGI Graph Creation Tool . Real Time: A real time simulated traffic plot using canned data from a pre-run experiment may be visualized here . A similar plot using live data may be plotted by visiting the same web page , and additionally passing it the hostname of the database config node of your experiment. You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject","title":"Feedback Case Study"},{"location":"orchestrator/feedback/#feedback-case-study","text":"","title":"Feedback Case Study"},{"location":"orchestrator/feedback/#introduction","text":"This case study demonstrates our system\u2019s ability to do real-time feedback from the experiment using triggers. We show how information from the experiment may be used to extend the Orchestrator autonomy and deterministically control dynamic experiments. This is an example where the active state of an experiment is a driving input to the control. The data management layer too plays an important role in enabling the flow of information. In this case study we show how, in a semi-controllable environment, the amount of traffic on a given link may be controlled using feedback from the experiment itself . The traffic on one of the links in the experiment must be maintained within a certain range; for this example, the range was 100-105 MB. We assume that the uncontrollable traffic on the link would not exceed the required maximum. This experiment is set up with the following characteristics: the monitored link has some noise (uncontrollable traffic) flowing through it. The noise has been artificially generated. We set the noise generating clients to pull randomly changing amount of traffic from the servers, in order to enact the noise. The solution is not dependent on the experiment topology. We deploy a traffic monitor agent on one of the end nodes of the link to be monitored. The traffic monitoring agent continuously monitors the traffic on the link. We also deploy a traffic generator agent (control client) that coordinates with the traffic monitor agent in real time and generates exactly the amount of traffic that would help maintain the total traffic on the link within the required limits. To demonstrate this case study, we set up an experiment topology similar to the one seen in the following figure, with 50 noise generating clients and 10 servers. We also tested a scaled up version of the experiment with 300 noise generating agents and 100 servers. However, due to the resource constraints on the testbed, we recommend you try this case with the smaller topology first. The scaling up of the experiment may be achieved with very simple modifications. We use two kinds of agents: A server agent (located on the server nodes) that runs an apache server and serves random data of requested size. A client agent (located on the client nodes) that periodically requests data from a randomally chosen server agent. The size of data that a client agent requests is configurable. The initial configuration is set to ~1.2 MB for each of the 50 noise generating clients, and ~40 MB for the control client, adding up to a total of ~100 MB. Now, to synthetically generate uncontrollable traffic, the size of data requested by the noise generating clients is changed randomly. We set the probability of change as: 80% - no change, 10% - increment, and 10% - reduction. The amount by which the fetch size is changed is calculated randomly. Then, to add control to this experiment, specifically on the traffic on the monitored link, we add a traffic monitor and a traffic generator (control client). We deploy a traffic monitor agent on the client-side end node of the link to be monitored. This agent continuously monitors all the traffic flowing through the node. Further, we attach a node (control client) to the traffic generator, and deploy a client agent on it. The Orchestrator, based on the real time traffic feedback, sends change load messages to the control client agent, in order to maintain the total traffic on the monitored link within the set limits. Event Streams This example has six events streams; the server stream, the noise stream, the noise modify stream, the control client stream, the control stream and the duration stream.","title":"Introduction"},{"location":"orchestrator/feedback/#mapping-to-the-topology","text":"The groups directive in the AAL file allows mapping a agent behavior to one or more nodes. #!php groups: server_group: &slist [s-0, s-1, s-2, s-3, s-4, s-5, s-6, s-7, s-8, s-9] noise_group: [uc-0, uc-1, uc-2, uc-3, uc-4, uc-5, uc-6, uc-7, uc-8, uc-9, uc-10, uc-11, uc-12, uc-13, uc-14, uc-15, uc-16, uc-17, uc-18, uc-19, uc-20, uc-21, uc-22, uc-23, uc-24, uc-25, uc-26, uc-27, uc-28, uc-29, uc-30, uc-31, uc-32, uc-33, uc-34, uc-35, uc-36, uc-37, uc-38, uc-39, uc-40, uc-41, uc-42, uc-43, uc-44, uc-45, uc-46, uc-47, uc-48, uc-49] sensor_group: [rc] client_group: [c-0] In this example, we observe that there are four groups server_group, noise_group, sensor_group and client_group. The server_group consists of 10 servers. The noise_group consists of 50 noise generating clients. The sensor_group consists of the lone sensor node and the client_group consists of the controlling client. Additionally,we use yaml pointers to annotate the server_group as \u201cslist\u201d. The slist annotation is used to refer to the list of servers for configuring the client agents in the section below. Configuring the Agents There are four types of agents, server_agent, noise_agent, monitor_agent, and client_agent. Each of the server agent is used to start a web server serving garbage data of requested size. The noise agents are used to create the noise of the network. Each of the noise agent periodically fetches data of a randomally changing size from a ramdonly chosen server. server_agent: group: server_group path: /share/magi/modules/apache/apache.tar.gz execargs: [] noise_agent: group: noise_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '2', sizes: '120000'} monitor_agent: group: sensor_group path: /proj/montage/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} client_agent: group: client_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '2', sizes: '4000000'}","title":"Mapping to the Topology"},{"location":"orchestrator/feedback/#server-stream","text":"The server event stream consists of three states. The start state which generates a trigger, called serverStarted, once all the server agents are activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the noise stream and the client event stream. Once the trigger is received, it enters the stop state, where the server is deactivated or terminated. Noise Stream and Control Client Stream The noise and client event streams consists of five states. First, the agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all the agent nodes. The streams then synchronize with the server stream by waiting for the serverStarted trigger from the server nodes. Once they receive the trigger the agents are activated in the start state. Each agent fetches web pages from one of the listed servers. Next, the streams wait for the monitorStopped trigger from the monitorstream. Once the trigger is received the clients are instructed to stop fetching data. On termination, the agents sends a noiseStopped/clientStopped trigger that allows the server stream to synchronize and terminate the servers, which is done only after all the http client agents have terminated. Noise Modify Stream The noise modify stream starts once the noise generating agents start. It continously instructs the noise generating agents to randomly modify the amount of noise being generated. This is done to create an uncontrolled noise generation behaviour. Control Stream The control stream starts once the control client agent has started. It configures the traffic monitoring agent and instucts it to start monitoring. Once the monitoring starts, this stream continously monitors the amount of traffic flowing on the monitored link, and based on the traffic information, instructs the control client to modify the amount of traffic it is pulling, in order to maintain the total traffic on the monitored link within the required limits. Duration Stream The duration stream manages the time duration for which the experiment needs to run. Its starts once the monitor agent has started. The stream then waits for \u2206t before instructing the monitor agent stop and terminating all of the agents.","title":"Server Stream"},{"location":"orchestrator/feedback/#running-the-experiment","text":"","title":"Running the Experiment"},{"location":"orchestrator/feedback/#step-1-set-up-your-environment","text":"Set up your environment. Assuming your experiment is named myExp, your DETER project is myProj, and the AAL file is called procedure.aal. PROJ=myExp EXP=myProj AAL=procedure.aal","title":"Step 1: Set up your environment"},{"location":"orchestrator/feedback/#step-2-set-up-containerized-experiment","text":"As this experiment requires a lot of nodes, we should try and use containerized nodes. Create a containerized version of the experiment using this network description file: casestudy_feedback.tcl NS=fb_topology.tcl > /share/containers/containerize.py $PROJ $EXP $NS --packing=8 --openvz-diskspace 15G --pnode-type MicroCloud,pc2133","title":"Step 2: Set Up Containerized Experiment"},{"location":"orchestrator/feedback/#step-3-swap-in-your-experiment","text":"Swap in the newly created experiment.","title":"Step 3: Swap in your Experiment"},{"location":"orchestrator/feedback/#step-4-run-orchestrator","text":"Once the experiment is swapped in, run the Orchestrator, giving it this AAL: casestudy_feedback.aal , the experiment name and project name. > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseFeedback\u201d: The Orchestrator enacts an internally defined stream called initialization that is responsible for establishing all the groups and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , the initialization stream is complete. Now all of the six above mentioned streams start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The noiseStream and the controlClientStream on receiving the trigger serverStarted from the server_group, send the startClient event to the noise_group and the control_group, respectively. All memebers of both the groups start http clients and fire noiseStarted and controlClientStarted triggers. The noiseModifyStream on receiving the noiseStarted trigger joins a loop that sends a changeTraffic event to the noise_group, every two seconds. The control stream on receiving the controlClientStarted trigger sends a startCollection event to the monitor_group. The lone member of the monitor_group starts monitoring the interfaces on the node, and fires a monitorStarted trigger. The control stream then, joins a loop that sends a sense event to the monitor_group, every two seconds, and based on the return value in the response trigger intfSensed, sends a increaseTraffic or a reduceTraffic event to the control_group, if required. The duration stream after receiving the monitorStarted trigger, waits for 5 minutes. On completion, it sends a stopCollection event to the monitor_group. The monitor agent stop monitoring and sends back a monitorStopped trigger. Once the noiseStream and the controlClientStream recieve the monitorStopped trigger, they send out the stopClient event to their respective members. The http clients are stopped on all the members, and the noiseStopped and the controlClientStopped triggers are sent back to the orchestrator. The serverStream, on receiving the noiseStopped and the controlClientStopped triggers, sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the durationStream. On receiving the serverStopped trigger, the durationStream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment. Procedure: casestudy_feedback.aal Topology: casestudy_feedback.tcl Archive Logs: casestudy_feedback.tar.gz Orchestration: casestudy_feedback.orch.log","title":"Step 4: Run Orchestrator"},{"location":"orchestrator/feedback/#visualizing-experiment-results","text":"Offline: A traffic plot may be generated using the MAGI Graph Creation Tool . Real Time: A real time simulated traffic plot using canned data from a pre-run experiment may be visualized here . A similar plot using live data may be plotted by visiting the same web page , and additionally passing it the hostname of the database config node of your experiment. You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject","title":"Visualizing Experiment Results"},{"location":"orchestrator/flooder/","text":"Flooder Case Study \u00b6 In this experiment we demonstrate how one can setup a flooding agent and a victim server. We demonstrate three aspects of MAGI: Specifying multiple event streams, Synchronizing with triggers, and A special target called exit to unload agents. Event Streams \u00b6 This experiment has three streams: the flooder stream , the server stream , and the cleanup stream . The co-ordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers or time-based triggers. The triggers are indicated as wait states in gray. The group formation and loading the agents, which is also automated by the orchestrator tool, is not illustrated above. Server Stream \u00b6 The server event stream consists of three states. The start state which generates a trigger, called serverStarted, once the server agent is activated on the experiment nodes. It then enters the wait state and stays there for a period \u2206t and terminates the server agent in stop state. The AAL description is below: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ { timeout: 180000 } ] - type: event agent: server_agent method: stopServer trigger: serverStopped args: {} Flooder Stream \u00b6 The flooder stream consists of five states. First, the flooder agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The flooder stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger the flooder agent is activated in the start state. Next, the flooder stream waits for a period of \u2206t before repeating the start and stop events one more time while waiting for a period of \u2206t in between. Finally, the flooder stream terminates the flooder agent in the stop state. The AAL description is as follows: flooderstream: - type: event agent: flooder_agent method: setConfiguration args: dst: '10.1.1.3' proto: 'udp' length: 'minmax(64, 1024)' ratetype: 'flat' highrate: '5000' lowrate: '5000' sport: 'minmax(1024, 4096)' dport: 'minmax(1024, 4096)' - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: flooder_agent method: startFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: stopFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: startFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: stopFlood args: {} Cleanup Stream \u00b6 The last event stream, the cleanup stream consists of two states. First, it waits for the server to stop and then it enters the exit state. The exit state unload and tears down all the comminucation mechanisms between the agents. The exit state is entered by the key target is used to transfer control to a reserved state internal to the orchestrator. It causes the orchestrator to send agent unload and disband group messages to all the experiment node and then it exits the orchestrator. cleanupstream: - type: trigger triggers: [ {event: serverStopped, target: exit} ] Running the experiment \u00b6 Swap in the experiment using the network description file given below. Set up your environment. Assuming your experiment is named myExp, your DETER project is myProj, and the AAL file is called procedure.aal. $ export PROJ=myExp $ export EXP=myProj $ export AAL=procedure.aal Once the experiment is swapped in, run the orchestrator, giving it the AAL above. The orchestrator needs an AAL file, and the experiment and project name. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseClientServer\u201d. $ /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The output will be as follows: stream groupBuildS... : sent : (16:17:14) joinGroup flooder_group --> __ALL__ stream groupBuildS... : sent : (16:17:14) joinGroup server_group --> __ALL__ stream groupBuildS... : trig : (16:17:15) trigger completed: GroupBuildDone: {'group': 'flooder_group'} stream groupBuildS... : trig : (16:17:15) trigger completed: GroupBuildDone: {'group': 'server_group'} stream groupBuildS... : sent : (16:17:15) groupPing(['flooder_group']) --> flooder_group stream groupBuildS... : sent : (16:17:15) groupPing(['server_group']) --> server_group stream groupBuildS... : trig : (16:17:15) trigger completed: GroupPong: {'group': 'flooder_group'} stream groupBuildS... : trig : (16:17:15) trigger completed: GroupPong: {'group': 'server_group'} stream groupBuildS... : DONE : (16:17:15) complete. stream loadAgentSt... : sent : (16:17:15) loadAgent flooder_agent --> flooder_group stream loadAgentSt... : sent : (16:17:15) loadAgent server_agent --> server_group stream loadAgentSt... : trig : (16:17:27) trigger completed: AgentLoadDone: {'agent': 'flooder_agent'} stream loadAgentSt... : trig : (16:18:03) trigger completed: AgentLoadDone: {'agent': 'server_agent'} stream loadAgentSt... : DONE : (16:18:03) complete. 04-24 16:18:03.371 magi.orchestrator.orchestrator INFO Running Event Streams stream flooderstream : sent : (16:18:03) setConfiguration(['udp', '10.1.1.3', ... ) --> flooder_group stream serverstream : sent : (16:18:03) startServer(None) --> server_group (fires trigger: serverStarted) stream flooderstream : trig : (16:18:05) trigger completed: serverStarted: {'retVal': True} stream flooderstream : sent : (16:18:05) startFlood(None) --> flooder_group 04-24 16:18:05.624 magi.orchestrator.orchestrator CRITICAL Got a runtime exception from an agent. Jumping to exit target. stream unknown : exit : (16:18:05) Run-time exception in agent <magi.modules.flooder_agent_code.flooder.flooder_agent object at 0x7f47e2243490> on node(s) floodernode in method _execute_child, line 1343, in file subprocess.py. Error: [Errno 2] No such file or directory 04-24 16:18:05.625 magi.orchestrator.orchestrator INFO Running Exit Streams stream unloadAgent... : sent : (16:18:05) unloadAgent flooder_agent --> flooder_group stream unloadAgent... : sent : (16:18:05) unloadAgent server_agent --> server_group stream unloadAgent... : trig : (16:18:06) trigger completed: AgentUnloadDone: {'agent': 'flooder_agent'} stream unloadAgent... : trig : (16:18:06) trigger completed: AgentUnloadDone: {'agent': 'server_agent'} stream unloadAgent... : DONE : (16:18:06) complete. stream groupLeaveS... : sent : (16:18:06) leaveGroup flooder_group --> __ALL__ stream groupLeaveS... : sent : (16:18:06) leaveGroup server_group --> __ALL__ stream groupLeaveS... : trig : (16:18:06) trigger completed: GroupTeardownDone: {'group': 'flooder_group'} stream groupLeaveS... : trig : (16:18:06) trigger completed: GroupTeardownDone: {'group': 'server_group'} stream groupLeaveS... : DONE : (16:18:06) complete. The orchestration tool runs an internally defined stream called initilization that is responsible for establishing the server_group and the flooder_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone, The initialization stream is complete. Now the serverstream, flooderstream and the cleanupstream start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The flooderstream on receiving the trigger serverStarted from the server_group, sends the startFlood event to the flooder_group. One minute later, the clientstream sends the trigger stopFlood to the flooder_group. This sequence of events repeats one more time before sending the final stopFlood trigger to the flooder_group, terminating the flooder_agent. Once the serverstream finishes the wait period, it sends out stopServer on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream. The procedure and topology file that were used for the casestudy are attached below. Procedure : casestudy_flooder.aal Topology : casestudy_flooder.tcl Archived logs : casestudy_flooder.tar.gz Visualizing Experiment Results \u00b6 In order to visulaize the traffic on the network, we modify the above mentioned procedure to add another stream called \u201cmonitor\u201d. This stream deploys a packet sensor agent on the server node to measure the traffic on the link in the experiment. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [servernode] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: serverStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the below mentioned tools to create a traffic plot. In order to populate the traffic data, re-orchestrate the experiment using the updated procedure. The updated procedure file and the corresponding logs are attached below. A plot of the traffic on the link containing the flooder and the victim server can be generated by the MAGI Graph Creation Tool . $ export GRAPHCONF=graph.conf $ /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o traffic_plot.png Procedure : flooder_monitor.aal Archived logs : casestudy_flooder_monitor.tar.gz Graph Config : graph.conf","title":"Flooder Case Study"},{"location":"orchestrator/flooder/#flooder-case-study","text":"In this experiment we demonstrate how one can setup a flooding agent and a victim server. We demonstrate three aspects of MAGI: Specifying multiple event streams, Synchronizing with triggers, and A special target called exit to unload agents.","title":"Flooder Case Study"},{"location":"orchestrator/flooder/#event-streams","text":"This experiment has three streams: the flooder stream , the server stream , and the cleanup stream . The co-ordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers or time-based triggers. The triggers are indicated as wait states in gray. The group formation and loading the agents, which is also automated by the orchestrator tool, is not illustrated above.","title":"Event Streams"},{"location":"orchestrator/flooder/#server-stream","text":"The server event stream consists of three states. The start state which generates a trigger, called serverStarted, once the server agent is activated on the experiment nodes. It then enters the wait state and stays there for a period \u2206t and terminates the server agent in stop state. The AAL description is below: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ { timeout: 180000 } ] - type: event agent: server_agent method: stopServer trigger: serverStopped args: {}","title":"Server Stream"},{"location":"orchestrator/flooder/#flooder-stream","text":"The flooder stream consists of five states. First, the flooder agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The flooder stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger the flooder agent is activated in the start state. Next, the flooder stream waits for a period of \u2206t before repeating the start and stop events one more time while waiting for a period of \u2206t in between. Finally, the flooder stream terminates the flooder agent in the stop state. The AAL description is as follows: flooderstream: - type: event agent: flooder_agent method: setConfiguration args: dst: '10.1.1.3' proto: 'udp' length: 'minmax(64, 1024)' ratetype: 'flat' highrate: '5000' lowrate: '5000' sport: 'minmax(1024, 4096)' dport: 'minmax(1024, 4096)' - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: flooder_agent method: startFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: stopFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: startFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: stopFlood args: {}","title":"Flooder Stream"},{"location":"orchestrator/flooder/#cleanup-stream","text":"The last event stream, the cleanup stream consists of two states. First, it waits for the server to stop and then it enters the exit state. The exit state unload and tears down all the comminucation mechanisms between the agents. The exit state is entered by the key target is used to transfer control to a reserved state internal to the orchestrator. It causes the orchestrator to send agent unload and disband group messages to all the experiment node and then it exits the orchestrator. cleanupstream: - type: trigger triggers: [ {event: serverStopped, target: exit} ]","title":"Cleanup Stream"},{"location":"orchestrator/flooder/#running-the-experiment","text":"Swap in the experiment using the network description file given below. Set up your environment. Assuming your experiment is named myExp, your DETER project is myProj, and the AAL file is called procedure.aal. $ export PROJ=myExp $ export EXP=myProj $ export AAL=procedure.aal Once the experiment is swapped in, run the orchestrator, giving it the AAL above. The orchestrator needs an AAL file, and the experiment and project name. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseClientServer\u201d. $ /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The output will be as follows: stream groupBuildS... : sent : (16:17:14) joinGroup flooder_group --> __ALL__ stream groupBuildS... : sent : (16:17:14) joinGroup server_group --> __ALL__ stream groupBuildS... : trig : (16:17:15) trigger completed: GroupBuildDone: {'group': 'flooder_group'} stream groupBuildS... : trig : (16:17:15) trigger completed: GroupBuildDone: {'group': 'server_group'} stream groupBuildS... : sent : (16:17:15) groupPing(['flooder_group']) --> flooder_group stream groupBuildS... : sent : (16:17:15) groupPing(['server_group']) --> server_group stream groupBuildS... : trig : (16:17:15) trigger completed: GroupPong: {'group': 'flooder_group'} stream groupBuildS... : trig : (16:17:15) trigger completed: GroupPong: {'group': 'server_group'} stream groupBuildS... : DONE : (16:17:15) complete. stream loadAgentSt... : sent : (16:17:15) loadAgent flooder_agent --> flooder_group stream loadAgentSt... : sent : (16:17:15) loadAgent server_agent --> server_group stream loadAgentSt... : trig : (16:17:27) trigger completed: AgentLoadDone: {'agent': 'flooder_agent'} stream loadAgentSt... : trig : (16:18:03) trigger completed: AgentLoadDone: {'agent': 'server_agent'} stream loadAgentSt... : DONE : (16:18:03) complete. 04-24 16:18:03.371 magi.orchestrator.orchestrator INFO Running Event Streams stream flooderstream : sent : (16:18:03) setConfiguration(['udp', '10.1.1.3', ... ) --> flooder_group stream serverstream : sent : (16:18:03) startServer(None) --> server_group (fires trigger: serverStarted) stream flooderstream : trig : (16:18:05) trigger completed: serverStarted: {'retVal': True} stream flooderstream : sent : (16:18:05) startFlood(None) --> flooder_group 04-24 16:18:05.624 magi.orchestrator.orchestrator CRITICAL Got a runtime exception from an agent. Jumping to exit target. stream unknown : exit : (16:18:05) Run-time exception in agent <magi.modules.flooder_agent_code.flooder.flooder_agent object at 0x7f47e2243490> on node(s) floodernode in method _execute_child, line 1343, in file subprocess.py. Error: [Errno 2] No such file or directory 04-24 16:18:05.625 magi.orchestrator.orchestrator INFO Running Exit Streams stream unloadAgent... : sent : (16:18:05) unloadAgent flooder_agent --> flooder_group stream unloadAgent... : sent : (16:18:05) unloadAgent server_agent --> server_group stream unloadAgent... : trig : (16:18:06) trigger completed: AgentUnloadDone: {'agent': 'flooder_agent'} stream unloadAgent... : trig : (16:18:06) trigger completed: AgentUnloadDone: {'agent': 'server_agent'} stream unloadAgent... : DONE : (16:18:06) complete. stream groupLeaveS... : sent : (16:18:06) leaveGroup flooder_group --> __ALL__ stream groupLeaveS... : sent : (16:18:06) leaveGroup server_group --> __ALL__ stream groupLeaveS... : trig : (16:18:06) trigger completed: GroupTeardownDone: {'group': 'flooder_group'} stream groupLeaveS... : trig : (16:18:06) trigger completed: GroupTeardownDone: {'group': 'server_group'} stream groupLeaveS... : DONE : (16:18:06) complete. The orchestration tool runs an internally defined stream called initilization that is responsible for establishing the server_group and the flooder_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone, The initialization stream is complete. Now the serverstream, flooderstream and the cleanupstream start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The flooderstream on receiving the trigger serverStarted from the server_group, sends the startFlood event to the flooder_group. One minute later, the clientstream sends the trigger stopFlood to the flooder_group. This sequence of events repeats one more time before sending the final stopFlood trigger to the flooder_group, terminating the flooder_agent. Once the serverstream finishes the wait period, it sends out stopServer on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream. The procedure and topology file that were used for the casestudy are attached below. Procedure : casestudy_flooder.aal Topology : casestudy_flooder.tcl Archived logs : casestudy_flooder.tar.gz","title":"Running the experiment"},{"location":"orchestrator/flooder/#visualizing-experiment-results","text":"In order to visulaize the traffic on the network, we modify the above mentioned procedure to add another stream called \u201cmonitor\u201d. This stream deploys a packet sensor agent on the server node to measure the traffic on the link in the experiment. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [servernode] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: serverStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the below mentioned tools to create a traffic plot. In order to populate the traffic data, re-orchestrate the experiment using the updated procedure. The updated procedure file and the corresponding logs are attached below. A plot of the traffic on the link containing the flooder and the victim server can be generated by the MAGI Graph Creation Tool . $ export GRAPHCONF=graph.conf $ /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o traffic_plot.png Procedure : flooder_monitor.aal Archived logs : casestudy_flooder_monitor.tar.gz Graph Config : graph.conf","title":"Visualizing Experiment Results"},{"location":"orchestrator/magi-desktop/","text":"MAGI Desktop \u00b6 Step 1: Install the following dependencies. \u00b6 The source code for all of these is available on Deter Ops ( users.deterlab.net ) under /share/magi/tarfiles/ : python-yaml python-pydot python-networkx py-matplotlib Mongo DB python-pymongo Yaml C Library (required for c-based agents, otherwise too helps improve performance) Mongo C Library (required only for c-based agents) All python packages can be installed using pip. Install pip, if not already available. curl -O https://bootstrap.pypa.io/get-pip.py python get-pip.py pip install pyyaml pip install pydot pip install networkx pip install matplotlib pip install pymongo Step 2: Download and install MAGI \u00b6 $ git clone https://github.com/deter-project/magi.git $ cd magi $ sudo python setup.py install Step 3: Run a simple two node MAGI setup. \u00b6 $ tools/magi_desktop_bootstrap.py -n node1,node2 # This should start two MAGI daemon processes with names node1 and node2 # The script creates sample config files and uses them to run MAGI daemons # by default the config and logs files should be available under /tmp/<node_name> Step 4: Check if both MAGI daemons are running. \u00b6 $ magi_status.py -b node1 -n node1,node2 # This should at the end say \"Received reply back from all the required nodes\"","title":"MAGI Desktop"},{"location":"orchestrator/magi-desktop/#magi-desktop","text":"","title":"MAGI Desktop"},{"location":"orchestrator/magi-desktop/#step-1-install-the-following-dependencies","text":"The source code for all of these is available on Deter Ops ( users.deterlab.net ) under /share/magi/tarfiles/ : python-yaml python-pydot python-networkx py-matplotlib Mongo DB python-pymongo Yaml C Library (required for c-based agents, otherwise too helps improve performance) Mongo C Library (required only for c-based agents) All python packages can be installed using pip. Install pip, if not already available. curl -O https://bootstrap.pypa.io/get-pip.py python get-pip.py pip install pyyaml pip install pydot pip install networkx pip install matplotlib pip install pymongo","title":"Step 1: Install the following dependencies."},{"location":"orchestrator/magi-desktop/#step-2-download-and-install-magi","text":"$ git clone https://github.com/deter-project/magi.git $ cd magi $ sudo python setup.py install","title":"Step 2: Download and install MAGI"},{"location":"orchestrator/magi-desktop/#step-3-run-a-simple-two-node-magi-setup","text":"$ tools/magi_desktop_bootstrap.py -n node1,node2 # This should start two MAGI daemon processes with names node1 and node2 # The script creates sample config files and uses them to run MAGI daemons # by default the config and logs files should be available under /tmp/<node_name>","title":"Step 3: Run a simple two node MAGI setup."},{"location":"orchestrator/magi-desktop/#step-4-check-if-both-magi-daemons-are-running","text":"$ magi_status.py -b node1 -n node1,node2 # This should at the end say \"Received reply back from all the required nodes\"","title":"Step 4: Check if both MAGI daemons are running."},{"location":"orchestrator/magi-dev/","text":"MAGI Development Codebase \u00b6 The MAGI codebase is maintained in three branches. Current/Released : Stable released version Development : Stable with added features after the last release Test : Unstable To be able to work with the development branch, you will need to change the bootstrap command. Instead of: sudo python /share/magi/current/magi_bootstrap.py Use: sudo python /share/magi/dev/magi_bootstrap.py This will install MAGI from the development code base. Also, if you run MAGI tools from the Deter Ops ( users.deterlab.net ) machine, then make the tools point to the development code base: export PYTHONPATH=/share/magi/dev_src /share/magi/dev/magi_orchestrator.py -c bridgeNode -f eventsFile However, in case you use one of the experiment nodes to run MAGI tools, you can use them the same way as while running the current version of MAGI.","title":"MAGI Development Codebase"},{"location":"orchestrator/magi-dev/#magi-development-codebase","text":"The MAGI codebase is maintained in three branches. Current/Released : Stable released version Development : Stable with added features after the last release Test : Unstable To be able to work with the development branch, you will need to change the bootstrap command. Instead of: sudo python /share/magi/current/magi_bootstrap.py Use: sudo python /share/magi/dev/magi_bootstrap.py This will install MAGI from the development code base. Also, if you run MAGI tools from the Deter Ops ( users.deterlab.net ) machine, then make the tools point to the development code base: export PYTHONPATH=/share/magi/dev_src /share/magi/dev/magi_orchestrator.py -c bridgeNode -f eventsFile However, in case you use one of the experiment nodes to run MAGI tools, you can use them the same way as while running the current version of MAGI.","title":"MAGI Development Codebase"},{"location":"orchestrator/magi-tools/","text":"MAGI Tools \u00b6 The following tools are available for MAGI experiments in DETERLab: magi_status.py magi_graph.py magi_status.py : Check status of a MAGI experiment, reboot MAGI daemon, download logs \u00b6 Use magi_status.py to: check MAGI\u2019s status on experiment nodes reboot MAGI daemon process download logs from experiment nodes This tool is run for one experiment at a time. The user needs to provide the project name and the experiment name to the tool. This tool, by default, works for all of the nodes corresponding to the given experiment. However, it can be made to work with a restricted set of nodes, either by directly providing the set of interested nodes, or by providing an AAL (experiment procedure) file to fetch the set of desired nodes. This tool by default only informs if the MAGI daemon process on a node is reachable or not. Specific options can be used to fetch group membership details and information about active agents. If you want to reboot the MAGI daemon, magi_status.py first reboots MAGI daemon processes on the experiment nodes, and then fetches their status. If the tool is asked to download logs, it just does that, and does not fetch the status. Usage: magi_status.py [options] Script to get the status of MAGI daemon processes on experiment nodes, to reboot them if required, and to download logs. Options: -h, --help show this help message and exit -p PROJECT, --project=PROJECT Project name -e EXPERIMENT, --experiment=EXPERIMENT Experiment name -n NODES, --nodes=NODES Comma-separated list of the nodes to reboot MAGI daemon -a AAL, --aal=AAL The yaml-based procedure file to extract the list of nodes -l, --logs Fetch logs. The -o/--logoutdir option is applicable only when fetching logs. -o LOGOUTDIR, --logoutdir=LOGOUTDIR Store logs under the given directory. Default: /tmp -g, --groupmembership Fetch group membership detail -i, --agentinfo Fetch loaded agent information -t TIMEOUT, --timeout=TIMEOUT Number of seconds to wait to receive the status reply from the nodes on the overlay -r, --reboot Reboot nodes. The following options are applicable only when rebooting. -d DISTPATH, --distpath=DISTPATH Location of the distribution -U, --noupdate Do not update the system before installing MAGI -N, --noinstall Do not install MAGI and the supporting libraries magi_graph.py : Create graphs for a MAGI experiment \u00b6 magi_graph.py is a graph generator for experiments executed on DETERLab using MAGI. The tool fetches the required data using MAGI\u2019s data management layer and generates a graph in PNG format. This tool may be executed from either the DETER Ops machine or a remote computer with access to internet. The data to be plotted and other graph features are configurable. The various commandline options are as follows: Usage: magi_graph.py [options] Plots the graph for an experiment based on parameters provided. Experiment Configuration File OR Project and Experiment Name needs to be provided to be able to connect to the experiment. Need to provide build a graph specific configuration for plotting. Options: -h, --help show this help message and exit -e EXPERIMENT, --experiment=EXPERIMENT Experiment name -p PROJECT, --project=PROJECT Project name -x EXPERIMENTCONFIG, --experimentConfig=EXPERIMENTCONFIG Experiment configuration file -c CONFIG, --config=CONFIG Graph configuration file -a AGENT, --agent=AGENT Agent name. This is used to fetch available database fields -l AAL, --aal=AAL AAL (experiment procedure) file. This is also used to fetch available database fields -o OUTPUT, --output=OUTPUT Output graph file. Default: graph.png -t, --tunnel Tell the tool to tunnel request through Deter Ops (users.deterlab.net). -u USERNAME, --username=USERNAME Username for creating tunnel. Required only if different from current shell username. This tool expects the user to provide a configuration file. The format of the configuration file needs to be similar to the sample configuration file provided below. graph: type: line xLabel: Time(sec) yLabel: Bytes title: Traffic plot db: agent: monitor_agent filter: host: servernode peerNode: clientnode trafficDirection: out xValue: created yValue: bytes The configuration is divided into two parts a) Graph options and b) Database options. Graph options are used to configure the type of graph and the various labels. The database options help the tool fetch the data to be plotted. Each record stored in the database using MAGI\u2019s database layer has the following three fields along with any other that an agent populates. agent: Agent Name host: Node of which the agent is hosted created: Timestamp of when the record is created In the above mentioned example, data populated by the agent named \u201cmonitor_agent\u201d hosted on the node named \u201cservernode\u201d will be fetched. The data would further be filtered on the configured values of peerNode and trafficDirection, which are agent specific fields. Among the fetched data, values corresponding to the fields, created and bytes, will be plotted correspoding to the x and the y axis, respectively.","title":"MAGI Tools"},{"location":"orchestrator/magi-tools/#magi-tools","text":"The following tools are available for MAGI experiments in DETERLab: magi_status.py magi_graph.py","title":"MAGI Tools"},{"location":"orchestrator/magi-tools/#magi_statuspy-check-status-of-a-magi-experiment-reboot-magi-daemon-download-logs","text":"Use magi_status.py to: check MAGI\u2019s status on experiment nodes reboot MAGI daemon process download logs from experiment nodes This tool is run for one experiment at a time. The user needs to provide the project name and the experiment name to the tool. This tool, by default, works for all of the nodes corresponding to the given experiment. However, it can be made to work with a restricted set of nodes, either by directly providing the set of interested nodes, or by providing an AAL (experiment procedure) file to fetch the set of desired nodes. This tool by default only informs if the MAGI daemon process on a node is reachable or not. Specific options can be used to fetch group membership details and information about active agents. If you want to reboot the MAGI daemon, magi_status.py first reboots MAGI daemon processes on the experiment nodes, and then fetches their status. If the tool is asked to download logs, it just does that, and does not fetch the status. Usage: magi_status.py [options] Script to get the status of MAGI daemon processes on experiment nodes, to reboot them if required, and to download logs. Options: -h, --help show this help message and exit -p PROJECT, --project=PROJECT Project name -e EXPERIMENT, --experiment=EXPERIMENT Experiment name -n NODES, --nodes=NODES Comma-separated list of the nodes to reboot MAGI daemon -a AAL, --aal=AAL The yaml-based procedure file to extract the list of nodes -l, --logs Fetch logs. The -o/--logoutdir option is applicable only when fetching logs. -o LOGOUTDIR, --logoutdir=LOGOUTDIR Store logs under the given directory. Default: /tmp -g, --groupmembership Fetch group membership detail -i, --agentinfo Fetch loaded agent information -t TIMEOUT, --timeout=TIMEOUT Number of seconds to wait to receive the status reply from the nodes on the overlay -r, --reboot Reboot nodes. The following options are applicable only when rebooting. -d DISTPATH, --distpath=DISTPATH Location of the distribution -U, --noupdate Do not update the system before installing MAGI -N, --noinstall Do not install MAGI and the supporting libraries","title":"magi_status.py: Check status of a MAGI experiment, reboot MAGI daemon, download logs"},{"location":"orchestrator/magi-tools/#magi_graphpy-create-graphs-for-a-magi-experiment","text":"magi_graph.py is a graph generator for experiments executed on DETERLab using MAGI. The tool fetches the required data using MAGI\u2019s data management layer and generates a graph in PNG format. This tool may be executed from either the DETER Ops machine or a remote computer with access to internet. The data to be plotted and other graph features are configurable. The various commandline options are as follows: Usage: magi_graph.py [options] Plots the graph for an experiment based on parameters provided. Experiment Configuration File OR Project and Experiment Name needs to be provided to be able to connect to the experiment. Need to provide build a graph specific configuration for plotting. Options: -h, --help show this help message and exit -e EXPERIMENT, --experiment=EXPERIMENT Experiment name -p PROJECT, --project=PROJECT Project name -x EXPERIMENTCONFIG, --experimentConfig=EXPERIMENTCONFIG Experiment configuration file -c CONFIG, --config=CONFIG Graph configuration file -a AGENT, --agent=AGENT Agent name. This is used to fetch available database fields -l AAL, --aal=AAL AAL (experiment procedure) file. This is also used to fetch available database fields -o OUTPUT, --output=OUTPUT Output graph file. Default: graph.png -t, --tunnel Tell the tool to tunnel request through Deter Ops (users.deterlab.net). -u USERNAME, --username=USERNAME Username for creating tunnel. Required only if different from current shell username. This tool expects the user to provide a configuration file. The format of the configuration file needs to be similar to the sample configuration file provided below. graph: type: line xLabel: Time(sec) yLabel: Bytes title: Traffic plot db: agent: monitor_agent filter: host: servernode peerNode: clientnode trafficDirection: out xValue: created yValue: bytes The configuration is divided into two parts a) Graph options and b) Database options. Graph options are used to configure the type of graph and the various labels. The database options help the tool fetch the data to be plotted. Each record stored in the database using MAGI\u2019s database layer has the following three fields along with any other that an agent populates. agent: Agent Name host: Node of which the agent is hosted created: Timestamp of when the record is created In the above mentioned example, data populated by the agent named \u201cmonitor_agent\u201d hosted on the node named \u201cservernode\u201d will be fetched. The data would further be filtered on the configured values of peerNode and trafficDirection, which are agent specific fields. Among the fetched data, values corresponding to the fields, created and bytes, will be plotted correspoding to the x and the y axis, respectively.","title":"magi_graph.py: Create graphs for a MAGI experiment"},{"location":"orchestrator/orchestrator-case-studies/","text":"Orchestrator Case Studies \u00b6 This section includes more detailed descriptions of how to conduct an experiment in DETERLab using MAGI Orchestrator. Each case study also includes a complete archive with logs and data files. Before you try out the examples below, we recommend reading the MAGI Orchestrator Guide . Simple Client Server \u00b6 Scaled Client Server \u00b6 Feedback \u00b6","title":"Orchestrator Case Studies"},{"location":"orchestrator/orchestrator-case-studies/#orchestrator-case-studies","text":"This section includes more detailed descriptions of how to conduct an experiment in DETERLab using MAGI Orchestrator. Each case study also includes a complete archive with logs and data files. Before you try out the examples below, we recommend reading the MAGI Orchestrator Guide .","title":"Orchestrator Case Studies"},{"location":"orchestrator/orchestrator-case-studies/#simple-client-server","text":"","title":"Simple Client Server"},{"location":"orchestrator/orchestrator-case-studies/#scaled-client-server","text":"","title":"Scaled Client Server"},{"location":"orchestrator/orchestrator-case-studies/#feedback","text":"","title":"Feedback"},{"location":"orchestrator/orchestrator-config/","text":"MAGI Configuration \u00b6 Experiment Configuration \u00b6 The MAGI experiment-wide configuration ( experiment.conf ) is a YAML-based configuration file. A customized experiment.conf can be provided to the MAGI bootstrap process. magi_bootstrap.py --expconf /path/to/experiment.conf It is optional for a user to provide one. In case it is not provided, the bootstrap process creates a default configuration file. Also, in cases where a user needs to customize a part of the configuration, the user can provide an experiment configuration file with only the parameters that need to be customized. The MAGI configuration validation process would update the user provided configuration to fill in the missing configuration. The experiment wide configuration file consists of three sections. MesDL : Messaging overlay configuration DBDL : Data layer configuration ExpDL : Other experiment information MesDL: Configure the MAGI Control Overlay \u00b6 The MAGI bootstrap process establishes a networking overlay over which it communicates with the experiment nodes. The control overlay provides a control plane across heterogeneous resources, such as containers, federation, and specialized hardware, on the DETERLab testbed. The control overlay provides a network to reliably propagate discrete control events across the experiment. Hence, it is critical to establish a robust and scalable overlay to ensure the experiments are orchestrated correctly. MAGI provides multi-point to multi-point group communication. A \u201cgroup\u201d has a set of nodes as members. Each group has a logical name. Any member of the group can send a message to any other member of the group. By default, one MAGI overlay is established for the experiment and all the control messages are passed over it. However, it may be necessary to establish two or more overlays based on the experiment topology embedding, control and data management requirements. The required overlay structure can be explicitly specified during the experiment bootstrap process. The overlay configuration, Messaging Description Language (MesDL), is a configuration of the required overlays and bridges in the experiment. The MesDL section of experiment.conf defines all the overlays and bridges for the experiment. The MAGI \u201coverlay\u201d is built on top of the control network on the DETER testbed. Nodes in the overlay can be thought of as being connected by virtual point-to-point links. The overlay provides a way to communicate with all the MAGI-enabled nodes across the testbed boundaries and even over the internet. \u201cbridges\u201d act as meeting places for all the members of the overlay it serves. For example: # Establish two bridges on node3. # Typically one is inward facing towards the experiment and one # is outward facing towards the experimenter to use with magi tools # magi tools can now connect to the experiment overlay through node3, # port 18808 bridges: - { TCPServer: node3, port: 18808 } - { TCPServer: node3, port: 28808 } # Establish two overlays for the experiment. overlay: # Members of this overlay rendezvous at node3. All experiment nodes # are part of this overlay. - { type: TCPTransport, members: [ '__ALL__' ], server: node3, port: 28808 } # Members of this overlay rendezvous at node2. # node4 and node7 are members of this overlay. - { type: TCPTransport, members: [ 'node4', 'node7' ], server: node2, port: 48808 } In the absence of the MesDL, MAGI creates a single overlay with all the experiment nodes as members. MAGI establishes a bridge node based on the following. Note MAGI establishes one of the nodes in the experiment as a bridge node. MAGI selects the bridge node based on the following rules: If there is a node named \u201ccontrol\u201d, it uses that node as the bridge node. If not, it establishes the node with the lowest alphanumeric node name as the bridge node. MAGI expects the selected bridge node to be MAGI-enabled. However, if that is not the case, the user must provide a custom MesDL. For example, here is the MesDL section of experiment.conf for the Client-Server tutorial mesdl: bridges: - {server: clientnode.clientserver.montage, type: TCPServer, port: 18808} - {server: clientnode.clientserver.montage, type: TCPServer, port: 28808} overlay: - members: [__ALL__] port: 28808 server: clientnode.clientserver.montage type: TCPTransport DBDL: Configure the MAGI Data Management Layer \u00b6 The database configuration contains the following parameters: dbdl: #whether the database management layer is enabled or not isDBEnabled: true #whether the database is sharded or not isDBSharded: false # mapping of sensor/data producer nodes to data collector nodes # __DEFAULT__ maps to the default collector node # if the user does not provide one, the first node in the alpha-numerically # sorted list of collectors is selected as the default collector # In case some of the sensor nodes are not mapped to a collector node, # MAGI would map them to the default collector node sensorToCollectorMap: {node1: node1, node2: node1, __DEFAULT__: node3} # the port at which the collectors listen collectorPort: 27018 #if sharded, where does the global server run globalServerHost: node-1 globalServerPort: 27017 By default, MAGI setups an unsharded setup with a centralized collector. All the sensors collect data at the same collector. However, an experimenter can choose to configure the database whichever way. All the collector nodes need to be MAGI-enabled. The data manager configuration needs to have a collector mapping for each of the MAGI-enabled experiment nodes. If one is not provided, any sensors on a node that does not have a mapped collector, would end up collecting at the default collector. The Data Management section has more information about MAGI\u2019s data management layer. Note The ports are not configurable, they are only for informational purpose. ExpDL: Other common experiment wide configuration \u00b6 The ExpDL part of the experiment configuration file contains common experiment-wide configuration that is used by the various MAGI tools. Most of the configuration in this section is automatically generated. A user can override the default AAL file location and the default directories for config files, log files, database files and temporary files. expdl: aal: /proj/montage/exp/clientserver/procedure.aal nodePaths: {config: /var/log/magi, db: /var/lib/mongodb, logs: /var/log/magi, temp: /tmp} distributionPath: /share/magi/dev experimentName: clientserver projectName: montage nodeList: [clientnode, servernode] testbedPaths: {experimentDir: /proj/montage/exp/clientserver} Node Configuration \u00b6 The MAGI daemon process runs using a node-specific configuration ( node.conf ). The experiment wide configuration ( experiment.conf ) is converted into a node specific configuration as part of the bootstrap process. The node configuration file contains some additional configuration apart from all the node specific configuration from the experiment wide configuration. A customized node.conf can be provided as an input to the MAGI bootstrap script, or to the MAGI daemon startup script, directly. Similar to the experiment.conf , a user can provide an incomplete configuration file, containing only customized parameters. The MAGI configuration validation process would fill in the missing configuration. transports: - {address: 0.0.0.0, class: TCPServer, port: 18808} - {address: 0.0.0.0, class: TCPServer, port: 28808} database: isDBEnabled: true configHost: clientnode sensorToCollectorMap: {clientnode: clientnode, servernode: servernode} localInfo: configDir: /var/log/magi logDir: /var/log/magi tempDir: /tmp dbDir: /var/lib/mongodb architecture: 32bit controlif: eth0 controlip: 172.16.111.95 distribution: Ubuntu 10.04 (lucid) nodename: clientnode hostname: clientnode.clientserver.montage.isi.deterlab.net interfaceInfo: eth1: expif: eth1 expip: 10.0.0.1 expmac: 00:00:00:00:00:01 linkname: link peernodes: [servernode] software: - {dir: /share/magi/dev/Linux-Ubuntu10.04-i686, type: rpmfile} - {dir: /share/magi/dev/Linux-Ubuntu10.04-i686, type: archive} - {type: apt} - {dir: /share/magi/dev/source, type: source} - {dir: /tmp/src, type: source} Bootstrap Process \u00b6 The magi_bootstrap.py is used to configure the overlay and start MAGI on the experiment nodes. The magi_bootstrap.py tool is typically called when the node starts as follows: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\" The magi bootstrap script can be used to install, configure, and start MAGI. The various command line options are as follows. Usage: magi_bootstrap.py [options] Bootstrap script that can be used to install, configure, and start MAGI Options: -h, --help show this help message and exit -p RPATH, --path=RPATH Location of the distribution -U, --noupdate Do not update the system before installing Magi -N, --noinstall Do not install magi and the supporting libraries -v, --verbose Include debugging information -e EXPCONF, --expconf=EXPCONF Path to the experiment wide configuration file -c NODECONF, --nodeconf=NODECONF Path to the node specific configuration file. Cannot use along with -f (see below) -n NODEDIR, --nodedir=NODEDIR Directory to put MAGI daemon specific files -f, --force Recreate node configuration file, even if present. Cannot use along with -c (see above) -D, --nodataman Do not install and setup data manager -o LOGFILE, --logfile=LOGFILE Log file. Default: /tmp/magi_bootstrap.log","title":"MAGI Configuration"},{"location":"orchestrator/orchestrator-config/#magi-configuration","text":"","title":"MAGI Configuration"},{"location":"orchestrator/orchestrator-config/#experiment-configuration","text":"The MAGI experiment-wide configuration ( experiment.conf ) is a YAML-based configuration file. A customized experiment.conf can be provided to the MAGI bootstrap process. magi_bootstrap.py --expconf /path/to/experiment.conf It is optional for a user to provide one. In case it is not provided, the bootstrap process creates a default configuration file. Also, in cases where a user needs to customize a part of the configuration, the user can provide an experiment configuration file with only the parameters that need to be customized. The MAGI configuration validation process would update the user provided configuration to fill in the missing configuration. The experiment wide configuration file consists of three sections. MesDL : Messaging overlay configuration DBDL : Data layer configuration ExpDL : Other experiment information","title":"Experiment Configuration"},{"location":"orchestrator/orchestrator-config/#mesdl-configure-the-magi-control-overlay","text":"The MAGI bootstrap process establishes a networking overlay over which it communicates with the experiment nodes. The control overlay provides a control plane across heterogeneous resources, such as containers, federation, and specialized hardware, on the DETERLab testbed. The control overlay provides a network to reliably propagate discrete control events across the experiment. Hence, it is critical to establish a robust and scalable overlay to ensure the experiments are orchestrated correctly. MAGI provides multi-point to multi-point group communication. A \u201cgroup\u201d has a set of nodes as members. Each group has a logical name. Any member of the group can send a message to any other member of the group. By default, one MAGI overlay is established for the experiment and all the control messages are passed over it. However, it may be necessary to establish two or more overlays based on the experiment topology embedding, control and data management requirements. The required overlay structure can be explicitly specified during the experiment bootstrap process. The overlay configuration, Messaging Description Language (MesDL), is a configuration of the required overlays and bridges in the experiment. The MesDL section of experiment.conf defines all the overlays and bridges for the experiment. The MAGI \u201coverlay\u201d is built on top of the control network on the DETER testbed. Nodes in the overlay can be thought of as being connected by virtual point-to-point links. The overlay provides a way to communicate with all the MAGI-enabled nodes across the testbed boundaries and even over the internet. \u201cbridges\u201d act as meeting places for all the members of the overlay it serves. For example: # Establish two bridges on node3. # Typically one is inward facing towards the experiment and one # is outward facing towards the experimenter to use with magi tools # magi tools can now connect to the experiment overlay through node3, # port 18808 bridges: - { TCPServer: node3, port: 18808 } - { TCPServer: node3, port: 28808 } # Establish two overlays for the experiment. overlay: # Members of this overlay rendezvous at node3. All experiment nodes # are part of this overlay. - { type: TCPTransport, members: [ '__ALL__' ], server: node3, port: 28808 } # Members of this overlay rendezvous at node2. # node4 and node7 are members of this overlay. - { type: TCPTransport, members: [ 'node4', 'node7' ], server: node2, port: 48808 } In the absence of the MesDL, MAGI creates a single overlay with all the experiment nodes as members. MAGI establishes a bridge node based on the following. Note MAGI establishes one of the nodes in the experiment as a bridge node. MAGI selects the bridge node based on the following rules: If there is a node named \u201ccontrol\u201d, it uses that node as the bridge node. If not, it establishes the node with the lowest alphanumeric node name as the bridge node. MAGI expects the selected bridge node to be MAGI-enabled. However, if that is not the case, the user must provide a custom MesDL. For example, here is the MesDL section of experiment.conf for the Client-Server tutorial mesdl: bridges: - {server: clientnode.clientserver.montage, type: TCPServer, port: 18808} - {server: clientnode.clientserver.montage, type: TCPServer, port: 28808} overlay: - members: [__ALL__] port: 28808 server: clientnode.clientserver.montage type: TCPTransport","title":"MesDL: Configure the MAGI Control Overlay"},{"location":"orchestrator/orchestrator-config/#dbdl-configure-the-magi-data-management-layer","text":"The database configuration contains the following parameters: dbdl: #whether the database management layer is enabled or not isDBEnabled: true #whether the database is sharded or not isDBSharded: false # mapping of sensor/data producer nodes to data collector nodes # __DEFAULT__ maps to the default collector node # if the user does not provide one, the first node in the alpha-numerically # sorted list of collectors is selected as the default collector # In case some of the sensor nodes are not mapped to a collector node, # MAGI would map them to the default collector node sensorToCollectorMap: {node1: node1, node2: node1, __DEFAULT__: node3} # the port at which the collectors listen collectorPort: 27018 #if sharded, where does the global server run globalServerHost: node-1 globalServerPort: 27017 By default, MAGI setups an unsharded setup with a centralized collector. All the sensors collect data at the same collector. However, an experimenter can choose to configure the database whichever way. All the collector nodes need to be MAGI-enabled. The data manager configuration needs to have a collector mapping for each of the MAGI-enabled experiment nodes. If one is not provided, any sensors on a node that does not have a mapped collector, would end up collecting at the default collector. The Data Management section has more information about MAGI\u2019s data management layer. Note The ports are not configurable, they are only for informational purpose.","title":"DBDL: Configure the MAGI Data Management Layer"},{"location":"orchestrator/orchestrator-config/#expdl-other-common-experiment-wide-configuration","text":"The ExpDL part of the experiment configuration file contains common experiment-wide configuration that is used by the various MAGI tools. Most of the configuration in this section is automatically generated. A user can override the default AAL file location and the default directories for config files, log files, database files and temporary files. expdl: aal: /proj/montage/exp/clientserver/procedure.aal nodePaths: {config: /var/log/magi, db: /var/lib/mongodb, logs: /var/log/magi, temp: /tmp} distributionPath: /share/magi/dev experimentName: clientserver projectName: montage nodeList: [clientnode, servernode] testbedPaths: {experimentDir: /proj/montage/exp/clientserver}","title":"ExpDL: Other common experiment wide configuration"},{"location":"orchestrator/orchestrator-config/#node-configuration","text":"The MAGI daemon process runs using a node-specific configuration ( node.conf ). The experiment wide configuration ( experiment.conf ) is converted into a node specific configuration as part of the bootstrap process. The node configuration file contains some additional configuration apart from all the node specific configuration from the experiment wide configuration. A customized node.conf can be provided as an input to the MAGI bootstrap script, or to the MAGI daemon startup script, directly. Similar to the experiment.conf , a user can provide an incomplete configuration file, containing only customized parameters. The MAGI configuration validation process would fill in the missing configuration. transports: - {address: 0.0.0.0, class: TCPServer, port: 18808} - {address: 0.0.0.0, class: TCPServer, port: 28808} database: isDBEnabled: true configHost: clientnode sensorToCollectorMap: {clientnode: clientnode, servernode: servernode} localInfo: configDir: /var/log/magi logDir: /var/log/magi tempDir: /tmp dbDir: /var/lib/mongodb architecture: 32bit controlif: eth0 controlip: 172.16.111.95 distribution: Ubuntu 10.04 (lucid) nodename: clientnode hostname: clientnode.clientserver.montage.isi.deterlab.net interfaceInfo: eth1: expif: eth1 expip: 10.0.0.1 expmac: 00:00:00:00:00:01 linkname: link peernodes: [servernode] software: - {dir: /share/magi/dev/Linux-Ubuntu10.04-i686, type: rpmfile} - {dir: /share/magi/dev/Linux-Ubuntu10.04-i686, type: archive} - {type: apt} - {dir: /share/magi/dev/source, type: source} - {dir: /tmp/src, type: source}","title":"Node Configuration"},{"location":"orchestrator/orchestrator-config/#bootstrap-process","text":"The magi_bootstrap.py is used to configure the overlay and start MAGI on the experiment nodes. The magi_bootstrap.py tool is typically called when the node starts as follows: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\" The magi bootstrap script can be used to install, configure, and start MAGI. The various command line options are as follows. Usage: magi_bootstrap.py [options] Bootstrap script that can be used to install, configure, and start MAGI Options: -h, --help show this help message and exit -p RPATH, --path=RPATH Location of the distribution -U, --noupdate Do not update the system before installing Magi -N, --noinstall Do not install magi and the supporting libraries -v, --verbose Include debugging information -e EXPCONF, --expconf=EXPCONF Path to the experiment wide configuration file -c NODECONF, --nodeconf=NODECONF Path to the node specific configuration file. Cannot use along with -f (see below) -n NODEDIR, --nodedir=NODEDIR Directory to put MAGI daemon specific files -f, --force Recreate node configuration file, even if present. Cannot use along with -c (see above) -D, --nodataman Do not install and setup data manager -o LOGFILE, --logfile=LOGFILE Log file. Default: /tmp/magi_bootstrap.log","title":"Bootstrap Process"},{"location":"orchestrator/orchestrator-guide/","text":"Orchestrator Guide \u00b6 In this tutorial we walk you through setting up a basic orchestrated experiment. This page also includes common advanced topics. Detailed descriptions of the commands and configuration files are available in the reference section . Note If you are a student, go to the education.deterlab.net site for classroom-specific instructions. Basic MAGI Tutorial \u00b6 In this tutorial, we demonstrate how to set up client and server traffic generators with only one server and one client. (For more complex examples, see the Case Studies .) The basic steps in creating an orchestrated experiment are: Write the AAL file that describes the experiment's workflows. Include a special start command in your topology. Create or use a physical experiment in DETERLab. Run the Orchestrator tool on a physical experiment on users.deterlab.net . The following sections describe each step in detail. Step 1. Write the AAL file \u00b6 Describe the experiment procedure (ie, workflow) in an AAL (.aal) file. First we'll cover the parts of an AAL file and then we'll walk through writing the AAL file for this tutorial (we also provide the AAL file itself). AAL File Overview \u00b6 Agent Activation Language (AAL) is a YAML-based descriptive language that describes an experiment\u2019s workflow. It identifies groups (nodes with similar behaviors), agents (a set of behaviors that may be invoked as events) and events/triggers (the different things you want agents to do and the things that trigger them). An AAL specification has mainly three parts: groups, agents and event streams. Groups \u00b6 Groups define sets of one or more nodes with the same behavior and enable a coupling between the experiment procedure and the experiment nodes. Example: groups: clients: [ node1, node2, node7 ] defender: [ router3 ] Agents \u00b6 Agents map a functional behavior onto a set of experiment nodes. An agent provides a set of behaviors that may be invoked with events. The agent directive requires the following keys: agent Name for the set of nodes that represent the behavior group A set of experiment nodes the will functional as the agent path The path to the agent implementation code code Directory name of the agent implementation. The code directive is used in the absence of a path directive. MAGI requires the agent implementation to be part of the python distribution if the path directive is not specified. execargs Zero or more arguments that will be passed to the agent during initialization. Example: agents: smallwebclient: group: smallclients path: /share/magi/modules/http_client/http_client.tar.gz execargs: { servers: [ servernode ], interval: '2', sizes: 'minmax(300,500)'} Event Streams \u00b6 Event Streams are lists of events and triggers that are parsed and executed by the Orchestrator tool. A procedure typically contains multiple event streams. Different event streams execute concurrently and are synchronized with each other using triggers. The set of event streams listed using the streamstarts directive are invoked at the start of procedure. However, note that the Orchestrator will perform several setup actions, such as create groups, load agents, get status, before the event streams start. Events \u00b6 Events invoke a procedure implemented in an agent. An event is sent to a group. An event directive requires the following keys: agent :: The agent to send the event. method :: The method to be invoked. args :: Zero or more arguments required by the method. Additionally, it may also contain the trigger key to flag the return status from the method. The return status may be either True or False . Example: - type: event agent: server_agent method: startServer trigger: serverStartedSuccessfully args: {} Triggers \u00b6 Triggers are used as a synchronization mechanism, guard points, or rendezvous points in an experiment procedure. There are two types of triggers that may be combined in several different ways: Event-based triggers are received from agents after a method. The serverStartedSuccessfully is an example of an event-based trigger. The Orchestrator keeps track of outstanding triggers to follow the experiment execution state space. When the server_agent returns True after the method startServer , the Orchestrator tags it as a received trigger. Example: - type: trigger triggers: [ {event: ClientStopped} ] Time-based triggers wait for a specified amount time to elapse at the Orchestrator before proceeding. Example: - type: trigger triggers: { [ timeout: 60000 ] } # wait for 60 seconds You may find several specific examples of declaring groups, agents, events, and triggers in the Case Studies . For this basic tutorial, save this code to a file named procedure.aal and save it to the experiment folder. Our AAL Example \u00b6 Now we'll write an AAL that demonstrates three aspects of MAGI: specifying multiple event streams, synchronizing with triggers, and a special target called exit to unload agents. Event Streams \u00b6 This example has three events streams; the server stream, the client stream, and the cleanup stream. The coordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers (such as after the server has started) or time-based triggers (such as wait for 30 seconds). The triggers are indicated as wait states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above. Server Stream \u00b6 The server event stream consists of three states. The start state generates a trigger, called serverStarted , once the server agent is activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, when the server is deactivated or terminated. Here is the relevant AAL description: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ {event: ClientStopped} ] - type: event agent: server_agent method: stopServer trigger: ServerStopped args: {} Client Stream \u00b6 The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger, the client agent is activated in the start state. Next, the client stream waits for a period of time and then terminates the client agents in the stop state. On termination, the client agents send a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated. Here is the relevant AAL description: clientstream: - type: trigger triggers: [ {event: ServerStarted} ] - type: event agent: client_agent method: startClient args: {} - type: trigger triggers: [ {timeout: 60000} ] - type: event agent: client_agent method: stopClient trigger: clientStopped args: {} Cleanup Stream \u00b6 The last event stream, the cleanup stream consists of two states. First, it waits for all the servers to stop. Then it enters the exit state. The exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator. It causes the Orchestrator to send agent unload and disband group messages to all of the experiment nodes and then it exits the Orchestrator. Here is the relevant AAL code: cleanup: - type: trigger triggers: [ {event: ServerStopped, target: exit} ] You can see all of the code together in this file: casestudy_clientserver.aal . Step 2: Swap in the the physical experiment using topology with MAGI start command \u00b6 Swap in the experiment using this network description file: casestudy_clientserver.tcl . This start command installs MAGI and supporting tools on all nodes at startup. The normal syntax is as follows: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\" where $NodeName is the control node. If you look at this file, you'll see the MAGI start command is added as a variable and then used for two nodes: the clientnode and servernode. In this example, we set the start command as a variable: set magi_start \"sudo python /share/magi/current/magi_bootstrap.py\" and then use it Step 2: Set up your environment \u00b6 Set up environment variables for your environment, replacing the value for myExp with your experiment name and myProj with your project name. PROJ=myExp EXP=myProj AAL=casestudy_clientserver.aal Create/Use an experiment in DETERLab \u00b6 MAGI needs to be enabled on a new or existing swapped-in DETERLab experiment (via interface or using startexp on the commandline). You will need its Experiment Name and Project Name when you run the Orchestrator in the next step. Make sure you\u2019ve swapped-in resources before the next step. Run the magi_orchestrator.py tool \u00b6 The MAGI Orchestrator tool, magi_orchestrator.py , is a tool that reads the procedure's AAL file and orchestrates an experiment based on the specified procedures. The Orchestrator does the following in this order: Joins Groups - The Orchestrator iterates over the list of groups and for each group sends a request to all the mapped nodes to join the group. A corresponding reply adds the replier to the group. Messages addressed to a group are sent to all the nodes that are part of the group. Loads Agents - The Orchestrator iterates over the list of agents and for each agent sends an agent load request to the mapped groups. An agent load message tells the Orchestrator to start an instance of the agent implementation and to put it in a listening mode, where the instance waits for further messages. Executes Event Streams - Next, the Orchestrator concurrently executes all the event streams listed as part of streamstarts . The Orchestrator has a predefined event stream called exit . The purpose of this event stream is to unload all the agents and disjoin groups. All workflows should end with executing this stream for a clean exit. From your home directory on users.deterlab.net , run the following command: /share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal where: clientnode equals the node you want to start with myExp is the Experiment Name myProj is the Project Name procedural.aal is the name of the AAL file. The various command line options are as follows Usage: magi_orchestrator.py [options] Options: -h, --help show this help message and exit -c CONTROL, --control=CONTROL The control node to connect to (i.e. control.exp.proj) -f EVENTS, --events=EVENTS The events.aal file(s) to use. Can be specified multiple times for multiple AAL files -l LOGLEVEL, --loglevel=LOGLEVEL The level at which to log. Must be one of none, debug, info, warning, error, or critical. Default is info. -o LOGFILE, --logfile=LOGFILE If given, log to the file instead of the console (stdout). -e EXITONFAILURE, --exitOnFailure=EXITONFAILURE If any method call fails (returns False), then exit all streams, unload all agents, and exit the orchestrator. Default value is True -g GROUPBUILDTIMEOUT, --groupBuildTimeout=GROUPBUILDTIMEOUT When building the initial groups for agents in the given AAL, use the timeout given (in milliseconds) when waiting for group formation to complete. --nocolor If given, do not use color in output. -v, --verbose Tell orchestrator to print info about what its doing -n, --tunnel Tell orchestrator to tunnel data through Deter Ops (users.deterlab.net).","title":"Orchestrator Guide"},{"location":"orchestrator/orchestrator-guide/#orchestrator-guide","text":"In this tutorial we walk you through setting up a basic orchestrated experiment. This page also includes common advanced topics. Detailed descriptions of the commands and configuration files are available in the reference section . Note If you are a student, go to the education.deterlab.net site for classroom-specific instructions.","title":"Orchestrator Guide"},{"location":"orchestrator/orchestrator-guide/#basic-magi-tutorial","text":"In this tutorial, we demonstrate how to set up client and server traffic generators with only one server and one client. (For more complex examples, see the Case Studies .) The basic steps in creating an orchestrated experiment are: Write the AAL file that describes the experiment's workflows. Include a special start command in your topology. Create or use a physical experiment in DETERLab. Run the Orchestrator tool on a physical experiment on users.deterlab.net . The following sections describe each step in detail.","title":"Basic MAGI Tutorial"},{"location":"orchestrator/orchestrator-guide/#step-1-write-the-aal-file","text":"Describe the experiment procedure (ie, workflow) in an AAL (.aal) file. First we'll cover the parts of an AAL file and then we'll walk through writing the AAL file for this tutorial (we also provide the AAL file itself).","title":"Step 1. Write the AAL file"},{"location":"orchestrator/orchestrator-guide/#aal-file-overview","text":"Agent Activation Language (AAL) is a YAML-based descriptive language that describes an experiment\u2019s workflow. It identifies groups (nodes with similar behaviors), agents (a set of behaviors that may be invoked as events) and events/triggers (the different things you want agents to do and the things that trigger them). An AAL specification has mainly three parts: groups, agents and event streams.","title":"AAL File Overview"},{"location":"orchestrator/orchestrator-guide/#groups","text":"Groups define sets of one or more nodes with the same behavior and enable a coupling between the experiment procedure and the experiment nodes. Example: groups: clients: [ node1, node2, node7 ] defender: [ router3 ]","title":"Groups"},{"location":"orchestrator/orchestrator-guide/#agents","text":"Agents map a functional behavior onto a set of experiment nodes. An agent provides a set of behaviors that may be invoked with events. The agent directive requires the following keys: agent Name for the set of nodes that represent the behavior group A set of experiment nodes the will functional as the agent path The path to the agent implementation code code Directory name of the agent implementation. The code directive is used in the absence of a path directive. MAGI requires the agent implementation to be part of the python distribution if the path directive is not specified. execargs Zero or more arguments that will be passed to the agent during initialization. Example: agents: smallwebclient: group: smallclients path: /share/magi/modules/http_client/http_client.tar.gz execargs: { servers: [ servernode ], interval: '2', sizes: 'minmax(300,500)'}","title":"Agents"},{"location":"orchestrator/orchestrator-guide/#event-streams","text":"Event Streams are lists of events and triggers that are parsed and executed by the Orchestrator tool. A procedure typically contains multiple event streams. Different event streams execute concurrently and are synchronized with each other using triggers. The set of event streams listed using the streamstarts directive are invoked at the start of procedure. However, note that the Orchestrator will perform several setup actions, such as create groups, load agents, get status, before the event streams start.","title":"Event Streams"},{"location":"orchestrator/orchestrator-guide/#events","text":"Events invoke a procedure implemented in an agent. An event is sent to a group. An event directive requires the following keys: agent :: The agent to send the event. method :: The method to be invoked. args :: Zero or more arguments required by the method. Additionally, it may also contain the trigger key to flag the return status from the method. The return status may be either True or False . Example: - type: event agent: server_agent method: startServer trigger: serverStartedSuccessfully args: {}","title":"Events"},{"location":"orchestrator/orchestrator-guide/#triggers","text":"Triggers are used as a synchronization mechanism, guard points, or rendezvous points in an experiment procedure. There are two types of triggers that may be combined in several different ways: Event-based triggers are received from agents after a method. The serverStartedSuccessfully is an example of an event-based trigger. The Orchestrator keeps track of outstanding triggers to follow the experiment execution state space. When the server_agent returns True after the method startServer , the Orchestrator tags it as a received trigger. Example: - type: trigger triggers: [ {event: ClientStopped} ] Time-based triggers wait for a specified amount time to elapse at the Orchestrator before proceeding. Example: - type: trigger triggers: { [ timeout: 60000 ] } # wait for 60 seconds You may find several specific examples of declaring groups, agents, events, and triggers in the Case Studies . For this basic tutorial, save this code to a file named procedure.aal and save it to the experiment folder.","title":"Triggers"},{"location":"orchestrator/orchestrator-guide/#our-aal-example","text":"Now we'll write an AAL that demonstrates three aspects of MAGI: specifying multiple event streams, synchronizing with triggers, and a special target called exit to unload agents.","title":"Our AAL Example"},{"location":"orchestrator/orchestrator-guide/#event-streams_1","text":"This example has three events streams; the server stream, the client stream, and the cleanup stream. The coordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers (such as after the server has started) or time-based triggers (such as wait for 30 seconds). The triggers are indicated as wait states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above.","title":"Event Streams"},{"location":"orchestrator/orchestrator-guide/#server-stream","text":"The server event stream consists of three states. The start state generates a trigger, called serverStarted , once the server agent is activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, when the server is deactivated or terminated. Here is the relevant AAL description: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ {event: ClientStopped} ] - type: event agent: server_agent method: stopServer trigger: ServerStopped args: {}","title":"Server Stream"},{"location":"orchestrator/orchestrator-guide/#client-stream","text":"The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger, the client agent is activated in the start state. Next, the client stream waits for a period of time and then terminates the client agents in the stop state. On termination, the client agents send a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated. Here is the relevant AAL description: clientstream: - type: trigger triggers: [ {event: ServerStarted} ] - type: event agent: client_agent method: startClient args: {} - type: trigger triggers: [ {timeout: 60000} ] - type: event agent: client_agent method: stopClient trigger: clientStopped args: {}","title":"Client Stream"},{"location":"orchestrator/orchestrator-guide/#cleanup-stream","text":"The last event stream, the cleanup stream consists of two states. First, it waits for all the servers to stop. Then it enters the exit state. The exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator. It causes the Orchestrator to send agent unload and disband group messages to all of the experiment nodes and then it exits the Orchestrator. Here is the relevant AAL code: cleanup: - type: trigger triggers: [ {event: ServerStopped, target: exit} ] You can see all of the code together in this file: casestudy_clientserver.aal .","title":"Cleanup Stream"},{"location":"orchestrator/orchestrator-guide/#step-2-swap-in-the-the-physical-experiment-using-topology-with-magi-start-command","text":"Swap in the experiment using this network description file: casestudy_clientserver.tcl . This start command installs MAGI and supporting tools on all nodes at startup. The normal syntax is as follows: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\" where $NodeName is the control node. If you look at this file, you'll see the MAGI start command is added as a variable and then used for two nodes: the clientnode and servernode. In this example, we set the start command as a variable: set magi_start \"sudo python /share/magi/current/magi_bootstrap.py\" and then use it","title":"Step 2: Swap in the the physical experiment using topology with MAGI start command"},{"location":"orchestrator/orchestrator-guide/#step-2-set-up-your-environment","text":"Set up environment variables for your environment, replacing the value for myExp with your experiment name and myProj with your project name. PROJ=myExp EXP=myProj AAL=casestudy_clientserver.aal","title":"Step 2: Set up your environment"},{"location":"orchestrator/orchestrator-guide/#createuse-an-experiment-in-deterlab","text":"MAGI needs to be enabled on a new or existing swapped-in DETERLab experiment (via interface or using startexp on the commandline). You will need its Experiment Name and Project Name when you run the Orchestrator in the next step. Make sure you\u2019ve swapped-in resources before the next step.","title":"Create/Use an experiment in DETERLab"},{"location":"orchestrator/orchestrator-guide/#run-the-magi_orchestratorpy-tool","text":"The MAGI Orchestrator tool, magi_orchestrator.py , is a tool that reads the procedure's AAL file and orchestrates an experiment based on the specified procedures. The Orchestrator does the following in this order: Joins Groups - The Orchestrator iterates over the list of groups and for each group sends a request to all the mapped nodes to join the group. A corresponding reply adds the replier to the group. Messages addressed to a group are sent to all the nodes that are part of the group. Loads Agents - The Orchestrator iterates over the list of agents and for each agent sends an agent load request to the mapped groups. An agent load message tells the Orchestrator to start an instance of the agent implementation and to put it in a listening mode, where the instance waits for further messages. Executes Event Streams - Next, the Orchestrator concurrently executes all the event streams listed as part of streamstarts . The Orchestrator has a predefined event stream called exit . The purpose of this event stream is to unload all the agents and disjoin groups. All workflows should end with executing this stream for a clean exit. From your home directory on users.deterlab.net , run the following command: /share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal where: clientnode equals the node you want to start with myExp is the Experiment Name myProj is the Project Name procedural.aal is the name of the AAL file. The various command line options are as follows Usage: magi_orchestrator.py [options] Options: -h, --help show this help message and exit -c CONTROL, --control=CONTROL The control node to connect to (i.e. control.exp.proj) -f EVENTS, --events=EVENTS The events.aal file(s) to use. Can be specified multiple times for multiple AAL files -l LOGLEVEL, --loglevel=LOGLEVEL The level at which to log. Must be one of none, debug, info, warning, error, or critical. Default is info. -o LOGFILE, --logfile=LOGFILE If given, log to the file instead of the console (stdout). -e EXITONFAILURE, --exitOnFailure=EXITONFAILURE If any method call fails (returns False), then exit all streams, unload all agents, and exit the orchestrator. Default value is True -g GROUPBUILDTIMEOUT, --groupBuildTimeout=GROUPBUILDTIMEOUT When building the initial groups for agents in the given AAL, use the timeout given (in milliseconds) when waiting for group formation to complete. --nocolor If given, do not use color in output. -v, --verbose Tell orchestrator to print info about what its doing -n, --tunnel Tell orchestrator to tunnel data through Deter Ops (users.deterlab.net).","title":"Run the magi_orchestrator.py tool"},{"location":"orchestrator/orchestrator-quickstart/","text":"Orchestrator Quickstart \u00b6 This page describes basic information about the MAGI Orchestrator and provides a high-level overview of how to use it. More details are available in the Orchestrator Guide . What is the MAGI Orchestrator? \u00b6 MAGI allows you to automate and manage the procedures of a DETERLab experiment which is very useful for highly complex experiments. It's essentially a workflow management system for DETERLab that provides deterministic control and orchestration over event streams, repeatable enactment of procedures and control and data management for experiments. MAGI replaces the SEER experimentation framework and is part of the DETER experiment lifecycle management tools. How does it work? \u00b6 The procedure for a MAGI experiment is expressed in a YAML-based Agent Activation Language (AAL) file. The MAGI Orchestrator tool parses the procedure AAL file and maintains experiment-wide state to execute the procedure. The Orchestrator then sends and receives events from agents on the experiment nodes and enforces synchronization points -- called as triggers \u2014 to deterministically execute the procedure. How do I use the Orchestrator? \u00b6 1. Include a special start command in your topology \u00b6 Add a special start command in your topology to install MAGI and supporting tools on all nodes at startup. The command will be similar to the following: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\" 2. Write the AAL file that describes the experiment's workflows \u00b6 Describe the experiment procedure (ie, workflow) in a YAML-based AAL (.aal) file that describes: groups - one or more nodes of with similar behavior. agents - sets of behaviors event streams - list of events and triggers that make up a procedure. events - invoke a procedure implemented in an agent triggers - synchronization mechanism based on events or time. The following is an example of an event in the AAL file: - type: event agent: server_agent method: startServer trigger: serverStartedSuccessfully args: {} You'll find more detailed information about writing the AAL file in the Orchestrator Guide . 3. Run the magi_orchestrator.py tool on a physical experiment in DETERLab \u00b6 Similar to Containers , you run the Orchestrator tool in conjunction with a swapped-in physical experiment in DETERLab. magi_orchestrator.py reads the procedure's AAL file and orchestrates an experiment based on the specified procedures. You would run a command similar to the following on users : /share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal 4. View results by accessing nodes, modify the experiment as needed. \u00b6 In an orchestrated experiment, you can access the virtual nodes with the same directories mounted as in a Core DETERLab experiment. You can load and run software and conduct experiments as you would in a Core experiment. 5. Save your work and swap out your experiment (release the resources) \u00b6 As with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users. More Information \u00b6 For more detailed information about the Orchestrator, read the following: Orchestrator Guide - This guide walks you through a basic example of using the Orchestrator and includes some advanced topics. Orchestrator Case Studies - Includes details of real-world examples of using Orchestrator. Orchestrator Reference - This reference includes commands, configuration details and logs.","title":"Orchestrator Quickstart"},{"location":"orchestrator/orchestrator-quickstart/#orchestrator-quickstart","text":"This page describes basic information about the MAGI Orchestrator and provides a high-level overview of how to use it. More details are available in the Orchestrator Guide .","title":"Orchestrator Quickstart"},{"location":"orchestrator/orchestrator-quickstart/#what-is-the-magi-orchestrator","text":"MAGI allows you to automate and manage the procedures of a DETERLab experiment which is very useful for highly complex experiments. It's essentially a workflow management system for DETERLab that provides deterministic control and orchestration over event streams, repeatable enactment of procedures and control and data management for experiments. MAGI replaces the SEER experimentation framework and is part of the DETER experiment lifecycle management tools.","title":"What is the MAGI Orchestrator?"},{"location":"orchestrator/orchestrator-quickstart/#how-does-it-work","text":"The procedure for a MAGI experiment is expressed in a YAML-based Agent Activation Language (AAL) file. The MAGI Orchestrator tool parses the procedure AAL file and maintains experiment-wide state to execute the procedure. The Orchestrator then sends and receives events from agents on the experiment nodes and enforces synchronization points -- called as triggers \u2014 to deterministically execute the procedure.","title":"How does it work?"},{"location":"orchestrator/orchestrator-quickstart/#how-do-i-use-the-orchestrator","text":"","title":"How do I use the Orchestrator?"},{"location":"orchestrator/orchestrator-quickstart/#1-include-a-special-start-command-in-your-topology","text":"Add a special start command in your topology to install MAGI and supporting tools on all nodes at startup. The command will be similar to the following: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\"","title":"1. Include a special start command in your topology"},{"location":"orchestrator/orchestrator-quickstart/#2-write-the-aal-file-that-describes-the-experiments-workflows","text":"Describe the experiment procedure (ie, workflow) in a YAML-based AAL (.aal) file that describes: groups - one or more nodes of with similar behavior. agents - sets of behaviors event streams - list of events and triggers that make up a procedure. events - invoke a procedure implemented in an agent triggers - synchronization mechanism based on events or time. The following is an example of an event in the AAL file: - type: event agent: server_agent method: startServer trigger: serverStartedSuccessfully args: {} You'll find more detailed information about writing the AAL file in the Orchestrator Guide .","title":"2. Write the AAL file that describes the experiment's workflows"},{"location":"orchestrator/orchestrator-quickstart/#3-run-the-magi_orchestratorpy-tool-on-a-physical-experiment-in-deterlab","text":"Similar to Containers , you run the Orchestrator tool in conjunction with a swapped-in physical experiment in DETERLab. magi_orchestrator.py reads the procedure's AAL file and orchestrates an experiment based on the specified procedures. You would run a command similar to the following on users : /share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal","title":"3. Run the magi_orchestrator.py tool on a physical experiment in DETERLab"},{"location":"orchestrator/orchestrator-quickstart/#4-view-results-by-accessing-nodes-modify-the-experiment-as-needed","text":"In an orchestrated experiment, you can access the virtual nodes with the same directories mounted as in a Core DETERLab experiment. You can load and run software and conduct experiments as you would in a Core experiment.","title":"4. View results by accessing nodes, modify the experiment as needed."},{"location":"orchestrator/orchestrator-quickstart/#5-save-your-work-and-swap-out-your-experiment-release-the-resources","text":"As with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users.","title":"5. Save your work and swap out your experiment (release the resources)"},{"location":"orchestrator/orchestrator-quickstart/#more-information","text":"For more detailed information about the Orchestrator, read the following: Orchestrator Guide - This guide walks you through a basic example of using the Orchestrator and includes some advanced topics. Orchestrator Case Studies - Includes details of real-world examples of using Orchestrator. Orchestrator Reference - This reference includes commands, configuration details and logs.","title":"More Information"},{"location":"orchestrator/orchestrator-reference/","text":"Orchestrator Reference \u00b6 The following sections contain reference information for the MAGI orchestrator system components: MAGI System Files MAGI Agent Library MAGI Tools MAGI Desktop MAGI Developer Codebase","title":"Orchestrator Reference"},{"location":"orchestrator/orchestrator-reference/#orchestrator-reference","text":"The following sections contain reference information for the MAGI orchestrator system components: MAGI System Files MAGI Agent Library MAGI Tools MAGI Desktop MAGI Developer Codebase","title":"Orchestrator Reference"},{"location":"orchestrator/scaled-client-server/","text":"Scaled Client Server Case Study \u00b6 In this example we demonstrate how to set up client server traffic generators in a larger topology. This case study is identical to Simple Client Case Study except the topology is significantly larger. Event Streams \u00b6 As in the simpler case , this example has three events streams; the server stream, the client stream, and the cleanup stream. Mapping to the Topology \u00b6 The groups directive in the AAL file allows mapping an agent behavior to one or more nodes. groups: client_group: [clientnode-1, clientnode-2, clientnode-3, clientnode-4, clientnode-5, clientnode-6, clientnode-7, clientnode-8, clientnode-9, clientnode-10, clientnode-11, clientnode-12, clientnode-13, clientnode-14, clientnode-15, clientnode-16, clientnode-17, clientnode-18, clientnode-19, clientnode-20, clientnode-21, clientnode-22, clientnode-23, clientnode-24, clientnode-25, clientnode-26, clientnode-27, clientnode-28, clientnode-29, clientnode-30, clientnode-31, clientnode-32, clientnode-33, clientnode-34, clientnode-35, clientnode-36, clientnode-37, clientnode-38, clientnode-39, clientnode-40, clientnode-41, clientnode-42, clientnode-43, clientnode-44, clientnode-45, clientnode-46, clientnode-47, clientnode-48, clientnode-49, clientnode-50 ] server_group: &slist [ servernode-1, servernode-2, servernode-3, servernode-4, servernode-5 ] In this example, we observe that there are two groups: client_group which consists of all 50 clientnodes, and server_group which consists of 5 servernodes. Additionally, we use YAML pointers to annotate the server_group as slist . The slist annotation is used to refer to the list of servers for configuring the client_agent in the section below. Configuring the Agents \u00b6 There are two types of agents, a client_agent and a server_agent. Each agent description consists of at least three directives; group, path and execargs. group : indicates the set of nodes that the client_agent should be deployed on. path : indicates the path to the agent code (also called an agent module). execargs : can be used to parameterize the agents at load time. The agents may be reconfigured later in the AAL also using the setConfiguration method. agents: client_agent: group: client_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '5', sizes: 'minmax(1000,10000)'} server_agent: group: server_group path: /share/magi/modules/apache/apache.tar.gz execargs: [] Server Stream \u00b6 The server event stream consists of three states. The start state which generates a trigger, called serverStarted , once all the server agents are activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, where the server is deactivated or terminated. The AAL description is the same as the one used in the Simple Client case study . Client Stream \u00b6 The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all 50 nodes. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger the client agent is activated in the start state. Each client_agent fetches web pages for one of the listed servers. Next, the client stream waits for a period (\\Delta) t and then terminates the client agents in the stop state. On termination, the client agents sends a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated. Running the Experiment \u00b6 Step 1: Swap in the experiment \u00b6 Swap in the experiment using this network description file: cs55_topology.tcl Step 2: Set up your environment \u00b6 Set up your environment. Assuming your experiment is named myExp , your DETER project is myProj , and the AAL file is called procedure.aal . PROJ=myExp EXP=myProj AAL=procedure.aal Step 3: Run the Orchestrator \u00b6 Once the experiment is swapped in, run the orchestrator, giving it the AAL above and the experiment and project name. > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseClientServer\u201d: The Orchestrator enacts an internally defined stream called initilization that is responsible for establishing the server_group and the client_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , The initialization stream is complete. Now the serverstream, clientstream and the cleanup stream start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The clienstream on receiving the trigger serverStarted from the server_group, sends the startClient event to the client_group. One minute later, the clientstream sends the event stopClient to the client_group and terminates the clientstream. All members of the client_group, terminate the client_agent and generate a clientStopped trigger which is sent back to the orchestrator. Once the serverstream receives the clientStopped trigger from the client_group, it sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream. On receiving the serverStopped trigger, the cleanupstream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment. Procedure: casestudy_clientserver55.aal Topology: casestudy_clientserver55.tcl Archive Logs: casestudy_clientserver55.tar.gz Orchestration: casestudy_clientserver55.orch.log Visualizing Experiment Results \u00b6 Note This process is the same as for the Simple Client case - we are reproducing here for your convenience. In order to visualize the traffic on the network, modify the above mentioned procedure to add another stream called \u201cmonitorstream\u201d. This stream deploys a packet sensor agent on the router node to measure the aggregated traffic between the server nodes and the client nodes. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [router] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: clientStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the below mentioned tools to create a traffic plot. In order to populate the traffic data, re-run the experiment using the updated procedure. The updated procedure file and the corresponding logs are attached below. The aggregated traffic can then be plotted in two ways: Offline: A plot of the traffic flowing through the router node connecting the clients and the servers can be generated using the MAGI Graph Creation Tool. > GRAPHCONF=cs55_magi_graph.conf > /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png Real Time: A real time simulated traffic plot using canned data from a pre-run experiment can be visualized here . A similar plot using live data can be plotted by visiting the same web page, and additionally passing it the hostname of the database config node of your experiment. You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject The procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below. Procedure: casestudy_clientserver55_monitor.aal Topology: cs55_topology.tcl Archived Logs: casestudy_clientserver55_monitor.tar.gz Graph Config: casestudy_clientserver55_magi_graph.conf","title":"Scaled Client Server Case Study"},{"location":"orchestrator/scaled-client-server/#scaled-client-server-case-study","text":"In this example we demonstrate how to set up client server traffic generators in a larger topology. This case study is identical to Simple Client Case Study except the topology is significantly larger.","title":"Scaled Client Server Case Study"},{"location":"orchestrator/scaled-client-server/#event-streams","text":"As in the simpler case , this example has three events streams; the server stream, the client stream, and the cleanup stream.","title":"Event Streams"},{"location":"orchestrator/scaled-client-server/#mapping-to-the-topology","text":"The groups directive in the AAL file allows mapping an agent behavior to one or more nodes. groups: client_group: [clientnode-1, clientnode-2, clientnode-3, clientnode-4, clientnode-5, clientnode-6, clientnode-7, clientnode-8, clientnode-9, clientnode-10, clientnode-11, clientnode-12, clientnode-13, clientnode-14, clientnode-15, clientnode-16, clientnode-17, clientnode-18, clientnode-19, clientnode-20, clientnode-21, clientnode-22, clientnode-23, clientnode-24, clientnode-25, clientnode-26, clientnode-27, clientnode-28, clientnode-29, clientnode-30, clientnode-31, clientnode-32, clientnode-33, clientnode-34, clientnode-35, clientnode-36, clientnode-37, clientnode-38, clientnode-39, clientnode-40, clientnode-41, clientnode-42, clientnode-43, clientnode-44, clientnode-45, clientnode-46, clientnode-47, clientnode-48, clientnode-49, clientnode-50 ] server_group: &slist [ servernode-1, servernode-2, servernode-3, servernode-4, servernode-5 ] In this example, we observe that there are two groups: client_group which consists of all 50 clientnodes, and server_group which consists of 5 servernodes. Additionally, we use YAML pointers to annotate the server_group as slist . The slist annotation is used to refer to the list of servers for configuring the client_agent in the section below.","title":"Mapping to the Topology"},{"location":"orchestrator/scaled-client-server/#configuring-the-agents","text":"There are two types of agents, a client_agent and a server_agent. Each agent description consists of at least three directives; group, path and execargs. group : indicates the set of nodes that the client_agent should be deployed on. path : indicates the path to the agent code (also called an agent module). execargs : can be used to parameterize the agents at load time. The agents may be reconfigured later in the AAL also using the setConfiguration method. agents: client_agent: group: client_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '5', sizes: 'minmax(1000,10000)'} server_agent: group: server_group path: /share/magi/modules/apache/apache.tar.gz execargs: []","title":"Configuring the Agents"},{"location":"orchestrator/scaled-client-server/#server-stream","text":"The server event stream consists of three states. The start state which generates a trigger, called serverStarted , once all the server agents are activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, where the server is deactivated or terminated. The AAL description is the same as the one used in the Simple Client case study .","title":"Server Stream"},{"location":"orchestrator/scaled-client-server/#client-stream","text":"The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all 50 nodes. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger the client agent is activated in the start state. Each client_agent fetches web pages for one of the listed servers. Next, the client stream waits for a period (\\Delta) t and then terminates the client agents in the stop state. On termination, the client agents sends a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated.","title":"Client Stream"},{"location":"orchestrator/scaled-client-server/#running-the-experiment","text":"","title":"Running the Experiment"},{"location":"orchestrator/scaled-client-server/#step-1-swap-in-the-experiment","text":"Swap in the experiment using this network description file: cs55_topology.tcl","title":"Step 1: Swap in the experiment"},{"location":"orchestrator/scaled-client-server/#step-2-set-up-your-environment","text":"Set up your environment. Assuming your experiment is named myExp , your DETER project is myProj , and the AAL file is called procedure.aal . PROJ=myExp EXP=myProj AAL=procedure.aal","title":"Step 2: Set up your environment"},{"location":"orchestrator/scaled-client-server/#step-3-run-the-orchestrator","text":"Once the experiment is swapped in, run the orchestrator, giving it the AAL above and the experiment and project name. > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseClientServer\u201d: The Orchestrator enacts an internally defined stream called initilization that is responsible for establishing the server_group and the client_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , The initialization stream is complete. Now the serverstream, clientstream and the cleanup stream start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The clienstream on receiving the trigger serverStarted from the server_group, sends the startClient event to the client_group. One minute later, the clientstream sends the event stopClient to the client_group and terminates the clientstream. All members of the client_group, terminate the client_agent and generate a clientStopped trigger which is sent back to the orchestrator. Once the serverstream receives the clientStopped trigger from the client_group, it sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream. On receiving the serverStopped trigger, the cleanupstream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment. Procedure: casestudy_clientserver55.aal Topology: casestudy_clientserver55.tcl Archive Logs: casestudy_clientserver55.tar.gz Orchestration: casestudy_clientserver55.orch.log","title":"Step 3: Run the Orchestrator"},{"location":"orchestrator/scaled-client-server/#visualizing-experiment-results","text":"Note This process is the same as for the Simple Client case - we are reproducing here for your convenience. In order to visualize the traffic on the network, modify the above mentioned procedure to add another stream called \u201cmonitorstream\u201d. This stream deploys a packet sensor agent on the router node to measure the aggregated traffic between the server nodes and the client nodes. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [router] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: clientStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the below mentioned tools to create a traffic plot. In order to populate the traffic data, re-run the experiment using the updated procedure. The updated procedure file and the corresponding logs are attached below. The aggregated traffic can then be plotted in two ways: Offline: A plot of the traffic flowing through the router node connecting the clients and the servers can be generated using the MAGI Graph Creation Tool. > GRAPHCONF=cs55_magi_graph.conf > /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png Real Time: A real time simulated traffic plot using canned data from a pre-run experiment can be visualized here . A similar plot using live data can be plotted by visiting the same web page, and additionally passing it the hostname of the database config node of your experiment. You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject The procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below. Procedure: casestudy_clientserver55_monitor.aal Topology: cs55_topology.tcl Archived Logs: casestudy_clientserver55_monitor.tar.gz Graph Config: casestudy_clientserver55_magi_graph.conf","title":"Visualizing Experiment Results"},{"location":"orchestrator/simple-client-server/","text":"Simple Client Server Case Study \u00b6 In this example, we demonstrate how to set up client and server traffic generators with only one server and one client. (In the Scaled Server Case Study , we will show how the same procedure can be used for a significantly larger topology.) We demonstrate three aspects of MAGI: specifying multiple event streams, synchronizing with triggers, and a special target called exit to unload agents. Event Streams \u00b6 This example has three events streams; the server stream, the client stream, and the cleanup stream. The coordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers (such as after the server has started) or time-based triggers (such as wait for 30 seconds). The triggers are indicated as wait states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above. Server Stream \u00b6 The server event stream consists of three states. The start state generates a trigger, called serverStarted , once the server agent is activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, when the server is deactivated or terminated. Here is the relevant AAL description: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ {event: ClientStopped} ] - type: event agent: server_agent method: stopServer trigger: ServerStopped args: {} Client Stream \u00b6 The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger, the client agent is activated in the start state. Next, the client stream waits for a period of time and then terminates the client agents in the stop state. On termination, the client agents send a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated. Here is the relevant AAL description: clientstream: - type: trigger triggers: [ {event: ServerStarted} ] - type: event agent: client_agent method: startClient args: {} - type: trigger triggers: [ {timeout: 60000} ] - type: event agent: client_agent method: stopClient trigger: clientStopped args: {} Cleanup Stream \u00b6 The last event stream, the cleanup stream consists of two states. First, it waits for all the servers to stop. Then it enters the exit state. The exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator. It causes the Orchestrator to send agent unload and disband group messages to all of the experiment nodes and then it exits the Orchestrator. Here is the relevant AAL code: cleanup: - type: trigger triggers: [ {event: ServerStopped, target: exit} ] Running the Experiment \u00b6 Step 1: Swap in the experiment \u00b6 Swap in the experiment using this network description file: casestudy_clientserver.tcl . Step 2: Set up your environment \u00b6 Assuming your experiment is named myExp , your project is named myProj , and the AAL file is called casestudy_clientserver.aal , include this in your environment: PROJ=myExp EXP=myProj AAL=casestudy_clientserver.aal Step 3: Run the Orchestrator \u00b6 Once the experiment is swapped in, run the Orchestrator using this AAL file: casestudy_clientserver.aal . > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project montage with experiment caseClientServer . The output will be as follows: The Orchestrator runs an internally defined stream called initilization that is responsible for establishing the server_group and the client_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , the initialization stream is complete. Now the serverstream , clientstream and the cleanup stream start concurrently. The serverstream sends the startServer event to the server_group . All members of the server_group start the server and fire a trigger serverStarted . The clientstream then sends the startClient event to the client_group. One minute later, the clientstream sends the event stopClient to the client_group and terminates the clientstream . All members of the client_group terminate the client_agent and generate a clientStopped trigger which is sent back to the Orchestrator. Once the serverstream receives the clientStopped trigger from the client_group, it sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream . Upon receiving the serverStopped trigger, the cleanupstream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the case study are attached below. Procedure: casestudy_clientserver.aal Topology: casestudy_clientserver.tcl Archived Logs: casestudy_clientserver.tar.gz Visualizing Experiment Results \u00b6 In order to visualize the traffic on the network, modify the above-mentioned procedure to add another stream called monitorstream . This stream deploys a packet sensor agent on the server node to measure the traffic on the link in the experiment. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [servernode] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: clientStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the [below mentioned] tools to create a traffic plot. In order to populate the traffic data, re-run the experiment using the updated procedure file: casestudy_clientserver_monitor.aal . The corresponding logs are also available here . The traffic may then be plotted in two ways: Offline Plotting \u00b6 A plot of the traffic on the link connecting the client and the server can be generated by the MAGI Graph Creation Tool. > GRAPHCONF=cs_magi_graph.conf > /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png Real-time Plotting \u00b6 A real time simulated traffic plot using canned data from a pre-run experiment may be visualized here . A similar plot using live data may be plotted by visiting the same web page and additionally passing it the hostname of the database config node of your experiment. You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf The database config node is as follows: dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject The procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below. Procedure: casestudy_clientserver_monitor.aal Archived Logs: casestudy_clientserver_monitor.tar.gz Graph Config: cs_magi_graph.conf Scaling the Experiment \u00b6 Now suppose you wanted to generate web traffic for a larger topology. We discuss how the above AAL can be applied to a topology of 55 nodes in the next tutorial .","title":"Simple Client Server Case Study"},{"location":"orchestrator/simple-client-server/#simple-client-server-case-study","text":"In this example, we demonstrate how to set up client and server traffic generators with only one server and one client. (In the Scaled Server Case Study , we will show how the same procedure can be used for a significantly larger topology.) We demonstrate three aspects of MAGI: specifying multiple event streams, synchronizing with triggers, and a special target called exit to unload agents.","title":"Simple Client Server Case Study"},{"location":"orchestrator/simple-client-server/#event-streams","text":"This example has three events streams; the server stream, the client stream, and the cleanup stream. The coordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers (such as after the server has started) or time-based triggers (such as wait for 30 seconds). The triggers are indicated as wait states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above.","title":"Event Streams"},{"location":"orchestrator/simple-client-server/#server-stream","text":"The server event stream consists of three states. The start state generates a trigger, called serverStarted , once the server agent is activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, when the server is deactivated or terminated. Here is the relevant AAL description: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ {event: ClientStopped} ] - type: event agent: server_agent method: stopServer trigger: ServerStopped args: {}","title":"Server Stream"},{"location":"orchestrator/simple-client-server/#client-stream","text":"The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger, the client agent is activated in the start state. Next, the client stream waits for a period of time and then terminates the client agents in the stop state. On termination, the client agents send a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated. Here is the relevant AAL description: clientstream: - type: trigger triggers: [ {event: ServerStarted} ] - type: event agent: client_agent method: startClient args: {} - type: trigger triggers: [ {timeout: 60000} ] - type: event agent: client_agent method: stopClient trigger: clientStopped args: {}","title":"Client Stream"},{"location":"orchestrator/simple-client-server/#cleanup-stream","text":"The last event stream, the cleanup stream consists of two states. First, it waits for all the servers to stop. Then it enters the exit state. The exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator. It causes the Orchestrator to send agent unload and disband group messages to all of the experiment nodes and then it exits the Orchestrator. Here is the relevant AAL code: cleanup: - type: trigger triggers: [ {event: ServerStopped, target: exit} ]","title":"Cleanup Stream"},{"location":"orchestrator/simple-client-server/#running-the-experiment","text":"","title":"Running the Experiment"},{"location":"orchestrator/simple-client-server/#step-1-swap-in-the-experiment","text":"Swap in the experiment using this network description file: casestudy_clientserver.tcl .","title":"Step 1: Swap in the experiment"},{"location":"orchestrator/simple-client-server/#step-2-set-up-your-environment","text":"Assuming your experiment is named myExp , your project is named myProj , and the AAL file is called casestudy_clientserver.aal , include this in your environment: PROJ=myExp EXP=myProj AAL=casestudy_clientserver.aal","title":"Step 2: Set up your environment"},{"location":"orchestrator/simple-client-server/#step-3-run-the-orchestrator","text":"Once the experiment is swapped in, run the Orchestrator using this AAL file: casestudy_clientserver.aal . > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project montage with experiment caseClientServer . The output will be as follows: The Orchestrator runs an internally defined stream called initilization that is responsible for establishing the server_group and the client_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , the initialization stream is complete. Now the serverstream , clientstream and the cleanup stream start concurrently. The serverstream sends the startServer event to the server_group . All members of the server_group start the server and fire a trigger serverStarted . The clientstream then sends the startClient event to the client_group. One minute later, the clientstream sends the event stopClient to the client_group and terminates the clientstream . All members of the client_group terminate the client_agent and generate a clientStopped trigger which is sent back to the Orchestrator. Once the serverstream receives the clientStopped trigger from the client_group, it sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream . Upon receiving the serverStopped trigger, the cleanupstream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the case study are attached below. Procedure: casestudy_clientserver.aal Topology: casestudy_clientserver.tcl Archived Logs: casestudy_clientserver.tar.gz","title":"Step 3: Run the Orchestrator"},{"location":"orchestrator/simple-client-server/#visualizing-experiment-results","text":"In order to visualize the traffic on the network, modify the above-mentioned procedure to add another stream called monitorstream . This stream deploys a packet sensor agent on the server node to measure the traffic on the link in the experiment. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [servernode] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: clientStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the [below mentioned] tools to create a traffic plot. In order to populate the traffic data, re-run the experiment using the updated procedure file: casestudy_clientserver_monitor.aal . The corresponding logs are also available here . The traffic may then be plotted in two ways:","title":"Visualizing Experiment Results"},{"location":"orchestrator/simple-client-server/#offline-plotting","text":"A plot of the traffic on the link connecting the client and the server can be generated by the MAGI Graph Creation Tool. > GRAPHCONF=cs_magi_graph.conf > /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png","title":"Offline Plotting"},{"location":"orchestrator/simple-client-server/#real-time-plotting","text":"A real time simulated traffic plot using canned data from a pre-run experiment may be visualized here . A similar plot using live data may be plotted by visiting the same web page and additionally passing it the hostname of the database config node of your experiment. You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf The database config node is as follows: dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject The procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below. Procedure: casestudy_clientserver_monitor.aal Archived Logs: casestudy_clientserver_monitor.tar.gz Graph Config: cs_magi_graph.conf","title":"Real-time Plotting"},{"location":"orchestrator/simple-client-server/#scaling-the-experiment","text":"Now suppose you wanted to generate web traffic for a larger topology. We discuss how the above AAL can be applied to a topology of 55 nodes in the next tutorial .","title":"Scaling the Experiment"},{"location":"orchestrator/system-files/","text":"MAGI System Organization \u00b6 The MAGI system is divided into magicore and magimodules . magicore \u00b6 The magicore consists of the group communication, control, and data management infrastructure. The latest available version for use on DETERLab may be found on users at: /share/magi/current/ The MAGI codebase is also available at a publically accessible repository. https://github.com/deter-project/magi.git magimodules \u00b6 The magimodules are agent function implementations that enable a particular behavior on the experiment nodes. The agents along with supporting documentation and file are located on users at: /share/magi/modules The MAGi modules are also available at a publically accessible repository. https://github.com/deter-project/magi-modules.git Logs \u00b6 You can find helpful logs in these locations in users LOG_DIR Check experiment configuration file. Default: /var/log/magi MAGI Bootstrap Log /tmp/magi_bootstrap.log MAGI Daemon Log $LOG_DIR/daemon.log MongoDB Log $LOG_DIR/mongo.log Configuration Files \u00b6 The following configuration files are available in users : Agent Name for the set of nodes that represent the behavior Experiment Configuration /proj/<project_name>/exp/<experiment_name>/experiment.conf CONF_DIR Check experiment configuration file. Default: /var/log/magi Node Configuration $CONF_DIR/node.conf","title":"MAGI System Organization"},{"location":"orchestrator/system-files/#magi-system-organization","text":"The MAGI system is divided into magicore and magimodules .","title":"MAGI System Organization"},{"location":"orchestrator/system-files/#magicore","text":"The magicore consists of the group communication, control, and data management infrastructure. The latest available version for use on DETERLab may be found on users at: /share/magi/current/ The MAGI codebase is also available at a publically accessible repository. https://github.com/deter-project/magi.git","title":"magicore"},{"location":"orchestrator/system-files/#magimodules","text":"The magimodules are agent function implementations that enable a particular behavior on the experiment nodes. The agents along with supporting documentation and file are located on users at: /share/magi/modules The MAGi modules are also available at a publically accessible repository. https://github.com/deter-project/magi-modules.git","title":"magimodules"},{"location":"orchestrator/system-files/#logs","text":"You can find helpful logs in these locations in users LOG_DIR Check experiment configuration file. Default: /var/log/magi MAGI Bootstrap Log /tmp/magi_bootstrap.log MAGI Daemon Log $LOG_DIR/daemon.log MongoDB Log $LOG_DIR/mongo.log","title":"Logs"},{"location":"orchestrator/system-files/#configuration-files","text":"The following configuration files are available in users : Agent Name for the set of nodes that represent the behavior Experiment Configuration /proj/<project_name>/exp/<experiment_name>/experiment.conf CONF_DIR Check experiment configuration file. Default: /var/log/magi Node Configuration $CONF_DIR/node.conf","title":"Configuration Files"},{"location":"orchestrator/writing-agents/","text":"Specialized User \u00b6 This page gives you a brief introduction on writing your own Magi Agent. It is designed to give you sample code, briefly explain it, then show you the pieces needed to run it. After reading this page you should be able to write and run a basic MAGI Agent. Further details and more advanced agent information may be found in the Magi Agent Library document (link). Basic Agent Information \u00b6 An Agent runs in two modes: a threaded mode and a process mode. Threaded mode: The MAGI Daemon loads python codes directly, and runs the Agent in a thread in its own process space. Process mode: The MAGI Daemon runs the agent in a process space separate from itself and communicates with the Agent via a pipe or a socket. DispatchAgent class \u00b6 In most cases you will want to use the Orchestrator (link) and an AAL file (link) to run and coordinate your Agent actions. In order to get the basic Agent control (via remote procedure calls), you'll need to derive your agent from the DispatchAgent base class. The DispatchAgent implements the simple remote procedure call (RPC) mechanism used by the Orchestrator (and available through the MAGI python API). This allows any method in a class derived from DispatchAgent to be invoked in an AAL (or by a MagiMessage if using the MAGI python interface directly). The DispatchAgent code simply loops, parsing incoming messages, looking for an event message. When it finds one, it attempts to call the method specified in the message with the arguments given. You needn't worry about message handling or parsing when you derive from DispatchAgent , you can simply write your code and call that code from the AAL. Basic Elements of Writing a Client \u00b6 To write and execute your agent you need the following three things: The Agent code to implement the Agent. Also, every Agent must implement a method named getAgent which returns an instance of the Agent to run. The MAGI Daemon uses this method to get an instance of the Agent to run in a local thread and communicate with the Agent instance. An Interface Description Language (IDL) file to describe the Agent function and specify things the MAGI Daemon needs to know to load and execute the Agent code (among these is the location of the Agent code and the execution mode). An AAL file (as described here ) Deploying and Executing a Sample Agent \u00b6 Step 1: Create a local directory named \"FileCreator\" \u00b6 MAGI Agents are usually contained in a single directory. Step 2: Create the Agent implementation code file \u00b6 Copy the following Agent implementation code to the file \"FileCreator/FileCreator.py\". This example Agent code has the following characteristics: It creates a simple Agent which creates a single file on a host. The agent is called FileCreator. It has a single method, createFile , which creates the file /tmp/newfile when called For this agent, we will always run in threaded-mode. from magi.util.agent import DispatchAgent, agentmethod from magi.util.processAgent import initializeProcessAgent # The FileCreator agent implementation, derived from DispatchAgent. class FileCreator(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) self.filename = '/tmp/newfile' # A single method which creates the file named by self.filename. # (The @agentmethod() decorator is not required, but is encouraged. # it does nothing of substance now, but may in the future.) @agentmethod() def createFile(self, msg): '''Create a file on the host.''' # open and immediately close the file to create it. open(self.filename, 'w').close() # the getAgent() method must be defined somewhere for all agents. # The Magi daemon invokes this mehod to get a reference to an # agent. It uses this reference to run and interact with an agent # instance. def getAgent(): agent = FileCreator() return agent # In case the agent is run as a separate process, we need to # create an instance of the agent, initialize the required # parameters based on the received arguments, and then call the # run method defined in DispatchAgent. if __name__ == \"__main__\": agent = FileCreator() initializeProcessAgent(agent, sys.argv) agent.run() Step 3: Create the IDL file \u00b6 Copy the IDL below to a file named 'FileCreator/FileCreator.idl'. Note The file and directory may be named anything, but if you deviate from the naming scheme given, make sure the mainfile setting in the IDL and the code setting in your AAL (below) matches your naming scheme. The following example IDL file has the following characteristics: The execution mode is \"thread\" and the inheritance is specified as \"DispatchAgent\". When you run this, you must specify the name of your implementation file (i.e., the Agent code from the previous step). This example assumes the file is in the local directory and is named \"FileCreator.py\". It lists methods and internal variables that the author wants exposed to external configuration. In our case, we expose the variable filename , but currently only use the default setting. Later we will describe how to set this outside of the Agent implementation. name: FileCreator display: File Creator Agent description: This agent creates a file on the test node. execute: thread mainfile: FileCreator.py inherits: - DispatchAgent methods: - name: createFile help: Create the file args: [] variables: - name: filename help: the full path of the file to create type: string Step 4: Create the AAL file \u00b6 Copy the sample AAL code below to a file named \"FileCreator/myEvents.aal\". Make sure to: Replace PATH with the full path to your \"FileCreator\" directory. Note: the PATH you use must include the NFS-mounted location on the test nodes. Replace NODES with the comma-separated list of nodes on your testbed on which you want to run the Agent. streamstarts: [main] groups: myFileCreatorGroup [NODES] agents: myFileCreators: group: myFileCreatorGroup # (note: the \"code\" argument is the Agent directory. The # directory must contain an IDL and Agent implementation.) code: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: createFile args: { } Note The AAL is in YAML format; therefore, it cannot have tabs. If you cut and paste the above code, make sure to remove any possible inserted tabs. Because your Agent code is on an NFS-mounted filesystem, all MAGI Daemons may read the code directly. Step 5: Run Orchestrator \u00b6 Run the MAGI Orchestrator to run the event streams in your AAL file - and thus your agent code: magi_orchestrator --control $control --events myEvents.aal -o run.log -v Where $control is the fully qualified domain of your DETERLab node, i.e. myNode.myGroup.myProject } This command runs the Orchestrator, which connects to the $control node, runs the events in the myEvents.aal file and writes verbose output to run.log . In this example, the method createFile will be called on all test nodes associated with myAgentGroup in the AAL file. On standard out, you should see Orchestrator output similar to the following: stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. This output shows the two execution streams the orchestrator runs. The first, initialization , is internal to the Orchestrator and sets up group communication and loads the agents. The second, main , is the event stream specified in your AAL file. If you do not see the \"correct\" output above, refer to the [#troubleshooting Troubleshooting] section below. To confirm the Agent ran and executed the createFile method, run the following (from users ): ssh myNode.myExperiment.myGroup ls -l /tmp/newfile Where myNode.myExperiment.myGroup is the domain name of a node on which you executed the Agent. You may download the sample code as a tar file here: [FileCreator.tbz] (attach). Runtime Agent Configuration \u00b6 The sample Agent FileCreator always creates the same file, each time it is run. What if you wanted to create a different file? Or a series of files? It is possible to specify Agent configuration in an AAL file - configuration that can modify the internal variables of your Agent at run time. See MAGI Agent Configuration for details. Troubleshooting \u00b6 Look at the Magi Daemon log file at /var/log/magi/daemon.log on your control and test nodes looking for errors. If there are syntax errors in Agent execution, they may show up here. Error: You see a 'No such file' error \u00b6 You may see an error indicating there is no such file as follows: Run-time exception in agent <Daemon(daemon, started 140555403872000)> on node(s) control in method __init__, line 71, in file threadInterface.py. Error: [Errno 2] No such file or directory Solution: You probably did not specify the correct \"mainfile\" in the IDL file. It must not be pathed out and must match the name of the main Agent implementation file, for example, \"myAgent.py\". Error: You see a \"no attribute 'getAgent'\" message \u00b6 Solution: The Magi Daemon needs the well-known method getAgent to exist in the Agent module. Add it to your Agent code. Error: 'module' object has no attribute 'getAgent' \u00b6 Solution: Make sure your agent defines (and exports or make available) a getAgent method and that it returns an instance of your agent.","title":"Specialized User"},{"location":"orchestrator/writing-agents/#specialized-user","text":"This page gives you a brief introduction on writing your own Magi Agent. It is designed to give you sample code, briefly explain it, then show you the pieces needed to run it. After reading this page you should be able to write and run a basic MAGI Agent. Further details and more advanced agent information may be found in the Magi Agent Library document (link).","title":"Specialized User"},{"location":"orchestrator/writing-agents/#basic-agent-information","text":"An Agent runs in two modes: a threaded mode and a process mode. Threaded mode: The MAGI Daemon loads python codes directly, and runs the Agent in a thread in its own process space. Process mode: The MAGI Daemon runs the agent in a process space separate from itself and communicates with the Agent via a pipe or a socket.","title":"Basic Agent Information"},{"location":"orchestrator/writing-agents/#dispatchagent-class","text":"In most cases you will want to use the Orchestrator (link) and an AAL file (link) to run and coordinate your Agent actions. In order to get the basic Agent control (via remote procedure calls), you'll need to derive your agent from the DispatchAgent base class. The DispatchAgent implements the simple remote procedure call (RPC) mechanism used by the Orchestrator (and available through the MAGI python API). This allows any method in a class derived from DispatchAgent to be invoked in an AAL (or by a MagiMessage if using the MAGI python interface directly). The DispatchAgent code simply loops, parsing incoming messages, looking for an event message. When it finds one, it attempts to call the method specified in the message with the arguments given. You needn't worry about message handling or parsing when you derive from DispatchAgent , you can simply write your code and call that code from the AAL.","title":"DispatchAgent class"},{"location":"orchestrator/writing-agents/#basic-elements-of-writing-a-client","text":"To write and execute your agent you need the following three things: The Agent code to implement the Agent. Also, every Agent must implement a method named getAgent which returns an instance of the Agent to run. The MAGI Daemon uses this method to get an instance of the Agent to run in a local thread and communicate with the Agent instance. An Interface Description Language (IDL) file to describe the Agent function and specify things the MAGI Daemon needs to know to load and execute the Agent code (among these is the location of the Agent code and the execution mode). An AAL file (as described here )","title":"Basic Elements of Writing a Client"},{"location":"orchestrator/writing-agents/#deploying-and-executing-a-sample-agent","text":"","title":"Deploying and Executing a Sample Agent"},{"location":"orchestrator/writing-agents/#step-1-create-a-local-directory-named-filecreator","text":"MAGI Agents are usually contained in a single directory.","title":"Step 1: Create a local directory named \"FileCreator\""},{"location":"orchestrator/writing-agents/#step-2-create-the-agent-implementation-code-file","text":"Copy the following Agent implementation code to the file \"FileCreator/FileCreator.py\". This example Agent code has the following characteristics: It creates a simple Agent which creates a single file on a host. The agent is called FileCreator. It has a single method, createFile , which creates the file /tmp/newfile when called For this agent, we will always run in threaded-mode. from magi.util.agent import DispatchAgent, agentmethod from magi.util.processAgent import initializeProcessAgent # The FileCreator agent implementation, derived from DispatchAgent. class FileCreator(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) self.filename = '/tmp/newfile' # A single method which creates the file named by self.filename. # (The @agentmethod() decorator is not required, but is encouraged. # it does nothing of substance now, but may in the future.) @agentmethod() def createFile(self, msg): '''Create a file on the host.''' # open and immediately close the file to create it. open(self.filename, 'w').close() # the getAgent() method must be defined somewhere for all agents. # The Magi daemon invokes this mehod to get a reference to an # agent. It uses this reference to run and interact with an agent # instance. def getAgent(): agent = FileCreator() return agent # In case the agent is run as a separate process, we need to # create an instance of the agent, initialize the required # parameters based on the received arguments, and then call the # run method defined in DispatchAgent. if __name__ == \"__main__\": agent = FileCreator() initializeProcessAgent(agent, sys.argv) agent.run()","title":"Step 2: Create the Agent implementation code file"},{"location":"orchestrator/writing-agents/#step-3-create-the-idl-file","text":"Copy the IDL below to a file named 'FileCreator/FileCreator.idl'. Note The file and directory may be named anything, but if you deviate from the naming scheme given, make sure the mainfile setting in the IDL and the code setting in your AAL (below) matches your naming scheme. The following example IDL file has the following characteristics: The execution mode is \"thread\" and the inheritance is specified as \"DispatchAgent\". When you run this, you must specify the name of your implementation file (i.e., the Agent code from the previous step). This example assumes the file is in the local directory and is named \"FileCreator.py\". It lists methods and internal variables that the author wants exposed to external configuration. In our case, we expose the variable filename , but currently only use the default setting. Later we will describe how to set this outside of the Agent implementation. name: FileCreator display: File Creator Agent description: This agent creates a file on the test node. execute: thread mainfile: FileCreator.py inherits: - DispatchAgent methods: - name: createFile help: Create the file args: [] variables: - name: filename help: the full path of the file to create type: string","title":"Step 3: Create the IDL file"},{"location":"orchestrator/writing-agents/#step-4-create-the-aal-file","text":"Copy the sample AAL code below to a file named \"FileCreator/myEvents.aal\". Make sure to: Replace PATH with the full path to your \"FileCreator\" directory. Note: the PATH you use must include the NFS-mounted location on the test nodes. Replace NODES with the comma-separated list of nodes on your testbed on which you want to run the Agent. streamstarts: [main] groups: myFileCreatorGroup [NODES] agents: myFileCreators: group: myFileCreatorGroup # (note: the \"code\" argument is the Agent directory. The # directory must contain an IDL and Agent implementation.) code: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: createFile args: { } Note The AAL is in YAML format; therefore, it cannot have tabs. If you cut and paste the above code, make sure to remove any possible inserted tabs. Because your Agent code is on an NFS-mounted filesystem, all MAGI Daemons may read the code directly.","title":"Step 4: Create the AAL file"},{"location":"orchestrator/writing-agents/#step-5-run-orchestrator","text":"Run the MAGI Orchestrator to run the event streams in your AAL file - and thus your agent code: magi_orchestrator --control $control --events myEvents.aal -o run.log -v Where $control is the fully qualified domain of your DETERLab node, i.e. myNode.myGroup.myProject } This command runs the Orchestrator, which connects to the $control node, runs the events in the myEvents.aal file and writes verbose output to run.log . In this example, the method createFile will be called on all test nodes associated with myAgentGroup in the AAL file. On standard out, you should see Orchestrator output similar to the following: stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. This output shows the two execution streams the orchestrator runs. The first, initialization , is internal to the Orchestrator and sets up group communication and loads the agents. The second, main , is the event stream specified in your AAL file. If you do not see the \"correct\" output above, refer to the [#troubleshooting Troubleshooting] section below. To confirm the Agent ran and executed the createFile method, run the following (from users ): ssh myNode.myExperiment.myGroup ls -l /tmp/newfile Where myNode.myExperiment.myGroup is the domain name of a node on which you executed the Agent. You may download the sample code as a tar file here: [FileCreator.tbz] (attach).","title":"Step 5: Run Orchestrator"},{"location":"orchestrator/writing-agents/#runtime-agent-configuration","text":"The sample Agent FileCreator always creates the same file, each time it is run. What if you wanted to create a different file? Or a series of files? It is possible to specify Agent configuration in an AAL file - configuration that can modify the internal variables of your Agent at run time. See MAGI Agent Configuration for details.","title":"Runtime Agent Configuration"},{"location":"orchestrator/writing-agents/#troubleshooting","text":"Look at the Magi Daemon log file at /var/log/magi/daemon.log on your control and test nodes looking for errors. If there are syntax errors in Agent execution, they may show up here.","title":"Troubleshooting"},{"location":"orchestrator/writing-agents/#error-you-see-a-no-such-file-error","text":"You may see an error indicating there is no such file as follows: Run-time exception in agent <Daemon(daemon, started 140555403872000)> on node(s) control in method __init__, line 71, in file threadInterface.py. Error: [Errno 2] No such file or directory Solution: You probably did not specify the correct \"mainfile\" in the IDL file. It must not be pathed out and must match the name of the main Agent implementation file, for example, \"myAgent.py\".","title":"Error: You see a 'No such file' error"},{"location":"orchestrator/writing-agents/#error-you-see-a-no-attribute-getagent-message","text":"Solution: The Magi Daemon needs the well-known method getAgent to exist in the Agent module. Add it to your Agent code.","title":"Error: You see a \"no attribute 'getAgent'\" message"},{"location":"orchestrator/writing-agents/#error-module-object-has-no-attribute-getagent","text":"Solution: Make sure your agent defines (and exports or make available) a getAgent method and that it returns an instance of your agent.","title":"Error: 'module' object has no attribute 'getAgent'"},{"location":"support/elab-elab-ssh-proxy/","text":"Setting Up FoxyProxy \u00b6 This only needs to be done once. Install the FoxyProxy extension for Firefox. Restart your browser to complete the installation. Follow these steps to set up the proxy. Text you should input is '''bold'''. Click the \"Add New Proxy\" button Proxy Details tab: Use \"Manual Proxy Configuration\". Host: localhost Port: 1080 Check the box for SOCKS 5 proxy URL Patterns tab: Click \"Add New Pattern\" Pattern Name: '''elab in elab''' (or anything you want) URL pattern: '''https?://myboss.. deterlab.net/''' (for MacOS, append '''. ''') Change \"Pattern Contains\" to Regular expression Click OK General tab: Give it a good name, like '''Elab in Elab tunnel''' Click OK Use the \"Move Up\" button to move the new pattern to the top of the list Change the Mode at the top to Use Proxies based on their pre-defined patterns and priorities . Connecting to Inner Boss \u00b6 Once you've set up FoxyProxy, whenever you want to connect to inner boss you must start an SSH SOCKS proxy. To do this using the standard OpenSSH command line client on linux, BSD, or OsX: ssh -D 1080 username@users.isi.deterlab.net You can also do this under windows using the PuTTY client: In the PuTTY configuration window expand \"Connection\", then \"SSH\", and click on tunnels. Hit the radio button for dynamic, fill in the port 1080, and click ADD. Don't specify the host here - do it after clicking on the main Session Category. If you save the parameters you won't have to do this again. You may then access inner boss using the url: http://myboss.${experiment_name}.${project_name}.isi.deterlab.net/ Fill in ${experiment_name} and ${project_name} with your experiment and project. For instance, the experiment test123 in the example project would use this URL: http://myboss.test123.example.isi.deterlab.net/ Once you are done, you may close the SSH tunnel connection. You may have to use Ctrl-C if it does not disconnect after you log out.","title":"Configuring E-lab in E-lab"},{"location":"support/elab-elab-ssh-proxy/#setting-up-foxyproxy","text":"This only needs to be done once. Install the FoxyProxy extension for Firefox. Restart your browser to complete the installation. Follow these steps to set up the proxy. Text you should input is '''bold'''. Click the \"Add New Proxy\" button Proxy Details tab: Use \"Manual Proxy Configuration\". Host: localhost Port: 1080 Check the box for SOCKS 5 proxy URL Patterns tab: Click \"Add New Pattern\" Pattern Name: '''elab in elab''' (or anything you want) URL pattern: '''https?://myboss.. deterlab.net/''' (for MacOS, append '''. ''') Change \"Pattern Contains\" to Regular expression Click OK General tab: Give it a good name, like '''Elab in Elab tunnel''' Click OK Use the \"Move Up\" button to move the new pattern to the top of the list Change the Mode at the top to Use Proxies based on their pre-defined patterns and priorities .","title":"Setting Up FoxyProxy"},{"location":"support/elab-elab-ssh-proxy/#connecting-to-inner-boss","text":"Once you've set up FoxyProxy, whenever you want to connect to inner boss you must start an SSH SOCKS proxy. To do this using the standard OpenSSH command line client on linux, BSD, or OsX: ssh -D 1080 username@users.isi.deterlab.net You can also do this under windows using the PuTTY client: In the PuTTY configuration window expand \"Connection\", then \"SSH\", and click on tunnels. Hit the radio button for dynamic, fill in the port 1080, and click ADD. Don't specify the host here - do it after clicking on the main Session Category. If you save the parameters you won't have to do this again. You may then access inner boss using the url: http://myboss.${experiment_name}.${project_name}.isi.deterlab.net/ Fill in ${experiment_name} and ${project_name} with your experiment and project. For instance, the experiment test123 in the example project would use this URL: http://myboss.test123.example.isi.deterlab.net/ Once you are done, you may close the SSH tunnel connection. You may have to use Ctrl-C if it does not disconnect after you log out.","title":"Connecting to Inner Boss"},{"location":"support/faqs/","text":"Frequently Asked questions \u00b6 Why can't I log in? \u00b6 Too many failed attempts to log into the web interface will result in your account being locked. You will get a message saying that your account has been frozen if you trigger it. If you are a student, please contact your TA. Otherwise, please \u200b contact us . Also: - You must use your actual account name , not an email address, to log into users.deterlab.net . - Too many failed attempts to log into users.deterlab.net will result in an IP address ban . We automatically whitelist all IP addresses that have successfully logged into the \u200bweb interface and this list is synchronized every 15 minutes. So if you find yourself banned from connecting to users.deterlab.net please log into the \u200bweb interface and then wait 15 minutes. How do I copy files from my workstation to a node in an experiment? \u00b6 Your home directory from users is available on the nodes in your experiment. Copy your files to users.deterlab.net using scp or sftp to make them available on your nodes. How can I copy files from a node in the testbed to my workstation? \u00b6 The reverse of the previous question: copy the files you want to your home directory, then download them from users.deterlab.net using scp or sftp . How can I install software on my nodes? \u00b6 The currently supported operating system images (see the \u200b Recommended list for currently supported OS images ) have access to full package repositories on a local mirror. Depending on your OS you may use yum , apt-get , or pkg_add to install software that has been pre-packaged for each OS. If there is no package for the software you wish to install, you may install from source. Copy the source tarball to the testbed (see How do I copy files from my workstation to a node in an experiment? or use wget or curl on users ), then follow the package's installation instructions. While we will do everything we can to assist any issues you face, we do not have the resources to help individual users install software. How do I connect the web browser on my workstation to the inner boss within an Emulab-in-Emulab experiment? \u00b6 See Setting Up FoxyProxy . I try to swap in and get the error: Admission Control: $project/$experiment has too many nodes allocated! \u00b6 If you are a class user: the maximum number of nodes that can be allocated for a class is limited (see Class Resource Limits for details). Wait for some of your classmates to free up resources before trying to swap in again. You are less likely to encounter this during non-peak hours (late night and early morning) and when deadlines are distant. If you are NOT a class user: please file a ticket , because something is broken. I try to swap out and get the error: /usr/testbed/bin/nfree: Please cleanup the previous errors. \u00b6 This may not be so frequent an error, but may arise when the experiment deliberately brings an interface down without bringing it back up prior to swap out. Try scheduling the link back up before the end of experiment. Your site claims that my new password is in the dictionary. I checked the dictionary and 'qwerty1234' is not in it. \u00b6 Please see our Passwords page .","title":"FAQs"},{"location":"support/faqs/#frequently-asked-questions","text":"","title":"Frequently Asked questions"},{"location":"support/faqs/#why-cant-i-log-in","text":"Too many failed attempts to log into the web interface will result in your account being locked. You will get a message saying that your account has been frozen if you trigger it. If you are a student, please contact your TA. Otherwise, please \u200b contact us . Also: - You must use your actual account name , not an email address, to log into users.deterlab.net . - Too many failed attempts to log into users.deterlab.net will result in an IP address ban . We automatically whitelist all IP addresses that have successfully logged into the \u200bweb interface and this list is synchronized every 15 minutes. So if you find yourself banned from connecting to users.deterlab.net please log into the \u200bweb interface and then wait 15 minutes.","title":"Why can't I log in?"},{"location":"support/faqs/#how-do-i-copy-files-from-my-workstation-to-a-node-in-an-experiment","text":"Your home directory from users is available on the nodes in your experiment. Copy your files to users.deterlab.net using scp or sftp to make them available on your nodes.","title":"How do I copy files from my workstation to a node in an experiment?"},{"location":"support/faqs/#how-can-i-copy-files-from-a-node-in-the-testbed-to-my-workstation","text":"The reverse of the previous question: copy the files you want to your home directory, then download them from users.deterlab.net using scp or sftp .","title":"How can I copy files from a node in the testbed to my workstation?"},{"location":"support/faqs/#how-can-i-install-software-on-my-nodes","text":"The currently supported operating system images (see the \u200b Recommended list for currently supported OS images ) have access to full package repositories on a local mirror. Depending on your OS you may use yum , apt-get , or pkg_add to install software that has been pre-packaged for each OS. If there is no package for the software you wish to install, you may install from source. Copy the source tarball to the testbed (see How do I copy files from my workstation to a node in an experiment? or use wget or curl on users ), then follow the package's installation instructions. While we will do everything we can to assist any issues you face, we do not have the resources to help individual users install software.","title":"How can I install software on my nodes?"},{"location":"support/faqs/#how-do-i-connect-the-web-browser-on-my-workstation-to-the-inner-boss-within-an-emulab-in-emulab-experiment","text":"See Setting Up FoxyProxy .","title":"How do I connect the web browser on my workstation to the inner boss within an Emulab-in-Emulab experiment?"},{"location":"support/faqs/#i-try-to-swap-in-and-get-the-error-admission-control-projectexperiment-has-too-many-nodes-allocated","text":"If you are a class user: the maximum number of nodes that can be allocated for a class is limited (see Class Resource Limits for details). Wait for some of your classmates to free up resources before trying to swap in again. You are less likely to encounter this during non-peak hours (late night and early morning) and when deadlines are distant. If you are NOT a class user: please file a ticket , because something is broken.","title":"I try to swap in and get the error: Admission Control: $project/$experiment has too many nodes allocated!"},{"location":"support/faqs/#i-try-to-swap-out-and-get-the-error-usrtestbedbinnfree-please-cleanup-the-previous-errors","text":"This may not be so frequent an error, but may arise when the experiment deliberately brings an interface down without bringing it back up prior to swap out. Try scheduling the link back up before the end of experiment.","title":"I try to swap out and get the error: /usr/testbed/bin/nfree: Please cleanup the previous errors."},{"location":"support/faqs/#your-site-claims-that-my-new-password-is-in-the-dictionary-i-checked-the-dictionary-and-qwerty1234-is-not-in-it","text":"Please see our Passwords page .","title":"Your site claims that my new password is in the dictionary. I checked the dictionary and 'qwerty1234' is not in it."},{"location":"support/getting-help/","text":"Getting Help with DETER \u00b6 Summary: take all of the details about your problem (project/experiment IDs, logs, error output, etc) and file a ticket in our Trac issue-tracking system. Note You must log into Trac using your DETERLab username and password. What to do first if you are having trouble? \u00b6 Read the FAQ page to see if your question has already been answered. You can also search the documentation (see the search box at the top of the left sidebar) and trouble tickets on the Trac Search page . If you still need help, create a Trac support ticket \u00b6 Step 1: Collect the information we need to help you \u00b6 Please include the following information in your ticket for best results: Your DETERLab username Your project name Your experiment (if applicable) Include a link in this format: [experiment:pid:eid] where pid is the project name and eid is the experiment name. A description of the problem. Be as detailed as you can. The following are all helpful: What you typed or clicked, what you expected to happen as a result, and what did happen Any error messages or logs (verbatim errors are best) Which nodes in your experiment are having a problem (if applicable) If your problem is with an experiment, please leave a copy swapped in for us (disable idle swap) Step 2: Create a new support ticket \u00b6 Go to the new ticket page and log in again with your DETERLab username and password. Note: you will receive an Error:Forbidden message when you first click the link. This is expected (the ticket system currently requires a separate login). Fill in all the details you gathered in the previous step as well as the following: Summary : Use a brief description of the issue Reporter : Use your DETERLab username (this should be pre-filled) Message : This is where you include the project name and all of the other details you collected in the previous step. Attachments : You may include screenshots and any other files (logs, etc) that could be useful to us in understanding your issue. We are notified when new tickets are opened and respond to them quickly, often as soon as we receive them (during business hours, US Pacific time). You may use the View Tickets button in the navigation bar to find any tickets you have created. Tips for the fastest response \u00b6 If you are reporting a support problem, do not email us. Instead, follow the instructions above for filing a support ticket. If for some reason you must email us, DETERLab testbed operations may be reached at testbed-ops at deterlab.net . Response to this email is typically slower than response to a ticket. Always send email to the testbed-ops address and not just an individual who responded to you. Then, if the individual is not available, someone will still be aware of your email.","title":"Getting Help"},{"location":"support/getting-help/#getting-help-with-deter","text":"Summary: take all of the details about your problem (project/experiment IDs, logs, error output, etc) and file a ticket in our Trac issue-tracking system. Note You must log into Trac using your DETERLab username and password.","title":"Getting Help with DETER"},{"location":"support/getting-help/#what-to-do-first-if-you-are-having-trouble","text":"Read the FAQ page to see if your question has already been answered. You can also search the documentation (see the search box at the top of the left sidebar) and trouble tickets on the Trac Search page .","title":"What to do first if you are having trouble?"},{"location":"support/getting-help/#if-you-still-need-help-create-a-trac-support-ticket","text":"","title":"If you still need help, create a Trac support ticket"},{"location":"support/getting-help/#step-1-collect-the-information-we-need-to-help-you","text":"Please include the following information in your ticket for best results: Your DETERLab username Your project name Your experiment (if applicable) Include a link in this format: [experiment:pid:eid] where pid is the project name and eid is the experiment name. A description of the problem. Be as detailed as you can. The following are all helpful: What you typed or clicked, what you expected to happen as a result, and what did happen Any error messages or logs (verbatim errors are best) Which nodes in your experiment are having a problem (if applicable) If your problem is with an experiment, please leave a copy swapped in for us (disable idle swap)","title":"Step 1: Collect the information we need to help you"},{"location":"support/getting-help/#step-2-create-a-new-support-ticket","text":"Go to the new ticket page and log in again with your DETERLab username and password. Note: you will receive an Error:Forbidden message when you first click the link. This is expected (the ticket system currently requires a separate login). Fill in all the details you gathered in the previous step as well as the following: Summary : Use a brief description of the issue Reporter : Use your DETERLab username (this should be pre-filled) Message : This is where you include the project name and all of the other details you collected in the previous step. Attachments : You may include screenshots and any other files (logs, etc) that could be useful to us in understanding your issue. We are notified when new tickets are opened and respond to them quickly, often as soon as we receive them (during business hours, US Pacific time). You may use the View Tickets button in the navigation bar to find any tickets you have created.","title":"Step 2: Create a new support ticket"},{"location":"support/getting-help/#tips-for-the-fastest-response","text":"If you are reporting a support problem, do not email us. Instead, follow the instructions above for filing a support ticket. If for some reason you must email us, DETERLab testbed operations may be reached at testbed-ops at deterlab.net . Response to this email is typically slower than response to a ticket. Always send email to the testbed-ops address and not just an individual who responded to you. Then, if the individual is not available, someone will still be aware of your email.","title":"Tips for the fastest response"},{"location":"support/passwords/","text":"Password Guidelines \u00b6 We are a computer security testbed, so please use a strong password . You may be reading this because you were told that your new password, 'qwerty1234', is in the dictionary. We do not mean the Oxford English Dictionary here. What we use is a large list of dictionary words combined with actual passwords that have been found in the wild. For example, the \u200b RockYou hack ended up revealing the unencrypted passwords of 32 million people (and about 14 million unique passwords). Since this list is one of the go-to lists for the bad guys, we use it too. This means that many passwords that seem clever or obscure fail our test because someone else thought up the same thing. Password tips: The longer your password, the less likely it is to be in the dictionary. Try combining multiple words mixed with numbers and symbols. If you are interested in learning more about password security and cracking, this arstechnica article is a pretty good introduction: \u200bAnatomy of a hack: How crackers ransack passwords like \u201cqeadzcwrsfxv1331\u201d .","title":"Passwords"},{"location":"support/passwords/#password-guidelines","text":"We are a computer security testbed, so please use a strong password . You may be reading this because you were told that your new password, 'qwerty1234', is in the dictionary. We do not mean the Oxford English Dictionary here. What we use is a large list of dictionary words combined with actual passwords that have been found in the wild. For example, the \u200b RockYou hack ended up revealing the unencrypted passwords of 32 million people (and about 14 million unique passwords). Since this list is one of the go-to lists for the bad guys, we use it too. This means that many passwords that seem clever or obscure fail our test because someone else thought up the same thing. Password tips: The longer your password, the less likely it is to be in the dictionary. Try combining multiple words mixed with numbers and symbols. If you are interested in learning more about password security and cracking, this arstechnica article is a pretty good introduction: \u200bAnatomy of a hack: How crackers ransack passwords like \u201cqeadzcwrsfxv1331\u201d .","title":"Password Guidelines"},{"location":"support/registering/","text":"Registering to use DETERLab \u00b6 Who may apply for a DETER project or request a user account? \u00b6 Researchers from academia, government, and industry -- as well as educators from academic institutions -- may apply for a DETERLab project account. A student must have their professor or appropriate faculty member apply for an account, and once it is activated the student can then apply for membership to that project. Note For more information on using DETERLab for education, please refer to the \u200bDETERLab Education Site . To Register \u00b6 DETER accounts are grouped by projects, therefore the project leader or PI must first request a project, then other users apply for membership to that project. Note If you already have a DETERLab account, please login first. This will help streamline the process. Requesting a New Project \u00b6 If you are a PI, project leader or instructor who wants to request a new project on DETERLab, fill out the \u200b New Project Application Form . No other users (such as students) should apply for a new project. You will be asked a number of questions about your project and how you intend to use DETER. Please be detailed, especially with respect to any possible risks from your experiment. A DETER staff member may contact you to discuss or clarify any potential issues. The project leader is responsible for ensuring that the project adheres to the Project Plan included in the application form. Instructors should indicate this project is for educational purposes. Once the project is created, you will receive further instructions including how to create accounts for your students. Upon submission, your application must be approved by the DETER Executive Committee ; this generally takes a few days. They may contact you and ask for clarification. You will receive an email notification upon approval and your user account will be active. You may then log in with the username and password you entered on the form. If you are curious about the progress on your application, you may \u200bcontact us . Getting a User Account to Join an Existing Project \u00b6 If you are a team member who needs to join an existing project , submit the \u200b Apply for Project Membership Form . The project leader will be informed via email of your request and will be required to log in to approve your account. Note to Students \u00b6 If you are a student who wants to use DETERLab for your own research: Your faculty sponsor must first fill out the \u200b New Project Application Form . A student may not create their own project. Once this has been done, the sponsor will instruct you to \u200bapply for project membership, which will create your DETERLab account. You must obtain the name of the DETERLab project from your sponsor to correctly fill out the form. If you are a student taking a class that uses DETERLab: You do not need to take any action \u2013 your instructor will create accounts and assign them to students. If you need assistance \u00b6 If you run into trouble, please \u200b contact us and we will be happy to assist.","title":"Registering with DETERLab"},{"location":"support/registering/#registering-to-use-deterlab","text":"","title":"Registering to use DETERLab"},{"location":"support/registering/#who-may-apply-for-a-deter-project-or-request-a-user-account","text":"Researchers from academia, government, and industry -- as well as educators from academic institutions -- may apply for a DETERLab project account. A student must have their professor or appropriate faculty member apply for an account, and once it is activated the student can then apply for membership to that project. Note For more information on using DETERLab for education, please refer to the \u200bDETERLab Education Site .","title":"Who may apply for a DETER project or request a user account?"},{"location":"support/registering/#to-register","text":"DETER accounts are grouped by projects, therefore the project leader or PI must first request a project, then other users apply for membership to that project. Note If you already have a DETERLab account, please login first. This will help streamline the process.","title":"To Register"},{"location":"support/registering/#requesting-a-new-project","text":"If you are a PI, project leader or instructor who wants to request a new project on DETERLab, fill out the \u200b New Project Application Form . No other users (such as students) should apply for a new project. You will be asked a number of questions about your project and how you intend to use DETER. Please be detailed, especially with respect to any possible risks from your experiment. A DETER staff member may contact you to discuss or clarify any potential issues. The project leader is responsible for ensuring that the project adheres to the Project Plan included in the application form. Instructors should indicate this project is for educational purposes. Once the project is created, you will receive further instructions including how to create accounts for your students. Upon submission, your application must be approved by the DETER Executive Committee ; this generally takes a few days. They may contact you and ask for clarification. You will receive an email notification upon approval and your user account will be active. You may then log in with the username and password you entered on the form. If you are curious about the progress on your application, you may \u200bcontact us .","title":"Requesting a New Project"},{"location":"support/registering/#getting-a-user-account-to-join-an-existing-project","text":"If you are a team member who needs to join an existing project , submit the \u200b Apply for Project Membership Form . The project leader will be informed via email of your request and will be required to log in to approve your account.","title":"Getting a User Account to Join an Existing Project"},{"location":"support/registering/#note-to-students","text":"If you are a student who wants to use DETERLab for your own research: Your faculty sponsor must first fill out the \u200b New Project Application Form . A student may not create their own project. Once this has been done, the sponsor will instruct you to \u200bapply for project membership, which will create your DETERLab account. You must obtain the name of the DETERLab project from your sponsor to correctly fill out the form. If you are a student taking a class that uses DETERLab: You do not need to take any action \u2013 your instructor will create accounts and assign them to students.","title":"Note to Students"},{"location":"support/registering/#if-you-need-assistance","text":"If you run into trouble, please \u200b contact us and we will be happy to assist.","title":"If you need assistance"},{"location":"support/usage-policy/","text":"Deterlab Usage Policy \u00b6 Be a good citizen \u00b6 DETER is a shared resource. We expect users to be good citizens by not abusing or wasting the resources we make available to them for free. We ask that you: Read the Core tutorial and give us feedback! Do not share accounts. We will close accounts that we suspect to be shared. Use good passwords. We are a computer security testbed, so please use a strong password. For more details, see our Passwords page . Swap out your experiment when it is finished. People forget to do this all the time. We do have an idle detection mechanism, but it is not perfect and may keep thinking your experiment is active long since you have collected your results, published your paper, and achieved tenure. Please log back in and free up your nodes so that other researchers can use them . Do not abuse the no idle-swap feature. Only turn off idle swap if there is a true need. Take the time to script the setup process for your experiment or create custom disk images. Make good use of disk space. Our goal at DETER is to assist you in running experiments. We are happy to provide you with all the storage necessary to accomplish your experimental goals. Also, we maintain nightly backups going back a few weeks. However, we are not a substitute for personal or institutional storage. We are not a backup service. Please keep your important files backed up offsite at your institution. Our main machine room is technically subject to earthquakes, tsunamis, and liquefaction. Take a look at rsync . We are not an archive. Please clean up or move offsite any large log files and traces after your experiment is done and paper is published. This makes sure that storage is available to researchers who are actively using the testbed. Keep things tidy. Remove unused custom operating system images. Experts tell us that 22% of custom operating system images are never used even once. Why keep that around on disk so that it is backed up day after day? It slows down our backups and crash recovery disk checks, and takes resources away from other researchers. If you do need more space, just contact us. Quota limits and housekeeping requests allows us to have extra space available to allocate to you when you need it. Let us know if you are done with your project. - We'll clean it up for you! Talk to us if you need something - we are here to help you. We often can provide useful suggestions about running experiments on the testbed. First come, first served. Sort of\u2026 \u00b6 We don't have an advanced scheduler. Unfortunately, our software does not support advance reservations. But we are always willing to work with users to help them acquire nodes they need on time, and then help them hold those nodes as long as they need them. Sometimes this involves blocking the nodes out for a period of time. Other times this involves swapping the experiment in the night before the demo, when most nodes are free. Please submit a ticket if you need a reservation and we will work with you. We may swap out idle experiments. We audit the testbed for idle experiments. When the testbed is close to full we will ping the users with long-running, idle experiments and ask if they can be swapped out. If no response is received within 8 hours, we will swap out the experiment. Note that you may prevent this by promptly replying to our email and telling us you need the nodes. Very rarely, we may restrict use to a part of the testbed. There are times when a large-scale demonstration requires us to block off a part of our testbed. At these times, you will see reduced node availability but your swapped in experiments will not be disturbed. We will inform you about these planned events at least two weeks ahead, via news items on DETERLab's web page. Watch out for downtimes. We have regular weekly downtimes where we reserve the right to perform service-interrupting work on the testbed (most weeks we sail right through without any noticeable interruption of service) and sometimes special downtimes are required. We usually give a few days notice before the special downtimes. Keep an eye on the DETERLab News Page for notices. Privacy \u00b6 DETER is a resource shared by users around the world. While we do our best to keep experiments separated from each other, we can not provide any guarantee of privacy between projects. If you are concerned about privacy, please make sure you understand how to use UNIX permissions and encrypt your files when they are stored on our main file server. Feel free to contact us if you have any questions. Usage is also governed by the University of Southern California\u2019s Privacy Policy .","title":"Usage Policy"},{"location":"support/usage-policy/#deterlab-usage-policy","text":"","title":"Deterlab Usage Policy"},{"location":"support/usage-policy/#be-a-good-citizen","text":"DETER is a shared resource. We expect users to be good citizens by not abusing or wasting the resources we make available to them for free. We ask that you: Read the Core tutorial and give us feedback! Do not share accounts. We will close accounts that we suspect to be shared. Use good passwords. We are a computer security testbed, so please use a strong password. For more details, see our Passwords page . Swap out your experiment when it is finished. People forget to do this all the time. We do have an idle detection mechanism, but it is not perfect and may keep thinking your experiment is active long since you have collected your results, published your paper, and achieved tenure. Please log back in and free up your nodes so that other researchers can use them . Do not abuse the no idle-swap feature. Only turn off idle swap if there is a true need. Take the time to script the setup process for your experiment or create custom disk images. Make good use of disk space. Our goal at DETER is to assist you in running experiments. We are happy to provide you with all the storage necessary to accomplish your experimental goals. Also, we maintain nightly backups going back a few weeks. However, we are not a substitute for personal or institutional storage. We are not a backup service. Please keep your important files backed up offsite at your institution. Our main machine room is technically subject to earthquakes, tsunamis, and liquefaction. Take a look at rsync . We are not an archive. Please clean up or move offsite any large log files and traces after your experiment is done and paper is published. This makes sure that storage is available to researchers who are actively using the testbed. Keep things tidy. Remove unused custom operating system images. Experts tell us that 22% of custom operating system images are never used even once. Why keep that around on disk so that it is backed up day after day? It slows down our backups and crash recovery disk checks, and takes resources away from other researchers. If you do need more space, just contact us. Quota limits and housekeeping requests allows us to have extra space available to allocate to you when you need it. Let us know if you are done with your project. - We'll clean it up for you! Talk to us if you need something - we are here to help you. We often can provide useful suggestions about running experiments on the testbed.","title":"Be a good citizen"},{"location":"support/usage-policy/#first-come-first-served-sort-of","text":"We don't have an advanced scheduler. Unfortunately, our software does not support advance reservations. But we are always willing to work with users to help them acquire nodes they need on time, and then help them hold those nodes as long as they need them. Sometimes this involves blocking the nodes out for a period of time. Other times this involves swapping the experiment in the night before the demo, when most nodes are free. Please submit a ticket if you need a reservation and we will work with you. We may swap out idle experiments. We audit the testbed for idle experiments. When the testbed is close to full we will ping the users with long-running, idle experiments and ask if they can be swapped out. If no response is received within 8 hours, we will swap out the experiment. Note that you may prevent this by promptly replying to our email and telling us you need the nodes. Very rarely, we may restrict use to a part of the testbed. There are times when a large-scale demonstration requires us to block off a part of our testbed. At these times, you will see reduced node availability but your swapped in experiments will not be disturbed. We will inform you about these planned events at least two weeks ahead, via news items on DETERLab's web page. Watch out for downtimes. We have regular weekly downtimes where we reserve the right to perform service-interrupting work on the testbed (most weeks we sail right through without any noticeable interruption of service) and sometimes special downtimes are required. We usually give a few days notice before the special downtimes. Keep an eye on the DETERLab News Page for notices.","title":"First come, first served. Sort of\u2026"},{"location":"support/usage-policy/#privacy","text":"DETER is a resource shared by users around the world. While we do our best to keep experiments separated from each other, we can not provide any guarantee of privacy between projects. If you are concerned about privacy, please make sure you understand how to use UNIX permissions and encrypt your files when they are stored on our main file server. Feel free to contact us if you have any questions. Usage is also governed by the University of Southern California\u2019s Privacy Policy .","title":"Privacy"},{"location":"tools/","text":"DETERLab Tools \u00b6 This page links to information on useful tools available for DETERLab: Internet Atlas Tool","title":"Tools Index"},{"location":"tools/#deterlab-tools","text":"This page links to information on useful tools available for DETERLab: Internet Atlas Tool","title":"DETERLab Tools"},{"location":"tools/internet-atlas/","text":"Internet Atlas to NS2 \u00b6 Internet Atlas to NS2 is a continuation of Internet Atlas , which provides a geographically anchored representation of the physical Internet including (i) nodes (e.g., hosting facilities and data centers), (ii) conduits/links that connect these nodes, and (iii) relevant meta data (e.g., source provenance). This tool adds the feature of extrapolating the generated map to a Network Simulator 2 topology and allows you to customize it by filtering the topology based on geo-coordinates. The customized map of network topology can be converted into a Network Simulator script that can then be run on an NS engine. Download the Code \u00b6 To use this tool, (obtain the source code from /deterlab/library/geocoder) and copy it to your local workstation. Then following these instructions: Prerequisites \u00b6 Python above version 2.7. Note: If you are using Python 3, you will need to convert the syntax of the geocoding.py script and troubleshoot any bugs the conversion may introduce. pip (to install the required package) Install the geocoder Python package via pip: sudo pip install geocoder How To \u00b6 Run the python script \u00b6 Run the geocoding python script. You only need to run this once - if you see geocoded.json in the source directory, you may skip this step. python geocoding.py Using the form \u00b6 Run python's small server locally to host the Internet Atlas form, fill out the form to generate the desired map and then download the .NS file of your new topology: Run the following from the montage directory: python -m SimpleHTTPServer & Launch a browser (preferably a recent version of Google Chrome, Firefox, or Safari). In a browser window, go to: http://localhost:8000/home.html Specify any of the parameters of your choice in the web page. Remember to include the minus sign (-) if applicable. For example, to target nodes North and East of Tucson, AZ, you would enter 32.2217 in the North field and -110.9265 in the East field. Click Generate Map to get a preview of the topology. The above parameters may generate a map such as this: Once you are satisfied with the topology, click Generate NS File to download the corresponding .NS file. Further details \u00b6 Internet Atlas is a visualization and analysis portal for diverse Internet measurement data. Initial ventures include but are not limited to: R. Durairajan, S. Ghosh, X. Tang, P. Barford, and B. Eriksson. Internet Atlas: A Geographic Database of the Internet. In Proceedings of ACM HotPlanet, 2013. R. Durairajan, P. Barford, J.Sommers and W. Willinger. InterTubes: A Study of the US Long-haul Fiber-optic Infrastructure. In Proceedings of ACM SIGCOMM, 2015.","title":"Internet Atlas"},{"location":"tools/internet-atlas/#internet-atlas-to-ns2","text":"Internet Atlas to NS2 is a continuation of Internet Atlas , which provides a geographically anchored representation of the physical Internet including (i) nodes (e.g., hosting facilities and data centers), (ii) conduits/links that connect these nodes, and (iii) relevant meta data (e.g., source provenance). This tool adds the feature of extrapolating the generated map to a Network Simulator 2 topology and allows you to customize it by filtering the topology based on geo-coordinates. The customized map of network topology can be converted into a Network Simulator script that can then be run on an NS engine.","title":"Internet Atlas to NS2"},{"location":"tools/internet-atlas/#download-the-code","text":"To use this tool, (obtain the source code from /deterlab/library/geocoder) and copy it to your local workstation. Then following these instructions:","title":"Download the Code"},{"location":"tools/internet-atlas/#prerequisites","text":"Python above version 2.7. Note: If you are using Python 3, you will need to convert the syntax of the geocoding.py script and troubleshoot any bugs the conversion may introduce. pip (to install the required package) Install the geocoder Python package via pip: sudo pip install geocoder","title":"Prerequisites"},{"location":"tools/internet-atlas/#how-to","text":"","title":"How To"},{"location":"tools/internet-atlas/#run-the-python-script","text":"Run the geocoding python script. You only need to run this once - if you see geocoded.json in the source directory, you may skip this step. python geocoding.py","title":"Run the python script"},{"location":"tools/internet-atlas/#using-the-form","text":"Run python's small server locally to host the Internet Atlas form, fill out the form to generate the desired map and then download the .NS file of your new topology: Run the following from the montage directory: python -m SimpleHTTPServer & Launch a browser (preferably a recent version of Google Chrome, Firefox, or Safari). In a browser window, go to: http://localhost:8000/home.html Specify any of the parameters of your choice in the web page. Remember to include the minus sign (-) if applicable. For example, to target nodes North and East of Tucson, AZ, you would enter 32.2217 in the North field and -110.9265 in the East field. Click Generate Map to get a preview of the topology. The above parameters may generate a map such as this: Once you are satisfied with the topology, click Generate NS File to download the corresponding .NS file.","title":"Using the form"},{"location":"tools/internet-atlas/#further-details","text":"Internet Atlas is a visualization and analysis portal for diverse Internet measurement data. Initial ventures include but are not limited to: R. Durairajan, S. Ghosh, X. Tang, P. Barford, and B. Eriksson. Internet Atlas: A Geographic Database of the Internet. In Proceedings of ACM HotPlanet, 2013. R. Durairajan, P. Barford, J.Sommers and W. Willinger. InterTubes: A Study of the US Long-haul Fiber-optic Infrastructure. In Proceedings of ACM SIGCOMM, 2015.","title":"Further details"}]}