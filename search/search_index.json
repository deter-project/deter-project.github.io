{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. These pages constitute the end-user documentation for DETERLab documentation. What is DETERLab? \uf0c1 DETERLab is a state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. To date, DETERLab-based projects have included behavior analysis and defensive technologies including DDoS attacks, worm and botnet attacks, encryption, pattern detection, and intrusion-tolerant storage protocols. Important Links \uf0c1 Testbed - this is the web interface to DETERLab (requires registration ) Support - this website provides support for DETERLab using a ticketing system. Organization \uf0c1 This site is organized via menu on the left to provide guidelines on how to use DETERLab for different user communities. Please explore the menu to learn more. You can start with quickstart documentation","title":"Welcome to the DETERLab Documentation"},{"location":"#what-is-deterlab","text":"DETERLab is a state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. To date, DETERLab-based projects have included behavior analysis and defensive technologies including DDoS attacks, worm and botnet attacks, encryption, pattern detection, and intrusion-tolerant storage protocols.","title":"What is DETERLab?"},{"location":"#important-links","text":"Testbed - this is the web interface to DETERLab (requires registration ) Support - this website provides support for DETERLab using a ticketing system.","title":"Important Links"},{"location":"#organization","text":"This site is organized via menu on the left to provide guidelines on how to use DETERLab for different user communities. Please explore the menu to learn more. You can start with quickstart documentation","title":"Organization"},{"location":"Glossary/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. DETERLab Glossary \uf0c1 Agent Abstraction Language (AAL) A YAML based descriptive language that the MAGI Orchestrator uses to describe an experiment\u2019s procedural workflow. The entire experiment procedure needs to be expressed as part of an AAL file. Find more information in the Orchestrator Guide . agent In MAGI Orchestrator , a piece of code that instruments a given functional behavior. Find more information in the Orchestrator Guide . Boss network ( myboss.isi.deterlab.net ) The main testbed server that runs DETERLab. Users are not allowed to log directly into it. collectors/collector nodes In MAGI Orchestrator , a set of nodes that capture and store experiment status and data. container_image.py In the Containers (Virtualization) system , a command that draws a picture of the topology of an experiment. This is helpful in keeping track of how virtual nodes are connected. For more information, see the Containers Reference . containerize.py In Containers (Virtualization) system , the command that creates a DETER experiment made up of containers. For more information, see containerize.py . container In the Containers (Virtualization) system , any one of various virtualization technologies, from an Openvz container to a physical machine to a simulation. The Containers system allows you to create interconnections of containers (in our sense) holding different experiment elements. data management layer In the MAGI Orchestrator , helps agents collect data. Users can query for the collected data by connecting to the data management layer. DBDL In the MAGI Orchestrator , the section of the experiment config file that configures the data layer. DETER web interface (isi.deterlab.net) The browser-based web portal for starting and defining experiments in DETERLab. DETER Stands for cyber DEfense Technology Experimental Research and is an organization out of ISI/USC conducting research (the DETER Project) as well as the operator of a leading cyber security experimentation lab, DETERLab. DETER's mission is to readily enable the research community to conduct advanced research in cyber security through use of DETERLab's innovative methods and advanced tools -- that allow for repeatable, scalable and scientifically verifiable experimentation -- for homeland security and critical infrastructure protection. DETER Project A research project run by DETER that focuses on answering key questions about how best to conduct cyber-security research, what are the best methods and tools to carry out this kind of research, and how to conduct cyber research in a repeatable, archivable, hypothesis-based way. For more information, see https://www.deter-project.org/about_deter_project . DETERLab A state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. For more information, see https://www.deter-project.org/about_deterlab . Emulab A network testbed designed to provide researchers a wide range of environments in which to develop, debug, and evaluate their systems. The DETERLab system is based on Emulab. endnode tracing/monitoring Refers to putting the trace hooks on the end nodes of a link, instead of on delay nodes. This happens when there are no delay nodes in use or if you have explicitly requested end node shaping to reduce the number of nodes you need for an experiment. event groups In MAGI Orchestrator , a group of events that enable a behavior in the experiment, for example web server events. event streams In MAGI Orchestrator , the order of events in a particular MAGI (orchestrated) experiment. Events are one of two things, events or triggers. Events are similar to remote procedure calls on agents. Triggers are synchronization and/or branching points in your event stream. A simple example of an event stream could be: configure the agent, tell it to begin monitoring the local node, wait for 60 seconds, then tell the agent to stop monitoring. When the event stream has no more events or triggers, the orchestrator will exit. event system In MAGI Orchestrator , a messaging system to send and receive orchestration messages. events In MAGI Orchestrator , events are one of two things, events or triggers. Events are similar to remote procedure calls on agents. Triggers are synchronization and/or branching points in your event stream. ExpDL In the MAGI Orchestrator , the section of the experiment config file that configures common experiment wide configuration that is used by the various MAGI tools. Most of the configuration in this section is automatically generated. experiment Work in DETERLab is organized by experiments within project. experiment.conf In the MAGI Orchestrator , the experiment-wide, YAML-based configuration file. federation In general, refers to the ability to join disparate resources as if they were the same resource. DETERLab offers federated architecture for creating experiments that span multiple testbeds through dynamically acquiring resources from other testbeds. fidelity In general, means the degree to which something is faithfully reproduced. In DETERLab, refers to varying degrees different elements of a large-scale experiment (in the Containers (Virtualization) system , for example) need to be fully reproduced. Some processes require high degree of fidelity while others do not. gatekeeper Protects the internet facing side of the testbed and serves as a NAT machine for the Private Internet Network using a bridging firewall. image IDs A descriptor for a disk image. This can be an image of an entire disk, a partition of a disk, or a set of partitions of a disk. image Refers to a disk image. interface description file (IDL) In the MAGI Orchestrator , refers to a YAML-based file that describes an agent's interface. This is required when writing your own agents and allows an agent to be integrated with the experiment GUIs. link tracing/monitoring The ability to follow the path of a link or LAN in a DETERLab experiment. MAGI A DETERLab system that allows you to orchestrate very complex experiments. Stands for Montage AGent Infrastructure (Montage was originally an experiment lifecycle manager in DETERLab). For more information, see MAGI Orchestrator docs . MAGI Graph In the MAGI Orchestrator , a graph generator for experiments executed on DETER Lab using MAGI. The tool fetches the required data using MAGI\u2019s data management layer and generates a graph in PNG format. MAGI Status In the MAGI Orchestrator , a status tool that allows you to check MAGI\u2019s status on experiment nodes, reboot MAGI daemon process, and download logs from experiment nodes. magi_bootstrap.py In the MAGI Orchestrator , installs the MAGI distribution along with the supporting tools when setting up a MAGI-enabled experiment in DETERLab. magi_orchestrator.py In the MAGI Orchestrator , a tool that parses an AAL file and orchestrates an experiment based on the specified procedures. Messaging Description Language (MESDL) In the MAGI Orchestrator , the section of the experiment config file that defines all the overlays and bridges for the experiment. The MAGI \u201coverlay\u201d is built on top of the control network on the DETERLab testbed. Nodes in the overlay can be thought of as being connected by virtual point-to-point links. The overlay provides a way to communicate with all the MAGI-enabled nodes across the testbed boundaries and even over the internet. \u201cBridges\u201d act as meeting places for all the members of the overlay it serves. node.conf In the MAGI Orchestrator , a node-specific configuration file used by the MAGI daemon process. As part of the bootstrap process, the experiment-wide config file (experiment.conf) is already broken down into node-level configuration. But you may customize node configuration via this file and it may be provided as an input to the MAGI bootstrap script, or to the MAGI daemon startup script, directly. nodes A node simply means any computer allocated to an experiment. NS (Network Simulators) files syntax Used to describe topologies in network experiments. DETERLab-specific information may be found in the topology guide and further documentation is available at http://www.isi.edu/nsnam/ns/ . operating system images (OSIDs) Describes an operating system which resides on a partition of a disk image. Every ImageID will have at least one OSID associated with it. orchestrator see [orchestrator.py], in the MAGI Orchestrator . project Each group of users (or team) using DETERLab is grouped into 'projects', identified by a project ID (PID). swapin The process where DETERLab allocates resources for your experiment and runs it. swapout The process where DETERLab frees up the resources that were being used for your experiment. It is very important to do so when you are no longer using your experiment so that these resources are available for other experiments. topology A description of the various elements of a computer network. In DETERLab, your experiment requires a topology in NS syntax that describes the links, nodes, etc of your experiment. triggers In the MAGI Orchestrator , these are synchronization and/or branching points in a stream of events. users network ( users.deterlab.net ) DETERLab's file server that serves as a shell host for testbed users. YAML A simple format for describing data, used as the syntax for many configuration files throughout DETERLab systems. In general, just follow configuration file documentation, but if you are curious about specifications, you may find the latest here: http://www.yaml.org/spec/1.2/spec.html","title":"Glossary"},{"location":"Glossary/#deterlab-glossary","text":"Agent Abstraction Language (AAL) A YAML based descriptive language that the MAGI Orchestrator uses to describe an experiment\u2019s procedural workflow. The entire experiment procedure needs to be expressed as part of an AAL file. Find more information in the Orchestrator Guide . agent In MAGI Orchestrator , a piece of code that instruments a given functional behavior. Find more information in the Orchestrator Guide . Boss network ( myboss.isi.deterlab.net ) The main testbed server that runs DETERLab. Users are not allowed to log directly into it. collectors/collector nodes In MAGI Orchestrator , a set of nodes that capture and store experiment status and data. container_image.py In the Containers (Virtualization) system , a command that draws a picture of the topology of an experiment. This is helpful in keeping track of how virtual nodes are connected. For more information, see the Containers Reference . containerize.py In Containers (Virtualization) system , the command that creates a DETER experiment made up of containers. For more information, see containerize.py . container In the Containers (Virtualization) system , any one of various virtualization technologies, from an Openvz container to a physical machine to a simulation. The Containers system allows you to create interconnections of containers (in our sense) holding different experiment elements. data management layer In the MAGI Orchestrator , helps agents collect data. Users can query for the collected data by connecting to the data management layer. DBDL In the MAGI Orchestrator , the section of the experiment config file that configures the data layer. DETER web interface (isi.deterlab.net) The browser-based web portal for starting and defining experiments in DETERLab. DETER Stands for cyber DEfense Technology Experimental Research and is an organization out of ISI/USC conducting research (the DETER Project) as well as the operator of a leading cyber security experimentation lab, DETERLab. DETER's mission is to readily enable the research community to conduct advanced research in cyber security through use of DETERLab's innovative methods and advanced tools -- that allow for repeatable, scalable and scientifically verifiable experimentation -- for homeland security and critical infrastructure protection. DETER Project A research project run by DETER that focuses on answering key questions about how best to conduct cyber-security research, what are the best methods and tools to carry out this kind of research, and how to conduct cyber research in a repeatable, archivable, hypothesis-based way. For more information, see https://www.deter-project.org/about_deter_project . DETERLab A state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. For more information, see https://www.deter-project.org/about_deterlab . Emulab A network testbed designed to provide researchers a wide range of environments in which to develop, debug, and evaluate their systems. The DETERLab system is based on Emulab. endnode tracing/monitoring Refers to putting the trace hooks on the end nodes of a link, instead of on delay nodes. This happens when there are no delay nodes in use or if you have explicitly requested end node shaping to reduce the number of nodes you need for an experiment. event groups In MAGI Orchestrator , a group of events that enable a behavior in the experiment, for example web server events. event streams In MAGI Orchestrator , the order of events in a particular MAGI (orchestrated) experiment. Events are one of two things, events or triggers. Events are similar to remote procedure calls on agents. Triggers are synchronization and/or branching points in your event stream. A simple example of an event stream could be: configure the agent, tell it to begin monitoring the local node, wait for 60 seconds, then tell the agent to stop monitoring. When the event stream has no more events or triggers, the orchestrator will exit. event system In MAGI Orchestrator , a messaging system to send and receive orchestration messages. events In MAGI Orchestrator , events are one of two things, events or triggers. Events are similar to remote procedure calls on agents. Triggers are synchronization and/or branching points in your event stream. ExpDL In the MAGI Orchestrator , the section of the experiment config file that configures common experiment wide configuration that is used by the various MAGI tools. Most of the configuration in this section is automatically generated. experiment Work in DETERLab is organized by experiments within project. experiment.conf In the MAGI Orchestrator , the experiment-wide, YAML-based configuration file. federation In general, refers to the ability to join disparate resources as if they were the same resource. DETERLab offers federated architecture for creating experiments that span multiple testbeds through dynamically acquiring resources from other testbeds. fidelity In general, means the degree to which something is faithfully reproduced. In DETERLab, refers to varying degrees different elements of a large-scale experiment (in the Containers (Virtualization) system , for example) need to be fully reproduced. Some processes require high degree of fidelity while others do not. gatekeeper Protects the internet facing side of the testbed and serves as a NAT machine for the Private Internet Network using a bridging firewall. image IDs A descriptor for a disk image. This can be an image of an entire disk, a partition of a disk, or a set of partitions of a disk. image Refers to a disk image. interface description file (IDL) In the MAGI Orchestrator , refers to a YAML-based file that describes an agent's interface. This is required when writing your own agents and allows an agent to be integrated with the experiment GUIs. link tracing/monitoring The ability to follow the path of a link or LAN in a DETERLab experiment. MAGI A DETERLab system that allows you to orchestrate very complex experiments. Stands for Montage AGent Infrastructure (Montage was originally an experiment lifecycle manager in DETERLab). For more information, see MAGI Orchestrator docs . MAGI Graph In the MAGI Orchestrator , a graph generator for experiments executed on DETER Lab using MAGI. The tool fetches the required data using MAGI\u2019s data management layer and generates a graph in PNG format. MAGI Status In the MAGI Orchestrator , a status tool that allows you to check MAGI\u2019s status on experiment nodes, reboot MAGI daemon process, and download logs from experiment nodes. magi_bootstrap.py In the MAGI Orchestrator , installs the MAGI distribution along with the supporting tools when setting up a MAGI-enabled experiment in DETERLab. magi_orchestrator.py In the MAGI Orchestrator , a tool that parses an AAL file and orchestrates an experiment based on the specified procedures. Messaging Description Language (MESDL) In the MAGI Orchestrator , the section of the experiment config file that defines all the overlays and bridges for the experiment. The MAGI \u201coverlay\u201d is built on top of the control network on the DETERLab testbed. Nodes in the overlay can be thought of as being connected by virtual point-to-point links. The overlay provides a way to communicate with all the MAGI-enabled nodes across the testbed boundaries and even over the internet. \u201cBridges\u201d act as meeting places for all the members of the overlay it serves. node.conf In the MAGI Orchestrator , a node-specific configuration file used by the MAGI daemon process. As part of the bootstrap process, the experiment-wide config file (experiment.conf) is already broken down into node-level configuration. But you may customize node configuration via this file and it may be provided as an input to the MAGI bootstrap script, or to the MAGI daemon startup script, directly. nodes A node simply means any computer allocated to an experiment. NS (Network Simulators) files syntax Used to describe topologies in network experiments. DETERLab-specific information may be found in the topology guide and further documentation is available at http://www.isi.edu/nsnam/ns/ . operating system images (OSIDs) Describes an operating system which resides on a partition of a disk image. Every ImageID will have at least one OSID associated with it. orchestrator see [orchestrator.py], in the MAGI Orchestrator . project Each group of users (or team) using DETERLab is grouped into 'projects', identified by a project ID (PID). swapin The process where DETERLab allocates resources for your experiment and runs it. swapout The process where DETERLab frees up the resources that were being used for your experiment. It is very important to do so when you are no longer using your experiment so that these resources are available for other experiments. topology A description of the various elements of a computer network. In DETERLab, your experiment requires a topology in NS syntax that describes the links, nodes, etc of your experiment. triggers In the MAGI Orchestrator , these are synchronization and/or branching points in a stream of events. users network ( users.deterlab.net ) DETERLab's file server that serves as a shell host for testbed users. YAML A simple format for describing data, used as the syntax for many configuration files throughout DETERLab systems. In general, just follow configuration file documentation, but if you are curious about specifications, you may find the latest here: http://www.yaml.org/spec/1.2/spec.html","title":"DETERLab Glossary"},{"location":"ISIUCB/","text":"DETERLab Rooms \uf0c1 DETERLab has three machine rooms. Nodes are split more or less evenly between the ISI, USC, and Berkeley. A 10 Gigabit link from ISI to USC and another 10 Gigabit link from ISI to Berkeley provides complete connectivity. ISI \uf0c1 The primary machine room is located in Southern California at The University of Southern California Information Sciences Institute (ISI) . This is where users.deterlab.net and www.isi.deterlab.net are located. USC \uf0c1 Another machine room is located in a Southern California colocation facility at The University of Southern California Information Technology Services (USC ITS) . UCB \uf0c1 The other machine room is located in Northern California at The University of California Berkeley (UCB) . Nodes that start with the letter b are located at UCB.","title":"DETERLab Rooms"},{"location":"ISIUCB/#deterlab-rooms","text":"DETERLab has three machine rooms. Nodes are split more or less evenly between the ISI, USC, and Berkeley. A 10 Gigabit link from ISI to USC and another 10 Gigabit link from ISI to Berkeley provides complete connectivity.","title":"DETERLab Rooms"},{"location":"ISIUCB/#isi","text":"The primary machine room is located in Southern California at The University of Southern California Information Sciences Institute (ISI) . This is where users.deterlab.net and www.isi.deterlab.net are located.","title":"ISI"},{"location":"ISIUCB/#usc","text":"Another machine room is located in a Southern California colocation facility at The University of Southern California Information Technology Services (USC ITS) .","title":"USC"},{"location":"ISIUCB/#ucb","text":"The other machine room is located in Northern California at The University of California Berkeley (UCB) . Nodes that start with the letter b are located at UCB.","title":"UCB"},{"location":"quickstart/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Quickstart \uf0c1 This page describes basic information about DETERLab and its core functionality. Please make sure to read detailed information about each step by clicking on the links in the menu or on the shapes in the figure. What is DETERLab? \uf0c1 The DETERLab testbed is a testbed for cybersecurity experimentation. It is an open public testbed and it is free for research and educational use, as well as for commercial product development and testing. DETERLab users gain superuser access to physical or virtual machines, and can install and run applications of their choice. Connections between machines (topology and routes) can also be configured by the user to meet their research needs. DETERLab is similar to a cloud, in a sense that one can allocate multiple nodes for an experiment. However, in a cloud users often receive virtual machines , and set them up with similar configurations to perform computation tasks . In DETERLab, users request physical machines , and also specify their connections . Users often set up these machines with different configurations , and use them to perform system and network tasks , and to generate traffic . Using DETERLab \uf0c1 Research Users \uf0c1 If you want to use DETERLab in your research your interaction will resemble the illustration below (pale colored shapes denote optional steps). Please explore links just below the diagram to learn more about how to register for, and interact with DETERLab as a researcher. For research users \uf0c1 Create new research project Join existing research project Create topology Create experiment Swap in experiment Interact with experiment Swap out experiment Modify experiment Terminate experiment Class Users \uf0c1 If you want to use DETERLab to teach a class or if you are a student in the class that requires DETERLab use, your interaction will resemble the illustration below (pale colored shapes denote optional steps). Please explore links just below the diagram to learn more about how to register for, and interact with DETERLab as a class user. For instructors and TAs \uf0c1 Create new class project Set up class Enroll students/TAs Add materials and assignments Monitor progress Collect submissions for grading Modify deadlines Recycle class For students \uf0c1 Create topology Create experiment Swap in experiment Interact with experiment Swap out experiment Modify experiment Terminate experiment Submit your work","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"This page describes basic information about DETERLab and its core functionality. Please make sure to read detailed information about each step by clicking on the links in the menu or on the shapes in the figure.","title":"Quickstart"},{"location":"quickstart/#what-is-deterlab","text":"The DETERLab testbed is a testbed for cybersecurity experimentation. It is an open public testbed and it is free for research and educational use, as well as for commercial product development and testing. DETERLab users gain superuser access to physical or virtual machines, and can install and run applications of their choice. Connections between machines (topology and routes) can also be configured by the user to meet their research needs. DETERLab is similar to a cloud, in a sense that one can allocate multiple nodes for an experiment. However, in a cloud users often receive virtual machines , and set them up with similar configurations to perform computation tasks . In DETERLab, users request physical machines , and also specify their connections . Users often set up these machines with different configurations , and use them to perform system and network tasks , and to generate traffic .","title":"What is DETERLab?"},{"location":"quickstart/#using-deterlab","text":"","title":"Using DETERLab"},{"location":"quickstart/#research-users","text":"If you want to use DETERLab in your research your interaction will resemble the illustration below (pale colored shapes denote optional steps). Please explore links just below the diagram to learn more about how to register for, and interact with DETERLab as a researcher.","title":"Research Users"},{"location":"quickstart/#for-research-users","text":"Create new research project Join existing research project Create topology Create experiment Swap in experiment Interact with experiment Swap out experiment Modify experiment Terminate experiment","title":"For research users"},{"location":"quickstart/#class-users","text":"If you want to use DETERLab to teach a class or if you are a student in the class that requires DETERLab use, your interaction will resemble the illustration below (pale colored shapes denote optional steps). Please explore links just below the diagram to learn more about how to register for, and interact with DETERLab as a class user.","title":"Class Users"},{"location":"quickstart/#for-instructors-and-tas","text":"Create new class project Set up class Enroll students/TAs Add materials and assignments Monitor progress Collect submissions for grading Modify deadlines Recycle class","title":"For instructors and TAs"},{"location":"quickstart/#for-students","text":"Create topology Create experiment Swap in experiment Interact with experiment Swap out experiment Modify experiment Terminate experiment Submit your work","title":"For students"},{"location":"style-guide/","text":"Style guide \uf0c1 The following style guide should be followed when writing documentation for this site. When writing documentation for Wolf CMS, we highly appreciate people following this guide as it allows us to more easily incorporate contributed documentation. If the guide is lacking or faulty somehow, please do bring that to our attention. Thank you. Document title \uf0c1 Every document is starts with its own title and it is written as follows: Document title ============== Sub headings \uf0c1 The various levels of sub headings should be written as: Level two heading ----------------- ### Level three heading #### Level four heading ##### Level five heading ###### Level six heading Notes \uf0c1 Various types of notes can and should be used throughout the documentation to inform people of special cases, danger areas, give tips, etc. However, notes should not be used overly much. The various note types are: Note The informational note, written as: !!! note Some extra piece of information. Tip A note that gives a tip, written as: !!! tip A handy tip! Warning A note that gives a warning, written as: !!! warning Please make sure you backup your system before... Danger A note that warns of dangerous actions or settins, written as: !!! danger If you do this you will likely screw up your system! Code \uf0c1 When applicable, you should enhance your explanations using code examples. Settings, file names and code in general should either be written as short, inline entries using single backticks ( ` ) surrounding the code or as so called code blocks: ``` This is a code block. ``` Inline markup \uf0c1 one asterisk: *text* for emphasis ; two asterisks: **text** for strong emphasis ; Lists \uf0c1 List markup is natural: just place an asterisk at the start of a paragraph and indent properly. The same goes for numbered lists. * This is a bulleted list. * It has two items, the second item uses several lines which are all indented. 1. This is a numbered list. 2. It has two items too. Nested lists can be achieved by simply indenting them. Note When adding code blocks or similar structures to a list, make sure they are all indented, including any empty lines. Hyperlinks \uf0c1 When additional information is needed or when referencing other parts of the documentation, a link is required. You can link in several different ways depending on the requirements of the situation: Basic raw links <http://www.example.com/> Link using descriptive text [Link text](http://www.example.com/) Link to other pages within the documentation [Link text](path/to/doc.md) Link to an anchor within a page [Link text](yourpage.md#anchorname) Link to an same-page anchor [Link text](#anchorname) Please note that all headings are automatically anchors. Warning Due to a bug in the current MkDocs implementation, same-page anchor links should be written as [Link text](samepage.md#anchor) instead of the correct [Link text](#anchor) . Images \uf0c1 Images can be inserted either using markdown syntax or as basic HTML code when alignment or special styling is required. Whenever possible, stick to the markdown syntax. ![Alternate text](images/example-image.png) ![Alternate text](images/example-image.png \"Optional title\") <p align=\"center\"><img src=\"docs/images/example-image.png\" alt=\"Alternate text\"></p> Before you do this, you need to prepare image (crop, resize, mark...) and put it in the docs/images folder.","title":"Style guide"},{"location":"style-guide/#style-guide","text":"The following style guide should be followed when writing documentation for this site. When writing documentation for Wolf CMS, we highly appreciate people following this guide as it allows us to more easily incorporate contributed documentation. If the guide is lacking or faulty somehow, please do bring that to our attention. Thank you.","title":"Style guide"},{"location":"style-guide/#document-title","text":"Every document is starts with its own title and it is written as follows: Document title ==============","title":"Document title"},{"location":"style-guide/#sub-headings","text":"The various levels of sub headings should be written as: Level two heading ----------------- ### Level three heading #### Level four heading ##### Level five heading ###### Level six heading","title":"Sub headings"},{"location":"style-guide/#notes","text":"Various types of notes can and should be used throughout the documentation to inform people of special cases, danger areas, give tips, etc. However, notes should not be used overly much. The various note types are: Note The informational note, written as: !!! note Some extra piece of information. Tip A note that gives a tip, written as: !!! tip A handy tip! Warning A note that gives a warning, written as: !!! warning Please make sure you backup your system before... Danger A note that warns of dangerous actions or settins, written as: !!! danger If you do this you will likely screw up your system!","title":"Notes"},{"location":"style-guide/#code","text":"When applicable, you should enhance your explanations using code examples. Settings, file names and code in general should either be written as short, inline entries using single backticks ( ` ) surrounding the code or as so called code blocks: ``` This is a code block. ```","title":"Code"},{"location":"style-guide/#inline-markup","text":"one asterisk: *text* for emphasis ; two asterisks: **text** for strong emphasis ;","title":"Inline markup"},{"location":"style-guide/#lists","text":"List markup is natural: just place an asterisk at the start of a paragraph and indent properly. The same goes for numbered lists. * This is a bulleted list. * It has two items, the second item uses several lines which are all indented. 1. This is a numbered list. 2. It has two items too. Nested lists can be achieved by simply indenting them. Note When adding code blocks or similar structures to a list, make sure they are all indented, including any empty lines.","title":"Lists"},{"location":"style-guide/#hyperlinks","text":"When additional information is needed or when referencing other parts of the documentation, a link is required. You can link in several different ways depending on the requirements of the situation: Basic raw links <http://www.example.com/> Link using descriptive text [Link text](http://www.example.com/) Link to other pages within the documentation [Link text](path/to/doc.md) Link to an anchor within a page [Link text](yourpage.md#anchorname) Link to an same-page anchor [Link text](#anchorname) Please note that all headings are automatically anchors. Warning Due to a bug in the current MkDocs implementation, same-page anchor links should be written as [Link text](samepage.md#anchor) instead of the correct [Link text](#anchor) .","title":"Hyperlinks"},{"location":"style-guide/#images","text":"Images can be inserted either using markdown syntax or as basic HTML code when alignment or special styling is required. Whenever possible, stick to the markdown syntax. ![Alternate text](images/example-image.png) ![Alternate text](images/example-image.png \"Optional title\") <p align=\"center\"><img src=\"docs/images/example-image.png\" alt=\"Alternate text\"></p> Before you do this, you need to prepare image (crop, resize, mark...) and put it in the docs/images folder.","title":"Images"},{"location":"abac/","text":"ABAC \uf0c1 What is ABAC? \uf0c1 The ABAC project has designed and implemented tools for using Attribute-Based Access Control, a scalable authorization system based on formal logic. It maps principals to attributes and uses the attribute to make an authorization decision, e.g., if user1 has the login attribute the login program will allow them to log in. This library, libabac, is a base on which to build those tools. It is in use in the DETER Federation system and being integrated with the GENI network testbed . For more information, go to the ABAC site at: abac.deterlab.net/","title":"ABAC"},{"location":"abac/#abac","text":"","title":"ABAC"},{"location":"abac/#what-is-abac","text":"The ABAC project has designed and implemented tools for using Attribute-Based Access Control, a scalable authorization system based on formal logic. It maps principals to attributes and uses the attribute to make an authorization decision, e.g., if user1 has the login attribute the login program will allow them to log in. This library, libabac, is a base on which to build those tools. It is in use in the DETER Federation system and being integrated with the GENI network testbed . For more information, go to the ABAC site at: abac.deterlab.net/","title":"What is ABAC?"},{"location":"about/contributing/","text":"TBD","title":"Contributing"},{"location":"about/license/","text":"TBD","title":"License"},{"location":"about/release-notes/","text":"TBD","title":"Release notes"},{"location":"competitions/","text":"Important Our current support for competitions is limited. Please read this page carefully. Competitions on DETERLab \uf0c1 DETERLab can be used to run attack/defense exercises between teams of students or researchers. In such an exercise, a given experiment will be accessed by two teams - a blue (defense) team and a red (offense) team. DETERLab competitions UI enables: Easy creation of experiments and teams Specification of which teams can access which experiments and which machines in an experiment Experiment setup (swap in, application installation) Important Examples below assume that you want to run a competiton called MyComp in a class called MyClass on our new infrastructure. You will also need a project, which we will assume is called MyCompProject . Enabling competitions \uf0c1 If you wish to run competitions within your DETERLab project, please submit a ticket and provide rough estimates of how often you would run and how many machines you would need. We will review and enable competitions feature for your project. New competition \uf0c1 You can create a new competition from your \"My DETERLab\" view. There will be a \"Competitions\" tab once you have a competition-enabled project. From that tab, you can click on the left-hand menu option \"New competition\". The dialogue will ask you for the competition name and folder path. Currently you must specify a path to a folder that has a file called nodes , with each node's name on a separate line. Let's assume that you have specified a path to folder named MyComp . The dialogue will also ask you how many copies of the competition you need and whether team assignment should be \"paired\" or \"circular\". The figures below illustrate these two assignment types. In a circular assignment there are as many teams as there are experiments. Each team defends one experiment and attacks one other experiment. This team assignment enables participants on each team to play both defensive and offensive roles. In a paired assignment the number of teams is twice that of the experiments. Each team either defends one experiment or attacks one experiment. This team assignment places each participant into either offensive or defensive role. Important We currently only support the circular assignment. Once you click \"Submit\" DETERLab will create a teams for your competition. Writing start up scripts for competitions \uf0c1 Instead of start up scripts, please leverage our support for project-specific experiments, described here . These guidelines also describe how these materials need to be in a separate project on our new platform, and that project has to be in your class organization. Please create this project, which we will refer to as MyCompProject in the rest of this documentation. Managing your competition \uf0c1 Once your competition is created you can see it in the competitions tab. You can set it up, run it or destroy it. Set up \uf0c1 Set up includes: (1) specifying which teams can access which machines, (2) assigning participants to teams. Team access \uf0c1 Each experiment has a number of machines. You can specify which team can access each machine: defense (blue) team, offense (red) team or none. Blue team machines will be defended in a competition, red team machines will be used to launch attacks on blue team. You can use machines with no-team access for scoring or infrastructure set up (e.g. set up DNS root server that can be queried but not modified by teams). When you are finished, click \"Assign\" then \"Done\". You can always repeat this action, as long as the competition is not swapped in. Otherwise, you have to swap it out to change team access. Bulding teams \uf0c1 You can assign a participant to a team by dragging him/her into the gray blocks for the desired team. A participant can belong to at most one team. When you are finished click \"Assign\" then \"Done\". You can always repeat this action, as long as the competition is not swapped in. If a participant shows up after you have swapped in the competition, you have to swap out, add the participant, and swap in again. Run \uf0c1 The past two steps created some files in /proj/yourProjectOnOldInfrastructure/MyComp . On our new platform create a project for your competition, if you haven't already. Assume the project's name is MyCompProject . Create an XDC in the MyCompProject , assume its name is MyCompXDC . Copy the entire folder from /proj/yourProjectOnOldInfrastructure/MyComp over to MyCompXDC . See here how to copy files to your XDC. Something like this should work on users.deterlab.net : mrg xdc ssh upload -r /proj/yourProjectOnOldInfrastructure/MyComp MyCompXDC.MyCompProject If you haven't already, make all your students part of the MyCompProject by typing on MyCompXDC: /share/addstudents MyCompProject MyClass Then to create competition experiments run the following on MyCompXDC: /share/startcompetition MyCompProj exp_name num_teams folder_w_model MyComp This will simply generate a script, which you then need to run via bash and monitor to ensure that everything progressed smoothly. Imagine that you ran the following command: /share/startcompetition compddos ddos 4 ddosf /users/jelena/ddosc This would generate a script to create experiments named ddos1, ddos2, ddos3 and ddos4 in your project compddos on the new infrastructure. It would mine team membership information and which machines should be accessible to red and blue teams from /users/jelena/ddosc folder (which was previously copied over from old DeterLab) and it would use startup scripts and topology model from /organization/ddosf. Important It is currently very difficult to add a new student to a running competition so please ensure that your team assignments are complete before you start the competition. Retiring the competition \uf0c1 You must manually release the machines (swap out) when your competition is done. Destroy \uf0c1 When you are completely done with your competition, click the \"Destroy\" button. This will remove the teams. It is important to clean up your competitions when you don't need them anymore to preserve DETERLab's resources.","title":"Competitions"},{"location":"competitions/#competitions-on-deterlab","text":"DETERLab can be used to run attack/defense exercises between teams of students or researchers. In such an exercise, a given experiment will be accessed by two teams - a blue (defense) team and a red (offense) team. DETERLab competitions UI enables: Easy creation of experiments and teams Specification of which teams can access which experiments and which machines in an experiment Experiment setup (swap in, application installation) Important Examples below assume that you want to run a competiton called MyComp in a class called MyClass on our new infrastructure. You will also need a project, which we will assume is called MyCompProject .","title":"Competitions on DETERLab"},{"location":"competitions/#enabling-competitions","text":"If you wish to run competitions within your DETERLab project, please submit a ticket and provide rough estimates of how often you would run and how many machines you would need. We will review and enable competitions feature for your project.","title":"Enabling competitions"},{"location":"competitions/#new-competition","text":"You can create a new competition from your \"My DETERLab\" view. There will be a \"Competitions\" tab once you have a competition-enabled project. From that tab, you can click on the left-hand menu option \"New competition\". The dialogue will ask you for the competition name and folder path. Currently you must specify a path to a folder that has a file called nodes , with each node's name on a separate line. Let's assume that you have specified a path to folder named MyComp . The dialogue will also ask you how many copies of the competition you need and whether team assignment should be \"paired\" or \"circular\". The figures below illustrate these two assignment types. In a circular assignment there are as many teams as there are experiments. Each team defends one experiment and attacks one other experiment. This team assignment enables participants on each team to play both defensive and offensive roles. In a paired assignment the number of teams is twice that of the experiments. Each team either defends one experiment or attacks one experiment. This team assignment places each participant into either offensive or defensive role. Important We currently only support the circular assignment. Once you click \"Submit\" DETERLab will create a teams for your competition.","title":"New competition"},{"location":"competitions/#writing-start-up-scripts-for-competitions","text":"Instead of start up scripts, please leverage our support for project-specific experiments, described here . These guidelines also describe how these materials need to be in a separate project on our new platform, and that project has to be in your class organization. Please create this project, which we will refer to as MyCompProject in the rest of this documentation.","title":" Writing start up scripts for competitions"},{"location":"competitions/#managing-your-competition","text":"Once your competition is created you can see it in the competitions tab. You can set it up, run it or destroy it.","title":"Managing your competition"},{"location":"competitions/#set-up","text":"Set up includes: (1) specifying which teams can access which machines, (2) assigning participants to teams.","title":"Set up"},{"location":"competitions/#team-access","text":"Each experiment has a number of machines. You can specify which team can access each machine: defense (blue) team, offense (red) team or none. Blue team machines will be defended in a competition, red team machines will be used to launch attacks on blue team. You can use machines with no-team access for scoring or infrastructure set up (e.g. set up DNS root server that can be queried but not modified by teams). When you are finished, click \"Assign\" then \"Done\". You can always repeat this action, as long as the competition is not swapped in. Otherwise, you have to swap it out to change team access.","title":"Team access"},{"location":"competitions/#bulding-teams","text":"You can assign a participant to a team by dragging him/her into the gray blocks for the desired team. A participant can belong to at most one team. When you are finished click \"Assign\" then \"Done\". You can always repeat this action, as long as the competition is not swapped in. If a participant shows up after you have swapped in the competition, you have to swap out, add the participant, and swap in again.","title":"Bulding teams"},{"location":"competitions/#run","text":"The past two steps created some files in /proj/yourProjectOnOldInfrastructure/MyComp . On our new platform create a project for your competition, if you haven't already. Assume the project's name is MyCompProject . Create an XDC in the MyCompProject , assume its name is MyCompXDC . Copy the entire folder from /proj/yourProjectOnOldInfrastructure/MyComp over to MyCompXDC . See here how to copy files to your XDC. Something like this should work on users.deterlab.net : mrg xdc ssh upload -r /proj/yourProjectOnOldInfrastructure/MyComp MyCompXDC.MyCompProject If you haven't already, make all your students part of the MyCompProject by typing on MyCompXDC: /share/addstudents MyCompProject MyClass Then to create competition experiments run the following on MyCompXDC: /share/startcompetition MyCompProj exp_name num_teams folder_w_model MyComp This will simply generate a script, which you then need to run via bash and monitor to ensure that everything progressed smoothly. Imagine that you ran the following command: /share/startcompetition compddos ddos 4 ddosf /users/jelena/ddosc This would generate a script to create experiments named ddos1, ddos2, ddos3 and ddos4 in your project compddos on the new infrastructure. It would mine team membership information and which machines should be accessible to red and blue teams from /users/jelena/ddosc folder (which was previously copied over from old DeterLab) and it would use startup scripts and topology model from /organization/ddosf. Important It is currently very difficult to add a new student to a running competition so please ensure that your team assignments are complete before you start the competition.","title":"Run"},{"location":"competitions/#retiring-the-competition","text":"You must manually release the machines (swap out) when your competition is done.","title":"Retiring the competition"},{"location":"competitions/#destroy","text":"When you are completely done with your competition, click the \"Destroy\" button. This will remove the teams. It is important to clean up your competitions when you don't need them anymore to preserve DETERLab's resources.","title":"Destroy"},{"location":"competitions/comp-script-guide/","text":"Writing start up scripts for competitions \uf0c1 A good start up script for a competition has the following properties: Is project and experiment agnostic - it takes project and experiment names as arguments from command line. This enables scaling and portability. Limits access to specific experiment's nodes to red/blue team or removes access to all teams Installs needed software Starts scoring Makes any changes persist through reboots Ends up with node reboot Please start from our sample script with these features and version it to satisfy your needs. Tip Don't forget to make your script executable","title":"Writing start up scripts for competitions"},{"location":"competitions/comp-script-guide/#writing-start-up-scripts-for-competitions","text":"A good start up script for a competition has the following properties: Is project and experiment agnostic - it takes project and experiment names as arguments from command line. This enables scaling and portability. Limits access to specific experiment's nodes to red/blue team or removes access to all teams Installs needed software Starts scoring Makes any changes persist through reboots Ends up with node reboot Please start from our sample script with these features and version it to satisfy your needs. Tip Don't forget to make your script executable","title":"Writing start up scripts for competitions"},{"location":"containers/containers-guide/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Containers Guide \uf0c1 In this tutorial we walk you through setting up a basic containerized experiment. This page also includes common advanced topics. Detailed descriptions of the commands and configuration files are available in the reference section . Basic Containers Tutorial \uf0c1 This tutorial will set up a containerized experiment with a star topology. We'll create a central node and connect 9 other nodes to it Getting started \uf0c1 You will need a DETERLab account and be a member of a project. If you need help, see Opening Account documentation on the left sidebar. Step 1: Design the topology \uf0c1 First, we will describe a star topology. For this example we will use the standard DETER topology descriptions. If you are new to designing topologies, walk through the basic tutorial in our topology guide . The Containers system is largely compatible with the physical DETER interface. Download the DETERLab-compatible ns2 description of this topology at this link to your home directory on users.deterlab.net . It is a simple loop, along with the standard DETER boilerplate. This file will be used to create a 10-node (9 satellites and one central node) physical experiment on DETER, although there are not many physical nodes on DETER with 10 interfaces (one interface for control traffic). The following is the topology description: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtproto Static $ns run Note The central node is named \"center\" and each satellite is names \"n-0\", \"n-1\"... through \"n-8\". Each connection is a 100 Mb/s link with a 10ms delay. The round trip time from n-0 to center will be 20 ms and from n-0 to n-1 will be 40 ms. Step 2: Create the containerized experiment \uf0c1 Now we will run a command so the Containers system will build the containerized experiment on top of a new DETERLab physical experiment. Run the following command from the shell on users.deterlab.net and refer to the example topology you just saved in your home. $ /share/containers/containerize.py DeterTest example1 ~/example1.tcl where the first two parameters are the project and experiment name to hold the DETER experiment. This command creates an experiment called experiment1 in the DeterTest project. Throughout this tutorial, we will refer to your project as DeterTest , but make sure you actually use your actual project's name. You may use the experiment name experiment1 as long as another experiment with that name doesn't already exist Note As with any DETERLab experiment, you must be a member of the project with appropriate rights to create an experiment in it. containerize.py expects there to be no experiment with that name, and it will fail if one exists. To remove an experiment you may terminate it through the web interface or use the endexp command. Terminating an experiment is more final than swapping one out, so be sure that you want to replace the old experiment. You may also resolve the conflict by renaming your new containerized experiment. The last parameter is the file containing the topology. In this tutorial, we are referring to the ns2 file in our example but you may also use a topdl description. An ns2 description must end in .tcl or .ns . With these default parameters containerize.py will put each node into an Openvz container with at most 10 containers per physical node. The output of the above command should be something like the following: users:~$ /share/containers/containerize.py DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net/showexp.php3?pid=DeterTest&eid=example1 Now we can see what a containerized experiment looks like. The Contents of a Containerized Experiment \uf0c1 Follow the link provided in the containerize.py output. You will see a standard DETER experiment page that looks like this: You may be surprised to see that DETER thinks the experiment has only one node: The Containers system has rewritten the description file and stored additional information in the experiment's per-experiment directory that will be used to create the 10 node experiment inside the single-node DETER experiment. If you look at the ns file DETERLab has stored (click the NS File tab on the experiment page), you should see the following code: set ns [new Simulator] source tb_compat.tcl tb-make-soft-vtype container0 {pc2133 bpc2133 MicroCloud} set pnode(0000) [$ns node] tb-set-node-os ${pnode(0000)} CentOS6-64-openvz tb-set-hardware ${pnode(0000)} container0 tb-set-node-startcmd ${pnode(0000)} \"sudo /share/containers/setup/hv/bootstrap /proj/DeterTest/exp/example1/containers/site.conf >& /tmp/container.log\" tb-set-node-failure-action ${pnode(0000)} \"nonfatal\" $ns rtproto Static $ns run This looks nothing like the file we gave to containerize.py , but it does show us a little about what the Containers system has done: The single physical node ( pnode(0000) ) will run the CentOS6-64-openvz image and run on a few kinds of node. On startup, pnode(0000) will execute a command from the same /share/containers directory that containerize.py ran from using data in the per-experiment directory /proj/DeterTest/exp/example1/containers/site.conf . There is a separate /proj/DeterTest/exp/example1/containers/ directory for each experiment. The path element after /proj is replaced with the project under which the experiment was created -- DeterTest in this example -- and the element after exp is the experiment name -- example1 in this case. These directories are created for all DETERLab experiments. containers sub-directory \uf0c1 The containers sub-directory of a containerized experiment holds information specific to a containerized experiment. There are a few useful bits of data in that per-experiment containers directory that we can look at. Copy of the topology file: First, a copy of the topology that we gave to containerize.py is available in /proj/DeterTest/exp/example1/containers/experiment.tcl . If the experiment is created from a topdl file, the filename will be containers/experiment.tcl . Visualization of experiment: A simple visualization of the experiment is in containers/visualization.png . This is annotated with node and network names as well as interface IP addresses. The topology depiction above is an example. To view a larger version, click here . IP-to-hostname mapping: The containers/hosts file is a copy of the IP-to-hostname mapping found on each virtual machine in the topology. It can be useful in converting IP addresses back to names. It is installed in /etc/hosts or the equivalent on each machine. PID/EID: The two files /var/containers/pid and /var/containers/eid contain the project name and experiment name. Scripts can make use of these. The rest of the contents of that directory are primarily used internally by the implementation, but a more detailed listing is in the Containers Reference . Step 3: Swap-in resources \uf0c1 At this point, as with any DETER experiment, the topology does not yet have any resources attached. To get the resources, swap the experiment in from the web interface or using the swapexp command. See the DETERLab Commands for more information. Step 4: Verify virtual topology and access nodes \uf0c1 Once you have been notified that the physical experiment has finished its swap-in, the Containers system starts converting the physical topology into the virtual topology. At this time, you must manually verify when the virtual topology has been created by ping-ing or trying to SSH into individual nodes of an experiment. We are working towards offering a better notification system. Once the containerized elements have all started, the nodes are available as if they were physical nodes. For example, we may access node n-0 of the experiment we swapped in by running: $ ssh n-0.example1.detertest Be sure that you replace example1 with the experiment name you passed to containerize.py and DeterTest with the project you created the experiment under. This is a DNS name, so it is case-insensitive. When the SSH succeeds, you will have access to an Ubuntu 10.04 32-bit node with the same directories mounted as in a physical DETERLab experiment. Containerized nodes access the control net as well. Your home directory will be mounted, so your SSH keys will work for accessing the machine. Use the same node naming conventions as physical DETERLab experiments to ping and access other nodes. Here is a ping from n-0 to center and n-1 that confirms the containerized experiment is working as we expect. n-0:~$ ping -c 3 center PING center-tblink-l21 (10.0.0.2) 56(84) bytes of data. 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=1 ttl=64 time=20.4 ms 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=2 ttl=64 time=20.0 ms 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=3 ttl=64 time=20.0 ms --- center-tblink-l21 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2002ms rtt min/avg/max/mdev = 20.052/20.184/20.445/0.184 ms n-0:~$ ping -c 3 n-1 PING n-1-tblink-l5 (10.0.6.1) 56(84) bytes of data. 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=1 ttl=64 time=40.7 ms 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=2 ttl=64 time=40.0 ms 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=3 ttl=64 time=40.0 ms --- n-1-tblink-l5 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2003ms rtt min/avg/max/mdev = 40.094/40.318/40.764/0.355 ms The nodes have the expected round trip times. At this point you can load and run software and generally experiment normally. Start Commands \uf0c1 DETERLab Core provides a facility to run a command when a physical experiment starts, called start commands . A containerized experiment offers a similar facility with a few differences: The start commands are not coordinated across nodes. In a physical experiment, the start commands all execute when the last node has reported to the testbed that it has completed booting. In a containerized experiment, the start commands run when the containerized node has come up. Logs from the start command are available in /var/containers/log/start_command.out and /var/containers/log/start_command.err . This is true on embedded pnodes as well. Start commands must be shorter than in a physical experiment because the Containers system is also using the facility. The event system cannot be used to replay the start command. Notes While start commands that make use of shell syntax for multiple commands and simple file redirection (e.g, > or <) may work, errors parsing redirection or other shell commands will cause the start command to fail silently . If you are doing anything more complex than calling a single program, we recommend that you create a simple script and run the script from the per-experiment directory or your home directory. This makes it more likely that the log files created by containers will have useful debugging information. We strongly recommend removing all shell redirection characters from the text of your start command. Redirecting I/O in the text of the start command may fail silently . Start commands offer a simple workaround for detecting that all nodes in an experiment have started: #!/bin/sh STARTDIR=\"/proj/\"`cat /var/containers/pid`\"/exp/\"`cat /var/containers/eid`\"/startup\" mkdir $STARTDIR date > $STARTDIR/`hostname` If you make the script above the start command of all nodes, the Containers system will put the time that each local container came up in the startup directory under the per-experiment directories. For example, n-0.example1.DeterTest will create /proj/DeterTest/exp/example1/startup/n-0 . Then you may monitor that directory on users to know which nodes are up. Step 4: Releasing Resources \uf0c1 As with a physical DETER experiment, release resources by swapping the experiment out using the web interface or the swapexp command. Advanced Topics \uf0c1 The previous tutorial described how to create an experiment using only openvz containers packed 10 to a machine. This section describes how to change those parameters. Using Other Container Types \uf0c1 To change the container type that containerize.py assigns to nodes, use the --default-container option. Valid choices follow the [ContainersQuickstart#KindsofContainers kinds of containers] DETERLab supports. Specifically: Parameter Container embedded_pnode Physical Node qemu Qemu VM openvz Openvz Container You can try this on our example topology : users:~$ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example1.tcl Requested a QEMU node with more than 7 experimental interfaces. Qemu nodes can only support 7 experimental interfaces. The Containers system is now using qemu containers to build our experiment. Unfortunately qemu containers only support 7 experimental interfaces, an internal limit on the number of interfaces the virtual hardware supports. Run the command again but use the attached <a href=\"../../downloads/example2.tcl\" version of the topology with fewer satellites to containerize without error. $ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example2.tcl Containerized experiment DeterTest/example2 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example2 The qemu experiment looks much like the openvz experiment above, at this small scale. Qemu nodes more completely emulate hardware and the kernels are independent, unlike openvz containers. For example, a program can load kernel modules in a qemu VM, which it cannot do in an openvz container. The qemu containers load the Ubuntu 12.04 (32 bit) distribution by default. Mixing Containers \uf0c1 Mixing containers requires you to assign container types in the topology description. This is done by attaching an attribute to nodes. The attribute is named containers:node_type and it takes the same values as the --default-container parameter to containerize.py . If the experiment definition is in topdl , the attribute can be attached using the standard topdl routines . Attaching the attribute in ns2 is done using the DETERLab tb-add-node-attribute command. tb-add-node-attribute $node containers:node_type openvz Using this command in an ns2 topology description will set node to be placed in an openvz container. Using this feature, we can modify our first example topology to consist of qemu nodes and a single OpenVZ container in the center. The new topology file looks like this: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] # The center node is an openvz VM tb-add-node-attribute $center containers:node_type openvz # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Satellites are qemu nodes tb-add-node-attribute $n($i) containers:node_type qemu # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtproto Static $ns run Because we have explicitly set the container_node_type of each node, the --default-container parameter to containerize.py does nothing. Create this experiment by running: users:~$ /share/containers/containerize.py DeterTest example3 ~/example3.tcl Containerized experiment DeterTest/example3 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example3 When we swap it in, the experiment will have 10 satellite containers in qemu VMs and a central OpenVZ VM that forwards packets. Another interesting mixture of containers is to include a physical node. Here is a modified version of our mixed topology that places the n-8 satellite on a physical computer by setting its containers:node_type to embedded_pnode . After running that experiment you should have output similar to the following: users:~$ /share/containers/containerize.py DeterTest example4 ~/example4.tcl Containerized experiment DeterTest/example4 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example4 Follow the url to the DETERLab experiment page and look at the Visualization tab: The physical node n-8 shows up in the DETERLab visualization and otherwise acts as a physical node that is in a 10-node topology. This experiment uses three different container types: physical nodes, ViewOS processes, and Qemu VMs. (Note that ViewOS containers are not currently supported on DETER.) Limitations on Mixing Containers \uf0c1 Physical node containers are mapped one-to-one to physical nodes by definition. Qemu and OpenVZ use different underlying operating system images in DETERLab, therefore they cannot share physical hardware. Qemu VMs cannot share a physical machine with OpenVZ VMs. If these container types are mixed in an experiment, they will always be assigned different physical nodes. The first invocation of tb-add-node-attribute takes precedence. It is best to only call tb-add-node-attribute once per attribute assigned on each node. Setting Openvz Parameters \uf0c1 An advantage of openvz nodes is that you can set the OS flavor and CPU bit-width across experiments and per-node. Similarly, you can set the size of the disk allocated to each node. Openvz uses templates to look like various Linux installations. The choices of Linux distribution that openvz supports are: Template Distribution Bit-width centos-6-x86 CentOS 6 32 bit centos-6-x86_64 CentOS 6 64 bit ubuntu-12.04-x86 Ubuntu 12.04 LTS 32 bit ubuntu-12.04-x86_64 Ubuntu 12.04 LTS 64 bit ubuntu-14.04-x86 Ubuntu 14.04 LTS 32 bit ubuntu-14.04-x86_64 Ubuntu 14.04 LTS 64 bit The default template is ubuntu-14.04-x86_64 . To set a template across an entire topology, give --openvz-template and the template name from the list above. Invoking containerize.py on our original example as below will instantiate the experiment under 64-bit Ubuntu 12.04: users:~$ /share/containers/containerize.py --openvz-template ubuntu-12.04-x86_64 DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 To set the size of the file system of containers in the experiment, use --openvz-diskspace . The value of the parameter is determined by the suffix: Suffix Value G Gigabytes M Megabytes The default openvz file system size is 2GB. The most practical suffix for DETERLab nodes is \"G\": users:~$ /share/containers/containerize.py --openvz-diskspace 15G DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 Each of these parameters can be set on individual nodes using attributes. Use containers:openvz_template to set a template on a node and use containers:openvz_diskspace to set the disk space. This example topology sets these openvz parameters per node: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] tb-add-node-attribute $center containers:openvz_template ubuntu-12.04-x86_64 # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Set satellite disk sizes to be 20 GB tb-add-node-attribute $n($i) containers:openvz_diskspace 20G # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtptoto Static $ns run The center node will run Ubuntu 12.04 64 bit and the satellites will have 20GB file systems. Setting Qemu Parameters \uf0c1 The Containers system has a more limited ability to set qemu parameters. Right now, a custom image may be loaded using the containers::qemu_url attribute and the architecture of the qemu VM may be chosen using containers:qemu_arch . Valid qemu architectures are: Param Meaning i386 32 bit Intel x86_64 64-bit Intel The image URL must be reachable from inside DETERLab. The image must be a qcow2 image, optionally bzip2ed. Facilities to snapshot and store such images are in development. If you are using a qemu image that is not booting into containers, make sure grub is properly configured. Qemu images also mount users' home directories the same as DETERLab physical nodes do. In order to do this scalably, the Qemu VMs mount the users' directories from the physical node. The DETER infrastructure cannot support exporting users' directories to thousands of containers. However, a Qemu VM can only mount a few tens of user directories this way. The limit is 23 user directories (24 in experiments that are not instantiated in a group). Many projects have more than 23 users, but in practice only a few experimenters need access to the containers. To tell the Containers systems which user to mount, use the --qemu-prefer-users option to containerize.py . That option takes a comma-separated list of usernames (no spaces). When the Qemu nodes will always mount those users' home directories. Others will be mounted if there is room. For example: users:~$ /share/containers/containerize.py --qemu-prefer-users=faber,jjh DeterTest example4 ~/example4.tcl This command makes sure that users faber and jjh have their home directories mounted in any Qemu containers. Changing The Packing Factor \uf0c1 The containerize.py program decides how many virtual nodes to put on each physical machine. Because we have been using roughly the same number of nodes as the default packing target (10 nodes per machine) all of the examples so far have fit onto a single machine. If we change the packing factor by using the --packing parameter to containerize.py , we can put fewer nodes on each machine. For example: users:~$ /share/containers/containerize.py --packing 2 DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 This command calls containerize.py on our original topology with a low packing factor. The result is the same nodes spread across more physical machines, as we can see from the DETERLab web interface (on the Visualization tab): You will want to balance how many physical machines you use against how precisely you want to mimic them. User Packing \uf0c1 You may specify your own packing using the containers:partition attribute. This attribute must be assigned an integer value. All nodes with the same partition are allocated to the same machine. If nodes have that attribute attached to them, containers.py will assume that they all have been partitioned and use those. Nodes without a partition assigned are assumed to be embedded_pnode s. More Sophisticated Packing: Multiple Passes \uf0c1 The previous examples have all treated packing containers onto physical machines as a single-step process with a single parameter - the packing factor. In fact, we can divide containers into sets and pack each set independently using different parameters. For example in an experiment with many containers dedicated only to forwarding packets and a few to modeling servers, we could create two sets and pack the forwarders tightly (using a high packing factor) and the servers loosely. In exchange for providing greater control on packing, there is a price. When a set of containers is packed, the Containers system takes into account both the nodes to be packed and their interconnections. When subsets of containers are packed, the system cannot consider the interconnections between subsets. In some cases, the packing of subsets can lead to a DETERLab experiment that cannot be created successfully. This danger is mitigated by the fact that containers that are packed together are often related in ways that limit the number of connections between that set and another. To explore packing, we need to use a larger topology : source tb_compat.tcl set ns [new Simulator] set center [$ns node] tb-add-node-attribute $center \"containers:PartitionPass\" 0 for { set i 0} { $i < 3 } { incr i} { set lanlist $center for { set j 0 } { $j < 20} { incr j } { set idx [expr $i * 20 + $j] set n($idx) [$ns node] tb-add-node-attribute $n($idx) \"containers:PartitionPass\" [expr $i + 1] lappend lanlist $n($idx) } set lan($i) [$ns make-lan [join $lanlist \" \"] 100Mb 0] } # Creation boilerplate $ns rtptoto Static $ns run This creates three 20-node sub networks attached to a single central router. It looks like this: Each node in the topology is assigned a containers::PackingPass attribute that groups them into subsets. The containers:PackingPass attribute must be assigned an integer value. The nodes in each packing pass are considered \"together\" when packing. Each pass can be assigned different parameters. The passes are carried out in order, though that is rarely important. Our example topology assigns center to pass 0, the nodes on lan-0 (the tcl variable lan(0)) to pass 1, those on lan-1 to pass 2 and those on lan-2 to pass 3. We will use the --pass-pack parameter to specify the packing factor for each pass. Each packing factor specification looks like pass : factor where pass and factor are both integers. We can specify more than one, separated by commas, or specify --pass-pack more than once. For example, we can pack the experiment using the following factors: Pass Packing Factor 0 1 1 20 2 10 3 5 By issuing the following command: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 We can view the packing by using container_image.py to generate a visualization that includes the partitions: users:~$ /share/containers/container_image.py --experiment DeterTest/example6 --partitions --out ~/example6.png The output shows the topology with boxes drawn around the containers that share a physical node: That partitioning is surprising in that lan-1 is split into three partitions of 6 & 7 nodes rather than two partitions of 10. Similarly lan-2 is split into five groups of 4 rather than four groups of 5. The packing system is built on the metis graph partitioning software. Metis takes a graph and a number of partitions and finds the most balanced partitioning that has roughly equal node counts in each partition as well as low inter-partition communication costs. The Containers system calls metis with increasing numbers of partitions until a partitioning is found that meets the packing factor limits. When the system attempts to pack lan-1 into two partitions, metis balances the node counts and the communications costs to produce a partition with 9 containers in one machine and 11 on the other. That partitioning does not meet the 10 node limit, so it tries again with three partitions and succeeds. There are two ways to fit our topology onto fewer nodes. The first is to put slightly more slop into the packing factors: Pass Packing Factor 0 1 1 20 2 11 3 6 As in the following command: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:11,3:6 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 These parameters result in this packing, which fits in fewer nodes, but has the slight imbalances of splitting lan-1 into 9 and 11 containers and lan-2 into 4,5,and 6 container partitions. Again, this asymmetry is an attempt to consider the internode networking costs. If the packing constraints are exact - 11 containers on lan-1 is unacceptable - a second choice is to use the --nodes-only option. This sets the cost of each arc in the graph to 0. Metis ignores such arcs altogether, so the partitions are completely even. This may cause trouble in more complex network topologies. The result of running the command with the original packing factors and --nodes-only ): users:~$ /share/containers/containerize.py --nodes-only --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 is which has symmetric partitions. Sometimes it is more intuitive to think in terms of the number of machines that will be used to hold containers. The --size and -pass-size options let users express that. The --size= expsize option uses expsize machines to hold the whole experiment. If multiple passes are made, each is put into expsize physical machines. The --size option takes precedence over --packing . Per-pass sizing can be done using --pass-size which uses the same syntax as --pass-pack . Therefore the command: users:~$ /share/containers/containerize.py --pass-size 0:1,1:1,2:2,3:4 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 packs pass 0 into one physical machine, pass 1 into one physical machine, pass 2 into two physical machines and pass 3 into four physcial machines. The result looks like: Partitions have different numbers of containers in them because metis is considering network constraints as well. As with using packing, adding --nodes-only restores symmetry: The --pass-pack option is a per-pass generalization of the --packing option. The options that can be specified per-pass are: Single-pass Per-Pass Per-Pass Format Per-Pass Example --packing --pass-pack pass : packing (comma-separated) --pass-pack 0:1,1:20,2:11,3:6 --size --pass-size pass : size (comma-separated) --pass-size 0:1,1:1,2:2,3:4 --pnode-types --pass-pnodes pass : pnode [, pnode ...] (semicolon separated) --pass-pnodes 0:MicroCloud;1:bpc2133,pc2133 --nodes-only --pass-nodes-only pass (comma-separated) --pass-nodes-only 1,3,5 The single-pass version sets a default so that this invocation on our 4 pass topology : users:~$ /share/containers/containerize.py --packing 5 --pass-pack 0:1,1:20 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 will pack pass 0 with a factor of 1, pass 1 with a factor of 20 and passes 2 and 3 with a factor of 5. Similarly: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes '0:pc2133,bpc2133;1:MicroCloud' DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 will allocate either bpc2133 or pc2133 nodes to containers assigned by pass 0 and Microcloud physical nodes to the containers partitioned in pass 1. The rest will be allocated as the site configuration specifies. The single quotes around the --pass-pnodes option protects the semi-colon from the shell. Another choice is to specify the command as: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes 0:pc2133,bpc2133 --pass-pnodes 1:MicroCloud DeterTest example6 ~/example6.tcl That formulation avoids the quotes by avoiding the semicolon. All the per-pass options may be specified multiple times on the command line. You can mix and match sizes and packing factors. This invocation: /share/containers/containerize.py --pass-size 1:10 --pass-pack 2:5,3:10 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 Produces: Remember that --size sets a default pass size and that sizes have precedence over packing. If you specify --size , no --packing or --pass-packing value will take effect. To mix packing and sizes, use --pack-size for each sized pass, rather than --size . These per-pass variables and user-specified pass specifications give users fine grained control over the paritioning process, even if they do not want to do the partitioning themselves. If no containers:PartitionPass attributes are specified in the topology, and no containers:Partition attributes are specified either, ```containerize.py}} carries out -- at most -- two passes. Pass 0 paritions all openvz containers and pass 1 partitions all qemu containers. Further Reading \uf0c1 Hopefully these illustrative examples have given you an idea of how to use the containers system and what it is capable of. More details are available from the reference guide . Please see Getting Help if you have difficulties.","title":"Containers guide"},{"location":"containers/containers-guide/#containers-guide","text":"In this tutorial we walk you through setting up a basic containerized experiment. This page also includes common advanced topics. Detailed descriptions of the commands and configuration files are available in the reference section .","title":"Containers Guide"},{"location":"containers/containers-guide/#basic-containers-tutorial","text":"This tutorial will set up a containerized experiment with a star topology. We'll create a central node and connect 9 other nodes to it","title":"Basic Containers Tutorial"},{"location":"containers/containers-guide/#getting-started","text":"You will need a DETERLab account and be a member of a project. If you need help, see Opening Account documentation on the left sidebar.","title":"Getting started"},{"location":"containers/containers-guide/#step-1-design-the-topology","text":"First, we will describe a star topology. For this example we will use the standard DETER topology descriptions. If you are new to designing topologies, walk through the basic tutorial in our topology guide . The Containers system is largely compatible with the physical DETER interface. Download the DETERLab-compatible ns2 description of this topology at this link to your home directory on users.deterlab.net . It is a simple loop, along with the standard DETER boilerplate. This file will be used to create a 10-node (9 satellites and one central node) physical experiment on DETER, although there are not many physical nodes on DETER with 10 interfaces (one interface for control traffic). The following is the topology description: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtproto Static $ns run Note The central node is named \"center\" and each satellite is names \"n-0\", \"n-1\"... through \"n-8\". Each connection is a 100 Mb/s link with a 10ms delay. The round trip time from n-0 to center will be 20 ms and from n-0 to n-1 will be 40 ms.","title":"Step 1: Design the topology"},{"location":"containers/containers-guide/#step-2-create-the-containerized-experiment","text":"Now we will run a command so the Containers system will build the containerized experiment on top of a new DETERLab physical experiment. Run the following command from the shell on users.deterlab.net and refer to the example topology you just saved in your home. $ /share/containers/containerize.py DeterTest example1 ~/example1.tcl where the first two parameters are the project and experiment name to hold the DETER experiment. This command creates an experiment called experiment1 in the DeterTest project. Throughout this tutorial, we will refer to your project as DeterTest , but make sure you actually use your actual project's name. You may use the experiment name experiment1 as long as another experiment with that name doesn't already exist Note As with any DETERLab experiment, you must be a member of the project with appropriate rights to create an experiment in it. containerize.py expects there to be no experiment with that name, and it will fail if one exists. To remove an experiment you may terminate it through the web interface or use the endexp command. Terminating an experiment is more final than swapping one out, so be sure that you want to replace the old experiment. You may also resolve the conflict by renaming your new containerized experiment. The last parameter is the file containing the topology. In this tutorial, we are referring to the ns2 file in our example but you may also use a topdl description. An ns2 description must end in .tcl or .ns . With these default parameters containerize.py will put each node into an Openvz container with at most 10 containers per physical node. The output of the above command should be something like the following: users:~$ /share/containers/containerize.py DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net/showexp.php3?pid=DeterTest&eid=example1 Now we can see what a containerized experiment looks like.","title":"Step 2: Create the containerized experiment"},{"location":"containers/containers-guide/#the-contents-of-a-containerized-experiment","text":"Follow the link provided in the containerize.py output. You will see a standard DETER experiment page that looks like this: You may be surprised to see that DETER thinks the experiment has only one node: The Containers system has rewritten the description file and stored additional information in the experiment's per-experiment directory that will be used to create the 10 node experiment inside the single-node DETER experiment. If you look at the ns file DETERLab has stored (click the NS File tab on the experiment page), you should see the following code: set ns [new Simulator] source tb_compat.tcl tb-make-soft-vtype container0 {pc2133 bpc2133 MicroCloud} set pnode(0000) [$ns node] tb-set-node-os ${pnode(0000)} CentOS6-64-openvz tb-set-hardware ${pnode(0000)} container0 tb-set-node-startcmd ${pnode(0000)} \"sudo /share/containers/setup/hv/bootstrap /proj/DeterTest/exp/example1/containers/site.conf >& /tmp/container.log\" tb-set-node-failure-action ${pnode(0000)} \"nonfatal\" $ns rtproto Static $ns run This looks nothing like the file we gave to containerize.py , but it does show us a little about what the Containers system has done: The single physical node ( pnode(0000) ) will run the CentOS6-64-openvz image and run on a few kinds of node. On startup, pnode(0000) will execute a command from the same /share/containers directory that containerize.py ran from using data in the per-experiment directory /proj/DeterTest/exp/example1/containers/site.conf . There is a separate /proj/DeterTest/exp/example1/containers/ directory for each experiment. The path element after /proj is replaced with the project under which the experiment was created -- DeterTest in this example -- and the element after exp is the experiment name -- example1 in this case. These directories are created for all DETERLab experiments.","title":"The Contents of a Containerized Experiment"},{"location":"containers/containers-guide/#containers-sub-directory","text":"The containers sub-directory of a containerized experiment holds information specific to a containerized experiment. There are a few useful bits of data in that per-experiment containers directory that we can look at. Copy of the topology file: First, a copy of the topology that we gave to containerize.py is available in /proj/DeterTest/exp/example1/containers/experiment.tcl . If the experiment is created from a topdl file, the filename will be containers/experiment.tcl . Visualization of experiment: A simple visualization of the experiment is in containers/visualization.png . This is annotated with node and network names as well as interface IP addresses. The topology depiction above is an example. To view a larger version, click here . IP-to-hostname mapping: The containers/hosts file is a copy of the IP-to-hostname mapping found on each virtual machine in the topology. It can be useful in converting IP addresses back to names. It is installed in /etc/hosts or the equivalent on each machine. PID/EID: The two files /var/containers/pid and /var/containers/eid contain the project name and experiment name. Scripts can make use of these. The rest of the contents of that directory are primarily used internally by the implementation, but a more detailed listing is in the Containers Reference .","title":"containers sub-directory"},{"location":"containers/containers-guide/#step-3-swap-in-resources","text":"At this point, as with any DETER experiment, the topology does not yet have any resources attached. To get the resources, swap the experiment in from the web interface or using the swapexp command. See the DETERLab Commands for more information.","title":"Step 3: Swap-in resources"},{"location":"containers/containers-guide/#step-4-verify-virtual-topology-and-access-nodes","text":"Once you have been notified that the physical experiment has finished its swap-in, the Containers system starts converting the physical topology into the virtual topology. At this time, you must manually verify when the virtual topology has been created by ping-ing or trying to SSH into individual nodes of an experiment. We are working towards offering a better notification system. Once the containerized elements have all started, the nodes are available as if they were physical nodes. For example, we may access node n-0 of the experiment we swapped in by running: $ ssh n-0.example1.detertest Be sure that you replace example1 with the experiment name you passed to containerize.py and DeterTest with the project you created the experiment under. This is a DNS name, so it is case-insensitive. When the SSH succeeds, you will have access to an Ubuntu 10.04 32-bit node with the same directories mounted as in a physical DETERLab experiment. Containerized nodes access the control net as well. Your home directory will be mounted, so your SSH keys will work for accessing the machine. Use the same node naming conventions as physical DETERLab experiments to ping and access other nodes. Here is a ping from n-0 to center and n-1 that confirms the containerized experiment is working as we expect. n-0:~$ ping -c 3 center PING center-tblink-l21 (10.0.0.2) 56(84) bytes of data. 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=1 ttl=64 time=20.4 ms 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=2 ttl=64 time=20.0 ms 64 bytes from center-tblink-l21 (10.0.0.2): icmp_seq=3 ttl=64 time=20.0 ms --- center-tblink-l21 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2002ms rtt min/avg/max/mdev = 20.052/20.184/20.445/0.184 ms n-0:~$ ping -c 3 n-1 PING n-1-tblink-l5 (10.0.6.1) 56(84) bytes of data. 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=1 ttl=64 time=40.7 ms 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=2 ttl=64 time=40.0 ms 64 bytes from n-1-tblink-l5 (10.0.6.1): icmp_seq=3 ttl=64 time=40.0 ms --- n-1-tblink-l5 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2003ms rtt min/avg/max/mdev = 40.094/40.318/40.764/0.355 ms The nodes have the expected round trip times. At this point you can load and run software and generally experiment normally.","title":"Step 4: Verify virtual topology and access nodes"},{"location":"containers/containers-guide/#start-commands","text":"DETERLab Core provides a facility to run a command when a physical experiment starts, called start commands . A containerized experiment offers a similar facility with a few differences: The start commands are not coordinated across nodes. In a physical experiment, the start commands all execute when the last node has reported to the testbed that it has completed booting. In a containerized experiment, the start commands run when the containerized node has come up. Logs from the start command are available in /var/containers/log/start_command.out and /var/containers/log/start_command.err . This is true on embedded pnodes as well. Start commands must be shorter than in a physical experiment because the Containers system is also using the facility. The event system cannot be used to replay the start command. Notes While start commands that make use of shell syntax for multiple commands and simple file redirection (e.g, > or <) may work, errors parsing redirection or other shell commands will cause the start command to fail silently . If you are doing anything more complex than calling a single program, we recommend that you create a simple script and run the script from the per-experiment directory or your home directory. This makes it more likely that the log files created by containers will have useful debugging information. We strongly recommend removing all shell redirection characters from the text of your start command. Redirecting I/O in the text of the start command may fail silently . Start commands offer a simple workaround for detecting that all nodes in an experiment have started: #!/bin/sh STARTDIR=\"/proj/\"`cat /var/containers/pid`\"/exp/\"`cat /var/containers/eid`\"/startup\" mkdir $STARTDIR date > $STARTDIR/`hostname` If you make the script above the start command of all nodes, the Containers system will put the time that each local container came up in the startup directory under the per-experiment directories. For example, n-0.example1.DeterTest will create /proj/DeterTest/exp/example1/startup/n-0 . Then you may monitor that directory on users to know which nodes are up.","title":"Start Commands"},{"location":"containers/containers-guide/#step-4-releasing-resources","text":"As with a physical DETER experiment, release resources by swapping the experiment out using the web interface or the swapexp command.","title":"Step 4: Releasing Resources"},{"location":"containers/containers-guide/#advanced-topics","text":"The previous tutorial described how to create an experiment using only openvz containers packed 10 to a machine. This section describes how to change those parameters.","title":"Advanced Topics"},{"location":"containers/containers-guide/#using-other-container-types","text":"To change the container type that containerize.py assigns to nodes, use the --default-container option. Valid choices follow the [ContainersQuickstart#KindsofContainers kinds of containers] DETERLab supports. Specifically: Parameter Container embedded_pnode Physical Node qemu Qemu VM openvz Openvz Container You can try this on our example topology : users:~$ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example1.tcl Requested a QEMU node with more than 7 experimental interfaces. Qemu nodes can only support 7 experimental interfaces. The Containers system is now using qemu containers to build our experiment. Unfortunately qemu containers only support 7 experimental interfaces, an internal limit on the number of interfaces the virtual hardware supports. Run the command again but use the attached <a href=\"../../downloads/example2.tcl\" version of the topology with fewer satellites to containerize without error. $ /share/containers/containerize.py --default-container qemu DeterTest example2 ~/example2.tcl Containerized experiment DeterTest/example2 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example2 The qemu experiment looks much like the openvz experiment above, at this small scale. Qemu nodes more completely emulate hardware and the kernels are independent, unlike openvz containers. For example, a program can load kernel modules in a qemu VM, which it cannot do in an openvz container. The qemu containers load the Ubuntu 12.04 (32 bit) distribution by default.","title":"Using Other Container Types"},{"location":"containers/containers-guide/#mixing-containers","text":"Mixing containers requires you to assign container types in the topology description. This is done by attaching an attribute to nodes. The attribute is named containers:node_type and it takes the same values as the --default-container parameter to containerize.py . If the experiment definition is in topdl , the attribute can be attached using the standard topdl routines . Attaching the attribute in ns2 is done using the DETERLab tb-add-node-attribute command. tb-add-node-attribute $node containers:node_type openvz Using this command in an ns2 topology description will set node to be placed in an openvz container. Using this feature, we can modify our first example topology to consist of qemu nodes and a single OpenVZ container in the center. The new topology file looks like this: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] # The center node is an openvz VM tb-add-node-attribute $center containers:node_type openvz # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Satellites are qemu nodes tb-add-node-attribute $n($i) containers:node_type qemu # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtproto Static $ns run Because we have explicitly set the container_node_type of each node, the --default-container parameter to containerize.py does nothing. Create this experiment by running: users:~$ /share/containers/containerize.py DeterTest example3 ~/example3.tcl Containerized experiment DeterTest/example3 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example3 When we swap it in, the experiment will have 10 satellite containers in qemu VMs and a central OpenVZ VM that forwards packets. Another interesting mixture of containers is to include a physical node. Here is a modified version of our mixed topology that places the n-8 satellite on a physical computer by setting its containers:node_type to embedded_pnode . After running that experiment you should have output similar to the following: users:~$ /share/containers/containerize.py DeterTest example4 ~/example4.tcl Containerized experiment DeterTest/example4 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example4 Follow the url to the DETERLab experiment page and look at the Visualization tab: The physical node n-8 shows up in the DETERLab visualization and otherwise acts as a physical node that is in a 10-node topology. This experiment uses three different container types: physical nodes, ViewOS processes, and Qemu VMs. (Note that ViewOS containers are not currently supported on DETER.)","title":"Mixing Containers"},{"location":"containers/containers-guide/#limitations-on-mixing-containers","text":"Physical node containers are mapped one-to-one to physical nodes by definition. Qemu and OpenVZ use different underlying operating system images in DETERLab, therefore they cannot share physical hardware. Qemu VMs cannot share a physical machine with OpenVZ VMs. If these container types are mixed in an experiment, they will always be assigned different physical nodes. The first invocation of tb-add-node-attribute takes precedence. It is best to only call tb-add-node-attribute once per attribute assigned on each node.","title":"Limitations on Mixing Containers"},{"location":"containers/containers-guide/#setting-openvz-parameters","text":"An advantage of openvz nodes is that you can set the OS flavor and CPU bit-width across experiments and per-node. Similarly, you can set the size of the disk allocated to each node. Openvz uses templates to look like various Linux installations. The choices of Linux distribution that openvz supports are: Template Distribution Bit-width centos-6-x86 CentOS 6 32 bit centos-6-x86_64 CentOS 6 64 bit ubuntu-12.04-x86 Ubuntu 12.04 LTS 32 bit ubuntu-12.04-x86_64 Ubuntu 12.04 LTS 64 bit ubuntu-14.04-x86 Ubuntu 14.04 LTS 32 bit ubuntu-14.04-x86_64 Ubuntu 14.04 LTS 64 bit The default template is ubuntu-14.04-x86_64 . To set a template across an entire topology, give --openvz-template and the template name from the list above. Invoking containerize.py on our original example as below will instantiate the experiment under 64-bit Ubuntu 12.04: users:~$ /share/containers/containerize.py --openvz-template ubuntu-12.04-x86_64 DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 To set the size of the file system of containers in the experiment, use --openvz-diskspace . The value of the parameter is determined by the suffix: Suffix Value G Gigabytes M Megabytes The default openvz file system size is 2GB. The most practical suffix for DETERLab nodes is \"G\": users:~$ /share/containers/containerize.py --openvz-diskspace 15G DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 Each of these parameters can be set on individual nodes using attributes. Use containers:openvz_template to set a template on a node and use containers:openvz_diskspace to set the disk space. This example topology sets these openvz parameters per node: source tb_compat.tcl set ns [new Simulator] # Create the center node (named by its variable name) set center [$ns node] tb-add-node-attribute $center containers:openvz_template ubuntu-12.04-x86_64 # Connect 9 satellites for { set i 0} { $i < 9 } { incr i} { # Create node n-1 (tcl n($i) becomes n-$i in the experiment) set n($i) [$ns node] # Set satellite disk sizes to be 20 GB tb-add-node-attribute $n($i) containers:openvz_diskspace 20G # Connect center to $n($i) ns duplex-link $center $n($i) 100Mb 10ms DropTail } # Creation boilerplate $ns rtptoto Static $ns run The center node will run Ubuntu 12.04 64 bit and the satellites will have 20GB file systems.","title":"Setting Openvz Parameters"},{"location":"containers/containers-guide/#setting-qemu-parameters","text":"The Containers system has a more limited ability to set qemu parameters. Right now, a custom image may be loaded using the containers::qemu_url attribute and the architecture of the qemu VM may be chosen using containers:qemu_arch . Valid qemu architectures are: Param Meaning i386 32 bit Intel x86_64 64-bit Intel The image URL must be reachable from inside DETERLab. The image must be a qcow2 image, optionally bzip2ed. Facilities to snapshot and store such images are in development. If you are using a qemu image that is not booting into containers, make sure grub is properly configured. Qemu images also mount users' home directories the same as DETERLab physical nodes do. In order to do this scalably, the Qemu VMs mount the users' directories from the physical node. The DETER infrastructure cannot support exporting users' directories to thousands of containers. However, a Qemu VM can only mount a few tens of user directories this way. The limit is 23 user directories (24 in experiments that are not instantiated in a group). Many projects have more than 23 users, but in practice only a few experimenters need access to the containers. To tell the Containers systems which user to mount, use the --qemu-prefer-users option to containerize.py . That option takes a comma-separated list of usernames (no spaces). When the Qemu nodes will always mount those users' home directories. Others will be mounted if there is room. For example: users:~$ /share/containers/containerize.py --qemu-prefer-users=faber,jjh DeterTest example4 ~/example4.tcl This command makes sure that users faber and jjh have their home directories mounted in any Qemu containers.","title":"Setting Qemu Parameters"},{"location":"containers/containers-guide/#changing-the-packing-factor","text":"The containerize.py program decides how many virtual nodes to put on each physical machine. Because we have been using roughly the same number of nodes as the default packing target (10 nodes per machine) all of the examples so far have fit onto a single machine. If we change the packing factor by using the --packing parameter to containerize.py , we can put fewer nodes on each machine. For example: users:~$ /share/containers/containerize.py --packing 2 DeterTest example1 ~/example1.tcl Containerized experiment DeterTest/example1 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example1 This command calls containerize.py on our original topology with a low packing factor. The result is the same nodes spread across more physical machines, as we can see from the DETERLab web interface (on the Visualization tab): You will want to balance how many physical machines you use against how precisely you want to mimic them.","title":"Changing The Packing Factor"},{"location":"containers/containers-guide/#user-packing","text":"You may specify your own packing using the containers:partition attribute. This attribute must be assigned an integer value. All nodes with the same partition are allocated to the same machine. If nodes have that attribute attached to them, containers.py will assume that they all have been partitioned and use those. Nodes without a partition assigned are assumed to be embedded_pnode s.","title":"User Packing"},{"location":"containers/containers-guide/#more-sophisticated-packing-multiple-passes","text":"The previous examples have all treated packing containers onto physical machines as a single-step process with a single parameter - the packing factor. In fact, we can divide containers into sets and pack each set independently using different parameters. For example in an experiment with many containers dedicated only to forwarding packets and a few to modeling servers, we could create two sets and pack the forwarders tightly (using a high packing factor) and the servers loosely. In exchange for providing greater control on packing, there is a price. When a set of containers is packed, the Containers system takes into account both the nodes to be packed and their interconnections. When subsets of containers are packed, the system cannot consider the interconnections between subsets. In some cases, the packing of subsets can lead to a DETERLab experiment that cannot be created successfully. This danger is mitigated by the fact that containers that are packed together are often related in ways that limit the number of connections between that set and another. To explore packing, we need to use a larger topology : source tb_compat.tcl set ns [new Simulator] set center [$ns node] tb-add-node-attribute $center \"containers:PartitionPass\" 0 for { set i 0} { $i < 3 } { incr i} { set lanlist $center for { set j 0 } { $j < 20} { incr j } { set idx [expr $i * 20 + $j] set n($idx) [$ns node] tb-add-node-attribute $n($idx) \"containers:PartitionPass\" [expr $i + 1] lappend lanlist $n($idx) } set lan($i) [$ns make-lan [join $lanlist \" \"] 100Mb 0] } # Creation boilerplate $ns rtptoto Static $ns run This creates three 20-node sub networks attached to a single central router. It looks like this: Each node in the topology is assigned a containers::PackingPass attribute that groups them into subsets. The containers:PackingPass attribute must be assigned an integer value. The nodes in each packing pass are considered \"together\" when packing. Each pass can be assigned different parameters. The passes are carried out in order, though that is rarely important. Our example topology assigns center to pass 0, the nodes on lan-0 (the tcl variable lan(0)) to pass 1, those on lan-1 to pass 2 and those on lan-2 to pass 3. We will use the --pass-pack parameter to specify the packing factor for each pass. Each packing factor specification looks like pass : factor where pass and factor are both integers. We can specify more than one, separated by commas, or specify --pass-pack more than once. For example, we can pack the experiment using the following factors: Pass Packing Factor 0 1 1 20 2 10 3 5 By issuing the following command: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 We can view the packing by using container_image.py to generate a visualization that includes the partitions: users:~$ /share/containers/container_image.py --experiment DeterTest/example6 --partitions --out ~/example6.png The output shows the topology with boxes drawn around the containers that share a physical node: That partitioning is surprising in that lan-1 is split into three partitions of 6 & 7 nodes rather than two partitions of 10. Similarly lan-2 is split into five groups of 4 rather than four groups of 5. The packing system is built on the metis graph partitioning software. Metis takes a graph and a number of partitions and finds the most balanced partitioning that has roughly equal node counts in each partition as well as low inter-partition communication costs. The Containers system calls metis with increasing numbers of partitions until a partitioning is found that meets the packing factor limits. When the system attempts to pack lan-1 into two partitions, metis balances the node counts and the communications costs to produce a partition with 9 containers in one machine and 11 on the other. That partitioning does not meet the 10 node limit, so it tries again with three partitions and succeeds. There are two ways to fit our topology onto fewer nodes. The first is to put slightly more slop into the packing factors: Pass Packing Factor 0 1 1 20 2 11 3 6 As in the following command: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:11,3:6 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 These parameters result in this packing, which fits in fewer nodes, but has the slight imbalances of splitting lan-1 into 9 and 11 containers and lan-2 into 4,5,and 6 container partitions. Again, this asymmetry is an attempt to consider the internode networking costs. If the packing constraints are exact - 11 containers on lan-1 is unacceptable - a second choice is to use the --nodes-only option. This sets the cost of each arc in the graph to 0. Metis ignores such arcs altogether, so the partitions are completely even. This may cause trouble in more complex network topologies. The result of running the command with the original packing factors and --nodes-only ): users:~$ /share/containers/containerize.py --nodes-only --pass-pack 0:1,1:20,2:10,3:5 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 is which has symmetric partitions. Sometimes it is more intuitive to think in terms of the number of machines that will be used to hold containers. The --size and -pass-size options let users express that. The --size= expsize option uses expsize machines to hold the whole experiment. If multiple passes are made, each is put into expsize physical machines. The --size option takes precedence over --packing . Per-pass sizing can be done using --pass-size which uses the same syntax as --pass-pack . Therefore the command: users:~$ /share/containers/containerize.py --pass-size 0:1,1:1,2:2,3:4 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 packs pass 0 into one physical machine, pass 1 into one physical machine, pass 2 into two physical machines and pass 3 into four physcial machines. The result looks like: Partitions have different numbers of containers in them because metis is considering network constraints as well. As with using packing, adding --nodes-only restores symmetry: The --pass-pack option is a per-pass generalization of the --packing option. The options that can be specified per-pass are: Single-pass Per-Pass Per-Pass Format Per-Pass Example --packing --pass-pack pass : packing (comma-separated) --pass-pack 0:1,1:20,2:11,3:6 --size --pass-size pass : size (comma-separated) --pass-size 0:1,1:1,2:2,3:4 --pnode-types --pass-pnodes pass : pnode [, pnode ...] (semicolon separated) --pass-pnodes 0:MicroCloud;1:bpc2133,pc2133 --nodes-only --pass-nodes-only pass (comma-separated) --pass-nodes-only 1,3,5 The single-pass version sets a default so that this invocation on our 4 pass topology : users:~$ /share/containers/containerize.py --packing 5 --pass-pack 0:1,1:20 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 will pack pass 0 with a factor of 1, pass 1 with a factor of 20 and passes 2 and 3 with a factor of 5. Similarly: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes '0:pc2133,bpc2133;1:MicroCloud' DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 will allocate either bpc2133 or pc2133 nodes to containers assigned by pass 0 and Microcloud physical nodes to the containers partitioned in pass 1. The rest will be allocated as the site configuration specifies. The single quotes around the --pass-pnodes option protects the semi-colon from the shell. Another choice is to specify the command as: users:~$ /share/containers/containerize.py --pass-pack 0:1,1:20,2:10,3:5 --pass-pnodes 0:pc2133,bpc2133 --pass-pnodes 1:MicroCloud DeterTest example6 ~/example6.tcl That formulation avoids the quotes by avoiding the semicolon. All the per-pass options may be specified multiple times on the command line. You can mix and match sizes and packing factors. This invocation: /share/containers/containerize.py --pass-size 1:10 --pass-pack 2:5,3:10 DeterTest example6 ~/example6.tcl Containerized experiment DeterTest/example6 successfully created! Access it via http://www.isi.deterlab.net//showexp.php3?pid=DeterTest&eid=example6 Produces: Remember that --size sets a default pass size and that sizes have precedence over packing. If you specify --size , no --packing or --pass-packing value will take effect. To mix packing and sizes, use --pack-size for each sized pass, rather than --size . These per-pass variables and user-specified pass specifications give users fine grained control over the paritioning process, even if they do not want to do the partitioning themselves. If no containers:PartitionPass attributes are specified in the topology, and no containers:Partition attributes are specified either, ```containerize.py}} carries out -- at most -- two passes. Pass 0 paritions all openvz containers and pass 1 partitions all qemu containers.","title":"More Sophisticated Packing: Multiple Passes"},{"location":"containers/containers-guide/#further-reading","text":"Hopefully these illustrative examples have given you an idea of how to use the containers system and what it is capable of. More details are available from the reference guide . Please see Getting Help if you have difficulties.","title":"Further Reading"},{"location":"containers/containers-quickstart/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Containers Quickstart \uf0c1 This page describes basic information about DETERLab Containers and provides an overview of how to use it. More details are available in the Containers Guide . What are Containers? \uf0c1 The Containers system enables experimenters to create large-scale DETERLab topologies that support differing degrees of fidelity in individual elements. In order to create an experiment larger than the 400+ computers available in DETERLab Core, experimenters must use virtualization, simulation, or some other abstraction to represent their topology. The Containers system automates creation of virtual nodes to achieve desired scale. The Containers system is built on top of the resource allocation that underlies the DETERLab testbed , extending it to provide multiple implementations of virtual nodes. Most DETERLab tools that run on physical experiments may be used directly on virtualized nodes. Experimenters find working in a containerized experiment very similar to working in physical DETERLab experiments. How does it work? \uf0c1 The Containers system lays out the virtual computers into a physical layout of computers and uses the resource allocation system to allocate that physical layout. Then the system installs and configures the appropriate virtualization technologies in that environment to create the virtual environment. As in physical DETERLab experiments, the experiment's topology is written in an extended version of DETERLab's ns2 syntax, or in topdl , a topology description language. Currently experimenters specify the type of container they want to use in their topology description. Kinds of Containers \uf0c1 The Containers system gives us a way to create interconnections of containers (in our sense) holding different experiment elements. A containerized topology might include a physical machine, a qemu virtual machine and an openvz container that can all communicate transparently. The Containers system framework supports multiple kinds of containers, but at this point researchers may request these: Container Type Fidelity Scalability Physical Machine Complete fidelity 1 per physical machine Qemu virtual Machine Virtual hardware 10's of containers per physical machine Openvz container Partitioned resources in one Linux kernel 100's of contatiners per physical machine How Do I Use Containers? \uf0c1 In general, once you have a DETERLab account, you follow these steps. The DETERLab Containers Guide will walk you through a basic tutorial of these steps. 1. Design the topology \uf0c1 Create your NS file describing your large topology. Transfer the file to the \"users.deterlab.net\". 2. Run containerized experiment with the containerize.py command \uf0c1 The Containers system will build the containerized experiment on top of an existing DETERLab physical experiment by running the containerize.py command from the shell on users.deterlab.net , as in the following example: $ /share/containers/containerize.py DeterTest example1 ~/example1.tcl where ''DeterTest'' and ''example1'' are the project and experiment name, respectively, of the physical DETERLab experiment and ''example.tcl'' is the topology file. With these default parameters, containerize.py will put each node into an Openvz container with at most 10 containers per physical node. 3. View results by accessing nodes, modify the experiment as needed. \uf0c1 In a containerized experiment, you can access the virtual nodes with the same directories mounted as in a physical DETERLab experiment. You can load and run software and conduct experiments as you would in a physical experiment. 4. Save your work and swap out your experiment (release the resources) \uf0c1 As with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users. More Information \uf0c1 For more detailed information about Containers, read the following: Containers Guide - This guide walks you through a basic example of using Containers and includes some advanced topics. Containers Reference - This reference includes Containers commands, configuration details and information about different types of containers.","title":"Containers"},{"location":"containers/containers-quickstart/#containers-quickstart","text":"This page describes basic information about DETERLab Containers and provides an overview of how to use it. More details are available in the Containers Guide .","title":"Containers Quickstart"},{"location":"containers/containers-quickstart/#what-are-containers","text":"The Containers system enables experimenters to create large-scale DETERLab topologies that support differing degrees of fidelity in individual elements. In order to create an experiment larger than the 400+ computers available in DETERLab Core, experimenters must use virtualization, simulation, or some other abstraction to represent their topology. The Containers system automates creation of virtual nodes to achieve desired scale. The Containers system is built on top of the resource allocation that underlies the DETERLab testbed , extending it to provide multiple implementations of virtual nodes. Most DETERLab tools that run on physical experiments may be used directly on virtualized nodes. Experimenters find working in a containerized experiment very similar to working in physical DETERLab experiments.","title":"What are Containers?"},{"location":"containers/containers-quickstart/#how-does-it-work","text":"The Containers system lays out the virtual computers into a physical layout of computers and uses the resource allocation system to allocate that physical layout. Then the system installs and configures the appropriate virtualization technologies in that environment to create the virtual environment. As in physical DETERLab experiments, the experiment's topology is written in an extended version of DETERLab's ns2 syntax, or in topdl , a topology description language. Currently experimenters specify the type of container they want to use in their topology description.","title":"How does it work?"},{"location":"containers/containers-quickstart/#kinds-of-containers","text":"The Containers system gives us a way to create interconnections of containers (in our sense) holding different experiment elements. A containerized topology might include a physical machine, a qemu virtual machine and an openvz container that can all communicate transparently. The Containers system framework supports multiple kinds of containers, but at this point researchers may request these: Container Type Fidelity Scalability Physical Machine Complete fidelity 1 per physical machine Qemu virtual Machine Virtual hardware 10's of containers per physical machine Openvz container Partitioned resources in one Linux kernel 100's of contatiners per physical machine","title":"Kinds of Containers"},{"location":"containers/containers-quickstart/#how-do-i-use-containers","text":"In general, once you have a DETERLab account, you follow these steps. The DETERLab Containers Guide will walk you through a basic tutorial of these steps.","title":"How Do I Use Containers?"},{"location":"containers/containers-quickstart/#1-design-the-topology","text":"Create your NS file describing your large topology. Transfer the file to the \"users.deterlab.net\".","title":"1. Design the topology"},{"location":"containers/containers-quickstart/#2-run-containerized-experiment-with-the-containerizepy-command","text":"The Containers system will build the containerized experiment on top of an existing DETERLab physical experiment by running the containerize.py command from the shell on users.deterlab.net , as in the following example: $ /share/containers/containerize.py DeterTest example1 ~/example1.tcl where ''DeterTest'' and ''example1'' are the project and experiment name, respectively, of the physical DETERLab experiment and ''example.tcl'' is the topology file. With these default parameters, containerize.py will put each node into an Openvz container with at most 10 containers per physical node.","title":"2. Run containerized experiment with the containerize.py command"},{"location":"containers/containers-quickstart/#3-view-results-by-accessing-nodes-modify-the-experiment-as-needed","text":"In a containerized experiment, you can access the virtual nodes with the same directories mounted as in a physical DETERLab experiment. You can load and run software and conduct experiments as you would in a physical experiment.","title":"3. View results by accessing nodes, modify the experiment as needed."},{"location":"containers/containers-quickstart/#4-save-your-work-and-swap-out-your-experiment-release-the-resources","text":"As with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users.","title":"4. Save your work and swap out your experiment (release the resources)"},{"location":"containers/containers-quickstart/#more-information","text":"For more detailed information about Containers, read the following: Containers Guide - This guide walks you through a basic example of using Containers and includes some advanced topics. Containers Reference - This reference includes Containers commands, configuration details and information about different types of containers.","title":"More Information"},{"location":"containers/containers-reference/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Containers Reference \uf0c1 This document describes the details of the commands and data structures that make up the Containers system. The Containers Guide provides useful context about the workflows and goals of the system that inform these technical details. Commands \uf0c1 This section describes the command line interface to the Containers system. containerize.py \uf0c1 The containerize.py command creates a DETERLab experiment made up of containers. The containerize.py program is available from /share/containers/containerize.py on users.deterlab.net . A sample invocation is: $ /share/containers/containerize.py MyProject MyExperiment ~/mytopology.tcl It will create a new experiment in MyProject called MyExperiment containing the experiment topology in mytopology.tcl . All the topology creation commands supported by DETERLab are supported by the Containers system, but DETERLab program agents are not. DETERLab start commands are supported. Containers will create an experiment in a group if the project parameter is of the form project / group . To start an experiment in the testing group of the DETER project, the first parameter is specified as DETER/testing . Containers supports ns2 file or topdl descriptions. Ns2 descriptions must end with .tcl or .ns . Other files are assumed to be topdl descriptions. Names of substrates and nodes in ns2 files are restricted to valid tcl variable names. Names of substrates and nodes in topdl files are restricted to the characters A-Z, a-z, digits, the underscore and the hyphen (-). By default, containerize.py program will partition the topology into openvz containers, packing 10 containers per physical computer. If the topology is already partitioned - meaning at least one element has a containers::partition attribute - containerize.py will not partition it. The --force-partition flag causes containerize.py to partition the experiment regardless of the presence of containers:partition attributes. If container types have been assigned to nodes using the containers:node_type attribute, containerize.py will respect them. Valid container types for the containers:node_type attribute or the --default-container parameter are: Parameter Container embedded_pnode Physical Node qemu Qemu VM openvz Openvz Container The containerize.py command takes several parameters that can change its behavior: --default-container = kind Containerize nodes without a container type into kind . If no nodes have been assigned containers, this puts all them into kind containers. --force-partition Partition the experiment whether or not it has been partitioned already --packing= int Attempt to put int containers into each physical node. The default --packing is 10. --size= int Attempt to divide the experiment into int physical nodes. The default is to use packing. There are some nuances to this with mixed containers. See the Containers Guide for more details. --config= filename Read configuration variables from filename . Configuration values are discussed below . --pnode-types= type1[,type2...] Override the site configuration and request nodes of type1 (or type2 etc.) as host nodes. --end-node-shaping Attempt to do end node traffic shaping even in containers connected by VDE switches. This works with qemu nodes. Topologies that include both openvz nodes and qemu nodes that shape traffic should use this. See the discussion below . --vde-switch-shaping Do traffic shaping in VDE switches. Probably the default, but that is controlled in the site configuration . See the discussion below . --openvz-diskspace Set the default openvz disk space size. The suffixes G and M stand for 'gigabytes' and 'megabytes'. --openvz-template Set the default openvz template. Templates are described in the Containers Guide . --openvz-template-dir Add a directory to be searched for openvz templates. Templates must end in tar.gz and be accessible to the user at creation and swap time. They can only be located under the /proj or /share directories. --image Construct a visualization of the virtual topology and leave it in the experiment directories (default). --nodes-only Ignore network constraints when partitioning nodes. --no-image Do not construct a visualization of the virtual topology. --pass-pack= pass : packing [, pass : packing ...] Specify the packing factor for each partitioning pass. The [ContainersGuide#MoreSophisticatedPacking:MultiplePasses Containers Guide] describes this in detail. --pass-size= pass : size [, pass : size ...] Specify the number of physical machines to use for each partitioning pass. The Containers Guide describes this in detail. --pass-pnodes= pass : type [, type ...][; pass : type [, type ...]...] Specify the pnode types on which nodes packed in partitioning pass pass can be placed. The Containers Guide describes this in detail. --pass-nodes-only= pass [, pass ...] Specify the partitioning passes on which network connectivity is ignored. The Containers Guide describes this in detail. --prefer-qemu-users= user[,user...] Make sure that Qemu images mount the given users' home directories. Qemu nodes can mount at most 19 users' home directories and this ensures that the experimenters using the containers can reach their home directories. --debug Print additional diagnostics and leave failed DETER experiments on the testbed. --keep-tmp Do not remove temporary files - used for debugging only. This invocation: $ ./containerize.py --packing 25 --default-container=qemu --force-partition DeterTest faber-packem ~/experiment.xml takes the topology in ~/experiment.xml (which must be a topdl description), packs it into 25 qemu containers per physical node, and creates an experiment called 'DeterTest/faber-packem' that can be swapped-in. If experiment.xml is already partitioned, it will be re-partitioned. If some nodes in that topology are assigned to openvz nodes already, those nodes will be still be in openvz containers. The result of a successful containerize.py run is a DETERLab experiment that can be swapped in. More detailed examples are available in the Containers Guide . container_image.py \uf0c1 The container_image.py command draws a picture of the topology of an experiment. This is helpful in keeping track of how virtual nodes are connected. containerize.py calls this internally and stores the output in the per-experiment directory (unless --no-image is used). A researcher may call container_image.py directly to generate an image later or to generate one with the partitioning drawn. The simplest way to call container_image.py is: /share/containers/container_image.py topology.xml output.png The first parameter is a topdl description, for example the one in the per-experiment directory . The second parameter is the output file for the image. When drawing an experiment that has been containerized, the --experiment option is very useful. Options include: --experiment= project / experiment Draw the experiment in project / experiment , if it exists. Note that this is just the DETERLab experiment and project names. Omit any sub-group. --topo= filename Draw the topology in the filename indicated. --attr-prefix= prefix Prefix for containers attributes. Deprecated. --partitions Draw labeled boxes around nodes that share a physical node. --out= filename Save the image in the filename indicated. --program= programname Use programname to lay out the graph. programname must take a file in graphviz's dot language . This is given as the --program option to fedd_image.py internally. The default is fdp which works well when --partitions is given. If neither --topo nor --experiment is given, the first positional parameter is the topdl topology to draw. If --out is not given the next positional parameter (the first if neither --topo nor --experiment is given) is the output file. A common invocation looks like: /share/containers/container_image.py --experiment SAFER/testbed-containers ~/drawing.png Topdl Attributes For Containers \uf0c1 Several topdl attributes influence how an experiment is containerized. These can be added to nodes using the ns2 tb-add-node-attribute command (used throughout the Containers Guide ) or directly to the topdl. These attributes are all attached to nodes/Computers: containers:node_type The container that will hold this node. The full list is available here. containers:partition An identifier grouping nodes together in containers that will share a physical node. Generally assigned by containerize.py , but researchers can also directly assign them. The containerize.py command assigns integers, so if a researcher assigns other partition identifiers, containerize.py will not overwrite them. containers:openvz_template The flavor of Linux distribution to emulate on openvz. There is a list of valid choices in the Containers Guide . containers:openvz_diskspace Amount of disk space to allocate to an openvz container. Be sure to include the G (gigabyte) or M (megabyte) suffix or the size will be taken as disk blocks. containers:ghost If this attribute is true, resources will be allocated for this node, but it will not be started when the topology is created. containers:maverick_url A location to download the QEMU image for this container. The name is a legacy that will disappear. This is deprecated. There are a few other attributes that are meaningful to more applications. Users specifying ns2 files will not need to set these directly, as the DETERLab ns2 interpreter does so. On Computers: startup The start command. tb-set-node-startcmd sets this. On interfaces: ip4_address The IPv4 address of this interface. Set by the ns2 commands for fixing addresses. ip4_netmask The IPv4 netmask. ns2 sets this. Configuration Files \uf0c1 These files control the operation of the containers system. Per-experiment Directory \uf0c1 When an experiment is containerized, the data necessary to create it is stored in /proj/ project /exp/ experiment /containers . The path /proj/ project /exp/ experiment is created by DETERLab when the experiment is created, and used by experimenters for a variety of things. This directory is replicated on nodes under /var/containers/config . There are a few files in the per-experiment directory that most experimenters can use: experiment.tcl If the topology was passed to containerize.py as an ns file, this is a copy of that input file. Useful for seeing what the experimenter asked for, or as a basis for new experiments. experiment.xml The analog of experiment.tcl , this is the topology given as topdl . The topdl input file. visualization.png A drawing of the virtual topology in png format. Generated by container_image.py hosts The host to IP mapping that will be installed on each node as /etc/hosts . site.conf A clone of the site configuration file that holds the global variables that the container creation will use. Values overridden on the command line invocation of containerize.py will be present in this file. The rest of this directory is primarily of interest to developers. It includes: annotated.xml First version of the input topology after default container types have been added. Input to the partitioning step. assignment A yaml representation of the partition to virtual node mapping. backend_config.yaml The server and channel to use for grandstand communication. Encoded in YAML. children Directory containing the assignment, including all the levels of nested hypervisors. config.tgz The contents of the per-experiment directory (except config.tgz ) for distribution into the experiment. embedding.yaml A yaml-encoded representation of the children sub-directory ghosts Containers that are initially not started in the experiment. maverick_url Yaml encoding of the qemu images to be used on each node. openvz_guest_url Yaml encoding of the openvz templates to be used on each node. partitioned.xml Output of the partitioning process. A copy of annotated.xml that has been decorated with the partitions. phys_topo.ns The ns2 file used to create the DETERLab experiment. phys_topo.xml The topdl file used to generate phystopo.ns . pid_eid The DETERLab project and experiment name under which this topology will be created. Broken out into /var/containers/pid and /var/containers/eid on virtual nodes inside the topology. route A directory containing the routing tables for each node. shaping.yaml Yaml-encoded data about the per-network and per-node loss, delay, and capacity parameters. switch A directory containing the VDE switch topology for the experiment. switch_extra.yaml Yaml-encoded extra switch configuration information. Mostly VDE switch configuration esoterica. topo.xml The final topology representation from which the physical topology is extracted. Includes the virtual topology as well. This file can be used as input to container_image.py . traffic_shaping.pickle Pickled information for configuring endnode traffic shaping. wirefilters.yaml Specific parameters for configuring the delay elements in VDE switched topologies that implement traffic shaping. See below . Site Configuration File \uf0c1 The site configuration file controls how all experiments are containerized across DETERLab. The contents are primarily of interest to developers, but researchers may occasionally find the need to specify their own. The --config parameter to containerize.py does that. The site configuration file is an attribute-value pair file parsed by a python ConfigParser that sets overall container parameters. Many of these have legacy internal names. The default site configuration is in /share/containers/site.conf on users.deterlab.net . Acceptable values (and their DETERLab defaults) are: maverick_url Default image used by qemu containers. Default: http://scratch/benito/pangolinbz.img.bz2 url_base Base URL of the DETERLab web interface on which users can see experiments. Default: http://www.isi.deterlab.net/ qemu_host_hw Hardware used by containers. Default: pc2133,bpc2133,MicroCloud xmlrpc_server Host and port from which to request experiment creation. Default: boss.isi.deterlab.net:3069 qemu_host_os OSID to request for qemu container nodess. Default: Ubuntu1604-STD exec_root Root of the directory tree holding containers software and libraries. Developers often change this. Default: /share/containers openvz_host_os OSID to request for openvz nodes. Default CentOS6-64-openvz openvz_guest_url Location to load the openvz template from. Default: %(exec_root)s/images/ubuntu-10.04-x86.tar.gz switch_shaping True if switched containers (see below) should do traffic shaping in the VDE switch that connects them. Default: true switched_containers A list of the containers that are networked with VDE switches. Default: qemus openvz_template_dir The directory that stores openvz template files. Default: %(exec_root)s/images/ (that is the images directory in the exec_root directory defined in the site config file. This can be a comma-separated list that will be searched in order, after any template directories given on the command line. node_log The name of the file on experiment nodes used to log containers creation. Default is /tmp/containers.log topdl_converter The program used to convert ns2 descriptions to topdl. The default is fedd_ns2topdl.py --file but any program that takes a single ns2 file as a parameter and prints the topdl to standard output is viable. On DETERLab installations /usr/testbed/lib/ns2ir/parse.tcl -t -x 3 -m dummy dummy dummy dummy can be used to decouple containers from needing a running fedd. default_router The IP address of a router needed to reach testbed infrastructure default_dest The network on which testbed infrastructure lives that needs to be routed through default_router . backend_server Deprecated grandstand_port Deprecated Container Notes \uf0c1 Different container types have some quirks. This section lists limitations of each container, as well as issues in interconnecting them. Qemu \uf0c1 Qemu nodes are limited to 7 experimental interfaces. They currently run only Ubuntu 12.04 32 bit operating systems. Physical Nodes \uf0c1 Physical nodes can be incorporated into experiments, but should only use modern versions of Ubuntu, to allow the Containers system to run their start commands correctly and to initialize their routing tables. Interconnections: VDE switches and local networking \uf0c1 The various containers are interconnected using either local kernel virtual networking or VDE switches . Kernel networking is lower overhead because it does not require process context switching, but VDE switches are a more general solution. Network behavior changes such as loss, delay or rate limits are introduced into a network of containers using one of two mechanisms: inserting elements into a VDE switch topology or end node traffic shaping. Inserting elements into the VDE switch topology allows the system to modify the behavior for all packets passing through it. Generally this means all packets to or from a host, as the Containers system inserts these elements in the path between the node and the switch. This figure shows three containers sharing a virtual LAN (VLAN) on a VDE switch with no traffic shaping: The blue containers connect to the switch and the switch has interconnected their VDE ports into the red shared VLAN. To add delays to two of the nodes on that VLAN, the following VDE switch configuration would be used: The VDE switch connects the containers with shaped traffic to the delay elements, not to the shared VLAN. The delay elements are on the VLAN and delay all traffic passing through them. The Container system configures the delay elements to delay traffic symmetrically - traffic from the LAN and traffic from the container are both delayed. The VDE tools can be configured asymmetrically as well. This is a very flexible way to interconnect containers. That flexibility incurs a cost in overhead. Each delay element and the VDE switch is a process, do traffic passing from one delayed nodes to the other experiences 7 context switches: container -> switch, switch -> delay, delay -> switch, switch -> delay, delay -> switch, and switch -> container. The alternative mechanism is to do the traffic shaping inside the nodes, using Linux traffic shaping . In this case, traffic outbound from a container is delayed in the container for the full transit time to the next hop. The next node does the same. End-node shaping all happens in the kernel so it is relatively inexpensive at run time. Qemu nodes can make use of either end-node shaping or VDE shaping, and use VDE shaping by default. The --end-node-shaping and --vde-switch-shaping options to containerize.py force the choice in qemu. Openvz nodes only use end-node traffic shaping. They have no native VDE support so interconnecting openvz containers to VDE switches would include both extra kernel crossings and extra context switches. Because a primary attraction of VDE switches is their efficiency, the Containers system does not implement VDE interconnections to openvz. Similarly embedded physical nodes use only endnode traffic shaping, as routing outgoing traffic through a virtual switch infrastructure that just connects to its physical interfaces is at best confusing. Unfortunately, endnode traffic shaping and VDE shaping are incompatible. Because endnode shaping does not impose delays on arriving traffic, it cannot delay traffic from a VDE delayed node correctly. This is primarily of academic interest, unless a researcher wants to impose traffic shaping between containers using incompatible traffic shaping. There needs to be an unshaped link between the two kinds of traffic shaping. Bootable Qemu Images \uf0c1 For qemu images to boot reliably, they should not wait for a keypress at the grub command, which is distressingly common. To ensure that your image does not wait for grub , do the following: For Ubuntu 12.04 (and any system that uses grub2) edit /etc/default/grub . For example: GRUB_DEFAULT=0 GRUB_HIDDEN_TIMEOUT=0 GRUB_HIDDEN_TIMEOUT_QUIET=true GRUB_TIMEOUT=1 GRUB_DISTRIBUTOR=`lsb_release -i -s 2> /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\" GRUB_CMDLINE_LINUX=\"\" Just make sure the HIDDENs are not commented out and have true/0 values. You then must run a command on the system which generates all the new grub configurations: $ sudo update-grub sudo configuration \uf0c1 The Containers system adds all users to the admin group so that group should be able to use sudo without providing a password.","title":"Containers reference"},{"location":"containers/containers-reference/#containers-reference","text":"This document describes the details of the commands and data structures that make up the Containers system. The Containers Guide provides useful context about the workflows and goals of the system that inform these technical details.","title":"Containers Reference"},{"location":"containers/containers-reference/#commands","text":"This section describes the command line interface to the Containers system.","title":"Commands"},{"location":"containers/containers-reference/#containerizepy","text":"The containerize.py command creates a DETERLab experiment made up of containers. The containerize.py program is available from /share/containers/containerize.py on users.deterlab.net . A sample invocation is: $ /share/containers/containerize.py MyProject MyExperiment ~/mytopology.tcl It will create a new experiment in MyProject called MyExperiment containing the experiment topology in mytopology.tcl . All the topology creation commands supported by DETERLab are supported by the Containers system, but DETERLab program agents are not. DETERLab start commands are supported. Containers will create an experiment in a group if the project parameter is of the form project / group . To start an experiment in the testing group of the DETER project, the first parameter is specified as DETER/testing . Containers supports ns2 file or topdl descriptions. Ns2 descriptions must end with .tcl or .ns . Other files are assumed to be topdl descriptions. Names of substrates and nodes in ns2 files are restricted to valid tcl variable names. Names of substrates and nodes in topdl files are restricted to the characters A-Z, a-z, digits, the underscore and the hyphen (-). By default, containerize.py program will partition the topology into openvz containers, packing 10 containers per physical computer. If the topology is already partitioned - meaning at least one element has a containers::partition attribute - containerize.py will not partition it. The --force-partition flag causes containerize.py to partition the experiment regardless of the presence of containers:partition attributes. If container types have been assigned to nodes using the containers:node_type attribute, containerize.py will respect them. Valid container types for the containers:node_type attribute or the --default-container parameter are: Parameter Container embedded_pnode Physical Node qemu Qemu VM openvz Openvz Container The containerize.py command takes several parameters that can change its behavior: --default-container = kind Containerize nodes without a container type into kind . If no nodes have been assigned containers, this puts all them into kind containers. --force-partition Partition the experiment whether or not it has been partitioned already --packing= int Attempt to put int containers into each physical node. The default --packing is 10. --size= int Attempt to divide the experiment into int physical nodes. The default is to use packing. There are some nuances to this with mixed containers. See the Containers Guide for more details. --config= filename Read configuration variables from filename . Configuration values are discussed below . --pnode-types= type1[,type2...] Override the site configuration and request nodes of type1 (or type2 etc.) as host nodes. --end-node-shaping Attempt to do end node traffic shaping even in containers connected by VDE switches. This works with qemu nodes. Topologies that include both openvz nodes and qemu nodes that shape traffic should use this. See the discussion below . --vde-switch-shaping Do traffic shaping in VDE switches. Probably the default, but that is controlled in the site configuration . See the discussion below . --openvz-diskspace Set the default openvz disk space size. The suffixes G and M stand for 'gigabytes' and 'megabytes'. --openvz-template Set the default openvz template. Templates are described in the Containers Guide . --openvz-template-dir Add a directory to be searched for openvz templates. Templates must end in tar.gz and be accessible to the user at creation and swap time. They can only be located under the /proj or /share directories. --image Construct a visualization of the virtual topology and leave it in the experiment directories (default). --nodes-only Ignore network constraints when partitioning nodes. --no-image Do not construct a visualization of the virtual topology. --pass-pack= pass : packing [, pass : packing ...] Specify the packing factor for each partitioning pass. The [ContainersGuide#MoreSophisticatedPacking:MultiplePasses Containers Guide] describes this in detail. --pass-size= pass : size [, pass : size ...] Specify the number of physical machines to use for each partitioning pass. The Containers Guide describes this in detail. --pass-pnodes= pass : type [, type ...][; pass : type [, type ...]...] Specify the pnode types on which nodes packed in partitioning pass pass can be placed. The Containers Guide describes this in detail. --pass-nodes-only= pass [, pass ...] Specify the partitioning passes on which network connectivity is ignored. The Containers Guide describes this in detail. --prefer-qemu-users= user[,user...] Make sure that Qemu images mount the given users' home directories. Qemu nodes can mount at most 19 users' home directories and this ensures that the experimenters using the containers can reach their home directories. --debug Print additional diagnostics and leave failed DETER experiments on the testbed. --keep-tmp Do not remove temporary files - used for debugging only. This invocation: $ ./containerize.py --packing 25 --default-container=qemu --force-partition DeterTest faber-packem ~/experiment.xml takes the topology in ~/experiment.xml (which must be a topdl description), packs it into 25 qemu containers per physical node, and creates an experiment called 'DeterTest/faber-packem' that can be swapped-in. If experiment.xml is already partitioned, it will be re-partitioned. If some nodes in that topology are assigned to openvz nodes already, those nodes will be still be in openvz containers. The result of a successful containerize.py run is a DETERLab experiment that can be swapped in. More detailed examples are available in the Containers Guide .","title":"containerize.py"},{"location":"containers/containers-reference/#container_imagepy","text":"The container_image.py command draws a picture of the topology of an experiment. This is helpful in keeping track of how virtual nodes are connected. containerize.py calls this internally and stores the output in the per-experiment directory (unless --no-image is used). A researcher may call container_image.py directly to generate an image later or to generate one with the partitioning drawn. The simplest way to call container_image.py is: /share/containers/container_image.py topology.xml output.png The first parameter is a topdl description, for example the one in the per-experiment directory . The second parameter is the output file for the image. When drawing an experiment that has been containerized, the --experiment option is very useful. Options include: --experiment= project / experiment Draw the experiment in project / experiment , if it exists. Note that this is just the DETERLab experiment and project names. Omit any sub-group. --topo= filename Draw the topology in the filename indicated. --attr-prefix= prefix Prefix for containers attributes. Deprecated. --partitions Draw labeled boxes around nodes that share a physical node. --out= filename Save the image in the filename indicated. --program= programname Use programname to lay out the graph. programname must take a file in graphviz's dot language . This is given as the --program option to fedd_image.py internally. The default is fdp which works well when --partitions is given. If neither --topo nor --experiment is given, the first positional parameter is the topdl topology to draw. If --out is not given the next positional parameter (the first if neither --topo nor --experiment is given) is the output file. A common invocation looks like: /share/containers/container_image.py --experiment SAFER/testbed-containers ~/drawing.png","title":"container_image.py"},{"location":"containers/containers-reference/#topdl-attributes-for-containers","text":"Several topdl attributes influence how an experiment is containerized. These can be added to nodes using the ns2 tb-add-node-attribute command (used throughout the Containers Guide ) or directly to the topdl. These attributes are all attached to nodes/Computers: containers:node_type The container that will hold this node. The full list is available here. containers:partition An identifier grouping nodes together in containers that will share a physical node. Generally assigned by containerize.py , but researchers can also directly assign them. The containerize.py command assigns integers, so if a researcher assigns other partition identifiers, containerize.py will not overwrite them. containers:openvz_template The flavor of Linux distribution to emulate on openvz. There is a list of valid choices in the Containers Guide . containers:openvz_diskspace Amount of disk space to allocate to an openvz container. Be sure to include the G (gigabyte) or M (megabyte) suffix or the size will be taken as disk blocks. containers:ghost If this attribute is true, resources will be allocated for this node, but it will not be started when the topology is created. containers:maverick_url A location to download the QEMU image for this container. The name is a legacy that will disappear. This is deprecated. There are a few other attributes that are meaningful to more applications. Users specifying ns2 files will not need to set these directly, as the DETERLab ns2 interpreter does so. On Computers: startup The start command. tb-set-node-startcmd sets this. On interfaces: ip4_address The IPv4 address of this interface. Set by the ns2 commands for fixing addresses. ip4_netmask The IPv4 netmask. ns2 sets this.","title":"Topdl Attributes For Containers"},{"location":"containers/containers-reference/#configuration-files","text":"These files control the operation of the containers system.","title":"Configuration Files"},{"location":"containers/containers-reference/#per-experiment-directory","text":"When an experiment is containerized, the data necessary to create it is stored in /proj/ project /exp/ experiment /containers . The path /proj/ project /exp/ experiment is created by DETERLab when the experiment is created, and used by experimenters for a variety of things. This directory is replicated on nodes under /var/containers/config . There are a few files in the per-experiment directory that most experimenters can use: experiment.tcl If the topology was passed to containerize.py as an ns file, this is a copy of that input file. Useful for seeing what the experimenter asked for, or as a basis for new experiments. experiment.xml The analog of experiment.tcl , this is the topology given as topdl . The topdl input file. visualization.png A drawing of the virtual topology in png format. Generated by container_image.py hosts The host to IP mapping that will be installed on each node as /etc/hosts . site.conf A clone of the site configuration file that holds the global variables that the container creation will use. Values overridden on the command line invocation of containerize.py will be present in this file. The rest of this directory is primarily of interest to developers. It includes: annotated.xml First version of the input topology after default container types have been added. Input to the partitioning step. assignment A yaml representation of the partition to virtual node mapping. backend_config.yaml The server and channel to use for grandstand communication. Encoded in YAML. children Directory containing the assignment, including all the levels of nested hypervisors. config.tgz The contents of the per-experiment directory (except config.tgz ) for distribution into the experiment. embedding.yaml A yaml-encoded representation of the children sub-directory ghosts Containers that are initially not started in the experiment. maverick_url Yaml encoding of the qemu images to be used on each node. openvz_guest_url Yaml encoding of the openvz templates to be used on each node. partitioned.xml Output of the partitioning process. A copy of annotated.xml that has been decorated with the partitions. phys_topo.ns The ns2 file used to create the DETERLab experiment. phys_topo.xml The topdl file used to generate phystopo.ns . pid_eid The DETERLab project and experiment name under which this topology will be created. Broken out into /var/containers/pid and /var/containers/eid on virtual nodes inside the topology. route A directory containing the routing tables for each node. shaping.yaml Yaml-encoded data about the per-network and per-node loss, delay, and capacity parameters. switch A directory containing the VDE switch topology for the experiment. switch_extra.yaml Yaml-encoded extra switch configuration information. Mostly VDE switch configuration esoterica. topo.xml The final topology representation from which the physical topology is extracted. Includes the virtual topology as well. This file can be used as input to container_image.py . traffic_shaping.pickle Pickled information for configuring endnode traffic shaping. wirefilters.yaml Specific parameters for configuring the delay elements in VDE switched topologies that implement traffic shaping. See below .","title":"Per-experiment Directory"},{"location":"containers/containers-reference/#site-configuration-file","text":"The site configuration file controls how all experiments are containerized across DETERLab. The contents are primarily of interest to developers, but researchers may occasionally find the need to specify their own. The --config parameter to containerize.py does that. The site configuration file is an attribute-value pair file parsed by a python ConfigParser that sets overall container parameters. Many of these have legacy internal names. The default site configuration is in /share/containers/site.conf on users.deterlab.net . Acceptable values (and their DETERLab defaults) are: maverick_url Default image used by qemu containers. Default: http://scratch/benito/pangolinbz.img.bz2 url_base Base URL of the DETERLab web interface on which users can see experiments. Default: http://www.isi.deterlab.net/ qemu_host_hw Hardware used by containers. Default: pc2133,bpc2133,MicroCloud xmlrpc_server Host and port from which to request experiment creation. Default: boss.isi.deterlab.net:3069 qemu_host_os OSID to request for qemu container nodess. Default: Ubuntu1604-STD exec_root Root of the directory tree holding containers software and libraries. Developers often change this. Default: /share/containers openvz_host_os OSID to request for openvz nodes. Default CentOS6-64-openvz openvz_guest_url Location to load the openvz template from. Default: %(exec_root)s/images/ubuntu-10.04-x86.tar.gz switch_shaping True if switched containers (see below) should do traffic shaping in the VDE switch that connects them. Default: true switched_containers A list of the containers that are networked with VDE switches. Default: qemus openvz_template_dir The directory that stores openvz template files. Default: %(exec_root)s/images/ (that is the images directory in the exec_root directory defined in the site config file. This can be a comma-separated list that will be searched in order, after any template directories given on the command line. node_log The name of the file on experiment nodes used to log containers creation. Default is /tmp/containers.log topdl_converter The program used to convert ns2 descriptions to topdl. The default is fedd_ns2topdl.py --file but any program that takes a single ns2 file as a parameter and prints the topdl to standard output is viable. On DETERLab installations /usr/testbed/lib/ns2ir/parse.tcl -t -x 3 -m dummy dummy dummy dummy can be used to decouple containers from needing a running fedd. default_router The IP address of a router needed to reach testbed infrastructure default_dest The network on which testbed infrastructure lives that needs to be routed through default_router . backend_server Deprecated grandstand_port Deprecated","title":"Site Configuration File"},{"location":"containers/containers-reference/#container-notes","text":"Different container types have some quirks. This section lists limitations of each container, as well as issues in interconnecting them.","title":"Container Notes"},{"location":"containers/containers-reference/#qemu","text":"Qemu nodes are limited to 7 experimental interfaces. They currently run only Ubuntu 12.04 32 bit operating systems.","title":"Qemu"},{"location":"containers/containers-reference/#physical-nodes","text":"Physical nodes can be incorporated into experiments, but should only use modern versions of Ubuntu, to allow the Containers system to run their start commands correctly and to initialize their routing tables.","title":"Physical Nodes"},{"location":"containers/containers-reference/#interconnections-vde-switches-and-local-networking","text":"The various containers are interconnected using either local kernel virtual networking or VDE switches . Kernel networking is lower overhead because it does not require process context switching, but VDE switches are a more general solution. Network behavior changes such as loss, delay or rate limits are introduced into a network of containers using one of two mechanisms: inserting elements into a VDE switch topology or end node traffic shaping. Inserting elements into the VDE switch topology allows the system to modify the behavior for all packets passing through it. Generally this means all packets to or from a host, as the Containers system inserts these elements in the path between the node and the switch. This figure shows three containers sharing a virtual LAN (VLAN) on a VDE switch with no traffic shaping: The blue containers connect to the switch and the switch has interconnected their VDE ports into the red shared VLAN. To add delays to two of the nodes on that VLAN, the following VDE switch configuration would be used: The VDE switch connects the containers with shaped traffic to the delay elements, not to the shared VLAN. The delay elements are on the VLAN and delay all traffic passing through them. The Container system configures the delay elements to delay traffic symmetrically - traffic from the LAN and traffic from the container are both delayed. The VDE tools can be configured asymmetrically as well. This is a very flexible way to interconnect containers. That flexibility incurs a cost in overhead. Each delay element and the VDE switch is a process, do traffic passing from one delayed nodes to the other experiences 7 context switches: container -> switch, switch -> delay, delay -> switch, switch -> delay, delay -> switch, and switch -> container. The alternative mechanism is to do the traffic shaping inside the nodes, using Linux traffic shaping . In this case, traffic outbound from a container is delayed in the container for the full transit time to the next hop. The next node does the same. End-node shaping all happens in the kernel so it is relatively inexpensive at run time. Qemu nodes can make use of either end-node shaping or VDE shaping, and use VDE shaping by default. The --end-node-shaping and --vde-switch-shaping options to containerize.py force the choice in qemu. Openvz nodes only use end-node traffic shaping. They have no native VDE support so interconnecting openvz containers to VDE switches would include both extra kernel crossings and extra context switches. Because a primary attraction of VDE switches is their efficiency, the Containers system does not implement VDE interconnections to openvz. Similarly embedded physical nodes use only endnode traffic shaping, as routing outgoing traffic through a virtual switch infrastructure that just connects to its physical interfaces is at best confusing. Unfortunately, endnode traffic shaping and VDE shaping are incompatible. Because endnode shaping does not impose delays on arriving traffic, it cannot delay traffic from a VDE delayed node correctly. This is primarily of academic interest, unless a researcher wants to impose traffic shaping between containers using incompatible traffic shaping. There needs to be an unshaped link between the two kinds of traffic shaping.","title":"Interconnections: VDE switches and local networking"},{"location":"containers/containers-reference/#bootable-qemu-images","text":"For qemu images to boot reliably, they should not wait for a keypress at the grub command, which is distressingly common. To ensure that your image does not wait for grub , do the following: For Ubuntu 12.04 (and any system that uses grub2) edit /etc/default/grub . For example: GRUB_DEFAULT=0 GRUB_HIDDEN_TIMEOUT=0 GRUB_HIDDEN_TIMEOUT_QUIET=true GRUB_TIMEOUT=1 GRUB_DISTRIBUTOR=`lsb_release -i -s 2> /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\" GRUB_CMDLINE_LINUX=\"\" Just make sure the HIDDENs are not commented out and have true/0 values. You then must run a command on the system which generates all the new grub configurations: $ sudo update-grub","title":"Bootable Qemu Images"},{"location":"containers/containers-reference/#sudo-configuration","text":"The Containers system adds all users to the admin group so that group should be able to use sudo without providing a password.","title":"sudo configuration"},{"location":"core/DETERSSH/","text":"Advanced Notes on Accessing Testbeds using SSH \uf0c1 Please first see the guide on interacting with your nodes . The rest of the notes on this page relate to specific operating systems and specific, advanced, uses of SSH. Each node on the testbed is reachable via SSH . DETERLab nodes are not accessible directly from the internet. In order to log into your nodes, you must first log into '''users.deterlab.net''' using your DETER username (not your email address) and password (or your SSH public key). From users you can log into your nodes. To save on connections, you might want to look into using GNU screen on users . Also refer to the Tips and Tricks section below for ways to make accessing DETER easier. Uploading files to DETER \uf0c1 You can upload files to users.deterlab.net via SCP or SFTP . Files in your home directory and in your project directory will be made available to you on all of your testbed nodes via NFS . Linux directions are given in the guide on interacting with your nodes . An Example Session with Windows and Putty \uf0c1 Putty is a free, lightweight SSH client for Windows. Here is an example session in which I connect to my experimental node \"node0\" in my experiment \"jjh-ubuntu1004\" in the project \"DeterTest\". First we connect to '''users.deterlab.net''': We then enter in our just our username and password (the same password as the DETERLab web interface). Trying to use your email address or something like jjh@users.deterlab.net will '''not''' work: Now we have successfully logged into users.deterlab.net: From users, we now ssh into our experimental node, \"node0.jjh-ubuntu.detertest\": SSH port forwarding \uf0c1 If you are, for example, running an internal web server on one of your DETER nodes, you can access it via SSH through users. You can find the name of the node in MyDeterlab view for your experiment (see the red oval in the picture below). For example to redirect port 80 on node A (pc176) to your local machine on port 8080 you would do: ssh -L 8080:pc176:80 YourUsername@users.deterlab.net Once logged in, you should be able to access the web server on your DETER node by going to http://localhost:8080. For more information on port forwarding with SSH, please refer to the SSH man page. SSH port forwarding with Putty \uf0c1 To use putty for port forwarding, configure putty to open a connection to users.deterlab.net before you make that connection, set up the tunneling parameters. This example forwards local port 12345 to a remode desktop protocol server (port 3389) on a testbed node. select the Tunnels menu from under the SSH choice in the Connection menu on the left hand side. Add a forwarded port using the Local type, a local port number (12345 in the image) and the DETER hostname and port in the Destination field. In the example we are forwarding the connection to port 3389 (the remote desktop protocol) on pc102 . Be sure to press Add to add the port. The putty window will look like this: Now open that connection. You will see a login prompt, and you should log in to users. Now you should be able to point your local remote desktop viewer to localhost port 12345 and see the login screen of pc102. If the node is a Windows node, you will see something like this: Be sure that your local machine does not firewall the local port 12345. Replace pc102 with a node in your experiment and the forwarded port with the port your service uses. Uploading your SSH key from OS X \uf0c1 The Upload File dialog in Macintosh OS X does not show hidden directories by default. This creates and extra hassle when uploading SSH public keys from an OS X machine. In the \"Upload File\" dialog, use the shortcut '''Shift-Command-G''' and type in \"~/.ssh\" to navigate to the contents of your .ssh directory. Then you will be presented with the contents of your .ssh directory and will be able to upload your id_rsa.pub file to DETER: OpenSSH Configuration for Directly Logging into testbed nodes \uf0c1 These configuration tweaks should work for any operating system that runs OpenSSH (Linux, BSD, and OS X typically use OpenSSH as the default SSH client). It is possible to log directly into testbed nodes with a little SSH configuration tweaking. Adding the following statement to '''~/.ssh/config''' will allow you to skip logging into users in order to access a particular testbed node. Change MyProject to the name of your project. Host pc*.isi.deterlab.net ProxyCommand ssh users.deterlab.net nc %h %p StrictHostKeyChecking no Host *.MyProject.isi.deterlab.net ProxyCommand ssh users.deterlab.net nc %h %p StrictHostKeyChecking no With this configuration change and a proper SSH key setup, you will be able to directly log into nodes in your experiment. You will now be able to log into nodes in your experiment using either the actual node name, e.g. pc025.isi.deterlab.net, or YourNode.YourExperiment.YourProject.isi.deterlab.net. For example: jjhs-mac-mini:~ jjh$ ssh node0.jjh-ubuntu1004.DeterTest.isi.deterlab.net Warning: Permanently added 'node0.jjh-ubuntu1004.detertest.isi.deterlab.net' (RSA) to the list of known hosts. Linux node0.jjh-ubuntu1004.detertest.isi.deterlab.net 2.6.32-25-generic-pae #45-Ubuntu SMP Sat Oct 16 21:01:33 UTC 2010 i686 GNU/Linux Ubuntu 10.04.1 LTS Welcome to Ubuntu! * Documentation: https://help.ubuntu.com/ System information as of Wed Nov 10 20:41:19 PST 2010 System load: 0.0 Processes: 116 Usage of /: 12.3% of 14.67GB Users logged in: 0 Memory usage: 1% IP address for eth1: 192.168.1.26 Swap usage: 0% Graph this data and manage this system at https://landscape.canonical.com/ 4 packages can be updated. 2 updates are security updates. Last login: Wed Nov 10 20:13:15 2010 from users.deterlab.net node0:~> Accelerating Multiple Connections using OpenSSH Connection Multplexing \uf0c1 You can log in multiple times using the same SSH connection. This dramatically speeds up creating new connections. To enable SSH connection multiplexing, add the following lines to ~/.ssh/config. If you are on a multiuser machine, you may want to store the control socket someplace other than /tmp. Host users.deterlab.net ControlMaster auto ControlPath /tmp/%r@%h:%p To verify that it is working, you can use the '''-v''' option: jjhs-mac-mini:~ jjh$ ssh -v pc026.isi.deterlab.net OpenSSH_5.2p1, OpenSSL 0.9.8l 5 Nov 2009 debug1: Reading configuration data /Users/jjh/.ssh/config debug1: Applying options for *isi.deterlab.net debug1: Applying options for pc*.isi.deterlab.net debug1: Applying options for * debug1: Reading configuration data /etc/ssh_config debug1: auto-mux: Trying existing master Last login: Wed Nov 10 20:51:43 2010 from users.deterlab.net node0:~> If you try to close your master connection while other connections are active, the connection will stay running until the other sessions end.","title":"Advanced Notes on Accessing Testbeds using SSH"},{"location":"core/DETERSSH/#advanced-notes-on-accessing-testbeds-using-ssh","text":"Please first see the guide on interacting with your nodes . The rest of the notes on this page relate to specific operating systems and specific, advanced, uses of SSH. Each node on the testbed is reachable via SSH . DETERLab nodes are not accessible directly from the internet. In order to log into your nodes, you must first log into '''users.deterlab.net''' using your DETER username (not your email address) and password (or your SSH public key). From users you can log into your nodes. To save on connections, you might want to look into using GNU screen on users . Also refer to the Tips and Tricks section below for ways to make accessing DETER easier.","title":"Advanced Notes on Accessing Testbeds using SSH"},{"location":"core/DETERSSH/#uploading-files-to-deter","text":"You can upload files to users.deterlab.net via SCP or SFTP . Files in your home directory and in your project directory will be made available to you on all of your testbed nodes via NFS . Linux directions are given in the guide on interacting with your nodes .","title":"Uploading files to DETER"},{"location":"core/DETERSSH/#an-example-session-with-windows-and-putty","text":"Putty is a free, lightweight SSH client for Windows. Here is an example session in which I connect to my experimental node \"node0\" in my experiment \"jjh-ubuntu1004\" in the project \"DeterTest\". First we connect to '''users.deterlab.net''': We then enter in our just our username and password (the same password as the DETERLab web interface). Trying to use your email address or something like jjh@users.deterlab.net will '''not''' work: Now we have successfully logged into users.deterlab.net: From users, we now ssh into our experimental node, \"node0.jjh-ubuntu.detertest\":","title":"An Example Session with Windows and Putty"},{"location":"core/DETERSSH/#ssh-port-forwarding","text":"If you are, for example, running an internal web server on one of your DETER nodes, you can access it via SSH through users. You can find the name of the node in MyDeterlab view for your experiment (see the red oval in the picture below). For example to redirect port 80 on node A (pc176) to your local machine on port 8080 you would do: ssh -L 8080:pc176:80 YourUsername@users.deterlab.net Once logged in, you should be able to access the web server on your DETER node by going to http://localhost:8080. For more information on port forwarding with SSH, please refer to the SSH man page.","title":"SSH port forwarding"},{"location":"core/DETERSSH/#ssh-port-forwarding-with-putty","text":"To use putty for port forwarding, configure putty to open a connection to users.deterlab.net before you make that connection, set up the tunneling parameters. This example forwards local port 12345 to a remode desktop protocol server (port 3389) on a testbed node. select the Tunnels menu from under the SSH choice in the Connection menu on the left hand side. Add a forwarded port using the Local type, a local port number (12345 in the image) and the DETER hostname and port in the Destination field. In the example we are forwarding the connection to port 3389 (the remote desktop protocol) on pc102 . Be sure to press Add to add the port. The putty window will look like this: Now open that connection. You will see a login prompt, and you should log in to users. Now you should be able to point your local remote desktop viewer to localhost port 12345 and see the login screen of pc102. If the node is a Windows node, you will see something like this: Be sure that your local machine does not firewall the local port 12345. Replace pc102 with a node in your experiment and the forwarded port with the port your service uses.","title":"SSH port forwarding with Putty"},{"location":"core/DETERSSH/#uploading-your-ssh-key-from-os-x","text":"The Upload File dialog in Macintosh OS X does not show hidden directories by default. This creates and extra hassle when uploading SSH public keys from an OS X machine. In the \"Upload File\" dialog, use the shortcut '''Shift-Command-G''' and type in \"~/.ssh\" to navigate to the contents of your .ssh directory. Then you will be presented with the contents of your .ssh directory and will be able to upload your id_rsa.pub file to DETER:","title":"Uploading your SSH key from OS X "},{"location":"core/DETERSSH/#openssh-configuration-for-directly-logging-into-testbed-nodes","text":"These configuration tweaks should work for any operating system that runs OpenSSH (Linux, BSD, and OS X typically use OpenSSH as the default SSH client). It is possible to log directly into testbed nodes with a little SSH configuration tweaking. Adding the following statement to '''~/.ssh/config''' will allow you to skip logging into users in order to access a particular testbed node. Change MyProject to the name of your project. Host pc*.isi.deterlab.net ProxyCommand ssh users.deterlab.net nc %h %p StrictHostKeyChecking no Host *.MyProject.isi.deterlab.net ProxyCommand ssh users.deterlab.net nc %h %p StrictHostKeyChecking no With this configuration change and a proper SSH key setup, you will be able to directly log into nodes in your experiment. You will now be able to log into nodes in your experiment using either the actual node name, e.g. pc025.isi.deterlab.net, or YourNode.YourExperiment.YourProject.isi.deterlab.net. For example: jjhs-mac-mini:~ jjh$ ssh node0.jjh-ubuntu1004.DeterTest.isi.deterlab.net Warning: Permanently added 'node0.jjh-ubuntu1004.detertest.isi.deterlab.net' (RSA) to the list of known hosts. Linux node0.jjh-ubuntu1004.detertest.isi.deterlab.net 2.6.32-25-generic-pae #45-Ubuntu SMP Sat Oct 16 21:01:33 UTC 2010 i686 GNU/Linux Ubuntu 10.04.1 LTS Welcome to Ubuntu! * Documentation: https://help.ubuntu.com/ System information as of Wed Nov 10 20:41:19 PST 2010 System load: 0.0 Processes: 116 Usage of /: 12.3% of 14.67GB Users logged in: 0 Memory usage: 1% IP address for eth1: 192.168.1.26 Swap usage: 0% Graph this data and manage this system at https://landscape.canonical.com/ 4 packages can be updated. 2 updates are security updates. Last login: Wed Nov 10 20:13:15 2010 from users.deterlab.net node0:~>","title":"OpenSSH Configuration for Directly Logging into testbed nodes"},{"location":"core/DETERSSH/#accelerating-multiple-connections-using-openssh-connection-multplexing","text":"You can log in multiple times using the same SSH connection. This dramatically speeds up creating new connections. To enable SSH connection multiplexing, add the following lines to ~/.ssh/config. If you are on a multiuser machine, you may want to store the control socket someplace other than /tmp. Host users.deterlab.net ControlMaster auto ControlPath /tmp/%r@%h:%p To verify that it is working, you can use the '''-v''' option: jjhs-mac-mini:~ jjh$ ssh -v pc026.isi.deterlab.net OpenSSH_5.2p1, OpenSSL 0.9.8l 5 Nov 2009 debug1: Reading configuration data /Users/jjh/.ssh/config debug1: Applying options for *isi.deterlab.net debug1: Applying options for pc*.isi.deterlab.net debug1: Applying options for * debug1: Reading configuration data /etc/ssh_config debug1: auto-mux: Trying existing master Last login: Wed Nov 10 20:51:43 2010 from users.deterlab.net node0:~> If you try to close your master connection while other connections are active, the connection will stay running until the other sessions end.","title":"Accelerating Multiple Connections using OpenSSH Connection Multplexing"},{"location":"core/automate/","text":"Automating Your Experimentation \uf0c1 Once you have developed your experiment and set everything up, you may want to automate that setup so you can easily re-run the experimetn later. There are multiple ways to automate your experiment. Scripting \uf0c1 You can write scripts using languages like Bash and Python to run commands on your nodes in a given scenario. For example, if you wanted to ping B from A and then ping A from B, in an experiment called test and project Share , you could write the following script in bash: ssh A.test.Share \"ping -c 1 B\" ssh B.test.Share \"ping -c 1 A\" You can then save it, e.g., in a file called ping.sh and run it by typing on users.deterlab.net : bash ping.sh Similarly, in Python you can achieve the same with the following script: import paramiko ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) ssh.connect(\"A.test.Share\") stdin, stdout, stderr = ssh.exec_command(\"ping -c 1 B\") ssh.connect(\"B.test.Share\") stdin, stdout, stderr = ssh.exec_command(\"ping -c 1 A\") You can then save it, e.g., in a file called ping.py and run it on one of your experiment nodes (not on users.deterlab.net ) by typing: python ping.py DEW \uf0c1 You can use DEW - distributed experiment workflows to design your experiment in a human-readable format and generate NS file and bash scripts. ` We provide more guidance in DEW YouTube channel as well as in documentation on DEW Web site. If you use image Ubuntu-DEW on your nodes, all the commands you type and snippets of their outputs will be saved in your project directory. You can use the tool flight_log, which is automatically installed in that image, to remind yourself of the commands you ran in the past and to select those you want to include in a Bash script. The script will be automatically generated for you. More information about this direction is in DEW YouTube channel . Orchestrator \uf0c1 You can use our MAGI orchestrator to create scripts that will be more robust and readable than Bash scripts. Please see our orchestrator documentation for guidelines and examples.","title":"Automating Your Experimentation"},{"location":"core/automate/#automating-your-experimentation","text":"Once you have developed your experiment and set everything up, you may want to automate that setup so you can easily re-run the experimetn later. There are multiple ways to automate your experiment.","title":"Automating Your Experimentation"},{"location":"core/automate/#scripting","text":"You can write scripts using languages like Bash and Python to run commands on your nodes in a given scenario. For example, if you wanted to ping B from A and then ping A from B, in an experiment called test and project Share , you could write the following script in bash: ssh A.test.Share \"ping -c 1 B\" ssh B.test.Share \"ping -c 1 A\" You can then save it, e.g., in a file called ping.sh and run it by typing on users.deterlab.net : bash ping.sh Similarly, in Python you can achieve the same with the following script: import paramiko ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) ssh.connect(\"A.test.Share\") stdin, stdout, stderr = ssh.exec_command(\"ping -c 1 B\") ssh.connect(\"B.test.Share\") stdin, stdout, stderr = ssh.exec_command(\"ping -c 1 A\") You can then save it, e.g., in a file called ping.py and run it on one of your experiment nodes (not on users.deterlab.net ) by typing: python ping.py","title":"Scripting"},{"location":"core/automate/#dew","text":"You can use DEW - distributed experiment workflows to design your experiment in a human-readable format and generate NS file and bash scripts. ` We provide more guidance in DEW YouTube channel as well as in documentation on DEW Web site. If you use image Ubuntu-DEW on your nodes, all the commands you type and snippets of their outputs will be saved in your project directory. You can use the tool flight_log, which is automatically installed in that image, to remind yourself of the commands you ran in the past and to select those you want to include in a Bash script. The script will be automatically generated for you. More information about this direction is in DEW YouTube channel .","title":"DEW"},{"location":"core/automate/#orchestrator","text":"You can use our MAGI orchestrator to create scripts that will be more robust and readable than Bash scripts. Please see our orchestrator documentation for guidelines and examples.","title":"Orchestrator"},{"location":"core/core-quickstart/","text":"Quickstart \uf0c1 This page describes basic information about DETERLab and its core functionality. Please make sure to read detailed information about each step by clicking on the links in the menu or on the shapes in the figure. What is DETERLab? \uf0c1 The DETERLab testbed is a security and education-enhanced version of Emulab. Funded by the National Science Foundation and the Department of Homeland Security, DETERLab is hosted by USC/ISI and UC Berkeley. DETERLab (like Emulab) offers user accounts with assorted permissions associated with different experiment groups. Each group can have its own pre-configured experimental environments running on Linux, BSD, Windows, or other operating systems. Users running DETERLab experiments have full control of real hardware and networks running preconfigured software packages. How does it work? \uf0c1 The software running DETERLab loads operating system images (low level disk copies) onto free nodes in the testbed, and then reconfigures programmable switches to create VLANs with the newly-imaged nodes connected according to the topology specified by the experiment creator. After the system is fully imaged and configured, DETERLab executes specified scripts, unpacks tarballs, and/or installs RPM files according to the experiment's configuration. The end result is a live network of real machines, accessible via the Internet. Work in DETERLab is based on projects that include individual experiments and is accomplished either via the browser-based web interface (isi.deterlab.net) or via commandline on the DETERLab nodes. To access DETERLab, you need to create an account, which provides credentials for accessing both the web interface and nodes. How do I get a DETERLab account? \uf0c1 You may obtain a DETERLab account by either starting a new project (if you are a PI or instructor) or joining an existing project (if you are a project member or a student). If you are the project investigator or instructor, you must create a project and invite your team members or students to join. If you are the member of a team using DETERLab, your project leader will invite you to join the appropriate DETERLab project. If you are a student, you may not create a project. Your instructor must create the project and, once approved, will give you information for joining the project. See Getting Started for more information. How do I use DETERLab? \uf0c1 In general, once you have a DETERLab account, you follow these steps. The DETERLab Core Guide will walk you through a basic tutorial of these steps. 1. Design the topology \uf0c1 Every experiment in DETERLab is based on a network topology file written in NS format and saved on the users node. The following is a very basic example: # This is a simple ns script. Comments start with #. set ns [new Simulator] source tb_compat.tcl # Create 4 nodes set nodeA [$ns node] set nodeB [$ns node] set nodeC [$ns node] set nodeD [$ns node] # Link between nodeA and nodeB with no delay and 10Mb bandwidth set link0 [$ns duplex-link $nodeB $nodeA 10Mb 0ms DropTail] # LAN among nodeB, nodeC and nodeD with no delay and 100Mb bandwidth set lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms] # Set the OS on a couple of nodes # Ubuntu-STD is the preferred image tb-set-node-os $nodeA FBSD-STD tb-set-node-os $nodeC Ubuntu-STD # Set up routing $ns rtproto Static # Go! $ns run 2. Create, start and swap in (allocate resources for) an experiment \uf0c1 Using your topology file, you start a new experiment via menu options in the DETERLab web interface. 3. Generate traffic for your nodes \uf0c1 Now you can experiment and start generating traffic for your nodes. We provide a flexible framework to pull together the software you'll need. 4. View results by accessing nodes, modify the experiment as needed. \uf0c1 Once your experiment has started, you now can access nodes via SSH and conduct your desired experiments in your new environment. You may modify aspects of a running experiment through the \"Modify experiment\" page in the web interface or by making changes to the NS file. 5. Save your work and swap out your experiment (release the resources) \uf0c1 When you are ready to stop working on an experiment but know you will want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users. This is just a high-level overview. Go to the Core Guide for a basic hands-on example of using DETERLab Core.","title":"Quickstart"},{"location":"core/core-quickstart/#quickstart","text":"This page describes basic information about DETERLab and its core functionality. Please make sure to read detailed information about each step by clicking on the links in the menu or on the shapes in the figure.","title":"Quickstart"},{"location":"core/core-quickstart/#what-is-deterlab","text":"The DETERLab testbed is a security and education-enhanced version of Emulab. Funded by the National Science Foundation and the Department of Homeland Security, DETERLab is hosted by USC/ISI and UC Berkeley. DETERLab (like Emulab) offers user accounts with assorted permissions associated with different experiment groups. Each group can have its own pre-configured experimental environments running on Linux, BSD, Windows, or other operating systems. Users running DETERLab experiments have full control of real hardware and networks running preconfigured software packages.","title":"What is DETERLab?"},{"location":"core/core-quickstart/#how-does-it-work","text":"The software running DETERLab loads operating system images (low level disk copies) onto free nodes in the testbed, and then reconfigures programmable switches to create VLANs with the newly-imaged nodes connected according to the topology specified by the experiment creator. After the system is fully imaged and configured, DETERLab executes specified scripts, unpacks tarballs, and/or installs RPM files according to the experiment's configuration. The end result is a live network of real machines, accessible via the Internet. Work in DETERLab is based on projects that include individual experiments and is accomplished either via the browser-based web interface (isi.deterlab.net) or via commandline on the DETERLab nodes. To access DETERLab, you need to create an account, which provides credentials for accessing both the web interface and nodes.","title":"How does it work?"},{"location":"core/core-quickstart/#how-do-i-get-a-deterlab-account","text":"You may obtain a DETERLab account by either starting a new project (if you are a PI or instructor) or joining an existing project (if you are a project member or a student). If you are the project investigator or instructor, you must create a project and invite your team members or students to join. If you are the member of a team using DETERLab, your project leader will invite you to join the appropriate DETERLab project. If you are a student, you may not create a project. Your instructor must create the project and, once approved, will give you information for joining the project. See Getting Started for more information.","title":"How do I get a DETERLab account?"},{"location":"core/core-quickstart/#how-do-i-use-deterlab","text":"In general, once you have a DETERLab account, you follow these steps. The DETERLab Core Guide will walk you through a basic tutorial of these steps.","title":"How do I use DETERLab?"},{"location":"core/core-quickstart/#1-design-the-topology","text":"Every experiment in DETERLab is based on a network topology file written in NS format and saved on the users node. The following is a very basic example: # This is a simple ns script. Comments start with #. set ns [new Simulator] source tb_compat.tcl # Create 4 nodes set nodeA [$ns node] set nodeB [$ns node] set nodeC [$ns node] set nodeD [$ns node] # Link between nodeA and nodeB with no delay and 10Mb bandwidth set link0 [$ns duplex-link $nodeB $nodeA 10Mb 0ms DropTail] # LAN among nodeB, nodeC and nodeD with no delay and 100Mb bandwidth set lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms] # Set the OS on a couple of nodes # Ubuntu-STD is the preferred image tb-set-node-os $nodeA FBSD-STD tb-set-node-os $nodeC Ubuntu-STD # Set up routing $ns rtproto Static # Go! $ns run","title":"1. Design the topology"},{"location":"core/core-quickstart/#2-create-start-and-swap-in-allocate-resources-for-an-experiment","text":"Using your topology file, you start a new experiment via menu options in the DETERLab web interface.","title":"2. Create, start and swap in (allocate resources for) an experiment"},{"location":"core/core-quickstart/#3-generate-traffic-for-your-nodes","text":"Now you can experiment and start generating traffic for your nodes. We provide a flexible framework to pull together the software you'll need.","title":"3. Generate traffic for your nodes"},{"location":"core/core-quickstart/#4-view-results-by-accessing-nodes-modify-the-experiment-as-needed","text":"Once your experiment has started, you now can access nodes via SSH and conduct your desired experiments in your new environment. You may modify aspects of a running experiment through the \"Modify experiment\" page in the web interface or by making changes to the NS file.","title":"4. View results by accessing nodes, modify the experiment as needed."},{"location":"core/core-quickstart/#5-save-your-work-and-swap-out-your-experiment-release-the-resources","text":"When you are ready to stop working on an experiment but know you will want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users. This is just a high-level overview. Go to the Core Guide for a basic hands-on example of using DETERLab Core.","title":"5. Save your work and swap out your experiment (release the resources)"},{"location":"core/core-reference/","text":"TBD","title":"Core reference"},{"location":"core/create/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Create Experiment \uf0c1 You can use the web interface to create a new experiment. Log into DETERLab with your account credentials Click the Experimentation menu item, then click Begin an Experiment . Click Select Project and choose your project. Most people will be a member of just one project, and will not have a choice. If you are a member of multiple projects, be sure to select the correct project from the menu. In this example, we will refer to the project as YourProject . Leave the Group field set to Default Group unless otherwise instructed. Enter the Name field with an easily identifiable name for the experiment. The Name should be a single word (no spaces) identifier. If you are following our tutorial, use basic-experiment for the name. Enter the Description field with a brief description of the experiment. Any string will work here. In the Your NS File field, upload the basic.ns file you downloaded or any other NS file you created. You can also create the file on DETERLab by SSH-ing to users.deterlab.net and using a text editor like vi, vim, emacs, nano or pico. In that case you would specify the path to the file like /users/YourUsername/YourFilename.ns . The rest of the settings depend on the goals of your experiment. In our example, please set the Idle Swap field to 1 h and leave the rest of the settings for Swapping , Linktest Option , and BatchMode at their default. Click Submit . After submission, DETERLab will begin processing your request. Since you did not check Swap In Immediately box, DETERLab will simply check sytax of your NS file and save some state in the database. You will receive an email and a pop-up notification when this has been done. The process usually takes a few seconds.","title":"Create an experiment"},{"location":"core/create/#create-experiment","text":"You can use the web interface to create a new experiment. Log into DETERLab with your account credentials Click the Experimentation menu item, then click Begin an Experiment . Click Select Project and choose your project. Most people will be a member of just one project, and will not have a choice. If you are a member of multiple projects, be sure to select the correct project from the menu. In this example, we will refer to the project as YourProject . Leave the Group field set to Default Group unless otherwise instructed. Enter the Name field with an easily identifiable name for the experiment. The Name should be a single word (no spaces) identifier. If you are following our tutorial, use basic-experiment for the name. Enter the Description field with a brief description of the experiment. Any string will work here. In the Your NS File field, upload the basic.ns file you downloaded or any other NS file you created. You can also create the file on DETERLab by SSH-ing to users.deterlab.net and using a text editor like vi, vim, emacs, nano or pico. In that case you would specify the path to the file like /users/YourUsername/YourFilename.ns . The rest of the settings depend on the goals of your experiment. In our example, please set the Idle Swap field to 1 h and leave the rest of the settings for Swapping , Linktest Option , and BatchMode at their default. Click Submit . After submission, DETERLab will begin processing your request. Since you did not check Swap In Immediately box, DETERLab will simply check sytax of your NS file and save some state in the database. You will receive an email and a pop-up notification when this has been done. The process usually takes a few seconds.","title":"Create Experiment"},{"location":"core/custom-images/","text":"Creating Custom Operating System Images \uf0c1 What is an Operating System ID (OSID) versus an Image ID? \uf0c1 In order to make the best use of custom operating system images, it is important to understand the difference between these two concepts. An Image ID is a descriptor for a disk image. This can be an image of an entire disk, a partition of a disk, or a set of partitions of a disk. By supporting multiple partitions, we can technically support different operating systems within the same disk image. These are referred to as combo images . The Image ID points to a real file that is stored in the directory /proj/YourProjectName/images . Other critical information is associated with the Image ID, such as what node types are supported by the images and what operating systems are on each partition. You can view the Image IDs on the List ImageIDs page , which is in the Experimentation drop down menu on the testbed web interface. An OSID describes an operating system which resides on a partition of a disk image. Every Image ID will have at least one OSID associated with it. In typical testbed usage, the Image ID and OSID will be the same since we usually do not put multiple operating systems on a single image. You can view the OSIDs on the List OSIDs page , which is in the Experimentation drop down menu on the testbed web interface. Standard Testbed Images \uf0c1 We provide a number of supported testbed images here at DETERLab. These images can be viewed by looking at the Operating System ID list . Most new operating system images that we support are whole disk images, which is different from the more traditional scheme of using partition 1 for FreeBSD and partition 2 for Linux. To view what nodes a particular operating system image runs on and what sort of partition scheme it uses, please refer to the Image ID list page . The supported testbed images are listed on DETERLab (click on Experimentation from the top menu, then on Images ). Custom OS Images \uf0c1 If your set of operating system customizations cannot be easily contained within an RPM/TAR (or multiple RPM/TARs), then you can create your own custom OS image. DETERLab allows you to create your own disk images and load them on your experimental nodes automatically when your experiment is created or swapped in. Once you have created a custom disk image (and the associated ImageID/OSID descriptor for it, you can use that OSID in your NS file. When your experiment is swapped in, the testbed system will arrange for your disks to be loaded in parallel using a locally written multicast disk loading protocol. Note Experience has shown that it is much faster to load a disk image on 10 nodes at once, then it is to load a bunch of RPMS or tarballs on each node as it boots. So while it may seem like overkill to create your own disk image, we can assure you it is not. The most common approach is to use the New Image Descriptor form to create a disk image that contains a customized version of a standard Linux or the FreeBSD image. All you need to do is enter the node name in the form, and the testbed system will create the image for you automatically, notifying you via email when it is finished. You can then use that image in subsequent experiments by specifying the descriptor name in your NS file with the tb-set-node-os command. When the experiment is configured, the proper image will be loaded on each node automatically by the system. Creating Your Custom Image \uf0c1 A typical approach to creating your own disk image is using one of the default images as a base or template. To do this: Create a single-node Linux or FreeBSD experiment. In your NS file, use the appropriate tb-set-node-os command, as in the following example: tb-set-node-os $nodeA FBSD-STD tb-set-node-os $nodeA Ubuntu-STD After your experiment has swapped in (you have received the email saying it is running), log into the node and load all of the software packages that you wish to load. If you want to install the latest version of the Linux kernel on one of our standard disk images, or on your own custom Linux image, be sure to arrange for any programs that need to be started at boot time. It is a good idea to reboot the node and make sure that everything is running as expected when it comes up. Note the physical ( pcXXX ) name of the machine used! Create an image descriptor and image using the New Image Descriptor form. Wait for the email saying the image creation is done. Now you can create a second single-node experiment to test your new image. In your NS file, use tb-set-node-os to select the OSID that you just created. Be sure to remove any RPM or tarball directives. Submit that NS file and wait for the email notification. Then log into the new node and check to make sure everything is running normally. If everything is going well, terminate both of these single-node experiments. If not, release the experiment created in the previous step, and then go back and fix the original node ( pcXXX above). Recreate the image as needed: create_image -p <proj> <imageid> <node> Once your image is working properly, you can use it in any NS file by using the tb-set-node-os command. If you ever want to reload a node in your experiment, either with one of your images or with one of the default images, you can use the os_load command. Log into users and run: os_load -p <proj> -i <imageid> <node> This program will run in the foreground, waiting until the image has been loaded. At that point you should log in and make sure everything is working okay. If you want to load the default image, then simply run: os_load <node> Hints When Making New OS Images \uf0c1 Please, never try to create an image from a node or type that begins with the letter b , e.g. bpc183 or bpc2133 . These nodes are located in Berkeley which is physically located 400 miles away from the boss and users servers. After you have created an image, load it back and watch what happens through the serial port. Consider creating a two node experiment, one to create the image and the other to load it back. There is a command called os_load available on the users server: users% which os_load /usr/testbed/bin/os_load users% os_load -h option -h not recognized os_load [options] node [node ...] os_load [options] -e pid,eid where: -i Specify image name; otherwise load default image -p Specify project for finding image name (-i) -s Do *not* wait for nodes to finish reloading -m Specify internal image id (instead of -i and -p) -r Do *not* reboot nodes; do that yourself -e Reboot all nodes in an experiment node Node to reboot (pcXXX) While the second node is reloading, watch its progress in real time using the console command from the users server, ie: users% console pc193 If you think you've got a good image, but it flounders while coming up, create another experiment with an NS directive that says \"Even if you think the node has not booted, let my experiment swap in anyway.\" This may allow you to log in through the console and figure out what went wrong. An example of such a directive is: tb-set-node-failure-action $nodeA \"nonfatal\" Create whole disk images on a smaller machine rather than a single partition image. Updating your Custom Image \uf0c1 Once you have your image, it is easy to update it later by taking a new snapshot from a node running your image. Assuming you have swapped in an experiment with a node running your image and you have made changes to that node, use the DETERLab web interface to navigate to the descriptor page for your image: Use the Experimentation drop down menu, and choose List ImageIDs to see the entire list of Images you may access. Find your custom image and click on it. In the More Options menu, click on Snapshot Node ... Fill in the name of the node that is running your image, and click on Go . As in the above instructions, wait for the email saying your image has been updated before you try and use the image.","title":"Creating Custom Operating System Images"},{"location":"core/custom-images/#creating-custom-operating-system-images","text":"","title":"Creating Custom Operating System Images"},{"location":"core/custom-images/#what-is-an-operating-system-id-osid-versus-an-image-id","text":"In order to make the best use of custom operating system images, it is important to understand the difference between these two concepts. An Image ID is a descriptor for a disk image. This can be an image of an entire disk, a partition of a disk, or a set of partitions of a disk. By supporting multiple partitions, we can technically support different operating systems within the same disk image. These are referred to as combo images . The Image ID points to a real file that is stored in the directory /proj/YourProjectName/images . Other critical information is associated with the Image ID, such as what node types are supported by the images and what operating systems are on each partition. You can view the Image IDs on the List ImageIDs page , which is in the Experimentation drop down menu on the testbed web interface. An OSID describes an operating system which resides on a partition of a disk image. Every Image ID will have at least one OSID associated with it. In typical testbed usage, the Image ID and OSID will be the same since we usually do not put multiple operating systems on a single image. You can view the OSIDs on the List OSIDs page , which is in the Experimentation drop down menu on the testbed web interface.","title":"What is an Operating System ID (OSID) versus an Image ID?"},{"location":"core/custom-images/#standard-testbed-images","text":"We provide a number of supported testbed images here at DETERLab. These images can be viewed by looking at the Operating System ID list . Most new operating system images that we support are whole disk images, which is different from the more traditional scheme of using partition 1 for FreeBSD and partition 2 for Linux. To view what nodes a particular operating system image runs on and what sort of partition scheme it uses, please refer to the Image ID list page . The supported testbed images are listed on DETERLab (click on Experimentation from the top menu, then on Images ).","title":"Standard Testbed Images"},{"location":"core/custom-images/#custom-os-images","text":"If your set of operating system customizations cannot be easily contained within an RPM/TAR (or multiple RPM/TARs), then you can create your own custom OS image. DETERLab allows you to create your own disk images and load them on your experimental nodes automatically when your experiment is created or swapped in. Once you have created a custom disk image (and the associated ImageID/OSID descriptor for it, you can use that OSID in your NS file. When your experiment is swapped in, the testbed system will arrange for your disks to be loaded in parallel using a locally written multicast disk loading protocol. Note Experience has shown that it is much faster to load a disk image on 10 nodes at once, then it is to load a bunch of RPMS or tarballs on each node as it boots. So while it may seem like overkill to create your own disk image, we can assure you it is not. The most common approach is to use the New Image Descriptor form to create a disk image that contains a customized version of a standard Linux or the FreeBSD image. All you need to do is enter the node name in the form, and the testbed system will create the image for you automatically, notifying you via email when it is finished. You can then use that image in subsequent experiments by specifying the descriptor name in your NS file with the tb-set-node-os command. When the experiment is configured, the proper image will be loaded on each node automatically by the system.","title":"Custom OS Images"},{"location":"core/custom-images/#creating-your-custom-image","text":"A typical approach to creating your own disk image is using one of the default images as a base or template. To do this: Create a single-node Linux or FreeBSD experiment. In your NS file, use the appropriate tb-set-node-os command, as in the following example: tb-set-node-os $nodeA FBSD-STD tb-set-node-os $nodeA Ubuntu-STD After your experiment has swapped in (you have received the email saying it is running), log into the node and load all of the software packages that you wish to load. If you want to install the latest version of the Linux kernel on one of our standard disk images, or on your own custom Linux image, be sure to arrange for any programs that need to be started at boot time. It is a good idea to reboot the node and make sure that everything is running as expected when it comes up. Note the physical ( pcXXX ) name of the machine used! Create an image descriptor and image using the New Image Descriptor form. Wait for the email saying the image creation is done. Now you can create a second single-node experiment to test your new image. In your NS file, use tb-set-node-os to select the OSID that you just created. Be sure to remove any RPM or tarball directives. Submit that NS file and wait for the email notification. Then log into the new node and check to make sure everything is running normally. If everything is going well, terminate both of these single-node experiments. If not, release the experiment created in the previous step, and then go back and fix the original node ( pcXXX above). Recreate the image as needed: create_image -p <proj> <imageid> <node> Once your image is working properly, you can use it in any NS file by using the tb-set-node-os command. If you ever want to reload a node in your experiment, either with one of your images or with one of the default images, you can use the os_load command. Log into users and run: os_load -p <proj> -i <imageid> <node> This program will run in the foreground, waiting until the image has been loaded. At that point you should log in and make sure everything is working okay. If you want to load the default image, then simply run: os_load <node>","title":"Creating Your Custom Image"},{"location":"core/custom-images/#hints-when-making-new-os-images","text":"Please, never try to create an image from a node or type that begins with the letter b , e.g. bpc183 or bpc2133 . These nodes are located in Berkeley which is physically located 400 miles away from the boss and users servers. After you have created an image, load it back and watch what happens through the serial port. Consider creating a two node experiment, one to create the image and the other to load it back. There is a command called os_load available on the users server: users% which os_load /usr/testbed/bin/os_load users% os_load -h option -h not recognized os_load [options] node [node ...] os_load [options] -e pid,eid where: -i Specify image name; otherwise load default image -p Specify project for finding image name (-i) -s Do *not* wait for nodes to finish reloading -m Specify internal image id (instead of -i and -p) -r Do *not* reboot nodes; do that yourself -e Reboot all nodes in an experiment node Node to reboot (pcXXX) While the second node is reloading, watch its progress in real time using the console command from the users server, ie: users% console pc193 If you think you've got a good image, but it flounders while coming up, create another experiment with an NS directive that says \"Even if you think the node has not booted, let my experiment swap in anyway.\" This may allow you to log in through the console and figure out what went wrong. An example of such a directive is: tb-set-node-failure-action $nodeA \"nonfatal\" Create whole disk images on a smaller machine rather than a single partition image.","title":"Hints When Making New OS Images"},{"location":"core/custom-images/#updating-your-custom-image","text":"Once you have your image, it is easy to update it later by taking a new snapshot from a node running your image. Assuming you have swapped in an experiment with a node running your image and you have made changes to that node, use the DETERLab web interface to navigate to the descriptor page for your image: Use the Experimentation drop down menu, and choose List ImageIDs to see the entire list of Images you may access. Find your custom image and click on it. In the More Options menu, click on Snapshot Node ... Fill in the name of the node that is running your image, and click on Go . As in the above instructions, wait for the email saying your image has been updated before you try and use the image.","title":"Updating your Custom Image"},{"location":"core/dell-serial-console/","text":"Dell Serial Console \uf0c1 Most of the machines at DETERLab are Dell servers. When connected via the serial console, some special key sequences are available. Please refer to the node types page for more information on the types of machines available through DETERLab. KEY MAPPING FOR CONSOLE REDIRECTION: Use the <ESC><0> key sequence for <F10> Use the <ESC><@> key sequence for <F12> Use the <ESC><Ctrl><M> key sequence for <Ctrl><M> Use the <ESC><Ctrl><H> key sequence for <Ctrl><H> Use the <ESC><Ctrl><I> key sequence for <Ctrl><I> Use the <ESC><Ctrl><J> key sequence for <Ctrl><J> Use the <ESC><X><X> key sequence for <Alt><x>, where x is any letter key, and X is the upper case of that key Use the <ESC><R><ESC><r><ESC><R> key sequence for <Ctrl><Alt><Del>","title":"Dell Serial Console"},{"location":"core/dell-serial-console/#dell-serial-console","text":"Most of the machines at DETERLab are Dell servers. When connected via the serial console, some special key sequences are available. Please refer to the node types page for more information on the types of machines available through DETERLab. KEY MAPPING FOR CONSOLE REDIRECTION: Use the <ESC><0> key sequence for <F10> Use the <ESC><@> key sequence for <F12> Use the <ESC><Ctrl><M> key sequence for <Ctrl><M> Use the <ESC><Ctrl><H> key sequence for <Ctrl><H> Use the <ESC><Ctrl><I> key sequence for <Ctrl><I> Use the <ESC><Ctrl><J> key sequence for <Ctrl><J> Use the <ESC><X><X> key sequence for <Alt><x>, where x is any letter key, and X is the upper case of that key Use the <ESC><R><ESC><r><ESC><R> key sequence for <Ctrl><Alt><Del>","title":"Dell Serial Console"},{"location":"core/deterlab-commands/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. DETERLab Commands \uf0c1 The following commands are available from the commandline on users.deterlab.net . Note Commands should be pre-pended with the path: /usr/testbed/bin . For example, to start an experiment, you would use: /usr/testbed/bin/startexp [options] Transport Layers \uf0c1 The DETERLab XMLRPC server can be accessed via two different transport layers: SSH and SSL. How to use SSH keys You can edit your profile on DETERLab (from My DeterLab select Profile tab and then select Edit SSH keys ) to upload your public key. After that you will be able to SSH into DETERLab without supplying your password. How to use SSL You need to request a certificate from the DETERLab website in order to use the SSL based server. Click the ''My DETERLab'' menu item in the navbar, click the ''Profile'' tab on the page and then click on the ''Generate SSL Certificate'' link. Enter a passphrase to use to encrypt the private key. Once the key has been created, you will be given a link to download a text version (in PEM format). Simply provide this certificate as an input to your SSL client. Operational Commands \uf0c1 startexp : Start an DETERLab experiment \uf0c1 startexp [-q] [-i [-w]] [-f] [-N] [-E description] [-g gid] [-S reason] [-L reason] [-a <time>] [-l <time>] -p <pid> -e <eid> <nsfile> Options: -i Swapin immediately; by default, the experiment is batched. -w Wait for non-batchmode experiment to preload or swapin. -f Preload experiment (do not swapin or queue yet). -q Be less verbose. -S Experiment cannot be swapped; must provide reason. -L Experiment cannot be IDLE swapped; must provide reason. -a Auto swapout NN minutes after experiment is swapped in. -l Auto swapout NN minutes after experiment goes idle. -E A concise sentence describing your experiment. -g The subgroup in which to create the experiment. -p The project in which to create the experiment. -e The experiment name (unique, alphanumeric, no blanks). -N Suppress most email to the user and testbed-ops. nsfile NS file to parse for experiment batchexp : Synonym for startexp \uf0c1 This is a legacy command. See command description for startexp. endexp : Terminate an experiment \uf0c1 endexp [-w] [-N] -e pid,eid endexp [-w] [-N] pid eid Options: -w Wait for experiment to finish terminating. -e Project and Experiment ID. -N Suppress most email to the user and testbed-ops. Note Use with caution! This will tear down your experiment and you will not be able to swap it back in. By default, endexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent. The experiment may be terminated when it is currently swapped in ''or'' swapped out. delay_config : Change the link shaping characteristics for a link or LAN \uf0c1 delay_config [options] -e pid,eid link PARAM#value ... delay_config [options] pid eid link PARAM#value ... Options: -m Modify virtual experiment as well as current state. -s Select the source of the link to change. -e Project and Experiment ID to operate on. link Name of link from your NS file (ie: link1 ). Parameters: BANDWIDTH#NNN N#bandwidth (10-100000 Kbits per second) PLR#NNN N#lossrate (0 <# plr < 1) DELAY#NNN N#delay (one-way delay in milliseconds > 0) LIMIT#NNN The queue size in bytes or packets QUEUE-IN-BYTES#N 0 means in packets, 1 means in bytes RED/GRED Options: (only if link was specified as RED/GRED) MAXTHRESH#NNN Maximum threshold for the average Q size THRESH#NNN Minimum threshold for the average Q size LINTERM#NNN Packet dropping probability Q_WEIGHT#NNN For calculating the average queue size modexp : Modify experiment \uf0c1 modexp [-r] [-s] [-w] [-N] -e pid,eid nsfile modexp [-r] [-s] [-w] [-N] pid eid nsfile Options: -w Wait for experiment to finish swapping. -e Project and Experiment ID. -r Reboot nodes (when experiment is active). -s Restart event scheduler (when experiment is active). -N Suppress most email to the user and testbed-ops Note By default, modexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent. The experiment can be either swapped in ''or'' swapped out. If the experiment is swapped out, the new NS file replaces the existing NS file (the virtual topology is updated). If the experiment is swapped in (active), the physical topology is also updated, subject to the -r and -s options above. swapexp : Swap experiment in or out \uf0c1 swapexp -e pid,eid in|out swapexp pid eid in|out Options: -w Wait for experiment to finish swapping. -e Project and Experiment ID. -N Suppress most email to the user and testbed-ops. in Swap experiment in (must currently be swapped out). out Swap experiment out (must currently be swapped in) Note By default, swapexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent. create_image : Create a disk image from a node \uf0c1 create_image [options] imageid node Options: -w Wait for image to be created. -p Project ID of imageid. imageid Name of the image. node Node to create image from (pcXXX). Example: The following command creates or re-creates an image for a particular project: create_image -p <proj> <imageid> <node> eventsys_control : Start/Stop/Restart the event system \uf0c1 eventsys_control -e pid,eid start|stop|replay eventsys_control pid eid start|stop|replay Options: -e Project and Experiment ID. stop Stop the event scheduler. start Start the event stream from time index 0. replay Replay the event stream from time index 0 loghole : Downloads and manages an experiment's log files \uf0c1 This utility downloads log files from certain directories on the experimental nodes (e.g. /local/logs ) to the DETERLab users machine. After downloading, it can also be used to produce and manage archives of the log files. Using this utility to manage an experiment's log files is encouraged because it will transfer the logs in a network-friendly manner and is already integrated with the rest of DETERLab. For example, any programs executed using the DETERLab event-system will have their standard output/error automatically placed in the /local/logs directory. The tool can also be used to preserve multiple trials of an experiment by producing and managing zip archives of the logs. loghole [-hVdqva] [-e pid/eid] [-s server] [-P port] action [...] loghole sync [-nPs] [-r remotedir] [-l localdir] [node1 node2 ...] loghole validate loghole archive [-k (i-delete|space-is-needed)] [-a days] [-c comment] [-d] [archive-name] loghole change [-k (i-delete|space-is-needed)] [-a days] [-c comment] archive-name1 [archive-name2 ...] loghole list [-O1!Xo] [-m atmost] [-s megabytes] loghole show [archive-name] loghole clean [-fne] [node1 node2 ...] loghole gc [-n] [-m atmost] [-s megabytes] Options: -h, --help Print the usage message for the loghole utility as a whole or, if an action is given, the usage message for that action. -V, --version Print out version information and exit. -d, --debug Output debugging messages. -q, --quiet Decrease the level of verbosity, this is subtractive, so multiple uses of this option will make the utility quieter and quieter. The default level of verbosity is human-readable, below that is machine-readable, and below that is silent. For example, the default output from the \"list\" action looks like: [ ] foobar.1.zip 10/15 [!] foobar.0.zip 10/13 Using a single -q option changes the output to look like: foobar.1.zip foobar.0.zip -e, --experiment#PID/EID Specify the experiment(s) to operate on using the Project ID (or project name) and Experiment ID (or experiment name). If multiple -e options are given, the action will apply to all of them. This option overrides the default behavior of inferring the experiment from (Note: this sentence was cut off in the Emulab documentation). Examples: To synchronize the log directory for experiment neptune/myexp with the log holes on the experimental nodes: [vmars@users ~] loghole -e neptune/myexp sync To archive the newly recovered logs and print out just the name of the new log file: [vmars@users ~] loghole -e neptune/myexp -q archive More information: To see the detailed documentation of loghole , view the man page on users : loghole man os_load : Reload disks on selected nodes or all nodes in an experiment \uf0c1 os_load [options] node [node ...] os_load [options] -e pid,eid Options: -i Specify image name; otherwise load default image. -p Specify project for finding image name ( -i ). -s Do not wait for nodes to finish reloading. -m Specify internal image id (instead of -i and -p ). -r Do not reboot nodes; do that yourself. -e Reboot all nodes in an experiment. node Node to reboot (pcXXX). portstats : Get portstats from the switches \uf0c1 portstats <-p | pid eid> [vname ...] [vname:port ...] Options: -e Show only error counters. -a Show all stats. -z Zero out counts for selected counters after printing. -q Quiet: don't actually print counts - useful with -z . -c Print absolute, rather than relative, counts. -p The machines given are physical, not virtual, node IDs. No pid and eid should be given with this option. Warning If only the pid and eid are given, this command prints out information about all ports in the experiment. Otherwise, output is limited to the nodes and/or ports given. Note Statistics are reported from the switch's perspective. This means that ''In'' packets are those sent FROM the node, and ''Out'' packets are those sent TO the node. In the output, packets described as 'NUnicast' or 'NUcast' are non-unicast (broadcast or multicast) packets. node_reboot : Reboot selected nodes or all nodes in an experiment \uf0c1 Use this if you need to powercycle a node. node_reboot [options] node [node ...] node_reboot [options] -e pid,eid Options: -w Wait for nodes is come back up. -c Reconfigure nodes instead of rebooting. -f Power cycle nodes (skip reboot!) -e Reboot all nodes in an experiment. node Node to reboot (pcXXX). Note You may provide more than one node on the command line. Be aware that you may power cycle only nodes in projects that you are member of. node_reboot does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason, node_reboot may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above). You may reboot all the nodes in an experiment by using the -e option to specify the project and experiment names. This option is provided as a shorthand method for rebooting large groups of nodes. Example: The following command will reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project. node_reboot -e testbed,multicast expwait : Wait for experiment to reach a state \uf0c1 expwait [-t timeout] -e pid,eid state expwait [-t timeout] pid eid state Options: -e Project and Experiment ID in format <projectID>/<experimentID> . -t Maximum time to wait (in seconds). Informational Commands \uf0c1 node_list : Print physical mapping of nodes in an experiment \uf0c1 node_list [options] -e pid,eid Options: -e Project and Experiment ID to list. -p Print physical (DETER database) names (default). -P Like -p , but include node type. -v Print virtual (experiment assigned) names. -h Print physical name of host for virtual nodes. -c Print container VMs and physical nodes. Note This command now queries the XMLRPC interface as it used to do. Users who had been using script_wrapper.py node_list to access this function should use /usr/testbed/bin/node_list instead. The -c flag that outputs containerized node names has been modified in two ways. Names are produced without the DNS qualifiers as node names provided by other options of this command are. A node in a VM container named a will be reported as a not a.exp.proj as earlier versions of this feature did. This option now reports embedded_pnode containers as well (physical machines). If no containers VMs are present in an experiment, node_list -c and node_list -v produce identical output. The node_list command is now available as node_summary . expinfo : Get information about an experiment \uf0c1 expinfo [-n] [-m] [-l] [-d] [-a] -e pid,eid expinfo [-n] [-m] [-l] [-d] [-a] pid eid Options: -e Project and Experiment ID. -n Show node information. -m Show node mapping. -l Show link information. -a Show all of the above. node_avail : Print free node counts \uf0c1 node_avail [-p project] [-c class] [-t type] Options: -p project Specify project credentials for node types that are restricted. -c class The node class (Default: pc). -t type The node type. Example: The following command will print free nodes on pc850 nodes: $ node_avail -t pc850 node_avail_list : Print physical node_ids of available nodes \uf0c1 node_avail_list [-p project] [-c class] [-t type] [-n nodes] Options: -p project Specify project credentials for node types that are restricted. -c class The node class (Default: pc). -t type The node type. -n pcX,pcY,...,pcZ A list of physical node_ids. Example: The following command will print the physical node_ids for available pc850 nodes: $ node_avail_list -t pc850 nscheck : Check and NS file for parser errors \uf0c1 nscheck nsfile Option: nsfile Path to NS file you to wish check for parse errors.","title":"DETERLab commands"},{"location":"core/deterlab-commands/#deterlab-commands","text":"The following commands are available from the commandline on users.deterlab.net . Note Commands should be pre-pended with the path: /usr/testbed/bin . For example, to start an experiment, you would use: /usr/testbed/bin/startexp [options]","title":"DETERLab Commands"},{"location":"core/deterlab-commands/#transport-layers","text":"The DETERLab XMLRPC server can be accessed via two different transport layers: SSH and SSL. How to use SSH keys You can edit your profile on DETERLab (from My DeterLab select Profile tab and then select Edit SSH keys ) to upload your public key. After that you will be able to SSH into DETERLab without supplying your password. How to use SSL You need to request a certificate from the DETERLab website in order to use the SSL based server. Click the ''My DETERLab'' menu item in the navbar, click the ''Profile'' tab on the page and then click on the ''Generate SSL Certificate'' link. Enter a passphrase to use to encrypt the private key. Once the key has been created, you will be given a link to download a text version (in PEM format). Simply provide this certificate as an input to your SSL client.","title":"Transport Layers"},{"location":"core/deterlab-commands/#operational-commands","text":"","title":"Operational Commands"},{"location":"core/deterlab-commands/#startexp-start-an-deterlab-experiment","text":"startexp [-q] [-i [-w]] [-f] [-N] [-E description] [-g gid] [-S reason] [-L reason] [-a <time>] [-l <time>] -p <pid> -e <eid> <nsfile> Options: -i Swapin immediately; by default, the experiment is batched. -w Wait for non-batchmode experiment to preload or swapin. -f Preload experiment (do not swapin or queue yet). -q Be less verbose. -S Experiment cannot be swapped; must provide reason. -L Experiment cannot be IDLE swapped; must provide reason. -a Auto swapout NN minutes after experiment is swapped in. -l Auto swapout NN minutes after experiment goes idle. -E A concise sentence describing your experiment. -g The subgroup in which to create the experiment. -p The project in which to create the experiment. -e The experiment name (unique, alphanumeric, no blanks). -N Suppress most email to the user and testbed-ops. nsfile NS file to parse for experiment","title":"startexp: Start an DETERLab experiment "},{"location":"core/deterlab-commands/#batchexp-synonym-for-startexp","text":"This is a legacy command. See command description for startexp.","title":"batchexp: Synonym for startexp "},{"location":"core/deterlab-commands/#endexp-terminate-an-experiment","text":"endexp [-w] [-N] -e pid,eid endexp [-w] [-N] pid eid Options: -w Wait for experiment to finish terminating. -e Project and Experiment ID. -N Suppress most email to the user and testbed-ops. Note Use with caution! This will tear down your experiment and you will not be able to swap it back in. By default, endexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent. The experiment may be terminated when it is currently swapped in ''or'' swapped out.","title":"endexp: Terminate an experiment "},{"location":"core/deterlab-commands/#delay_config-change-the-link-shaping-characteristics-for-a-link-or-lan","text":"delay_config [options] -e pid,eid link PARAM#value ... delay_config [options] pid eid link PARAM#value ... Options: -m Modify virtual experiment as well as current state. -s Select the source of the link to change. -e Project and Experiment ID to operate on. link Name of link from your NS file (ie: link1 ). Parameters: BANDWIDTH#NNN N#bandwidth (10-100000 Kbits per second) PLR#NNN N#lossrate (0 <# plr < 1) DELAY#NNN N#delay (one-way delay in milliseconds > 0) LIMIT#NNN The queue size in bytes or packets QUEUE-IN-BYTES#N 0 means in packets, 1 means in bytes RED/GRED Options: (only if link was specified as RED/GRED) MAXTHRESH#NNN Maximum threshold for the average Q size THRESH#NNN Minimum threshold for the average Q size LINTERM#NNN Packet dropping probability Q_WEIGHT#NNN For calculating the average queue size","title":"delay_config: Change the link shaping characteristics for a link or LAN "},{"location":"core/deterlab-commands/#modexp-modify-experiment","text":"modexp [-r] [-s] [-w] [-N] -e pid,eid nsfile modexp [-r] [-s] [-w] [-N] pid eid nsfile Options: -w Wait for experiment to finish swapping. -e Project and Experiment ID. -r Reboot nodes (when experiment is active). -s Restart event scheduler (when experiment is active). -N Suppress most email to the user and testbed-ops Note By default, modexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent. The experiment can be either swapped in ''or'' swapped out. If the experiment is swapped out, the new NS file replaces the existing NS file (the virtual topology is updated). If the experiment is swapped in (active), the physical topology is also updated, subject to the -r and -s options above.","title":"modexp: Modify experiment "},{"location":"core/deterlab-commands/#swapexp-swap-experiment-in-or-out","text":"swapexp -e pid,eid in|out swapexp pid eid in|out Options: -w Wait for experiment to finish swapping. -e Project and Experiment ID. -N Suppress most email to the user and testbed-ops. in Swap experiment in (must currently be swapped out). out Swap experiment out (must currently be swapped in) Note By default, swapexp runs in the background, sending you email when the transition has completed. Use the -w option to wait in the foreground, returning exit status. Email is still sent.","title":"swapexp: Swap experiment in or out "},{"location":"core/deterlab-commands/#create_image-create-a-disk-image-from-a-node","text":"create_image [options] imageid node Options: -w Wait for image to be created. -p Project ID of imageid. imageid Name of the image. node Node to create image from (pcXXX). Example: The following command creates or re-creates an image for a particular project: create_image -p <proj> <imageid> <node>","title":"create_image: Create a disk image from a node "},{"location":"core/deterlab-commands/#eventsys_control-startstoprestart-the-event-system","text":"eventsys_control -e pid,eid start|stop|replay eventsys_control pid eid start|stop|replay Options: -e Project and Experiment ID. stop Stop the event scheduler. start Start the event stream from time index 0. replay Replay the event stream from time index 0","title":"eventsys_control: Start/Stop/Restart the event system "},{"location":"core/deterlab-commands/#loghole-downloads-and-manages-an-experiments-log-files","text":"This utility downloads log files from certain directories on the experimental nodes (e.g. /local/logs ) to the DETERLab users machine. After downloading, it can also be used to produce and manage archives of the log files. Using this utility to manage an experiment's log files is encouraged because it will transfer the logs in a network-friendly manner and is already integrated with the rest of DETERLab. For example, any programs executed using the DETERLab event-system will have their standard output/error automatically placed in the /local/logs directory. The tool can also be used to preserve multiple trials of an experiment by producing and managing zip archives of the logs. loghole [-hVdqva] [-e pid/eid] [-s server] [-P port] action [...] loghole sync [-nPs] [-r remotedir] [-l localdir] [node1 node2 ...] loghole validate loghole archive [-k (i-delete|space-is-needed)] [-a days] [-c comment] [-d] [archive-name] loghole change [-k (i-delete|space-is-needed)] [-a days] [-c comment] archive-name1 [archive-name2 ...] loghole list [-O1!Xo] [-m atmost] [-s megabytes] loghole show [archive-name] loghole clean [-fne] [node1 node2 ...] loghole gc [-n] [-m atmost] [-s megabytes] Options: -h, --help Print the usage message for the loghole utility as a whole or, if an action is given, the usage message for that action. -V, --version Print out version information and exit. -d, --debug Output debugging messages. -q, --quiet Decrease the level of verbosity, this is subtractive, so multiple uses of this option will make the utility quieter and quieter. The default level of verbosity is human-readable, below that is machine-readable, and below that is silent. For example, the default output from the \"list\" action looks like: [ ] foobar.1.zip 10/15 [!] foobar.0.zip 10/13 Using a single -q option changes the output to look like: foobar.1.zip foobar.0.zip -e, --experiment#PID/EID Specify the experiment(s) to operate on using the Project ID (or project name) and Experiment ID (or experiment name). If multiple -e options are given, the action will apply to all of them. This option overrides the default behavior of inferring the experiment from (Note: this sentence was cut off in the Emulab documentation). Examples: To synchronize the log directory for experiment neptune/myexp with the log holes on the experimental nodes: [vmars@users ~] loghole -e neptune/myexp sync To archive the newly recovered logs and print out just the name of the new log file: [vmars@users ~] loghole -e neptune/myexp -q archive More information: To see the detailed documentation of loghole , view the man page on users : loghole man","title":"loghole: Downloads and manages an experiment's log files "},{"location":"core/deterlab-commands/#os_load-reload-disks-on-selected-nodes-or-all-nodes-in-an-experiment","text":"os_load [options] node [node ...] os_load [options] -e pid,eid Options: -i Specify image name; otherwise load default image. -p Specify project for finding image name ( -i ). -s Do not wait for nodes to finish reloading. -m Specify internal image id (instead of -i and -p ). -r Do not reboot nodes; do that yourself. -e Reboot all nodes in an experiment. node Node to reboot (pcXXX).","title":"os_load: Reload disks on selected nodes or all nodes in an experiment "},{"location":"core/deterlab-commands/#portstats-get-portstats-from-the-switches","text":"portstats <-p | pid eid> [vname ...] [vname:port ...] Options: -e Show only error counters. -a Show all stats. -z Zero out counts for selected counters after printing. -q Quiet: don't actually print counts - useful with -z . -c Print absolute, rather than relative, counts. -p The machines given are physical, not virtual, node IDs. No pid and eid should be given with this option. Warning If only the pid and eid are given, this command prints out information about all ports in the experiment. Otherwise, output is limited to the nodes and/or ports given. Note Statistics are reported from the switch's perspective. This means that ''In'' packets are those sent FROM the node, and ''Out'' packets are those sent TO the node. In the output, packets described as 'NUnicast' or 'NUcast' are non-unicast (broadcast or multicast) packets.","title":"portstats: Get portstats from the switches "},{"location":"core/deterlab-commands/#node_reboot-reboot-selected-nodes-or-all-nodes-in-an-experiment","text":"Use this if you need to powercycle a node. node_reboot [options] node [node ...] node_reboot [options] -e pid,eid Options: -w Wait for nodes is come back up. -c Reconfigure nodes instead of rebooting. -f Power cycle nodes (skip reboot!) -e Reboot all nodes in an experiment. node Node to reboot (pcXXX). Note You may provide more than one node on the command line. Be aware that you may power cycle only nodes in projects that you are member of. node_reboot does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason, node_reboot may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above). You may reboot all the nodes in an experiment by using the -e option to specify the project and experiment names. This option is provided as a shorthand method for rebooting large groups of nodes. Example: The following command will reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project. node_reboot -e testbed,multicast","title":"node_reboot: Reboot selected nodes or all nodes in an experiment "},{"location":"core/deterlab-commands/#expwait-wait-for-experiment-to-reach-a-state","text":"expwait [-t timeout] -e pid,eid state expwait [-t timeout] pid eid state Options: -e Project and Experiment ID in format <projectID>/<experimentID> . -t Maximum time to wait (in seconds).","title":"expwait: Wait for experiment to reach a state "},{"location":"core/deterlab-commands/#informational-commands","text":"","title":"Informational Commands"},{"location":"core/deterlab-commands/#node_list-print-physical-mapping-of-nodes-in-an-experiment","text":"node_list [options] -e pid,eid Options: -e Project and Experiment ID to list. -p Print physical (DETER database) names (default). -P Like -p , but include node type. -v Print virtual (experiment assigned) names. -h Print physical name of host for virtual nodes. -c Print container VMs and physical nodes. Note This command now queries the XMLRPC interface as it used to do. Users who had been using script_wrapper.py node_list to access this function should use /usr/testbed/bin/node_list instead. The -c flag that outputs containerized node names has been modified in two ways. Names are produced without the DNS qualifiers as node names provided by other options of this command are. A node in a VM container named a will be reported as a not a.exp.proj as earlier versions of this feature did. This option now reports embedded_pnode containers as well (physical machines). If no containers VMs are present in an experiment, node_list -c and node_list -v produce identical output. The node_list command is now available as node_summary .","title":"node_list: Print physical mapping of nodes in an experiment "},{"location":"core/deterlab-commands/#expinfo-get-information-about-an-experiment","text":"expinfo [-n] [-m] [-l] [-d] [-a] -e pid,eid expinfo [-n] [-m] [-l] [-d] [-a] pid eid Options: -e Project and Experiment ID. -n Show node information. -m Show node mapping. -l Show link information. -a Show all of the above.","title":"expinfo: Get information about an experiment "},{"location":"core/deterlab-commands/#node_avail-print-free-node-counts","text":"node_avail [-p project] [-c class] [-t type] Options: -p project Specify project credentials for node types that are restricted. -c class The node class (Default: pc). -t type The node type. Example: The following command will print free nodes on pc850 nodes: $ node_avail -t pc850","title":"node_avail: Print free node counts "},{"location":"core/deterlab-commands/#node_avail_list-print-physical-node_ids-of-available-nodes","text":"node_avail_list [-p project] [-c class] [-t type] [-n nodes] Options: -p project Specify project credentials for node types that are restricted. -c class The node class (Default: pc). -t type The node type. -n pcX,pcY,...,pcZ A list of physical node_ids. Example: The following command will print the physical node_ids for available pc850 nodes: $ node_avail_list -t pc850","title":"node_avail_list: Print physical node_ids of available nodes "},{"location":"core/deterlab-commands/#nscheck-check-and-ns-file-for-parser-errors","text":"nscheck nsfile Option: nsfile Path to NS file you to wish check for parse errors.","title":"nscheck: Check and NS file for parser errors "},{"location":"core/faq/","text":"Frequently Asked Questions \uf0c1 How do I install software onto my node? \uf0c1 Each supported operating system has packages mirrored on a host called scratch and each operating system is configured to use this system to fetch packages from. Information for specific operating systems is documented there. How do I copy files onto my node? \uf0c1 Your home directory on users is automatically mounted via NFS on every node in your experiment. As are your project directory in /proj and a special filesystem called /share . I need root access! \uf0c1 If you need to customize the configuration, or perhaps reboot nodes, you can use the \"sudo\" command, located in /usr/local/bin on FreeBSD and /usr/bin Linux. All users are added to the Administrators group on Windows XP nodes. Our policy is very liberal; you can customize the configuration in any way you like, provided it does not interfere with the operation of the testbed. As as example, to reboot a node that is running FreeBSD: /usr/local/bin/sudo reboot Also, every testbed node has an automatically generated root password. Simply click on a node in the \"Reserved Nodes\" for your experiment and look at the root_password attribute. Can I access the nodes console? \uf0c1 Yes. Each of the PCs has its own serial console line connected to a serial server . You can connect to a nodes serial console through the users machine, using our console program located in '/usr/testbed/bin'. For example, to connect over serial line to \"pc001\" in your experiment, SSH into users.deterlab.net , and then type console pc001 at the Unix prompt. You may then interact with the serial console (hit \"enter\" to elicit output from the target machine). To exit the console program, type Ctrl-] ; it's just a telnet session. In any case, all console output from each node is saved so that you may look at it it later. For each node, the console log is stored as /var/log/tiplogs/pcXXX.run . This run file is created when nodes are first allocated to an experiment, and the Unix permissions of the run files permit only members of the project to view them. When the nodes are deallocated, the run files are cleared, so if you want to save them, you must do so before terminating the experiment. In addition, you can view the console logs from the web interface, on the Show Experiment page. Of course, you may not interact with the console, but you can at least view the current log. Escape codes for Dell serial consoles are documented here . My node is wedged! \uf0c1 Power cycling a node is easy since every node on the testbed is connected to a power controller. If you need to power cycle a node, log on to users.deterlab.net and use the \"node_reboot\" command: node_reboot <node> [node ... ] where node is the physical name, as listed in the node mapping table. You may provide more than one node on the command line. Be aware that you may power cycle only nodes in projects that you are member of. Also, node_reboot does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason, node_reboot may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above). You may also reboot all the nodes in an experiment by using the -e option to specify the project and experiment names. For example: node_reboot -e testbed,multicast will reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project. This option is provided as a shorthand method for rebooting large groups of nodes. I want to load a fresh operating system on my node \uf0c1 Scrogging your disk is certainly not as common, but it does happen. You can either swap your experiment out and then back in (which will allocate another group of nodes), or if you prefer you can reload the disk image yourself. You will of course lose anything you have stored on that disk; it is a good idea to store only data that can be easily recreated, or else store it in your project directory in /proj . Reloading your disk with a fresh copy of an image is easy, and requires no intervention by DETER staff: os_load [-i ImageName] [-p Project] <node> [node ... ] If you do not specify an image name, the default image for that node type will be loaded (typically Ubuntu1604-STD). For testbed wide images, you do not have to specify a project. The os_load command will wait (not exit) until the nodes have been reloaded, so that you do not need to check the console lines of each node to determine when the load is done. For example, to load the image 'testpc167' which is in the project 'DeterTest' onto pc167, we type: users > os_load -i testpc167 -p DeterTest pc167 osload (pc167): Changing default OS to [OS 998: DeterTest,testpc167] osload: Updating image signature. Setting up reload for pc167 (mode: Frisbee) osload: Issuing reboot for pc167 and then waiting ... reboot (pc167): Attempting to reboot ... reboot (pc167): Successful! reboot: Done. There were 0 failures. reboot (pc167): child returned 0 status. osload (pc167): still waiting; it has been 1 minute(s) osload (pc167): still waiting; it has been 2 minute(s) osload (pc167): still waiting; it has been 3 minute(s) osload (pc167): still waiting; it has been 4 minute(s) osload: Done! There were 0 failures. users > I only want certain types of nodes for my experiment \uf0c1 The NS command tb-set-hardware only lets you pick one type of hardware. If you are fine with a couple of different kinds of hardware, say you just want nodes that are at ISI part of the testbed, you can define a virtual node type in your NS file. For more information on virtual types, please refer to the Virtual Type Commands section of the NS command reference. Here is a quick example: tb-make-soft-vtype ISI {pc2133 pc3000 pc3060 pc3100} tb-make-soft-vtype UCB {bpc2133 bpc3000 bpc3060} tb-set-hardware $atISI ISI tb-set-hardware $atUCB UCB","title":"Frequently Asked Questions"},{"location":"core/faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"core/faq/#how-do-i-install-software-onto-my-node","text":"Each supported operating system has packages mirrored on a host called scratch and each operating system is configured to use this system to fetch packages from. Information for specific operating systems is documented there.","title":"How do I install software onto my node?"},{"location":"core/faq/#how-do-i-copy-files-onto-my-node","text":"Your home directory on users is automatically mounted via NFS on every node in your experiment. As are your project directory in /proj and a special filesystem called /share .","title":"How do I copy files onto my node?"},{"location":"core/faq/#i-need-root-access","text":"If you need to customize the configuration, or perhaps reboot nodes, you can use the \"sudo\" command, located in /usr/local/bin on FreeBSD and /usr/bin Linux. All users are added to the Administrators group on Windows XP nodes. Our policy is very liberal; you can customize the configuration in any way you like, provided it does not interfere with the operation of the testbed. As as example, to reboot a node that is running FreeBSD: /usr/local/bin/sudo reboot Also, every testbed node has an automatically generated root password. Simply click on a node in the \"Reserved Nodes\" for your experiment and look at the root_password attribute.","title":"I need root access!"},{"location":"core/faq/#can-i-access-the-nodes-console","text":"Yes. Each of the PCs has its own serial console line connected to a serial server . You can connect to a nodes serial console through the users machine, using our console program located in '/usr/testbed/bin'. For example, to connect over serial line to \"pc001\" in your experiment, SSH into users.deterlab.net , and then type console pc001 at the Unix prompt. You may then interact with the serial console (hit \"enter\" to elicit output from the target machine). To exit the console program, type Ctrl-] ; it's just a telnet session. In any case, all console output from each node is saved so that you may look at it it later. For each node, the console log is stored as /var/log/tiplogs/pcXXX.run . This run file is created when nodes are first allocated to an experiment, and the Unix permissions of the run files permit only members of the project to view them. When the nodes are deallocated, the run files are cleared, so if you want to save them, you must do so before terminating the experiment. In addition, you can view the console logs from the web interface, on the Show Experiment page. Of course, you may not interact with the console, but you can at least view the current log. Escape codes for Dell serial consoles are documented here .","title":"Can I access the nodes console?"},{"location":"core/faq/#my-node-is-wedged","text":"Power cycling a node is easy since every node on the testbed is connected to a power controller. If you need to power cycle a node, log on to users.deterlab.net and use the \"node_reboot\" command: node_reboot <node> [node ... ] where node is the physical name, as listed in the node mapping table. You may provide more than one node on the command line. Be aware that you may power cycle only nodes in projects that you are member of. Also, node_reboot does its very best to perform a clean reboot before resorting to cycling the power to the node. This is to prevent the damage that can occur from constant power cycling over a long period of time. For this reason, node_reboot may delay a minute or two if it detects that the machine is still responsive to network transmission. In any event, please try to reboot your nodes first (see above). You may also reboot all the nodes in an experiment by using the -e option to specify the project and experiment names. For example: node_reboot -e testbed,multicast will reboot all of the nodes reserved in the \"multicast\" experiment in the \"testbed\" project. This option is provided as a shorthand method for rebooting large groups of nodes.","title":"My node is wedged!"},{"location":"core/faq/#i-want-to-load-a-fresh-operating-system-on-my-node","text":"Scrogging your disk is certainly not as common, but it does happen. You can either swap your experiment out and then back in (which will allocate another group of nodes), or if you prefer you can reload the disk image yourself. You will of course lose anything you have stored on that disk; it is a good idea to store only data that can be easily recreated, or else store it in your project directory in /proj . Reloading your disk with a fresh copy of an image is easy, and requires no intervention by DETER staff: os_load [-i ImageName] [-p Project] <node> [node ... ] If you do not specify an image name, the default image for that node type will be loaded (typically Ubuntu1604-STD). For testbed wide images, you do not have to specify a project. The os_load command will wait (not exit) until the nodes have been reloaded, so that you do not need to check the console lines of each node to determine when the load is done. For example, to load the image 'testpc167' which is in the project 'DeterTest' onto pc167, we type: users > os_load -i testpc167 -p DeterTest pc167 osload (pc167): Changing default OS to [OS 998: DeterTest,testpc167] osload: Updating image signature. Setting up reload for pc167 (mode: Frisbee) osload: Issuing reboot for pc167 and then waiting ... reboot (pc167): Attempting to reboot ... reboot (pc167): Successful! reboot: Done. There were 0 failures. reboot (pc167): child returned 0 status. osload (pc167): still waiting; it has been 1 minute(s) osload (pc167): still waiting; it has been 2 minute(s) osload (pc167): still waiting; it has been 3 minute(s) osload (pc167): still waiting; it has been 4 minute(s) osload: Done! There were 0 failures. users >","title":"I want to load a fresh operating system on my node"},{"location":"core/faq/#i-only-want-certain-types-of-nodes-for-my-experiment","text":"The NS command tb-set-hardware only lets you pick one type of hardware. If you are fine with a couple of different kinds of hardware, say you just want nodes that are at ISI part of the testbed, you can define a virtual node type in your NS file. For more information on virtual types, please refer to the Virtual Type Commands section of the NS command reference. Here is a quick example: tb-make-soft-vtype ISI {pc2133 pc3000 pc3060 pc3100} tb-make-soft-vtype UCB {bpc2133 bpc3000 bpc3060} tb-set-hardware $atISI ISI tb-set-hardware $atUCB UCB","title":"I only want certain types of nodes for my experiment"},{"location":"core/generating-traffic/","text":"Generating Traffic with LegoTG \uf0c1 LegoTG is a flexible framework for pulling together the appropriate software for the traffic generation process. The key insight of LegoTG is that the definition of \"realism\" in traffic generation is entirely dependent on the experiment/scenario. LegoTG itself does not determine what \"realistic\" traffic means for a particular experiment or scenario. Rather, the definition of realism must come from the LegoTG user. LegoTG enables a modular and composable approach to the traffic generation process. LegoTG's Orchestrator handles tying together various modules which handle the different aspects of generation and allows a user to create a plug-and-play traffic generator. We are developing software to perform data extraction as well as working on a generator which will be generic enough to handle a variety of modeled dimensions. In the LegoTG framework, each traffic generation functionality is realized through a separate piece of code, called a TGblock. The framework works like a child\u2019s building block set: TGblocks combine in different ways to achieve customizable and composable traffic generation. This combination and customization is achieved through LegoTG\u2019s Orchestrator, which sets up, configures, deploys, runs, synchronizes and stops TGblocks in distributed experiments. The entire specification of the traffic generation process for an experiment is in an experiment configuration file\u2014called an ExFile, which is an input for the Orchestrator. The ExFile offers a convenient capture of all the details of an experiment\u2019s background traffic set up, which promotes sharing and reproducibility of experiments. For more information and to download software, please see the LegoTG project page.","title":"Generating Traffic with LegoTG"},{"location":"core/generating-traffic/#generating-traffic-with-legotg","text":"LegoTG is a flexible framework for pulling together the appropriate software for the traffic generation process. The key insight of LegoTG is that the definition of \"realism\" in traffic generation is entirely dependent on the experiment/scenario. LegoTG itself does not determine what \"realistic\" traffic means for a particular experiment or scenario. Rather, the definition of realism must come from the LegoTG user. LegoTG enables a modular and composable approach to the traffic generation process. LegoTG's Orchestrator handles tying together various modules which handle the different aspects of generation and allows a user to create a plug-and-play traffic generator. We are developing software to perform data extraction as well as working on a generator which will be generic enough to handle a variety of modeled dimensions. In the LegoTG framework, each traffic generation functionality is realized through a separate piece of code, called a TGblock. The framework works like a child\u2019s building block set: TGblocks combine in different ways to achieve customizable and composable traffic generation. This combination and customization is achieved through LegoTG\u2019s Orchestrator, which sets up, configures, deploys, runs, synchronizes and stops TGblocks in distributed experiments. The entire specification of the traffic generation process for an experiment is in an experiment configuration file\u2014called an ExFile, which is an input for the Orchestrator. The ExFile offers a convenient capture of all the details of an experiment\u2019s background traffic set up, which promotes sharing and reproducibility of experiments. For more information and to download software, please see the LegoTG project page.","title":"Generating Traffic with LegoTG"},{"location":"core/hostnames/","text":"Hostnames for your nodes \uf0c1 We set up names for your nodes in DNS and /etc/hosts files for use on the nodes in the experiment. Since our nodes have multiple interfaces (the control network, and, depending on the experiment, possibly several experimental interfaces) determining which name refers to which interface can be somewhat confusing. The rules below should help you figure this out. From users.deterlab.net - We set up names in the form of node.expt.proj in DNS. This name always refers to the node's control network interface, which is the only one reachable from users.deterlab.net . On the nodes themselves - There are three basic ways to refer to the interfaces of a node. The first is stored in DNS, and the second two are stored on the node in the /etc/hosts file. Short form - Within your experiment you should use just the node name (e..g, nodeA ) to refer to the nodes. This ensures that traffic between nodes in your experiment goes over the links you created Node-link form - You can refer to an individual experimental interface by suffixing it with the name of the link or LAN (as defined in your NS file) that it is a member of. For example, nodeA-link0 or server-serverLAN . This is the preferred way to refer to experimental interfaces, since it uniquely and unambiguously identifies an interface. Fully-qualified hostnames - These names are the same ones that use use on users.deterlab.net and look like node.expt.proj . This name always refers to the control network interface. Note It is a bad idea to pick virtual node names in your topology that clash with the physical node names in the testbed, such as \"pc45\".","title":"Hostnames for your nodes"},{"location":"core/hostnames/#hostnames-for-your-nodes","text":"We set up names for your nodes in DNS and /etc/hosts files for use on the nodes in the experiment. Since our nodes have multiple interfaces (the control network, and, depending on the experiment, possibly several experimental interfaces) determining which name refers to which interface can be somewhat confusing. The rules below should help you figure this out. From users.deterlab.net - We set up names in the form of node.expt.proj in DNS. This name always refers to the node's control network interface, which is the only one reachable from users.deterlab.net . On the nodes themselves - There are three basic ways to refer to the interfaces of a node. The first is stored in DNS, and the second two are stored on the node in the /etc/hosts file. Short form - Within your experiment you should use just the node name (e..g, nodeA ) to refer to the nodes. This ensures that traffic between nodes in your experiment goes over the links you created Node-link form - You can refer to an individual experimental interface by suffixing it with the name of the link or LAN (as defined in your NS file) that it is a member of. For example, nodeA-link0 or server-serverLAN . This is the preferred way to refer to experimental interfaces, since it uniquely and unambiguously identifies an interface. Fully-qualified hostnames - These names are the same ones that use use on users.deterlab.net and look like node.expt.proj . This name always refers to the control network interface. Note It is a bad idea to pick virtual node names in your topology that clash with the physical node names in the testbed, such as \"pc45\".","title":"Hostnames for your nodes"},{"location":"core/interact/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Interact With Your Experiment \uf0c1 When interacting with your experiment you may want to access your nodes , transfer files , generate traffic , collect statistics , or modify your experiment . Sections below provide more information about each of these needs. Modes of use \uf0c1 There are several ways in which you can use your nodes: Manual : As you start developing your experiment, you may want to SSH to your experiment and configure nodes or generate traffic manually. The rest of this document talks about manual use of your nodes. Automated : As your work progresses you may want to develop scripts (e.g., using Bash or Python or MAGI or Ansible) to automate running of your experiments. Please see our documentation on automating your experiments . Experiment Realization \uf0c1 Imagine an experiment with three nodes A, B, and C where A is connected to B and B to C. Figure below shows the actual connections within the testbed. Each node is connected to two switches - a control switch, which is used to reach into the node from the Internet, and an experimental switch, which is used to create the physical connections between nodes in the same experiment. To connect A to B and B to C, some network interfaces (NICs) connecting to the experimental switch will be configured (see yellow dots denoting these NICs). When a node becomes part of an experiment (in a given project), it gets at least two names : a short name like A , and a long name like A.test.Share . The long name is formed by putting together node name, experiment name and project name, separated by dots. In the figure above you can see that the short name is the name that relates to the node's connections to the experimental switch, and the long name relates to connections to the control switch. Imagine that you have opened a terminal window connected to node A. If you type ping C.test.Share this traffic will reach the node following the black dotted line on the left, going through the control switch. If you type ping C this traffic will reach the node following the black dotted line on the right, going through the experimental switch. More importantly, this traffic will go from A to B and then to C, just like you specified in your topology -- the only way to reach C from A is to go over B. Access your nodes \uf0c1 You can access your expreriment through use of SSH and terminal. Please see our advanced guide to SSH for some advanced topics. This section provides a basic reference. First SSH into users.deterlab.net using your DETERLab username and password. See the figure above (red dashed lines). Since DETERLab nodes are not directly connected to the Internet you need to SSH into them by first going through users and then from users into your experiment. Once you log in to users , you'll need to SSH again to your actual experimental nodes using long node names, which look like this: nodeName.experimentName.projectName . You will not need to re-authenticate. For example, imagine that you have swapped in test experiment under project Share and that your username is YourUsername . Imagine that you want to access node A . You would type the following in your terminal window on your device: ssh YourUsername@users.deterlab.net ssh A.test.Share If you have just started your experiment you may need to wait a few minutes before you can successfully SSH into experimental nodes, because nodes may not be fully set up. If you happen to terminate the SSH process on your node, or if you set up a firewall rule that blocks SSH traffic, another way to access your node is via serial console . Shared Filesystem \uf0c1 On DETERLab there are two folders that are mounted over the NFS (network file system) on each node. This includes users.deterlab.net and any experimental node that a user allocates. One folder is /users/YourUsername or your home folder. This is why you can create a file on one experimental node in your home folder and it becomes visible in another experimental node in that same folder . The second folder is /users/YourProject where \"YourProject\" is the project that you are a member of, and that you created your experiment in. That second folder is accessible not only by the user but by all users that are members of that same project (e.g., all students in a class). Transfer Files \uf0c1 You can transfer files between your device and your DETERLab experiment by using the scp command from the command line. The command has colon (:) and dot (.) characters in certain places. Make sure to type them as is shown in the examples. Our guide to SSH provides more details. Note Home folders on DETERLab are mounted remotely. This means that you can place a file in the home folder on one experimental node or on users.deterlab.net and it will be there when you log into a different node. Assume that you want to transfer file \"myfile.txt\" between your device and one of your experimental nodes (e.g., A.test.Share ). From your device to DETERLab home directory \uf0c1 On your device open a terminal window, navigate to the folder with the file \"myfile.txt\" and type the commands below: scp myfile.txt YourUsername@users.deterlab.net:. This will place the file into your home folder on DETERLab. This folder is your home folder on any experimental machine as well (see Behind the scenes ), and myfile.txt will be on node A.test.Share as well. From DETERLab home directory to your device \uf0c1 Assume that you want to transfer file \"myfile.txt\" from your home folder on DETERLab to your device. On your device open a terminal window, navigate to the folder where you want to store the file and type the commands below: scp YourUsername@users.deterlab.net:myfile.txt . From your device to DETERLab local folder on experimental node \uf0c1 Assume that you want to transfer file \"myfile.txt\" from your device to folder \"/tmp\" on node A.test.Share. You first have to move the file to your DETERLab home folder and then move it to the local node folder. On your device open a terminal window, navigate to the folder where you want to store the file and type the commands below: scp YourUsername@users.deterlab.net:myfile.txt . SSH to users.deterlab.net and then to A.test.Share and then type: cp ~/myfile.txt /tmp/myfile.txt From DETERLab local folder on experimental node to your device \uf0c1 Assume that you want to transfer file \"myfile.txt\" from folder \"/tmp\" on node A.test.Share to your device. Since this file is not in your home directory you have to move it there first. SSH to users.deterlab.net and then to A.test.Share and then type: cp /tmp/myfile.txt ~/myfile.txt Then on your device open a terminal window, navigate to the folder where you want to store the file and type the commands below: scp YourUsername@users.deterlab.net:myfile.txt . Mount More Disk Space \uf0c1 DETERLab nodes come with a modest amount of disk space. You can mount more disk space by typing the following instructions on your experimental node: sudo mkdir /mnt/local sudo /usr/local/etc/emulab/mkextrafs /mnt/local user=`whoami` sudo chown $user /mnt/local Install Software \uf0c1 To contain malicious traffic within experiments, DETERLab has no connectivity to the outside world. To install packages please use our mirrors of popular content: Install Ubuntu packages by typing sudo apt install [package name] Install Python packages by typing pip install -i https://fbsd-build.isi.deterlab.net/pypi/web/simple [package name] Install software from source by: (1) downloading the software into your home directory by typing on users.deterlab.net the following wget [software URL] , then (2) in your experiment access the software you downloaded in your home directory, extract it and follow install instructions If you have any other installation needs that are not met with the above instructions please file a ticket. Generate Traffic \uf0c1 You can generate traffic in your experiment in many ways. At first, as you are designing your experiment, you may want generate traffic manually and interactively . Later, you may want to write scripts for traffic generation. You may also want to emulate human actions in your experiments. Collect Statistics \uf0c1 For any statistics you collect, please store them locally on the experimental nodes, and not in your home directory. This will minimize traffic on the shared testbed infrastructure. If you are testing some custom system you wrote, you can log events locally, e.g., in the /tmp directory of the experimental nodes. You may also want to collect traffic traces, e.g., by using tcpdump . Please store them locally on the nodes as well. If you need more space to store your data please see our extended storage options . Modify Experiment \uf0c1 You can modify your experiment's topology by clicking on the experiment's name in \"My Deterlab\" view and then clicking on \"Modify Experiment\" on the left menu. This is best done when the experiment is not currently swapped in. You can also modify the experiment's settings such as idle swap and maximum duration if you click on the \"Modify Settings\" on the left menu.","title":"Use your nodes"},{"location":"core/interact/#interact-with-your-experiment","text":"When interacting with your experiment you may want to access your nodes , transfer files , generate traffic , collect statistics , or modify your experiment . Sections below provide more information about each of these needs.","title":"Interact With Your Experiment"},{"location":"core/interact/#modes-of-use","text":"There are several ways in which you can use your nodes: Manual : As you start developing your experiment, you may want to SSH to your experiment and configure nodes or generate traffic manually. The rest of this document talks about manual use of your nodes. Automated : As your work progresses you may want to develop scripts (e.g., using Bash or Python or MAGI or Ansible) to automate running of your experiments. Please see our documentation on automating your experiments .","title":"Modes of use"},{"location":"core/interact/#experiment-realization","text":"Imagine an experiment with three nodes A, B, and C where A is connected to B and B to C. Figure below shows the actual connections within the testbed. Each node is connected to two switches - a control switch, which is used to reach into the node from the Internet, and an experimental switch, which is used to create the physical connections between nodes in the same experiment. To connect A to B and B to C, some network interfaces (NICs) connecting to the experimental switch will be configured (see yellow dots denoting these NICs). When a node becomes part of an experiment (in a given project), it gets at least two names : a short name like A , and a long name like A.test.Share . The long name is formed by putting together node name, experiment name and project name, separated by dots. In the figure above you can see that the short name is the name that relates to the node's connections to the experimental switch, and the long name relates to connections to the control switch. Imagine that you have opened a terminal window connected to node A. If you type ping C.test.Share this traffic will reach the node following the black dotted line on the left, going through the control switch. If you type ping C this traffic will reach the node following the black dotted line on the right, going through the experimental switch. More importantly, this traffic will go from A to B and then to C, just like you specified in your topology -- the only way to reach C from A is to go over B.","title":"Experiment Realization"},{"location":"core/interact/#access-your-nodes","text":"You can access your expreriment through use of SSH and terminal. Please see our advanced guide to SSH for some advanced topics. This section provides a basic reference. First SSH into users.deterlab.net using your DETERLab username and password. See the figure above (red dashed lines). Since DETERLab nodes are not directly connected to the Internet you need to SSH into them by first going through users and then from users into your experiment. Once you log in to users , you'll need to SSH again to your actual experimental nodes using long node names, which look like this: nodeName.experimentName.projectName . You will not need to re-authenticate. For example, imagine that you have swapped in test experiment under project Share and that your username is YourUsername . Imagine that you want to access node A . You would type the following in your terminal window on your device: ssh YourUsername@users.deterlab.net ssh A.test.Share If you have just started your experiment you may need to wait a few minutes before you can successfully SSH into experimental nodes, because nodes may not be fully set up. If you happen to terminate the SSH process on your node, or if you set up a firewall rule that blocks SSH traffic, another way to access your node is via serial console .","title":"Access your nodes"},{"location":"core/interact/#shared-filesystem","text":"On DETERLab there are two folders that are mounted over the NFS (network file system) on each node. This includes users.deterlab.net and any experimental node that a user allocates. One folder is /users/YourUsername or your home folder. This is why you can create a file on one experimental node in your home folder and it becomes visible in another experimental node in that same folder . The second folder is /users/YourProject where \"YourProject\" is the project that you are a member of, and that you created your experiment in. That second folder is accessible not only by the user but by all users that are members of that same project (e.g., all students in a class).","title":"Shared Filesystem"},{"location":"core/interact/#transfer-files","text":"You can transfer files between your device and your DETERLab experiment by using the scp command from the command line. The command has colon (:) and dot (.) characters in certain places. Make sure to type them as is shown in the examples. Our guide to SSH provides more details. Note Home folders on DETERLab are mounted remotely. This means that you can place a file in the home folder on one experimental node or on users.deterlab.net and it will be there when you log into a different node. Assume that you want to transfer file \"myfile.txt\" between your device and one of your experimental nodes (e.g., A.test.Share ).","title":"Transfer Files"},{"location":"core/interact/#from-your-device-to-deterlab-home-directory","text":"On your device open a terminal window, navigate to the folder with the file \"myfile.txt\" and type the commands below: scp myfile.txt YourUsername@users.deterlab.net:. This will place the file into your home folder on DETERLab. This folder is your home folder on any experimental machine as well (see Behind the scenes ), and myfile.txt will be on node A.test.Share as well.","title":"From your device to DETERLab home directory"},{"location":"core/interact/#from-deterlab-home-directory-to-your-device","text":"Assume that you want to transfer file \"myfile.txt\" from your home folder on DETERLab to your device. On your device open a terminal window, navigate to the folder where you want to store the file and type the commands below: scp YourUsername@users.deterlab.net:myfile.txt .","title":"From DETERLab home directory to your device"},{"location":"core/interact/#from-your-device-to-deterlab-local-folder-on-experimental-node","text":"Assume that you want to transfer file \"myfile.txt\" from your device to folder \"/tmp\" on node A.test.Share. You first have to move the file to your DETERLab home folder and then move it to the local node folder. On your device open a terminal window, navigate to the folder where you want to store the file and type the commands below: scp YourUsername@users.deterlab.net:myfile.txt . SSH to users.deterlab.net and then to A.test.Share and then type: cp ~/myfile.txt /tmp/myfile.txt","title":"From your device to DETERLab local folder on experimental node"},{"location":"core/interact/#from-deterlab-local-folder-on-experimental-node-to-your-device","text":"Assume that you want to transfer file \"myfile.txt\" from folder \"/tmp\" on node A.test.Share to your device. Since this file is not in your home directory you have to move it there first. SSH to users.deterlab.net and then to A.test.Share and then type: cp /tmp/myfile.txt ~/myfile.txt Then on your device open a terminal window, navigate to the folder where you want to store the file and type the commands below: scp YourUsername@users.deterlab.net:myfile.txt .","title":"From DETERLab local folder on experimental node to your device"},{"location":"core/interact/#mount-more-disk-space","text":"DETERLab nodes come with a modest amount of disk space. You can mount more disk space by typing the following instructions on your experimental node: sudo mkdir /mnt/local sudo /usr/local/etc/emulab/mkextrafs /mnt/local user=`whoami` sudo chown $user /mnt/local","title":"Mount More Disk Space"},{"location":"core/interact/#install-software","text":"To contain malicious traffic within experiments, DETERLab has no connectivity to the outside world. To install packages please use our mirrors of popular content: Install Ubuntu packages by typing sudo apt install [package name] Install Python packages by typing pip install -i https://fbsd-build.isi.deterlab.net/pypi/web/simple [package name] Install software from source by: (1) downloading the software into your home directory by typing on users.deterlab.net the following wget [software URL] , then (2) in your experiment access the software you downloaded in your home directory, extract it and follow install instructions If you have any other installation needs that are not met with the above instructions please file a ticket.","title":"Install Software"},{"location":"core/interact/#generate-traffic","text":"You can generate traffic in your experiment in many ways. At first, as you are designing your experiment, you may want generate traffic manually and interactively . Later, you may want to write scripts for traffic generation. You may also want to emulate human actions in your experiments.","title":"Generate Traffic"},{"location":"core/interact/#collect-statistics","text":"For any statistics you collect, please store them locally on the experimental nodes, and not in your home directory. This will minimize traffic on the shared testbed infrastructure. If you are testing some custom system you wrote, you can log events locally, e.g., in the /tmp directory of the experimental nodes. You may also want to collect traffic traces, e.g., by using tcpdump . Please store them locally on the nodes as well. If you need more space to store your data please see our extended storage options .","title":"Collect Statistics"},{"location":"core/interact/#modify-experiment","text":"You can modify your experiment's topology by clicking on the experiment's name in \"My Deterlab\" view and then clicking on \"Modify Experiment\" on the left menu. This is best done when the experiment is not currently swapped in. You can also modify the experiment's settings such as idle swap and maximum duration if you click on the \"Modify Settings\" on the left menu.","title":"Modify Experiment"},{"location":"core/legacy-tools/","text":"Legacy Tools \uf0c1 This page includes links to tools that have been useful to DETER users in the past. There is no guarantee that they will perform with current DETER software and they are listed for legacy purposes. Benchmarks \uf0c1 DDoS Defense Benchmarks \uf0c1 Developed and maintained by University of Delaware, this tool contains: A benchmark suite with a set of scenarios to be used for defense evaluation, integrated with SEER, A set of performance metrics that characterize an attack's impact and a defense's performance, and A set of tools used for benchmark development, integration of benchmarks with the DETER testbed and calculation of performance metrics from tcpdump traces collected during DDoS experimentation. Website : http://www.isi.edu/~mirkovic/bench Runs on : Any platform Best for : Testing DDoS defenses For questions, contact : Jelena Mirkovic at ISI Legitimate Traffic Generators \uf0c1 SEER \uf0c1 The Security Experimentation EnviRonment (SEER), developed by SPARTA, Inc., is a GUI-based user interface to DETERLab, helping an experimenter to set up, script, and perform experiments in the DETER environment. The SEER back-end includes tools to generate legitimate traffic using Harpoon or custom-made Web, DNS, Ping, IRC, FTP and VoIP agents. Note that this tool is no longer supported and is offered as-is. Website : http://seer.deterlab.net/trac Runs on : All platforms, written in Java Best for : Legitimate traffic generation, DoS traffic generation, visualization of traffic levels in topology Tcpreplay \uf0c1 Tcpreplay is a suite of BSD licensed tools, which gives you the ability to inject previously captured traffic in libpcap format to test a variety of network devices. It allows you to classify traffic as client or server, rewrite Layer 2, 3 and 4 headers and finally replay the traffic back onto the network and through other devices such as switches, routers, firewalls, NIDS and IPS's. Tcpreplay supports both single and dual NIC modes for testing both sniffing and inline devices. Website : http://tcpreplay.synfin.net/trac/ Runs on : UNIX-flavored OSes and Win32 with Cygwin Best for : Replaying traces to regenerate same or similar traffic For questions, contact : Tcpreplay support Webstone \uf0c1 Webstone, a benchmark owned by Mindcraft Inc., measures performance of web server software and hardware products. Webstone consists of a program called the webmaster which can be installed on a client in the network or on a separate computer. The webmaster distributes web client software as well as configuration files for testing to the client computers, that contact the web server to retrieve web pages or files in order to test web server performance. Webstone also tests operating system software, CPU and network speeds. While it was developed with the idea of measuring the performance of web servers, it can be used to generate background traffic in a network as the multiple clients keep contacting the server over a period of time thereby simulating web traffic in the network. Website : http://www.mindcraft.com/webstone/ Runs on : UNIX-flavored OSes and Windows NT Best for : Web traffic generation Harpoon \uf0c1 Harpoon, developed at University of Wisconsin, is a flow-level traffic generator. It uses a set of distributional parameters that can be automatically extracted from Netflow traces to generate flows that exhibit the same statistical qualities present in measured Internet traces, including temporal and spatial characteristics. Harpoon can be used to generate representative background traffic for application or protocol testing, or for testing network switching hardware. Note, however, that while traffic dynamics will resemble the one found in traces, Harpoon traffic runs over HTTP and application behavior may be different from the real one. Website : https://github.com/jsommers/harpoon Runs on : UNIX-flavored OSes Best for : Generating traffic from traces or from high-level specifications. DoS and DDoS Attack Traffic Generators \uf0c1 SEER \uf0c1 ( See above ) SEER generates attack traffic using the Flooder tool, developed by SPARTA, and the Cleo tool developed by UCLA. Look at SEER's Web page for a more detailed description of these tools. The following collection of real DDoS tools has little new to offer with regard to attack traffic generation, when compared to SEER's capabilities. In general, SEER can generate same traffic variations as this tools, and is easier to control and customize. If, however, you are testing a defense that looks at control traffic of DoS networks these tools may be useful to you. They are all downloadable from third-party Web sites and are not maintained. Stacheldraht \uf0c1 Stacheldraht combines features of Trinoo and TFN tools and adds encrypted communication between the attacker and the masters. Stacheldraht uses TCP for encrypted communication between the attacker and the masters, and TCP or ICMP for communication between master and agents. Another added feature is the ability to perform automatic updates of agent code. Available attacks are UDP flood, TCP SYN flood, ICMP ECHO flood and Smurf attacks. Website : http://packetstormsecurity.org/distributed/stachel.tgz Mstream \uf0c1 Mstream generates a flood of TCP packets with the ACK bit set. Masters can be controlled remotely by one or more attackers using a password- protected interactive login. The communications between attacker and masters, and a master and agents, are configurable at compile time and have varied signif- icantly from incident to incident. Source addresses in attack packets are spoofed at random. The TCP ACK attack exhausts network resources and will likely cause a TCP RST to be sent to the spoofed source address (potentially also creating outgoing bandwidth consumption at the victim). Website : http://packetstormsecurity.org/distributed/mstream.txt Topology Generators and Convertors \uf0c1 Rocketfuel-to-ns \uf0c1 Rocketfuel-to-ns, developed by Purdue University, is a utility to convert RocketFuel-format data files into a set of configuration files runnable on am emulation testbed like the DETER testbed. Experiment configurations generated with this tool have the advantage of not being totally synthetic representations of the Internet; they provide a router-level topology based off real measurement data. This distribution also contains many sample NS files that represent real AS topologies. Website : http://www.cs.purdue.edu/homes/fahmy/software/rf2ns/index.html Runs on : UNIX Best for : Collecting real AS topologies and importing them into DETERLab. Inet \uf0c1 Inet, developed by University of Michigan, is a generator of representative Autonomous System (AS) level Internet topologies. Website : http://topology.eecs.umich.edu/inet/ Runs on : FreeBSD, Linux, Mac OS and Solaris Best for : Synthetic topology generation, following a power law. Brite \uf0c1 Brite, developed by Boston University, is a generator of flat AS, flat Router and hierarchical topologies, interoperable with various topology generators and simulators. Website : http://www.cs.bu.edu/brite/ Best for : Synthetic topology generation using different models and a GUI. GT-ITM \uf0c1 GT-ITM: Georgia Tech Internetwork Topology Models, developed by Georgia Tech, generates graphs that model the topological structure of internetworks. Website : http://www.cc.gatech.edu/projects/gtitm/ Runs on : SunOS and Linux Best for : Synthetic topology generation for small size topologies.","title":"Legacy Tools"},{"location":"core/legacy-tools/#legacy-tools","text":"This page includes links to tools that have been useful to DETER users in the past. There is no guarantee that they will perform with current DETER software and they are listed for legacy purposes.","title":"Legacy Tools"},{"location":"core/legacy-tools/#benchmarks","text":"","title":"Benchmarks"},{"location":"core/legacy-tools/#ddos-defense-benchmarks","text":"Developed and maintained by University of Delaware, this tool contains: A benchmark suite with a set of scenarios to be used for defense evaluation, integrated with SEER, A set of performance metrics that characterize an attack's impact and a defense's performance, and A set of tools used for benchmark development, integration of benchmarks with the DETER testbed and calculation of performance metrics from tcpdump traces collected during DDoS experimentation. Website : http://www.isi.edu/~mirkovic/bench Runs on : Any platform Best for : Testing DDoS defenses For questions, contact : Jelena Mirkovic at ISI","title":"DDoS Defense Benchmarks"},{"location":"core/legacy-tools/#legitimate-traffic-generators","text":"","title":"Legitimate Traffic Generators"},{"location":"core/legacy-tools/#seer","text":"The Security Experimentation EnviRonment (SEER), developed by SPARTA, Inc., is a GUI-based user interface to DETERLab, helping an experimenter to set up, script, and perform experiments in the DETER environment. The SEER back-end includes tools to generate legitimate traffic using Harpoon or custom-made Web, DNS, Ping, IRC, FTP and VoIP agents. Note that this tool is no longer supported and is offered as-is. Website : http://seer.deterlab.net/trac Runs on : All platforms, written in Java Best for : Legitimate traffic generation, DoS traffic generation, visualization of traffic levels in topology","title":"SEER"},{"location":"core/legacy-tools/#tcpreplay","text":"Tcpreplay is a suite of BSD licensed tools, which gives you the ability to inject previously captured traffic in libpcap format to test a variety of network devices. It allows you to classify traffic as client or server, rewrite Layer 2, 3 and 4 headers and finally replay the traffic back onto the network and through other devices such as switches, routers, firewalls, NIDS and IPS's. Tcpreplay supports both single and dual NIC modes for testing both sniffing and inline devices. Website : http://tcpreplay.synfin.net/trac/ Runs on : UNIX-flavored OSes and Win32 with Cygwin Best for : Replaying traces to regenerate same or similar traffic For questions, contact : Tcpreplay support","title":"Tcpreplay"},{"location":"core/legacy-tools/#webstone","text":"Webstone, a benchmark owned by Mindcraft Inc., measures performance of web server software and hardware products. Webstone consists of a program called the webmaster which can be installed on a client in the network or on a separate computer. The webmaster distributes web client software as well as configuration files for testing to the client computers, that contact the web server to retrieve web pages or files in order to test web server performance. Webstone also tests operating system software, CPU and network speeds. While it was developed with the idea of measuring the performance of web servers, it can be used to generate background traffic in a network as the multiple clients keep contacting the server over a period of time thereby simulating web traffic in the network. Website : http://www.mindcraft.com/webstone/ Runs on : UNIX-flavored OSes and Windows NT Best for : Web traffic generation","title":"Webstone"},{"location":"core/legacy-tools/#harpoon","text":"Harpoon, developed at University of Wisconsin, is a flow-level traffic generator. It uses a set of distributional parameters that can be automatically extracted from Netflow traces to generate flows that exhibit the same statistical qualities present in measured Internet traces, including temporal and spatial characteristics. Harpoon can be used to generate representative background traffic for application or protocol testing, or for testing network switching hardware. Note, however, that while traffic dynamics will resemble the one found in traces, Harpoon traffic runs over HTTP and application behavior may be different from the real one. Website : https://github.com/jsommers/harpoon Runs on : UNIX-flavored OSes Best for : Generating traffic from traces or from high-level specifications.","title":"Harpoon"},{"location":"core/legacy-tools/#dos-and-ddos-attack-traffic-generators","text":"","title":"DoS and DDoS Attack Traffic Generators"},{"location":"core/legacy-tools/#seer_1","text":"( See above ) SEER generates attack traffic using the Flooder tool, developed by SPARTA, and the Cleo tool developed by UCLA. Look at SEER's Web page for a more detailed description of these tools. The following collection of real DDoS tools has little new to offer with regard to attack traffic generation, when compared to SEER's capabilities. In general, SEER can generate same traffic variations as this tools, and is easier to control and customize. If, however, you are testing a defense that looks at control traffic of DoS networks these tools may be useful to you. They are all downloadable from third-party Web sites and are not maintained.","title":"SEER"},{"location":"core/legacy-tools/#stacheldraht","text":"Stacheldraht combines features of Trinoo and TFN tools and adds encrypted communication between the attacker and the masters. Stacheldraht uses TCP for encrypted communication between the attacker and the masters, and TCP or ICMP for communication between master and agents. Another added feature is the ability to perform automatic updates of agent code. Available attacks are UDP flood, TCP SYN flood, ICMP ECHO flood and Smurf attacks. Website : http://packetstormsecurity.org/distributed/stachel.tgz","title":"Stacheldraht"},{"location":"core/legacy-tools/#mstream","text":"Mstream generates a flood of TCP packets with the ACK bit set. Masters can be controlled remotely by one or more attackers using a password- protected interactive login. The communications between attacker and masters, and a master and agents, are configurable at compile time and have varied signif- icantly from incident to incident. Source addresses in attack packets are spoofed at random. The TCP ACK attack exhausts network resources and will likely cause a TCP RST to be sent to the spoofed source address (potentially also creating outgoing bandwidth consumption at the victim). Website : http://packetstormsecurity.org/distributed/mstream.txt","title":"Mstream"},{"location":"core/legacy-tools/#topology-generators-and-convertors","text":"","title":"Topology Generators and Convertors"},{"location":"core/legacy-tools/#rocketfuel-to-ns","text":"Rocketfuel-to-ns, developed by Purdue University, is a utility to convert RocketFuel-format data files into a set of configuration files runnable on am emulation testbed like the DETER testbed. Experiment configurations generated with this tool have the advantage of not being totally synthetic representations of the Internet; they provide a router-level topology based off real measurement data. This distribution also contains many sample NS files that represent real AS topologies. Website : http://www.cs.purdue.edu/homes/fahmy/software/rf2ns/index.html Runs on : UNIX Best for : Collecting real AS topologies and importing them into DETERLab.","title":"Rocketfuel-to-ns"},{"location":"core/legacy-tools/#inet","text":"Inet, developed by University of Michigan, is a generator of representative Autonomous System (AS) level Internet topologies. Website : http://topology.eecs.umich.edu/inet/ Runs on : FreeBSD, Linux, Mac OS and Solaris Best for : Synthetic topology generation, following a power law.","title":"Inet"},{"location":"core/legacy-tools/#brite","text":"Brite, developed by Boston University, is a generator of flat AS, flat Router and hierarchical topologies, interoperable with various topology generators and simulators. Website : http://www.cs.bu.edu/brite/ Best for : Synthetic topology generation using different models and a GUI.","title":"Brite"},{"location":"core/legacy-tools/#gt-itm","text":"GT-ITM: Georgia Tech Internetwork Topology Models, developed by Georgia Tech, generates graphs that model the topological structure of internetworks. Website : http://www.cc.gatech.edu/projects/gtitm/ Runs on : SunOS and Linux Best for : Synthetic topology generation for small size topologies.","title":"GT-ITM"},{"location":"core/link-delays/","text":"Per-Link Traffic Shaping (\"linkdelays\") \uf0c1 In order to conserve nodes, it is possible to specify that instead of doing traffic shaping on separate delay nodes (which eats up a node for every two shaped links), it be done on the nodes that are actually generating the traffic. If you are running FreeBSD on your nodes, end node traffic shaping uses IPFW to direct traffic into the proper Dummynet pipe. On each node in a duplex link or LAN, a set of IPFW rules and Dummynet pipes is set up. As traffic enters or leaves your node, IPFW looks at the packet and stuffs it into the proper Dummynet pipe. At the proper time, Dummynet takes the packet and sends it on its way. Under Linux, end node traffic shaping is performed by the packet scheduler modules, part of the kernel NET3 implementation. Each packet is added to the appropriate scheduler queue tree and shaped as specified in your NS file. Note that Linux traffic shaping currently only supports the drop-tail queueing discipline; gred and red are not available yet. To specify end node shaping in your NS file, simply set up a normal link or LAN, and then mark it as wanting to use end node traffic shaping. For example: set link0 [$ns duplex-link $nodeA $nodeD 10Mb 0ms DropTail] set lan0 [$ns make-lan \"nodeA nodeB nodeC\" 1Mb 0ms] tb-set-endnodeshaping $link0 1 tb-set-endnodeshaping $lan0 1 Please be aware though, that the kernels are different than the standard ones in a couple of ways: The kernel runs at a 1000HZ (1024HZ in Linux) clockrate instead of 100HZ. That is, the timer interrupts 1000 (1024) times per second instead of 100. This finer granularity allows the traffic shapers to do a better job of scheduling packets. Under FreeBSD, IPFW and Dummynet are compiled into the kernel, which affects the network stack; all incoming and outgoing packets are sent into IPFW to be matched on. Under Linux, packet scheduling exists implicitly, but uses lightweight modules by default. The packet timing mechanism in the linkdelay Linux kernel uses a slightly heavier (but more precise) method. Flow-based IP forwarding is turned off. This is also known as IP ''fast forwarding'' in the FreeBSD kernel. Note that regular IP packet forwarding is still enabled. To use end node traffic shaping globally, without having to specify per link or LAN, use the following in your NS file: tb-use-endnodeshaping 1 To specify non-shaped links, but perhaps control the shaping parameters later (increase delay, decrease bandwidth, etc.) after the experiment is swapped in, use the following in your NS file: tb-force-endnodeshaping 1 Multiplexed Links \uf0c1 Another feature we have added (FreeBSD only) is ''multiplexed'' (sometimes called ''emulated'') links. An emulated link is one that can be multiplexed over a physical link along with other links. Say your experimental nodes have just one physical interface (call it \"fxp0\"), but you want to create two duplex links on it: set link0 [$ns duplex-link $nodeA $nodeB 50Mb 0ms DropTail] set link1 [$ns duplex-link $nodeA $nodeC 50Mb 0ms DropTail] tb-set-multiplexed $link0 1 tb-set-multiplexed $link1 1 Without multiplexed links, your experiment would not be mappable since there are no nodes that can support the two duplex links that NodeA requires; there is only one physical interface. Using multiplexed links however, the testbed software will assign both links on NodeA to one physical interface. That is because each duplex link is only 50Mbs, while the physical link (fxp0) is 100Mbs. Of course, if your application actually tried to use more than 50Mbs on each multiplexed link, there would be a problem; a flow using more than its share on link0 would cause packets on link1 to be dropped when they otherwise would not be. ('''At this time, you cannot specify that a LAN use multiplexed links''') To prevent this problem, a multiplexed link is automatically setup to use [#LINKDELAYS per-link traffic shaping]. Each of the links in the above example would get a set of DummyNet pipes restricting their bandwidth to 50Mbs. Each link is forced to behave just as it would if the actual link bandwidth were 50Mbs. This allows the underlying physical link to support the aggregate bandwidth. Of course, the same caveats listed for per-link delays apply when using multiplexed links. As a concrete example, consider the following NS file which creates a router and attaches it to 12 other nodes: set maxnodes 12 set router [$ns node] for {set i 1} {$i <= $maxnodes} {incr i} { set node($i) [$ns node] set link($i) [$ns duplex-link $node($i) $router 30Mb 10ms DropTail] tb-set-multiplexed $link($i) 1 } tb-set-vlink-emulation vlan # Turn on routing. $ns rtproto Static Since each node has four 100Mbs interfaces, the above mapping would not be possible without the use of multiplexed links. However, since each link is defined to use 30Mbs, by using multiplexed links, the 12 links can be shared over the four physical interfaces, without oversubscribing the 400Mbs aggregate bandwidth available to the node that is assigned to the router. FreeBSD Technical Discussion \uf0c1 First, let's just look at what happens with per-link delays on a duplex link. In this case, an IPFW pipe is set up on each node. The rule for the pipe looks like: ipfw add pipe 10 ip from any to any out xmit fxp0 which says that any packet going out on fxp0 should be stuffed into pipe 10. Consider the case of a ping packet that traverses a duplex link from NodeA to NodeB: * Once the proper interface is chosen (based on routing or the fact that the destination is directly connected), the packet is handed off to IPFW, which determines that the interface (fxp0) matches the rule specified above. * The packet is then stuffed into the corresponding Dummynet pipe, to emerge sometime later (based on the traffic shaping parameters) and be placed on the wire. * The packet then arrives at NodeB. * A ping reply packet is created and addressed to NodeA, placed into the proper Dummynet pipe, and arrives at NodeA. As you can see, each packet traversed exactly one Dummynet pipe (or put another way, the entire ping/reply sequence traversed two pipes). Constructing delayed LANs is more complicated than duplex links because of the desire to allow each node in a LAN to see different delays when talking to any other node in the LAN. That is, the delay when traversing from NodeA to NodeB is different than when traversing from NodeA to NodeC. Further, the return delays might be specified completely differently so that the return trips take a different amount of time. To support this, it is necessary to insert two delay pipes for each node. One pipe is for traffic leaving the node for the LAN, and the other pipe is for traffic entering the node from the LAN. Why not create ''N'' pipes on each node for each possible destination address in the LAN, so that each packet traverses only one pipe? The reason is that a node on a LAN has only one connection to it, and multiple pipes would not respect the aggregate bandwidth cap specified. The rule for the second pipe looks like: ipfw add pipe 15 ip from any to any in recv fxp0 which says that any packet received on fxp0 should be stuffed into pipe 15. The packet is later handed up to the application, or forwarded on to the next hop, if appropriate. The addition of multiplexed links complicates things further. To multiplex several different links on a physical interface, one must use either encapsulation (ipinip, VLAN, etc) or IP interface aliases. We chose IP aliases because it does not affect the MTU size. The downside of IP aliases is that it is difficult (if not impossible) to determine what flow a packet is part of, and thus which IPFW pipe to stuff the packet into. In other words, the rules used above: ipfw add ... out xmit fxp0 ipfw add ... in recv fxp0 do not work because there are now multiple flows multiplexed onto the interface (multiple IPs) and so there is no way to distinguish which flow. Consider a duplex link in which we use the first rule above. If the packet is not addressed to a direct neighbor, the routing code lookup will return a nexthop, which '''does''' indicate the flow, but because the rule is based simply on the interface (fxp0), all flows match! Unfortunately, IPFW does not provide an interface for matching on the nexthop address, but seeing as we are kernel hackers, this is easy to deal with by adding new syntax to IPFW to allow matching on nexthop: ipfw add ... out xmit fxp0 nexthop 192.168.2.3:255.255.255.0 Now, no matter how the user alters the routing table, packets will be stuffed into the proper pipe since the nexthop indicates which directly connected virtual link the packet was sent over. The use of a mask allows for matching when directly connected to a LAN (a simplification). Multiplexed LANs present even worse problems because of the need to figure out which flow an incoming packet is part of. When a packet arrives at an interface, there is nothing in the packet to indicate which IP alias the packet was intended for (or which it came from) when the packet is not destined for the local node (is being forwarded). Linux Technical Discussion \uf0c1 Traffic shaping under Linux uses the NET3 packet scheduling modules, a hierarchically composable set of disciplines providing facilities such as bandwidth limiting, packet loss, and packet delay. As in the FreeBSD case, simplex (outgoing) link shaping is used on point-to-point links, while duplex shaping (going out, and coming in an interface) is used with LANs. See the previous section to understand why this is done. Unlike FreeBSD, Linux traffic shaping modules must be connected directly to a network device, and hence don't require a firewall directive to place packets into them. This means that all packets must pass through the shaping tree connected to a particular interface. Note that filters may be used on the shapers themselves to discriminate traffic flows, so it's not strictly the case that all traffic must be shaped if modules are attached. However, all traffic to an interface, at the least, is queued and de-queued through the root module of the shaping hierarchy. And all interfaces have at least a root module, but it is normally just a fast FIFO. Also of note is the fact that Linux traffic shaping normally only happens on the outgoing side of an interface, and requires a special virtual network device (known as an intermediate queuing device or IMQ) to capture incoming packets for shaping. This also requires the aid of the Linux firewalling facility, iptables, to divert the packets to the IMQs prior to routing. Here is an example duplex-link configuration with 50Mbps of bandwidth, a 0.05 PLR, and 20ms of delay in both directions: Outgoing side setup commands: # implicitly sets up class 1:1 tc qdisc add dev eth0 root handle 1 plr 0.05 # attach to class 1:1 and tell the module the default place to send # traffic is to class 2:1 (could attach filters to discriminate) tc qdisc add dev eth0 parent 1:1 handle 2 htb default 1 # class 2:1 does the actual limiting tc class add dev eth0 parent 2 classid 2:1 htb rate 50Mbit ceil 50Mbit # attach to class 2:1, also implicitly creates class 3:1, and attaches # a FIFO queue to it. tc qdisc add dev eth0 parent 2:1 handle 3 delay usecs 20000 The incoming side setup commands will look the same, but with eth0 replaced by imq0. Also, we have to tell the kernel to send packets coming into eth0 to imq0 (where they will be shaped): iptables -t mangle -A PREROUTING -i eth0 -j IMQ --todev 0 A flood ping sequence utilizing eth0 (echo->echo-reply) would experience a round trip delay of 40 ms, be restricted to 50Mbit, and have a 10% chance of losing packets. The doubling of numbers is due to shaping as packets go out, and come back in the interface. At the time of writing, we don't support multiplexed links under Linux, so no explicit matching against nexthop is necessary.","title":"Per-Link Traffic Shaping (\"linkdelays\")"},{"location":"core/link-delays/#per-link-traffic-shaping-linkdelays","text":"In order to conserve nodes, it is possible to specify that instead of doing traffic shaping on separate delay nodes (which eats up a node for every two shaped links), it be done on the nodes that are actually generating the traffic. If you are running FreeBSD on your nodes, end node traffic shaping uses IPFW to direct traffic into the proper Dummynet pipe. On each node in a duplex link or LAN, a set of IPFW rules and Dummynet pipes is set up. As traffic enters or leaves your node, IPFW looks at the packet and stuffs it into the proper Dummynet pipe. At the proper time, Dummynet takes the packet and sends it on its way. Under Linux, end node traffic shaping is performed by the packet scheduler modules, part of the kernel NET3 implementation. Each packet is added to the appropriate scheduler queue tree and shaped as specified in your NS file. Note that Linux traffic shaping currently only supports the drop-tail queueing discipline; gred and red are not available yet. To specify end node shaping in your NS file, simply set up a normal link or LAN, and then mark it as wanting to use end node traffic shaping. For example: set link0 [$ns duplex-link $nodeA $nodeD 10Mb 0ms DropTail] set lan0 [$ns make-lan \"nodeA nodeB nodeC\" 1Mb 0ms] tb-set-endnodeshaping $link0 1 tb-set-endnodeshaping $lan0 1 Please be aware though, that the kernels are different than the standard ones in a couple of ways: The kernel runs at a 1000HZ (1024HZ in Linux) clockrate instead of 100HZ. That is, the timer interrupts 1000 (1024) times per second instead of 100. This finer granularity allows the traffic shapers to do a better job of scheduling packets. Under FreeBSD, IPFW and Dummynet are compiled into the kernel, which affects the network stack; all incoming and outgoing packets are sent into IPFW to be matched on. Under Linux, packet scheduling exists implicitly, but uses lightweight modules by default. The packet timing mechanism in the linkdelay Linux kernel uses a slightly heavier (but more precise) method. Flow-based IP forwarding is turned off. This is also known as IP ''fast forwarding'' in the FreeBSD kernel. Note that regular IP packet forwarding is still enabled. To use end node traffic shaping globally, without having to specify per link or LAN, use the following in your NS file: tb-use-endnodeshaping 1 To specify non-shaped links, but perhaps control the shaping parameters later (increase delay, decrease bandwidth, etc.) after the experiment is swapped in, use the following in your NS file: tb-force-endnodeshaping 1","title":"Per-Link Traffic Shaping (\"linkdelays\")"},{"location":"core/link-delays/#multiplexed-links","text":"Another feature we have added (FreeBSD only) is ''multiplexed'' (sometimes called ''emulated'') links. An emulated link is one that can be multiplexed over a physical link along with other links. Say your experimental nodes have just one physical interface (call it \"fxp0\"), but you want to create two duplex links on it: set link0 [$ns duplex-link $nodeA $nodeB 50Mb 0ms DropTail] set link1 [$ns duplex-link $nodeA $nodeC 50Mb 0ms DropTail] tb-set-multiplexed $link0 1 tb-set-multiplexed $link1 1 Without multiplexed links, your experiment would not be mappable since there are no nodes that can support the two duplex links that NodeA requires; there is only one physical interface. Using multiplexed links however, the testbed software will assign both links on NodeA to one physical interface. That is because each duplex link is only 50Mbs, while the physical link (fxp0) is 100Mbs. Of course, if your application actually tried to use more than 50Mbs on each multiplexed link, there would be a problem; a flow using more than its share on link0 would cause packets on link1 to be dropped when they otherwise would not be. ('''At this time, you cannot specify that a LAN use multiplexed links''') To prevent this problem, a multiplexed link is automatically setup to use [#LINKDELAYS per-link traffic shaping]. Each of the links in the above example would get a set of DummyNet pipes restricting their bandwidth to 50Mbs. Each link is forced to behave just as it would if the actual link bandwidth were 50Mbs. This allows the underlying physical link to support the aggregate bandwidth. Of course, the same caveats listed for per-link delays apply when using multiplexed links. As a concrete example, consider the following NS file which creates a router and attaches it to 12 other nodes: set maxnodes 12 set router [$ns node] for {set i 1} {$i <= $maxnodes} {incr i} { set node($i) [$ns node] set link($i) [$ns duplex-link $node($i) $router 30Mb 10ms DropTail] tb-set-multiplexed $link($i) 1 } tb-set-vlink-emulation vlan # Turn on routing. $ns rtproto Static Since each node has four 100Mbs interfaces, the above mapping would not be possible without the use of multiplexed links. However, since each link is defined to use 30Mbs, by using multiplexed links, the 12 links can be shared over the four physical interfaces, without oversubscribing the 400Mbs aggregate bandwidth available to the node that is assigned to the router.","title":"Multiplexed Links"},{"location":"core/link-delays/#freebsd-technical-discussion","text":"First, let's just look at what happens with per-link delays on a duplex link. In this case, an IPFW pipe is set up on each node. The rule for the pipe looks like: ipfw add pipe 10 ip from any to any out xmit fxp0 which says that any packet going out on fxp0 should be stuffed into pipe 10. Consider the case of a ping packet that traverses a duplex link from NodeA to NodeB: * Once the proper interface is chosen (based on routing or the fact that the destination is directly connected), the packet is handed off to IPFW, which determines that the interface (fxp0) matches the rule specified above. * The packet is then stuffed into the corresponding Dummynet pipe, to emerge sometime later (based on the traffic shaping parameters) and be placed on the wire. * The packet then arrives at NodeB. * A ping reply packet is created and addressed to NodeA, placed into the proper Dummynet pipe, and arrives at NodeA. As you can see, each packet traversed exactly one Dummynet pipe (or put another way, the entire ping/reply sequence traversed two pipes). Constructing delayed LANs is more complicated than duplex links because of the desire to allow each node in a LAN to see different delays when talking to any other node in the LAN. That is, the delay when traversing from NodeA to NodeB is different than when traversing from NodeA to NodeC. Further, the return delays might be specified completely differently so that the return trips take a different amount of time. To support this, it is necessary to insert two delay pipes for each node. One pipe is for traffic leaving the node for the LAN, and the other pipe is for traffic entering the node from the LAN. Why not create ''N'' pipes on each node for each possible destination address in the LAN, so that each packet traverses only one pipe? The reason is that a node on a LAN has only one connection to it, and multiple pipes would not respect the aggregate bandwidth cap specified. The rule for the second pipe looks like: ipfw add pipe 15 ip from any to any in recv fxp0 which says that any packet received on fxp0 should be stuffed into pipe 15. The packet is later handed up to the application, or forwarded on to the next hop, if appropriate. The addition of multiplexed links complicates things further. To multiplex several different links on a physical interface, one must use either encapsulation (ipinip, VLAN, etc) or IP interface aliases. We chose IP aliases because it does not affect the MTU size. The downside of IP aliases is that it is difficult (if not impossible) to determine what flow a packet is part of, and thus which IPFW pipe to stuff the packet into. In other words, the rules used above: ipfw add ... out xmit fxp0 ipfw add ... in recv fxp0 do not work because there are now multiple flows multiplexed onto the interface (multiple IPs) and so there is no way to distinguish which flow. Consider a duplex link in which we use the first rule above. If the packet is not addressed to a direct neighbor, the routing code lookup will return a nexthop, which '''does''' indicate the flow, but because the rule is based simply on the interface (fxp0), all flows match! Unfortunately, IPFW does not provide an interface for matching on the nexthop address, but seeing as we are kernel hackers, this is easy to deal with by adding new syntax to IPFW to allow matching on nexthop: ipfw add ... out xmit fxp0 nexthop 192.168.2.3:255.255.255.0 Now, no matter how the user alters the routing table, packets will be stuffed into the proper pipe since the nexthop indicates which directly connected virtual link the packet was sent over. The use of a mask allows for matching when directly connected to a LAN (a simplification). Multiplexed LANs present even worse problems because of the need to figure out which flow an incoming packet is part of. When a packet arrives at an interface, there is nothing in the packet to indicate which IP alias the packet was intended for (or which it came from) when the packet is not destined for the local node (is being forwarded).","title":"FreeBSD Technical Discussion"},{"location":"core/link-delays/#linux-technical-discussion","text":"Traffic shaping under Linux uses the NET3 packet scheduling modules, a hierarchically composable set of disciplines providing facilities such as bandwidth limiting, packet loss, and packet delay. As in the FreeBSD case, simplex (outgoing) link shaping is used on point-to-point links, while duplex shaping (going out, and coming in an interface) is used with LANs. See the previous section to understand why this is done. Unlike FreeBSD, Linux traffic shaping modules must be connected directly to a network device, and hence don't require a firewall directive to place packets into them. This means that all packets must pass through the shaping tree connected to a particular interface. Note that filters may be used on the shapers themselves to discriminate traffic flows, so it's not strictly the case that all traffic must be shaped if modules are attached. However, all traffic to an interface, at the least, is queued and de-queued through the root module of the shaping hierarchy. And all interfaces have at least a root module, but it is normally just a fast FIFO. Also of note is the fact that Linux traffic shaping normally only happens on the outgoing side of an interface, and requires a special virtual network device (known as an intermediate queuing device or IMQ) to capture incoming packets for shaping. This also requires the aid of the Linux firewalling facility, iptables, to divert the packets to the IMQs prior to routing. Here is an example duplex-link configuration with 50Mbps of bandwidth, a 0.05 PLR, and 20ms of delay in both directions: Outgoing side setup commands: # implicitly sets up class 1:1 tc qdisc add dev eth0 root handle 1 plr 0.05 # attach to class 1:1 and tell the module the default place to send # traffic is to class 2:1 (could attach filters to discriminate) tc qdisc add dev eth0 parent 1:1 handle 2 htb default 1 # class 2:1 does the actual limiting tc class add dev eth0 parent 2 classid 2:1 htb rate 50Mbit ceil 50Mbit # attach to class 2:1, also implicitly creates class 3:1, and attaches # a FIFO queue to it. tc qdisc add dev eth0 parent 2:1 handle 3 delay usecs 20000 The incoming side setup commands will look the same, but with eth0 replaced by imq0. Also, we have to tell the kernel to send packets coming into eth0 to imq0 (where they will be shaped): iptables -t mangle -A PREROUTING -i eth0 -j IMQ --todev 0 A flood ping sequence utilizing eth0 (echo->echo-reply) would experience a round trip delay of 40 ms, be restricted to 50Mbit, and have a 10% chance of losing packets. The doubling of numbers is due to shaping as packets go out, and come back in the interface. At the time of writing, we don't support multiplexed links under Linux, so no explicit matching against nexthop is necessary.","title":"Linux Technical Discussion"},{"location":"core/logging-in/","text":"Logging into your Node \uf0c1 By the time you receive the email message listing your nodes, the DETER configuration system will have ensured that your nodes are fully configured and ready to use. If you have selected one of the DETER-supported operating system images see supported images ), this configuration process includes: * loading fresh disk images so that each node is in a known clean state; * rebooting each node so that it is running the OS specified in the NS script; * configuring each of the network interfaces so that each one is \"up\" and talking to its virtual LAN (VLAN); * creating user accounts for each of the project members; * mounting the projects NFS directory in /proj so that project files are easily shared amongst all the nodes in the experiment; * creating a /etc/hosts file on each node so that you may refer to the experimental interfaces of other nodes by name instead of IP number; * configuring all of the delay parameters; * configuring the serial console lines so that project members may access the console ports from users.deterlab.net. As this point you may log into any of the nodes in your experiment. You will need to use Secure Shell (ssh) to log into users.deterlab.net Your login name and password will be the same as your Web Interface login and password. Note Although you can log into the web interface using your email address instead of your login name, you must use your login name when logging into users.deterlab.net . Once logged into users you can then SSH to your nodes. You should use the `qualified name' from the nodes mapping table so that you do not form dependencies on any particular physical node. For more information on using SSH with DETER, please take a look at the DETER SSH wiki page.","title":"Logging into your Node"},{"location":"core/logging-in/#logging-into-your-node","text":"By the time you receive the email message listing your nodes, the DETER configuration system will have ensured that your nodes are fully configured and ready to use. If you have selected one of the DETER-supported operating system images see supported images ), this configuration process includes: * loading fresh disk images so that each node is in a known clean state; * rebooting each node so that it is running the OS specified in the NS script; * configuring each of the network interfaces so that each one is \"up\" and talking to its virtual LAN (VLAN); * creating user accounts for each of the project members; * mounting the projects NFS directory in /proj so that project files are easily shared amongst all the nodes in the experiment; * creating a /etc/hosts file on each node so that you may refer to the experimental interfaces of other nodes by name instead of IP number; * configuring all of the delay parameters; * configuring the serial console lines so that project members may access the console ports from users.deterlab.net. As this point you may log into any of the nodes in your experiment. You will need to use Secure Shell (ssh) to log into users.deterlab.net Your login name and password will be the same as your Web Interface login and password. Note Although you can log into the web interface using your email address instead of your login name, you must use your login name when logging into users.deterlab.net . Once logged into users you can then SSH to your nodes. You should use the `qualified name' from the nodes mapping table so that you do not form dependencies on any particular physical node. For more information on using SSH with DETER, please take a look at the DETER SSH wiki page.","title":"Logging into your Node"},{"location":"core/node-types/","text":"Node Types \uf0c1 This is not a complete list of all node types available at DETERLab, but below are the primary types. dl380g3 MicroCloud pc2133 (including pc2133 and bpc2133) pc3000 (including pc3000, bpc3000, pc3060, bpc3060, and pc3100) bvx2200 bpc2800 netfpga2 dl380g3 \uf0c1 There are 120 dl380g3 class nodes available at ISI . Machine: HP Proliant DL360 G8 Server Each node has: Dual Intel(R) Xeon(R) hexa-core processors running at 2.2 Ghz with 15MB cache Intel VT-x support 24GB of RAM One 1Tb SATA HP Proliant Disk Drive 7.2k rpm G8 (boot priority) One 240Gb SATA HP Proliant Solid State Drive G8 Two experimental interfaces: One Dual port PCIe Intel Ten Gigabit Ethernet card for experimental ports One Quad port PCIe Intel Gigabit Ethernet card, presently with one port wired to the control network MicroCloud \uf0c1 There are 128 MicroCloud nodes at ISI . Machine: High Density SuperMicro MicroCloud Chassis that fits 8 nodes in 3u of rack space. Each node has: One Intel(R) Xeon(R) E3-1260L quad-core processor running at 2.4 Ghz Intel VT-x and VT-d support 16GB of RAM One 250Gb SATA Western Digital RE4 Disk Drive 5 experimental interfaces One Dual port PCIe Intel Gigabit Ethernet card for the control network and an experimental port One Quad port PCIe Intel Gigabit Ethernet card for experimental network pc2133 \uf0c1 This node type includes pc2133 and bpc2133: There are 63 pc2133 nodes at ISI . There are 64 bpc2133 nodes at UCB . The pc2133 and bpc2133 machines have the following features: Dell PowerEdge 860 Chasis One Intel(R) Xeon(R) CPU X3210 quad core processor running at 2.13 Ghz 4GB of RAM One 250Gb SATA Disk Drive One Dual port PCI-X Intel Gigabit Ethernet card for the control network (only one port is used). One Quad port PCIe Intel Gigabit Ethernet card for experimental network. CPU flags: \uf0c1 fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx lm constant_tsc pni monitor ds_cpl '''vmx''' est tm2 ssse3 cx16 xtpr lahf_lm pc3000 \uf0c1 This node type includes pc3000, bpc3000, pc3060, bpc3060, and pc3100. There are: 0 pc3000 nodes at ISI 32 bpc3000 nodes at UCB 17 pc3060 nodes at ISI 32 bpc3060 nodes at UCB 4 pc3100 nodes at ISI pc3000 and bpc3000 have the following features: Dell PowerEdge 1850 Chassis. Dual 3Ghz Intel Xeon processors. 2 GB of RAM One 36Gb 15k RPM SCSI drive (bpc machines may be configured with two). 4 Intel Gigabit experimental network ports. 1 Intel Gigabit experimental network port. pc3060 and bpc3060 machines are the same as the pc3000/bpc3000 machines except that they have one more experimental network interface. pc3100 machines have a total of 9 experimental interfaces and 1 control network interface. There are only 4 of these type of machine. bvx2200 \uf0c1 There are 31 bvx2200 nodes at UCB . bvx2200 has the following features: Sun Microsystems Sun Fire X2100 M2 Chassis. Dual-Core 1.8 Ghz AMD Opteron(tm) Processor 1210. One 250Gb 7200 RPM SATA drive. 1 Broadcom NetXtreme Gigabit experimental network port. 2 Nvidia nForce MCP55 experimental network ports. 2 Intel Gigabit experimental network ports. bpc2800 \uf0c1 There are 30 bpc2800 at UCB . The bpc2800 machines have the following features: Sun Microsystems Sun Fire V60 Chassis One Intel(R) Xeon(R) CPU dual core processor running at 2.8 GHz 2 GB of RAM One 36 GB SCSI Disk Drive Two Dual port PCI-X Intel Gigabit Ethernet cards, 1 port for control network and 3 ports for experimental network One Single port PCI-X Intel Gigabit Ethernet card for experimental network CPU flags \uf0c1 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe pebs bts cid xtpr netfpga2 \uf0c1 There are 10 netfpga2 class nodes in the testbed. These are pc2133 class machines. Each of these nodes has a single NetFPGA card installed.","title":"Node Types"},{"location":"core/node-types/#node-types","text":"This is not a complete list of all node types available at DETERLab, but below are the primary types. dl380g3 MicroCloud pc2133 (including pc2133 and bpc2133) pc3000 (including pc3000, bpc3000, pc3060, bpc3060, and pc3100) bvx2200 bpc2800 netfpga2","title":"Node Types"},{"location":"core/node-types/#dl380g3","text":"There are 120 dl380g3 class nodes available at ISI . Machine: HP Proliant DL360 G8 Server Each node has: Dual Intel(R) Xeon(R) hexa-core processors running at 2.2 Ghz with 15MB cache Intel VT-x support 24GB of RAM One 1Tb SATA HP Proliant Disk Drive 7.2k rpm G8 (boot priority) One 240Gb SATA HP Proliant Solid State Drive G8 Two experimental interfaces: One Dual port PCIe Intel Ten Gigabit Ethernet card for experimental ports One Quad port PCIe Intel Gigabit Ethernet card, presently with one port wired to the control network","title":"dl380g3"},{"location":"core/node-types/#microcloud","text":"There are 128 MicroCloud nodes at ISI . Machine: High Density SuperMicro MicroCloud Chassis that fits 8 nodes in 3u of rack space. Each node has: One Intel(R) Xeon(R) E3-1260L quad-core processor running at 2.4 Ghz Intel VT-x and VT-d support 16GB of RAM One 250Gb SATA Western Digital RE4 Disk Drive 5 experimental interfaces One Dual port PCIe Intel Gigabit Ethernet card for the control network and an experimental port One Quad port PCIe Intel Gigabit Ethernet card for experimental network","title":"MicroCloud"},{"location":"core/node-types/#pc2133","text":"This node type includes pc2133 and bpc2133: There are 63 pc2133 nodes at ISI . There are 64 bpc2133 nodes at UCB . The pc2133 and bpc2133 machines have the following features: Dell PowerEdge 860 Chasis One Intel(R) Xeon(R) CPU X3210 quad core processor running at 2.13 Ghz 4GB of RAM One 250Gb SATA Disk Drive One Dual port PCI-X Intel Gigabit Ethernet card for the control network (only one port is used). One Quad port PCIe Intel Gigabit Ethernet card for experimental network.","title":"pc2133"},{"location":"core/node-types/#cpu-flags","text":"fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx lm constant_tsc pni monitor ds_cpl '''vmx''' est tm2 ssse3 cx16 xtpr lahf_lm","title":"CPU flags:"},{"location":"core/node-types/#pc3000","text":"This node type includes pc3000, bpc3000, pc3060, bpc3060, and pc3100. There are: 0 pc3000 nodes at ISI 32 bpc3000 nodes at UCB 17 pc3060 nodes at ISI 32 bpc3060 nodes at UCB 4 pc3100 nodes at ISI pc3000 and bpc3000 have the following features: Dell PowerEdge 1850 Chassis. Dual 3Ghz Intel Xeon processors. 2 GB of RAM One 36Gb 15k RPM SCSI drive (bpc machines may be configured with two). 4 Intel Gigabit experimental network ports. 1 Intel Gigabit experimental network port. pc3060 and bpc3060 machines are the same as the pc3000/bpc3000 machines except that they have one more experimental network interface. pc3100 machines have a total of 9 experimental interfaces and 1 control network interface. There are only 4 of these type of machine.","title":"pc3000"},{"location":"core/node-types/#bvx2200","text":"There are 31 bvx2200 nodes at UCB . bvx2200 has the following features: Sun Microsystems Sun Fire X2100 M2 Chassis. Dual-Core 1.8 Ghz AMD Opteron(tm) Processor 1210. One 250Gb 7200 RPM SATA drive. 1 Broadcom NetXtreme Gigabit experimental network port. 2 Nvidia nForce MCP55 experimental network ports. 2 Intel Gigabit experimental network ports.","title":"bvx2200"},{"location":"core/node-types/#bpc2800","text":"There are 30 bpc2800 at UCB . The bpc2800 machines have the following features: Sun Microsystems Sun Fire V60 Chassis One Intel(R) Xeon(R) CPU dual core processor running at 2.8 GHz 2 GB of RAM One 36 GB SCSI Disk Drive Two Dual port PCI-X Intel Gigabit Ethernet cards, 1 port for control network and 3 ports for experimental network One Single port PCI-X Intel Gigabit Ethernet card for experimental network","title":"bpc2800"},{"location":"core/node-types/#cpu-flags_1","text":"fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe pebs bts cid xtpr","title":"CPU flags"},{"location":"core/node-types/#netfpga2","text":"There are 10 netfpga2 class nodes in the testbed. These are pc2133 class machines. Each of these nodes has a single NetFPGA card installed.","title":"netfpga2"},{"location":"core/ns-commands/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Note This is advanced material. For beginners looking to learn how to create topologies please view topology documentation NS Commands \uf0c1 In order to use the testbed specific commands, you must include the following line near the top of your NS topology file (before any testbed commands are used): source tb_compat.tcl If you wish to use your file under NS, download tb_compat.tcl and place it in the same directory as your NS file. When run in this way under NS, the testbed commands will have no effect, but NS will be able to parse your file. TCL, NS, and node names \uf0c1 In your file, you will be creating nodes with something like the following line: set node1 [$ns node] With this command, the simulator, represented by $ns is creating a new node involving many internal data changes and returning a reference to it which is stored in the variable node1 . In almost all cases when you need to refer to a node, you will do it as $node1 , the $ indicating that you want the value of the variable node1 , i.e. the reference to the node. Thus you will be issuing commands like: $ns duplex-link $node1 $node2 100Mb 150ms DropTail tb-set-ip $node1 10.1.0.2 Note the instances of $ . You will notice that when your experiment is set up, the node names and such will be node1 , node2 , node3 , etc. This happens because the parser detects what variable you are using to store the node reference and uses that as the node name. In the case that you do something like: set node1 [$ns node2] set A $node1 The node will still be called node1 as that was the first variable to contain the reference. If you are dealing with many nodes you may store them in an array, using a command similar to the following: for {set i 0} {$i < 4} {incr i} { set nodes($i) [$ns node] } In this case, the names of the node will be nodes-0 , nodes-1 , nodes-2 , nodes-3 . In other words, the \"(\" character is replaced with \"-\", and \")\" is removed. This slightly different syntax is used to avoid any problems that \"()\" may cause later in the process. For example, the \"()\" characters may not appear in DNS entries. As a final note, everything said above for nodes applies equally to LANs, i.e.: set lan0 [$ns make-lan \"$node0 $node1\" 100Mb 0ms] tb-set-lan-loss $lan0 .02 Again, note the instances of $ . Links may also be named just like nodes and LANs. The names may then be used to set loss rates or IP addresses. This technique is the only way to set such attributes when there are multiple links between two nodes. set link1 [$ns duplex-link $node0 $node1 100Mb 0ms DropTail] tb-set-link-loss $link1 0.05 tb-set-ip-link $node0 $link1 10.1.0.128 Captured NS file parameters \uf0c1 A common convention when writing NS files is to place any parameters in an array named opt at the beginning of the file. For example: set opt(CLIENT_COUNT) 5 set opt(BW) 10mb; Link bandwidth set opt(LAT) 10ms; Link latency ... $ns duplex-link $server $router $opt(BW) $opt(LAT) DropTail for {set i 0} {$i < $opt(CLIENT_COUNT)} {incr i} { set nodes($i) [$ns node] ... } set serverprog [$server program-agent -command \"starter.sh\"] Normally, this convention is only used to help organize the parameters. In DETERLab, however, the contents of the opt array are captured and made available to the emulated environment. For instance, the parameters are added as environment variables to any commands run by program-agents. So in the above example of NS code, the starter.sh script will be able to reference parameters by name, like so: #! /bin/sh echo \"Testing with $CLIENT_COUNT clients.\" ... Note that the contents of the opt array are not ordered, so you should not reference other parameters and expect the shell to expand them appropriately: set opt(prefix) \"/foo/bar\" set opt(BINDIR) '$prefix/bin'; # BAD set opt(prefix) \"/foo/bar\" set opt(BINDIR) \"$opt(prefix)/bin\"; # Good Ordering Issues \uf0c1 tb- commands have the same status as all other Tcl and NS commands. Thus the order matters not only relative to each other but also relative to other commands. One common example of this is that IP commands must be issued after the links or LANs are created. Hardware Commands \uf0c1 tb-set-hardware \uf0c1 tb-set-hardware node type [args] tb-set-hardware $node3 pc tb-set-hardware $node4 shark where: node = The name of the node. type = The type of the node. Note Please see the Node Status page for a list of available types. pc is the default type. No current types have any additional arguments. IP Address Commands \uf0c1 Each node will be assigned an IP address for each interface that is in use. The following commands will allow you to explicitly set those IP addresses. IP addresses will be automatically generated for all nodes for which you do not explicitly set IP addresses. In most cases, the IP addresses on either side of a link must be in the same subnet. Likewise, all IP addresses on a LAN should be in the same subnet. Generally the same subnet should not be used for more than one link or LAN in a given experiment, nor should one node have multiple interfaces in the same subnet. Automatically generated IP addresses will conform to these requirements. If part of a link or LAN is explicitly specified with the commands below then the remainder will be automatically generated under the same subnet. IP address assignment is deterministic and tries to fill lower IP's first, starting at 2. Except in the partial specification case (see above), all automatic IP addresses are in the network 10 . tb-set-ip \uf0c1 tb-set-ip node ip tb-set-ip $node1 142.3.4.5 where: node = The node to assign the IP address to ip = The IP address. Note This command should only be used for nodes that have a single link. For nodes with multiple links the following commands should be used. Mixing tb-set-ip and any other IP command on the same node will result in an error. tb-set-ip-link \uf0c1 tb-set-ip-link node link ip tb-set-ip-link $node0 $link0 142.3.4.6 where: node = The node to set the IP for. link = The link to set the IP for. ip = The IP address. Note One way to think of the arguments is a link with the node specifying which side of the link to set the IP for. This command cannot be mixed with tb-set-ip on the same node. tb-set-ip-lan \uf0c1 tb-set-ip-lan node lan ip tb-set-ip-lan $node1 $lan0 142.3.4.6 where: node = The node to set the IP for. lan = The lan the IP is on. ip = The IP address. Note One way to think of the arguments is a node with the LAN specifying which port to set the IP address for. This command cannot be mixed with tb-set-ip on the same node. tb-set-ip-interface \uf0c1 tb-set-ip-interface node dst ip tb-set-ip-interface $node2 $node1 142.3.4.6 where: node = The node to set the IP for. dst = The destination of the link to set the IP for. IP = The IP address. Note This command cannot be mixed on the same node with tb-set-ip . (See above) In the case of multiple links between the same pair of nodes, there is no way to distinguish which link to the set the IP for. This should be fixed soon. This command is converted internally to either tb-set-ip-link or tb-set-ip-lan . It is possible that error messages will report either of those commands instead of tb-set-ip-interface . tb-set-netmask \uf0c1 tb-set-netmask lanlink netmask tb-set-netmask $link0 \"255.255.255.248\" where: lanlink = The lan or link to set the netmask for. netmask = The netmask in dotted notation. Note This command sets the netmask for a LAN or link. The mask must be big enough to support all of the nodes on the LAN or link! You may play with the bottom three octets (0xFFFFFXXX) of the mask; attempts to change the upper octets will cause an error. OS Commands \uf0c1 tb-set-node-os \uf0c1 tb-set-node-os node os tb-set-node-os $node1 FBSD-STD tb-set-node-os $node1 MY_OS where: node = The node to set the OS for. os = The id of the OS for that node. Note The OSID may either by one of the standard OS's we provide or a custom OSID, created via the web interface. If no OS is specified for a node, a default OS is chosen based on the nodes type. This is currently 'Ubuntu1604-STD ' for PCs. The currently available standard OS types are listed in the OS Images page. tb-set-node-rpms \uf0c1 tb-set-node-rpms node rpms... tb-set-node-rpms $node0 rpm1 rpm2 rpm3 Note This command sets which RPMs are to be installed on the node when it first boots after being assigned to an experiment. Each RPM can be either a path to a file or a URL. Paths must be to files that reside in the /proj or /groups directory. You are not allowed to place your RPMs in your home directory. http(s):// and ftp:// URLs will be fetched into the experiment's directory, and re-distributed from there. tb-set-node-startcmd \uf0c1 tb-set-node-startcmd node startupcmd tb-set-node-startcmd $node0 \"mystart.sh -a >& /tmp/node0.log\" Note Specify a script or program to be run when the node is booted. tb-set-node-cmdline \uf0c1 tb-set-node-cmdline node cmdline tb-set-node-cmdline $node0 {???} Note Set the commandline to be passed to the kernel when it is booted. Currently, this is supported on OSKit kernels only. tb-set-node-tarfiles \uf0c1 tb-set-node-tarfiles node install-dir1 tarfile1 ... The tb-set-node-tarfiles command is used to install one or more tar files onto a node's local disk. This command is useful for installing files that are used frequently, but will change very little during the course of your experiments. For example, if your software depends on a third-party library not provided in the standard disk images, you can produce a tarball and have the library ready for use on all the experimental nodes. Another example would be the data sets for your software. The benefit of installing files using this method is that they will reside on the node's local disk, so your experimental runs will not be disrupted by NFS traffic. Note Avoid using this command if the files are changing frequently because the tars are only (re)installed when the nodes boot. Installing individual tar files or RPMs is a midpoint in the spectrum of getting software onto the experimental nodes. At one extreme, you can read everything over NFS, which works well if the files are changing constantly, but can generate a great deal of strain on the control network and disrupt your experiment. The tar files and RPMs are also read over NFS when the nodes initially boot; however, there won't be any extra NFS traffic while you are running your experiment. Finally, if you need a lot of software installed on a large number of nodes, say greater than 20, it might be best to create a custom disk image . Using a disk image is easier on the control network since it is transferred using multicast, thus greatly reducing the amount of NFS traffic when the experiment is swapped in. Required Parameters: node - The node where the files should be installed. Each node has its own tar file list, which may or may not be different from the others. One or more install-dir and tarfile pairs are then listed in the order you wish them to be installed: install-dir - An existing directory on the node where the tar file should be unarchived (e.g. / , /usr , /usr/local ). The tar command will be run as \"root\" [#tb-set-node-tarfiles Note1], so all of the node's directories will be accessible to you. If the directory does not exist on the image or was not created by the unarchiving of a previous tar file, the installation will fail [#tb-set-node-tarfiles Note2]. tarfile - An existing tar file located in a project directory (e.g. /proj or /groups ) or an http , https , or ftp URL. In the case of URLs, they are downloaded when the experiment is swapped in and cached in the experiment's directory for future use. In either case, the tar file name is required to have one of the following extensions: .tar, .tar.Z, .tar.gz, or .tgz. Note that the tar file could have been created anywhere; however, if you want the unarchived files to have valid DETERLab user and group id's, you should create the tar file on ops or an experimental node. Example usage: # Overwrite files in /bin and /sbin. tb-set-node-tarfiles $node0 /bin /proj/foo/mybinmods.tar /sbin /proj/foo/mysbinmods.tar # Programmatically generate the list of tarballs. set tb [list] # Add a tarball located on a web site. lappend tb / http://foo.bar/bazzer.tgz # Add a tarball located in the DETER NFS space. lappend tb /usr/local /proj/foo/tarfiles/bar.tar.gz # Use 'eval' to expand the 'tb' list into individual # arguments to the tb-set-node-tarfiles command. eval tb-set-node-tarfiles $node1 $tb See also: tb-set-node-rpms Custom disk images Note Because the files are installed as root, care must be taken to protect the tar file so it cannot be replaced with a trojan that allowed less privileged users to become root. Currently, you can only tell how/why an installation failed by examining the node's console log on bootup. Link Loss Commands \uf0c1 This is the NS syntax for creating a link: $ns duplex-link $node1 $node2 100Mb 150ms DropTail Note This does not allow for specifying link loss rates. DETERLab does, however, support link loss. The following commands can be used to specify link loss rates. tb-set-link-loss \uf0c1 tb-set-link-loss src dst loss tb-set-link-loss link loss tb-set-link-loss $node1 $node2 0.05 tb-set-link-loss $link1 0.02 where: src , dst = Two nodes to describe the link. link = The link to set the rate for. loss = The loss rate (between 0 and 1). Note There are two syntaxes available. The first specifies a link by a source/destination pair. The second explicitly specifies the link. The source/destination pair is incapable of describing an individual link in the case of multiple links between two nodes. Use the second syntax for this case. tb-set-lan-loss \uf0c1 tb-set-lan-loss lan loss tb-set-lan-loss $lan1 0.3 Where: lan = The lan to set the loss rate for. loss = The loss rate (between 0 and 1). Note This command sets the loss rate for the entire LAN. tb-set-node-lan-delay \uf0c1 tb-set-node-lan-delay node lan delay tb-set-node-lan-delay $node0 $lan0 40ms Where: node = The node we are modifying the delay for. lan = Which LAN the node is in that we are affecting. delay = The new node to switch delay (see below). Note This command changes the delay between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node, possibly incurring additional latency from any delay parameters there. If this command is not used to overwrite the delay, then the delay for a given node to switch link is taken as one half of the delay passed to make-lan . Thus in a LAN where no tb-set-node-delay calls are made, the node-to-node latency will be the latency passed to make-lan . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times. Delays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. As a convenience, a delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other. tb-set-node-lan-bandwidth \uf0c1 tb-set-node-lan-bandwidth node lan bandwidth tb-set-node-lan-bandwidth $node0 $lan0 20Mb Where: node = The node we are modifying the bandwidth for. lan = Which LAN the node is in that we are affecting. bandwidth = The new node to switch bandwidth (see below). Note This command changes the bandwidth between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may have a lower bandwidth. If this command is not used to overwrite the bandwidth, then the bandwidth for a given node to switch link is taken directly from the bandwidth passed to make-lan . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times. tb-set-node-lan-loss \uf0c1 tb-set-node-lan-loss node lan loss tb-set-node-lan-loss $node0 $lan0 0.05 Where: node = The node we are modifying the loss for. lan = Which LAN the node is in that we are affecting. loss = The new node to switch loss (see below). Note This command changes the loss probability between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may also have a loss chance. Thus for packet going to switch with loss chance A and then going on the destination with loss chance B , the node-to-node loss chance is (1-(1-A)(1-B)) . If this command is not used to overwrite the loss, then the loss for a given node to switch link is taken from the loss rate passed to the make-lan command. If a loss rate of L is passed to make-lan then the node to switch loss rate for each node is set to (1-sqrt(1-L)) . Because each packet will have two such chances to be lost, the node-to-loss rate comes out as the desired L . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times. tb-set-node-lan-params \uf0c1 tb-set-node-lan-params node lan delay bandwidth loss tb-set-node-lan-params $node0 $lan0 40ms 20Mb 0.05 Where: node = The node we are modifying the loss for. lan = Which LAN the node is in that we are affecting. delay = The new node to switch delay. bandwidth = The new node to switch bandwidth. loss = The new node to switch loss. Note This command is exactly equivalent to calling each of the above three commands appropriately. See above for more information. tb-set-link-simplex-params \uf0c1 tb-set-link-simplex-params link src delay bw loss tb-set-link-simplex-params $link1 $srcnode 100ms 50Mb 0.2 Where: link = The link we are modifying. src = The source, defining which direction we are modifying. delay = The source to destination delay. bw = The source to destination bandwidth. loss = The source to destination loss. Note This commands modifies the delay characteristics of a link in a single direction. The other direction is unchanged. This command only applies to links. Use tb-set-lan-simplex-params below for LANs. tb-set-lan-simplex-params \uf0c1 tb-set-lan-simplex-params lan node todelay tobw toloss fromdelay frombw fromloss tb-set-lan-simplex-params $lan1 $node1 100ms 10Mb 0.1 5ms 100Mb 0 Where: lan = The lan we are modifying. node = The member of the lan we are modifying. todelay = Node to lan delay. tobw = Node to lan bandwidth. toloss = Node to lan loss. fromdelay = Lan to node delay. frombw = Lan to node bandwidth. fromloss = Lan to node loss. Note This command is exactly like tb-set-node-lan-params except that it allows the characteristics in each direction to be chosen separately. See all the notes for tb-set-node-lan-params . tb-set-endnodeshaping \uf0c1 tb-set-endnodeshaping link-or-lan enable tb-set-endnodeshaping $link1 1 tb-set-endnodeshaping $lan1 1 Where: link-or-lan = The link or LAN we are modifying. enable = Set to 1 to enable, 0 to disable. Note This command specifies whether end node shaping is used on the specified link or LAN (instead of a delay node). Disabled by default for all links and LANs. Only available when running the standard DETERLab FreeBSD or Linux kernels. See End Node Traffic Shaping and Multiplexed Links for more details. tb-set-noshaping \uf0c1 tb-set-noshaping link-or-lan enable tb-set-noshaping $link1 1 tb-set-noshaping $lan1 1 Where: link-or-lan = The link or LAN we are modifying. enable = Set to 1 to disable bandwidth shaping, 0 to enable. Note This command specifies whether link bandwidth shaping should be enforced on the specified link or LAN. When enabled, bandwidth limits indicated for a link or LAN will not be enforced. Disabled by default for all links and LANs. That is, link bandwidth shaping is enforced on all links and LANs by default. If the delay and loss values for a tb-set-noshaping link are zero (the default), then no delay node or end-node delay pipe will be associated with the link or LAN. This command is a hack. The primary purpose for this command is to subvert the topology mapper ( assign ). Assign always observes the physical bandwidth constraints of the testbed. By using tb-set-noshaping , you can convince assign that links are low-bandwidth and thus get your topology mapped, but then not actually have the links shaped. tb-use-endnodeshaping \uf0c1 tb-use-endnodeshaping enable tb-use-endnodeshaping 1 Where: enable = Set to 1 to enable end-node traffic shaping on all links and LANs. Note This command allows you to use end-node traffic shaping globally, without having to specify per link or LAN with tb-set-endnodeshaping . See End Node Traffic Shaping and Multiplexed Links for more details. tb-force-endnodeshaping \uf0c1 tb-force-endnodeshaping enable tb-force-endnodeshaping 1 Where: enable = Set to 1 to force end-node traffic shaping on all links and LANs. Note This command allows you to specify non-shaped links and LANs at creation time, but still control the shaping parameters later (e.g., increase delay, decrease bandwidth) after the experiment is swapped in. This command forces allocation of end-node shaping infrastructure for all links. There is no equivalent to force delay node allocation. See End Node Traffic Shaping and Multiplexed Links for more details. tb-set-multiplexed \uf0c1 tb-set-multiplexed link allow tb-set-multiplexed $link1 1 Where: link = The link we are modifying. 'allow` = Set to 1 to allow multiplexing of the link, 0 to disallow. Note This command allows a link to be multiplexed over a physical link along with other links. Disabled by default for all links. Only available when running the standard DETER FreeBSD (not Linux) and only for links (not LANs). See End Node Traffic Shaping and Multiplexed Links for more details. tb-set-vlink-emulation \uf0c1 tb-set-vlink-emulation style tb-set-vlink-emulation $link1 vlan Where: style = One of \"vlan\" or \"veth-ne\" Note It seems to be necessary to set the virtual link emulation style to vlan for multiplexed links to work under linux. Virtual Type Commands \uf0c1 Virtual Types are a method of defining fuzzy types, i.e. types that can be fulfilled by multiple different physical types. The advantage of virtual types, also known as 'vtypes ', is that all nodes of the same vtype will usually be the same physical type of node. In this way, vtypes allows logical grouping of nodes. As an example, imagine we have a network with internal routers connecting leaf nodes. We want the routers to all have the same hardware, and the leaf nodes to all have the same hardware, but the specifics do not matter. We have the following fragment in our NS file: ... tb-make-soft-vtype router {pc600 pc850} tb-make-soft-vtype leaf {pc600 pc850} tb-set-hardware $router1 router tb-set-hardware $router2 router tb-set-hardware $leaf1 leaf tb-set-hardware $leaf2 leaf Here we have set up two soft (see below) vtypes: router and leaf. Our router nodes are then specified to be of type router , and the leaf nodes of type leaf . When the experiment is swapped in, the testbed will attempt to make router1 and router2 be of the same type, and similarly, leaf1 and leaf2 of the same type. However, the routers/leafs may be pc600s or they may be pc850s, whichever is easier to fit in to the available resources. As a basic use, vtypes can be used to request nodes that are all the same type, but can be of any available type: ... tb-make-soft-vtype N {pc600 pc850} tb-set-hardware $node1 N tb-set-hardware $node2 N Vtypes come in two varieties: hard and soft. * 'Soft ' - With soft vtypes, the testbed will try to make all nodes of that vtype the same physical type, but may do otherwise if resources are tight. * 'Hard ' - Hard vtypes behave just like soft vtypes except that the testbed will give higher priority to vtype consistency and swapping in will fail if the vtypes cannot be satisfied. Therefore, if you use soft vtypes you are more likely to swap in but there is a chance your node of a specific vtype will not all be the same. If you use hard vtypes, all nodes of a given vtype will be the same, but swapping in may fail. Further, you can have weighted soft vtypes. Here you assign a weight from 0 to 1 exclusive to your vtype. The testbed will give higher priority to consistency in the higher weighted vtypes. The primary use of this is to rank multiple vtypes by importance of consistency. Soft vtypes have a weight of 0.5 by default. As a final note, when specifying the types of a vtype, use the most specific type possible. For example, the following command is not very useful: tb-make-soft-vtype router {pc pc600} This is because pc600 is a sub type of pc. You may very well end up with two routers as type pc with different hardware, as pc covers multiple types of hardware. tb-make-soft-vtype \uf0c1 tb-make-soft-vtype vtype {types} tb-make-hard-vtype vtype {types} tb-make-weighted-vtype vtype weight {types} tb-make-soft-vtype router {pc600 pc850} tb-make-hard-vtype leaf {pc600 pc850} tb-make-weighted-vtype A 0.1 {pc600 pc850} Where: vtype = The name of the vtype to create. types = One or more physical types. weight = The weight of the vtype, 0 < weight < 1. Note These commands create vtypes. See notes above for a description of vtypes and the difference between soft and hard. tb-make-soft-vtype creates vtypes with weight 0.5. vtype commands must appear before tb-set-hardware commands that use them. Do not use tb-fix-node with nodes that have a vtype. Misc. Commands \uf0c1 tb-fix-node \uf0c1 tb-fix-node vnode pnode tb-fix-node $node0 pc42 Where: vnode = The node we are fixing. pnode = The physical node we want used. Note This command forces the virtual node to be mapped to the specified physical node. Swap in will fail if this cannot be done. Do not use this command on nodes that are a virtual type. tb-fix-interface \uf0c1 tb-fix-interface vnode vlink iface tb-fix-interface $node0 $link0 \"eth0\" Where: vnode = The node we are fixing. vlink = The link connecting to that node that we want to set. iface = The DETERLab name for the interface that is to be used. Note The interface names used are the ones in the DETERLab database - we can make no guarantee that the OS image that boots on the node assigns the same name. Different types of nodes have different sets of interfaces, so this command is most useful if you are also using tb-fix-node and/or tb-set-hardware on the vnode . tb-set-uselatestwadata \uf0c1 tb-set-uselatestwadata 0 tb-set-uselatestwadata 1 Note This command indicates which widearea data to use when mapping widearea nodes to links. The default is 0, which says to use the aged data. Setting it to 1 says to use the most recent data. tb-set-wasolver-weights \uf0c1 tb-set-wasolver-weights delay bw plr tb-set-wasolver-weights 1 10 500 Where: delay = The weight to give delay when solving. bw = The weight to give bandwidth when solving. plr = The weight to give lossrate when solving. Note This command sets the relative weights to use when assigning widearea nodes to links. Specifying a zero says to ignore that particular metric when doing the assignment. Setting all three to zero results in an essentially random selection. tb-set-node-failure-action \uf0c1 tb-set-node-failure-action node action tb-set-node-failure-action $nodeA \"fatal\" tb-set-node-failure-action $nodeB \"nonfatal\" Where: node = The node name. action = One of \"fatal\" or \"nonfatal\". Note This command sets the failure mode for a node. When an experiment is swapped in, the default action is to abort the swapin if any nodes fail to come up normally. This is the \"fatal\" mode. You may also set a node to \"nonfatal\" which will cause node bootup failures to be reported, but otherwise ignored during swapin. Note that this can result in your experiment not working properly if a dependent node fails, but typically you can arrange your software to deal with this.","title":"NS commands"},{"location":"core/ns-commands/#ns-commands","text":"In order to use the testbed specific commands, you must include the following line near the top of your NS topology file (before any testbed commands are used): source tb_compat.tcl If you wish to use your file under NS, download tb_compat.tcl and place it in the same directory as your NS file. When run in this way under NS, the testbed commands will have no effect, but NS will be able to parse your file.","title":"NS Commands"},{"location":"core/ns-commands/#tcl-ns-and-node-names","text":"In your file, you will be creating nodes with something like the following line: set node1 [$ns node] With this command, the simulator, represented by $ns is creating a new node involving many internal data changes and returning a reference to it which is stored in the variable node1 . In almost all cases when you need to refer to a node, you will do it as $node1 , the $ indicating that you want the value of the variable node1 , i.e. the reference to the node. Thus you will be issuing commands like: $ns duplex-link $node1 $node2 100Mb 150ms DropTail tb-set-ip $node1 10.1.0.2 Note the instances of $ . You will notice that when your experiment is set up, the node names and such will be node1 , node2 , node3 , etc. This happens because the parser detects what variable you are using to store the node reference and uses that as the node name. In the case that you do something like: set node1 [$ns node2] set A $node1 The node will still be called node1 as that was the first variable to contain the reference. If you are dealing with many nodes you may store them in an array, using a command similar to the following: for {set i 0} {$i < 4} {incr i} { set nodes($i) [$ns node] } In this case, the names of the node will be nodes-0 , nodes-1 , nodes-2 , nodes-3 . In other words, the \"(\" character is replaced with \"-\", and \")\" is removed. This slightly different syntax is used to avoid any problems that \"()\" may cause later in the process. For example, the \"()\" characters may not appear in DNS entries. As a final note, everything said above for nodes applies equally to LANs, i.e.: set lan0 [$ns make-lan \"$node0 $node1\" 100Mb 0ms] tb-set-lan-loss $lan0 .02 Again, note the instances of $ . Links may also be named just like nodes and LANs. The names may then be used to set loss rates or IP addresses. This technique is the only way to set such attributes when there are multiple links between two nodes. set link1 [$ns duplex-link $node0 $node1 100Mb 0ms DropTail] tb-set-link-loss $link1 0.05 tb-set-ip-link $node0 $link1 10.1.0.128","title":"TCL, NS, and node names"},{"location":"core/ns-commands/#captured-ns-file-parameters","text":"A common convention when writing NS files is to place any parameters in an array named opt at the beginning of the file. For example: set opt(CLIENT_COUNT) 5 set opt(BW) 10mb; Link bandwidth set opt(LAT) 10ms; Link latency ... $ns duplex-link $server $router $opt(BW) $opt(LAT) DropTail for {set i 0} {$i < $opt(CLIENT_COUNT)} {incr i} { set nodes($i) [$ns node] ... } set serverprog [$server program-agent -command \"starter.sh\"] Normally, this convention is only used to help organize the parameters. In DETERLab, however, the contents of the opt array are captured and made available to the emulated environment. For instance, the parameters are added as environment variables to any commands run by program-agents. So in the above example of NS code, the starter.sh script will be able to reference parameters by name, like so: #! /bin/sh echo \"Testing with $CLIENT_COUNT clients.\" ... Note that the contents of the opt array are not ordered, so you should not reference other parameters and expect the shell to expand them appropriately: set opt(prefix) \"/foo/bar\" set opt(BINDIR) '$prefix/bin'; # BAD set opt(prefix) \"/foo/bar\" set opt(BINDIR) \"$opt(prefix)/bin\"; # Good","title":"Captured NS file parameters"},{"location":"core/ns-commands/#ordering-issues","text":"tb- commands have the same status as all other Tcl and NS commands. Thus the order matters not only relative to each other but also relative to other commands. One common example of this is that IP commands must be issued after the links or LANs are created.","title":"Ordering Issues"},{"location":"core/ns-commands/#hardware-commands","text":"","title":"Hardware Commands"},{"location":"core/ns-commands/#tb-set-hardware","text":"tb-set-hardware node type [args] tb-set-hardware $node3 pc tb-set-hardware $node4 shark where: node = The name of the node. type = The type of the node. Note Please see the Node Status page for a list of available types. pc is the default type. No current types have any additional arguments.","title":"tb-set-hardware"},{"location":"core/ns-commands/#ip-address-commands","text":"Each node will be assigned an IP address for each interface that is in use. The following commands will allow you to explicitly set those IP addresses. IP addresses will be automatically generated for all nodes for which you do not explicitly set IP addresses. In most cases, the IP addresses on either side of a link must be in the same subnet. Likewise, all IP addresses on a LAN should be in the same subnet. Generally the same subnet should not be used for more than one link or LAN in a given experiment, nor should one node have multiple interfaces in the same subnet. Automatically generated IP addresses will conform to these requirements. If part of a link or LAN is explicitly specified with the commands below then the remainder will be automatically generated under the same subnet. IP address assignment is deterministic and tries to fill lower IP's first, starting at 2. Except in the partial specification case (see above), all automatic IP addresses are in the network 10 .","title":"IP Address Commands"},{"location":"core/ns-commands/#tb-set-ip","text":"tb-set-ip node ip tb-set-ip $node1 142.3.4.5 where: node = The node to assign the IP address to ip = The IP address. Note This command should only be used for nodes that have a single link. For nodes with multiple links the following commands should be used. Mixing tb-set-ip and any other IP command on the same node will result in an error.","title":"tb-set-ip"},{"location":"core/ns-commands/#tb-set-ip-link","text":"tb-set-ip-link node link ip tb-set-ip-link $node0 $link0 142.3.4.6 where: node = The node to set the IP for. link = The link to set the IP for. ip = The IP address. Note One way to think of the arguments is a link with the node specifying which side of the link to set the IP for. This command cannot be mixed with tb-set-ip on the same node.","title":"tb-set-ip-link"},{"location":"core/ns-commands/#tb-set-ip-lan","text":"tb-set-ip-lan node lan ip tb-set-ip-lan $node1 $lan0 142.3.4.6 where: node = The node to set the IP for. lan = The lan the IP is on. ip = The IP address. Note One way to think of the arguments is a node with the LAN specifying which port to set the IP address for. This command cannot be mixed with tb-set-ip on the same node.","title":"tb-set-ip-lan"},{"location":"core/ns-commands/#tb-set-ip-interface","text":"tb-set-ip-interface node dst ip tb-set-ip-interface $node2 $node1 142.3.4.6 where: node = The node to set the IP for. dst = The destination of the link to set the IP for. IP = The IP address. Note This command cannot be mixed on the same node with tb-set-ip . (See above) In the case of multiple links between the same pair of nodes, there is no way to distinguish which link to the set the IP for. This should be fixed soon. This command is converted internally to either tb-set-ip-link or tb-set-ip-lan . It is possible that error messages will report either of those commands instead of tb-set-ip-interface .","title":"tb-set-ip-interface"},{"location":"core/ns-commands/#tb-set-netmask","text":"tb-set-netmask lanlink netmask tb-set-netmask $link0 \"255.255.255.248\" where: lanlink = The lan or link to set the netmask for. netmask = The netmask in dotted notation. Note This command sets the netmask for a LAN or link. The mask must be big enough to support all of the nodes on the LAN or link! You may play with the bottom three octets (0xFFFFFXXX) of the mask; attempts to change the upper octets will cause an error.","title":"tb-set-netmask"},{"location":"core/ns-commands/#os-commands","text":"","title":"OS Commands "},{"location":"core/ns-commands/#tb-set-node-os","text":"tb-set-node-os node os tb-set-node-os $node1 FBSD-STD tb-set-node-os $node1 MY_OS where: node = The node to set the OS for. os = The id of the OS for that node. Note The OSID may either by one of the standard OS's we provide or a custom OSID, created via the web interface. If no OS is specified for a node, a default OS is chosen based on the nodes type. This is currently 'Ubuntu1604-STD ' for PCs. The currently available standard OS types are listed in the OS Images page.","title":"tb-set-node-os"},{"location":"core/ns-commands/#tb-set-node-rpms","text":"tb-set-node-rpms node rpms... tb-set-node-rpms $node0 rpm1 rpm2 rpm3 Note This command sets which RPMs are to be installed on the node when it first boots after being assigned to an experiment. Each RPM can be either a path to a file or a URL. Paths must be to files that reside in the /proj or /groups directory. You are not allowed to place your RPMs in your home directory. http(s):// and ftp:// URLs will be fetched into the experiment's directory, and re-distributed from there.","title":"tb-set-node-rpms"},{"location":"core/ns-commands/#tb-set-node-startcmd","text":"tb-set-node-startcmd node startupcmd tb-set-node-startcmd $node0 \"mystart.sh -a >& /tmp/node0.log\" Note Specify a script or program to be run when the node is booted.","title":"tb-set-node-startcmd"},{"location":"core/ns-commands/#tb-set-node-cmdline","text":"tb-set-node-cmdline node cmdline tb-set-node-cmdline $node0 {???} Note Set the commandline to be passed to the kernel when it is booted. Currently, this is supported on OSKit kernels only.","title":"tb-set-node-cmdline"},{"location":"core/ns-commands/#tb-set-node-tarfiles","text":"tb-set-node-tarfiles node install-dir1 tarfile1 ... The tb-set-node-tarfiles command is used to install one or more tar files onto a node's local disk. This command is useful for installing files that are used frequently, but will change very little during the course of your experiments. For example, if your software depends on a third-party library not provided in the standard disk images, you can produce a tarball and have the library ready for use on all the experimental nodes. Another example would be the data sets for your software. The benefit of installing files using this method is that they will reside on the node's local disk, so your experimental runs will not be disrupted by NFS traffic. Note Avoid using this command if the files are changing frequently because the tars are only (re)installed when the nodes boot. Installing individual tar files or RPMs is a midpoint in the spectrum of getting software onto the experimental nodes. At one extreme, you can read everything over NFS, which works well if the files are changing constantly, but can generate a great deal of strain on the control network and disrupt your experiment. The tar files and RPMs are also read over NFS when the nodes initially boot; however, there won't be any extra NFS traffic while you are running your experiment. Finally, if you need a lot of software installed on a large number of nodes, say greater than 20, it might be best to create a custom disk image . Using a disk image is easier on the control network since it is transferred using multicast, thus greatly reducing the amount of NFS traffic when the experiment is swapped in. Required Parameters: node - The node where the files should be installed. Each node has its own tar file list, which may or may not be different from the others. One or more install-dir and tarfile pairs are then listed in the order you wish them to be installed: install-dir - An existing directory on the node where the tar file should be unarchived (e.g. / , /usr , /usr/local ). The tar command will be run as \"root\" [#tb-set-node-tarfiles Note1], so all of the node's directories will be accessible to you. If the directory does not exist on the image or was not created by the unarchiving of a previous tar file, the installation will fail [#tb-set-node-tarfiles Note2]. tarfile - An existing tar file located in a project directory (e.g. /proj or /groups ) or an http , https , or ftp URL. In the case of URLs, they are downloaded when the experiment is swapped in and cached in the experiment's directory for future use. In either case, the tar file name is required to have one of the following extensions: .tar, .tar.Z, .tar.gz, or .tgz. Note that the tar file could have been created anywhere; however, if you want the unarchived files to have valid DETERLab user and group id's, you should create the tar file on ops or an experimental node. Example usage: # Overwrite files in /bin and /sbin. tb-set-node-tarfiles $node0 /bin /proj/foo/mybinmods.tar /sbin /proj/foo/mysbinmods.tar # Programmatically generate the list of tarballs. set tb [list] # Add a tarball located on a web site. lappend tb / http://foo.bar/bazzer.tgz # Add a tarball located in the DETER NFS space. lappend tb /usr/local /proj/foo/tarfiles/bar.tar.gz # Use 'eval' to expand the 'tb' list into individual # arguments to the tb-set-node-tarfiles command. eval tb-set-node-tarfiles $node1 $tb See also: tb-set-node-rpms Custom disk images Note Because the files are installed as root, care must be taken to protect the tar file so it cannot be replaced with a trojan that allowed less privileged users to become root. Currently, you can only tell how/why an installation failed by examining the node's console log on bootup.","title":"tb-set-node-tarfiles"},{"location":"core/ns-commands/#link-loss-commands","text":"This is the NS syntax for creating a link: $ns duplex-link $node1 $node2 100Mb 150ms DropTail Note This does not allow for specifying link loss rates. DETERLab does, however, support link loss. The following commands can be used to specify link loss rates.","title":"Link Loss Commands"},{"location":"core/ns-commands/#tb-set-link-loss","text":"tb-set-link-loss src dst loss tb-set-link-loss link loss tb-set-link-loss $node1 $node2 0.05 tb-set-link-loss $link1 0.02 where: src , dst = Two nodes to describe the link. link = The link to set the rate for. loss = The loss rate (between 0 and 1). Note There are two syntaxes available. The first specifies a link by a source/destination pair. The second explicitly specifies the link. The source/destination pair is incapable of describing an individual link in the case of multiple links between two nodes. Use the second syntax for this case.","title":"tb-set-link-loss"},{"location":"core/ns-commands/#tb-set-lan-loss","text":"tb-set-lan-loss lan loss tb-set-lan-loss $lan1 0.3 Where: lan = The lan to set the loss rate for. loss = The loss rate (between 0 and 1). Note This command sets the loss rate for the entire LAN.","title":"tb-set-lan-loss"},{"location":"core/ns-commands/#tb-set-node-lan-delay","text":"tb-set-node-lan-delay node lan delay tb-set-node-lan-delay $node0 $lan0 40ms Where: node = The node we are modifying the delay for. lan = Which LAN the node is in that we are affecting. delay = The new node to switch delay (see below). Note This command changes the delay between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node, possibly incurring additional latency from any delay parameters there. If this command is not used to overwrite the delay, then the delay for a given node to switch link is taken as one half of the delay passed to make-lan . Thus in a LAN where no tb-set-node-delay calls are made, the node-to-node latency will be the latency passed to make-lan . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times. Delays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. As a convenience, a delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other.","title":"tb-set-node-lan-delay"},{"location":"core/ns-commands/#tb-set-node-lan-bandwidth","text":"tb-set-node-lan-bandwidth node lan bandwidth tb-set-node-lan-bandwidth $node0 $lan0 20Mb Where: node = The node we are modifying the bandwidth for. lan = Which LAN the node is in that we are affecting. bandwidth = The new node to switch bandwidth (see below). Note This command changes the bandwidth between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may have a lower bandwidth. If this command is not used to overwrite the bandwidth, then the bandwidth for a given node to switch link is taken directly from the bandwidth passed to make-lan . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times.","title":"tb-set-node-lan-bandwidth"},{"location":"core/ns-commands/#tb-set-node-lan-loss","text":"tb-set-node-lan-loss node lan loss tb-set-node-lan-loss $node0 $lan0 0.05 Where: node = The node we are modifying the loss for. lan = Which LAN the node is in that we are affecting. loss = The new node to switch loss (see below). Note This command changes the loss probability between the node and the switch of the LAN. This is only half of the trip a packet must take. The packet will also traverse the switch to the destination node which may also have a loss chance. Thus for packet going to switch with loss chance A and then going on the destination with loss chance B , the node-to-node loss chance is (1-(1-A)(1-B)) . If this command is not used to overwrite the loss, then the loss for a given node to switch link is taken from the loss rate passed to the make-lan command. If a loss rate of L is passed to make-lan then the node to switch loss rate for each node is set to (1-sqrt(1-L)) . Because each packet will have two such chances to be lost, the node-to-loss rate comes out as the desired L . The behavior of this command is not defined when used with nodes that are in the same LAN multiple times.","title":"tb-set-node-lan-loss"},{"location":"core/ns-commands/#tb-set-node-lan-params","text":"tb-set-node-lan-params node lan delay bandwidth loss tb-set-node-lan-params $node0 $lan0 40ms 20Mb 0.05 Where: node = The node we are modifying the loss for. lan = Which LAN the node is in that we are affecting. delay = The new node to switch delay. bandwidth = The new node to switch bandwidth. loss = The new node to switch loss. Note This command is exactly equivalent to calling each of the above three commands appropriately. See above for more information.","title":"tb-set-node-lan-params"},{"location":"core/ns-commands/#tb-set-link-simplex-params","text":"tb-set-link-simplex-params link src delay bw loss tb-set-link-simplex-params $link1 $srcnode 100ms 50Mb 0.2 Where: link = The link we are modifying. src = The source, defining which direction we are modifying. delay = The source to destination delay. bw = The source to destination bandwidth. loss = The source to destination loss. Note This commands modifies the delay characteristics of a link in a single direction. The other direction is unchanged. This command only applies to links. Use tb-set-lan-simplex-params below for LANs.","title":"tb-set-link-simplex-params"},{"location":"core/ns-commands/#tb-set-lan-simplex-params","text":"tb-set-lan-simplex-params lan node todelay tobw toloss fromdelay frombw fromloss tb-set-lan-simplex-params $lan1 $node1 100ms 10Mb 0.1 5ms 100Mb 0 Where: lan = The lan we are modifying. node = The member of the lan we are modifying. todelay = Node to lan delay. tobw = Node to lan bandwidth. toloss = Node to lan loss. fromdelay = Lan to node delay. frombw = Lan to node bandwidth. fromloss = Lan to node loss. Note This command is exactly like tb-set-node-lan-params except that it allows the characteristics in each direction to be chosen separately. See all the notes for tb-set-node-lan-params .","title":"tb-set-lan-simplex-params"},{"location":"core/ns-commands/#tb-set-endnodeshaping","text":"tb-set-endnodeshaping link-or-lan enable tb-set-endnodeshaping $link1 1 tb-set-endnodeshaping $lan1 1 Where: link-or-lan = The link or LAN we are modifying. enable = Set to 1 to enable, 0 to disable. Note This command specifies whether end node shaping is used on the specified link or LAN (instead of a delay node). Disabled by default for all links and LANs. Only available when running the standard DETERLab FreeBSD or Linux kernels. See End Node Traffic Shaping and Multiplexed Links for more details.","title":"tb-set-endnodeshaping"},{"location":"core/ns-commands/#tb-set-noshaping","text":"tb-set-noshaping link-or-lan enable tb-set-noshaping $link1 1 tb-set-noshaping $lan1 1 Where: link-or-lan = The link or LAN we are modifying. enable = Set to 1 to disable bandwidth shaping, 0 to enable. Note This command specifies whether link bandwidth shaping should be enforced on the specified link or LAN. When enabled, bandwidth limits indicated for a link or LAN will not be enforced. Disabled by default for all links and LANs. That is, link bandwidth shaping is enforced on all links and LANs by default. If the delay and loss values for a tb-set-noshaping link are zero (the default), then no delay node or end-node delay pipe will be associated with the link or LAN. This command is a hack. The primary purpose for this command is to subvert the topology mapper ( assign ). Assign always observes the physical bandwidth constraints of the testbed. By using tb-set-noshaping , you can convince assign that links are low-bandwidth and thus get your topology mapped, but then not actually have the links shaped.","title":"tb-set-noshaping"},{"location":"core/ns-commands/#tb-use-endnodeshaping","text":"tb-use-endnodeshaping enable tb-use-endnodeshaping 1 Where: enable = Set to 1 to enable end-node traffic shaping on all links and LANs. Note This command allows you to use end-node traffic shaping globally, without having to specify per link or LAN with tb-set-endnodeshaping . See End Node Traffic Shaping and Multiplexed Links for more details.","title":"tb-use-endnodeshaping"},{"location":"core/ns-commands/#tb-force-endnodeshaping","text":"tb-force-endnodeshaping enable tb-force-endnodeshaping 1 Where: enable = Set to 1 to force end-node traffic shaping on all links and LANs. Note This command allows you to specify non-shaped links and LANs at creation time, but still control the shaping parameters later (e.g., increase delay, decrease bandwidth) after the experiment is swapped in. This command forces allocation of end-node shaping infrastructure for all links. There is no equivalent to force delay node allocation. See End Node Traffic Shaping and Multiplexed Links for more details.","title":"tb-force-endnodeshaping"},{"location":"core/ns-commands/#tb-set-multiplexed","text":"tb-set-multiplexed link allow tb-set-multiplexed $link1 1 Where: link = The link we are modifying. 'allow` = Set to 1 to allow multiplexing of the link, 0 to disallow. Note This command allows a link to be multiplexed over a physical link along with other links. Disabled by default for all links. Only available when running the standard DETER FreeBSD (not Linux) and only for links (not LANs). See End Node Traffic Shaping and Multiplexed Links for more details.","title":"tb-set-multiplexed"},{"location":"core/ns-commands/#tb-set-vlink-emulation","text":"tb-set-vlink-emulation style tb-set-vlink-emulation $link1 vlan Where: style = One of \"vlan\" or \"veth-ne\" Note It seems to be necessary to set the virtual link emulation style to vlan for multiplexed links to work under linux.","title":"tb-set-vlink-emulation"},{"location":"core/ns-commands/#virtual-type-commands","text":"Virtual Types are a method of defining fuzzy types, i.e. types that can be fulfilled by multiple different physical types. The advantage of virtual types, also known as 'vtypes ', is that all nodes of the same vtype will usually be the same physical type of node. In this way, vtypes allows logical grouping of nodes. As an example, imagine we have a network with internal routers connecting leaf nodes. We want the routers to all have the same hardware, and the leaf nodes to all have the same hardware, but the specifics do not matter. We have the following fragment in our NS file: ... tb-make-soft-vtype router {pc600 pc850} tb-make-soft-vtype leaf {pc600 pc850} tb-set-hardware $router1 router tb-set-hardware $router2 router tb-set-hardware $leaf1 leaf tb-set-hardware $leaf2 leaf Here we have set up two soft (see below) vtypes: router and leaf. Our router nodes are then specified to be of type router , and the leaf nodes of type leaf . When the experiment is swapped in, the testbed will attempt to make router1 and router2 be of the same type, and similarly, leaf1 and leaf2 of the same type. However, the routers/leafs may be pc600s or they may be pc850s, whichever is easier to fit in to the available resources. As a basic use, vtypes can be used to request nodes that are all the same type, but can be of any available type: ... tb-make-soft-vtype N {pc600 pc850} tb-set-hardware $node1 N tb-set-hardware $node2 N Vtypes come in two varieties: hard and soft. * 'Soft ' - With soft vtypes, the testbed will try to make all nodes of that vtype the same physical type, but may do otherwise if resources are tight. * 'Hard ' - Hard vtypes behave just like soft vtypes except that the testbed will give higher priority to vtype consistency and swapping in will fail if the vtypes cannot be satisfied. Therefore, if you use soft vtypes you are more likely to swap in but there is a chance your node of a specific vtype will not all be the same. If you use hard vtypes, all nodes of a given vtype will be the same, but swapping in may fail. Further, you can have weighted soft vtypes. Here you assign a weight from 0 to 1 exclusive to your vtype. The testbed will give higher priority to consistency in the higher weighted vtypes. The primary use of this is to rank multiple vtypes by importance of consistency. Soft vtypes have a weight of 0.5 by default. As a final note, when specifying the types of a vtype, use the most specific type possible. For example, the following command is not very useful: tb-make-soft-vtype router {pc pc600} This is because pc600 is a sub type of pc. You may very well end up with two routers as type pc with different hardware, as pc covers multiple types of hardware.","title":"Virtual Type Commands"},{"location":"core/ns-commands/#tb-make-soft-vtype","text":"tb-make-soft-vtype vtype {types} tb-make-hard-vtype vtype {types} tb-make-weighted-vtype vtype weight {types} tb-make-soft-vtype router {pc600 pc850} tb-make-hard-vtype leaf {pc600 pc850} tb-make-weighted-vtype A 0.1 {pc600 pc850} Where: vtype = The name of the vtype to create. types = One or more physical types. weight = The weight of the vtype, 0 < weight < 1. Note These commands create vtypes. See notes above for a description of vtypes and the difference between soft and hard. tb-make-soft-vtype creates vtypes with weight 0.5. vtype commands must appear before tb-set-hardware commands that use them. Do not use tb-fix-node with nodes that have a vtype.","title":"tb-make-soft-vtype"},{"location":"core/ns-commands/#misc-commands","text":"","title":"Misc. Commands"},{"location":"core/ns-commands/#tb-fix-node","text":"tb-fix-node vnode pnode tb-fix-node $node0 pc42 Where: vnode = The node we are fixing. pnode = The physical node we want used. Note This command forces the virtual node to be mapped to the specified physical node. Swap in will fail if this cannot be done. Do not use this command on nodes that are a virtual type.","title":"tb-fix-node"},{"location":"core/ns-commands/#tb-fix-interface","text":"tb-fix-interface vnode vlink iface tb-fix-interface $node0 $link0 \"eth0\" Where: vnode = The node we are fixing. vlink = The link connecting to that node that we want to set. iface = The DETERLab name for the interface that is to be used. Note The interface names used are the ones in the DETERLab database - we can make no guarantee that the OS image that boots on the node assigns the same name. Different types of nodes have different sets of interfaces, so this command is most useful if you are also using tb-fix-node and/or tb-set-hardware on the vnode .","title":"tb-fix-interface"},{"location":"core/ns-commands/#tb-set-uselatestwadata","text":"tb-set-uselatestwadata 0 tb-set-uselatestwadata 1 Note This command indicates which widearea data to use when mapping widearea nodes to links. The default is 0, which says to use the aged data. Setting it to 1 says to use the most recent data.","title":"tb-set-uselatestwadata"},{"location":"core/ns-commands/#tb-set-wasolver-weights","text":"tb-set-wasolver-weights delay bw plr tb-set-wasolver-weights 1 10 500 Where: delay = The weight to give delay when solving. bw = The weight to give bandwidth when solving. plr = The weight to give lossrate when solving. Note This command sets the relative weights to use when assigning widearea nodes to links. Specifying a zero says to ignore that particular metric when doing the assignment. Setting all three to zero results in an essentially random selection.","title":"tb-set-wasolver-weights"},{"location":"core/ns-commands/#tb-set-node-failure-action","text":"tb-set-node-failure-action node action tb-set-node-failure-action $nodeA \"fatal\" tb-set-node-failure-action $nodeB \"nonfatal\" Where: node = The node name. action = One of \"fatal\" or \"nonfatal\". Note This command sets the failure mode for a node. When an experiment is swapped in, the default action is to abort the swapin if any nodes fail to come up normally. This is the \"fatal\" mode. You may also set a node to \"nonfatal\" which will cause node bootup failures to be reported, but otherwise ignored during swapin. Note that this can result in your experiment not working properly if a dependent node fails, but typically you can arrange your software to deal with this.","title":"tb-set-node-failure-action"},{"location":"core/os-images/","text":"Operating System Images \uf0c1 Here is the list of currently supported DETERLab operating system images. If you have a DETERLab account, you can view the most updated information as well as statistics on each machine on the OSID page on the testbed. Supported OS Images as of 01/19/2017 \uf0c1 Name OS Description FBSD10-STD FreeBSD FreeBSD 10.x Standard CentOS6-64-STD Linux CentOS6 64-Bit image CentOS7-STD Linux CentOS7 64 bit KALI-RLG Linux Kali 2016.1 Penetration Testing Metasploitable2 Linux An intentionally vulnerable system Ubuntu1404-32-STD Linux Ubuntu 14.04 LTS 32 bit Standard Image Ubuntu1404-64-STD Linux Ubuntu 14.04 LTS 64 bit Standard Image Ubuntu1604-STD Linux Ubuntu 16.04 LTS 64 bit Standard Image Updates for Custom Images \uf0c1 Updating Linux images made before Jan 25, 2013 \uf0c1 We made a change to make mounting NFS home directories more robust. You may update your custom images by running: sudo curl --output /usr/local/etc/emulab/liblocsetup.pm boss.isi.deterlab.net/downloads/client-update/linux-liblocsetup.pm sudo chmod a+rx /usr/local/etc/emulab/liblocsetup.pm and taking a snapshot. On CentOS-6-64-STD you must to install Time::HiRes by running: sudo yum install perl-Time-HiRes","title":"Operating System Images"},{"location":"core/os-images/#operating-system-images","text":"Here is the list of currently supported DETERLab operating system images. If you have a DETERLab account, you can view the most updated information as well as statistics on each machine on the OSID page on the testbed.","title":"Operating System Images"},{"location":"core/os-images/#supported-os-images-as-of-01192017","text":"Name OS Description FBSD10-STD FreeBSD FreeBSD 10.x Standard CentOS6-64-STD Linux CentOS6 64-Bit image CentOS7-STD Linux CentOS7 64 bit KALI-RLG Linux Kali 2016.1 Penetration Testing Metasploitable2 Linux An intentionally vulnerable system Ubuntu1404-32-STD Linux Ubuntu 14.04 LTS 32 bit Standard Image Ubuntu1404-64-STD Linux Ubuntu 14.04 LTS 64 bit Standard Image Ubuntu1604-STD Linux Ubuntu 16.04 LTS 64 bit Standard Image","title":"Supported OS Images as of 01/19/2017"},{"location":"core/os-images/#updates-for-custom-images","text":"","title":"Updates for Custom Images"},{"location":"core/os-images/#updating-linux-images-made-before-jan-25-2013","text":"We made a change to make mounting NFS home directories more robust. You may update your custom images by running: sudo curl --output /usr/local/etc/emulab/liblocsetup.pm boss.isi.deterlab.net/downloads/client-update/linux-liblocsetup.pm sudo chmod a+rx /usr/local/etc/emulab/liblocsetup.pm and taking a snapshot. On CentOS-6-64-STD you must to install Time::HiRes by running: sudo yum install perl-Time-HiRes","title":"Updating Linux images made before Jan 25, 2013"},{"location":"core/risky-experiment/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Risky Experiment Support \uf0c1 We define as a risky experiment each experiment that either uses some type of malware such as DoS tool, a worm, an exploit, etc, even when the malware code is written by the experimenter and/or requires connectivity to the outside directly from experimental nodes. We recognize all these experiment types are interesting and important to facilitate new research in security. At the same time there is potential risk to the testbed and the Internet that must be contained. This risk includes: Malware interfering with other experiments on DETER Malware escaping to the outside world Malware overwhelming critical infrastructure in DETER, such as users and boss machines and control network Malware from the outside world infecting experimental machines and spreading in DETER or propagating back to the outside (with implications of DETER unwittingly participating in attacks). We have developed strategies to contain experiment risk while allowing users to observe phenomena of interest to them. This means that containment is customized for each experiment. This customization is performed automatically depending on information a user specifies on the \"Begin Experiment Web\" page. Important A \"risky experiment\" in DETERLab is a heavyweight mechanism. We request that you only implement it if other approaches, such as SSH tunnels, do not work. Please file a ticket (you must log in to Trac with your DETERLab username and password) if you run into any problems. This code is still being developed and we welcome your feedback. Who Is Eligible to Run Risky Experiments? \uf0c1 To be able to create and run risky experiments you must first request permission from DETERLab. Please file a ticket with your project name, a description of your experiment and its risk. We may need to exchange a few emails with you before granting permission to use this feature. Attempts to create risky experiments without approval of DETER staff will fail. Specifying Risk and Connectivity Options \uf0c1 This section explains how to specify risk and connectivity options and what effect this will have on your experiment. Note Many containment strategies require automated modification of the experiment's NS file. We can currently only parse \"simple\" NS files, i.e. those that specify each node's features separately rather than using \"foreach\" option. We expect to extend our parsing ability in the near future. parsable.ns\u200b is a sample file we can parse, and unparsable.ns\u200b is the same file in a version we cannot parse yet. Further, it is unclear yet how containment works with NS options such as traffic generation via NS file, use of templates, use of Planetlab nodes, etc. Configuring for Malware \uf0c1 This section describes which options to check for malware. Experiment uses live malware If your experiment uses any type of malware such as DoS, worm code, exploit code, scanning code, etc. even if it is written by you, please check this box. You are not likely to experience any adverse effects if this box is checked. If you need no outside connectivity we may not take any special containment action, in addition to those already in place for all experiments. If you need outside connectivity we may log your traffic or filter it based on malware signatures to ensure no malware traffic escapes DETER. The type of containment will depend on your connectivity selections, and on type of malware you are using. Again, you are unlikely to experience any adverse effects from our actions. Malware self-propagates? Check this box if you use malware such as worms that can spread by itself, once launched, through a set of vulnerable nodes without human action. Also check this box if you use email or other types of viruses that spread when a human accesses them through a benign application (e.g. human opens an email, human opens an IM attachment with the virus, etc). Checking this box will increase DETER's monitoring and filtering of traffic generated by your experiment to the control net and the outside. Type of malware. This selection helps us customize our monitoring and filtering to a specific malware you are using. We expect this list to grow in the future. If no selection fits malware you are using, please select Unknown. Connectivity \uf0c1 Experiment needs outside connectivity. Check this box if you require the ability to initiate conversations with the Internet from your experiment. Connectivity will be provided by adding a special type of node, called tunnel to your topology. We will add this node automatically. Its name - tunnel - becomes a keyword so no other nodes in your topology can be named tunnel if this box is selected. The tunnel node will be linked to a random, well-connected node in your topology. We will also automatically set up routing on your nodes so they know they reach outside via the tunnel node. You will be able to see the added code in the NS file. Do not change it, otherwise you are likely to lose connectivity. You can change other parts of your NS file at will, including changing a link that connects the tunnel node with the topology to reconnect it to another experimental node. Do not create additional links to the tunnel node, this likely won't work. If at some point you do not need connectivity anymore, visit the Modify Experiment page and uncheck this box. Note You will never get an unlimited outside connectivity. Instead we install a firewall on a tunnel node and open communication between your experiment and remote IPs you have specified on fine-grain basis. The rest of the form allows you to specify allowed communication patterns. If you don't know which nodes you will need to talk to at experiment creation time please file a ticket and explain to us your experiment design. We'll work together with you to find a safe way to support your experiment. Select type of communication with the outside. If you plan to communicate with the Internet for benign purposes (e.g., to get rpms, to contact public Web servers in a benign way, to let others talk to a Web server you have set up in DETER) you would select the benign option. If you plan to let malware generate traffic to the outside or you plan to attract malware from the outside select Malware-generated. If you use multiple traffic generators that fall into different categories, select Mixed. Type of communication you select will have direct influence on the level of our monitoring and filtering on tunnel nodes. Names and ports of experiment nodes that receive outside connections. If you plan to set up a server on some of your experimental nodes that will receive communication from the outside specify the nodes, destination ports and protocols here. Names of the nodes are names from your NS file. Only single protocols, not protocol ranges, can be specified. Supported protocols are tcp and udp. This communication is NAT-ed via the tunnel node, thus remote clients will need to contact you on a specific, high-numbered port. You will be notified of NAT specifics in an email once you swap your experiment in. This information can also be viewed on your experiment's Web page. For example, let a node n1 in your topology have IP address 1.2.3.4 and wants to run a public Web server. You would specify n1/80/tcp in the textbox for experiment nodes that receive outside connections. NAT may map this into 5.6.7.8 port 10001. Remote clients would then type \u200bhttp://5.6.7.8:10001 to access your Web server. If this is confusing please file a ticket for more information. IPs and ports of outside nodes that receive connections from the experiment. If you need to contact remote servers from your experimental nodes (e.g., to send some statistics to a server in your institution) specify the IPs, destination ports and protocols here. Same restrictions for ports (single numbers, not ranges) and protocols (tcp and udp) apply. Note that a lot of public servers have multiple IP addresses. If you want to contact such servers (e.g. Google) you will need to list all IP addresses to ensure that when you try to access them via their URL, the corresponding IP is on our list of allowed remote hosts. Otherwise you could list only one IP for such public servers and contact them using this IP address and not the URL (e.g., you could type 74.125.19.103 instead of www.google.com in your browser's address bar and this would guarantee that you always go to this one specific Google server). Logging on Tunnel Nodes \uf0c1 For security reasons, all prohibited traffic on tunnel nodes is logged. This means that traffic you specifically allowed via our forms for experiment creation and modification will not be logged but any other traffic to/from tunnel nodes and any other transit traffic will be logged. Accessing Tunnel Nodes \uf0c1 For security reasons, users do not have sudoer access to tunnel nodes. Rebooting and Modifying Experiments with Tunnel Nodes \uf0c1 You can safely reboot tunnel nodes if you want - they should come back up with all the correct settings. If you reboot any other nodes in your experiment you will need to set up routing to the outside. You can do this by logging on the rebooted nodes and running /share/t1t2/set_route if you have sudo privileges on that node. If you run into problems please contact us and we will help restore the connectivity. The safest way to change something in your experiment is via Modify Experiment Web page. This should properly set up all the routes, firewall rules, etc. in your experiment. You will not be allowed to modify tunnel node's settings such as OS image and name but you can modify where this node links with your topology. If you need to add some start commands on experiment nodes (except the tunnel node) the best way is to copy the commands we have automatically inserted into your startup script, add commands you want, then replace the start-cmd options in the NS file with the name of your startup script. If you mess up startcmd options in the NS file routing on experiment nodes will not be set up properly. To amend the problem you can delete the entire \"tunnelcode\" section of the NS file, make sure that the connectivity box is checked and run Modify Experiment and the proper code for connectivity will be automatically inserted.","title":"Risky Experiments"},{"location":"core/risky-experiment/#risky-experiment-support","text":"We define as a risky experiment each experiment that either uses some type of malware such as DoS tool, a worm, an exploit, etc, even when the malware code is written by the experimenter and/or requires connectivity to the outside directly from experimental nodes. We recognize all these experiment types are interesting and important to facilitate new research in security. At the same time there is potential risk to the testbed and the Internet that must be contained. This risk includes: Malware interfering with other experiments on DETER Malware escaping to the outside world Malware overwhelming critical infrastructure in DETER, such as users and boss machines and control network Malware from the outside world infecting experimental machines and spreading in DETER or propagating back to the outside (with implications of DETER unwittingly participating in attacks). We have developed strategies to contain experiment risk while allowing users to observe phenomena of interest to them. This means that containment is customized for each experiment. This customization is performed automatically depending on information a user specifies on the \"Begin Experiment Web\" page. Important A \"risky experiment\" in DETERLab is a heavyweight mechanism. We request that you only implement it if other approaches, such as SSH tunnels, do not work. Please file a ticket (you must log in to Trac with your DETERLab username and password) if you run into any problems. This code is still being developed and we welcome your feedback.","title":"Risky Experiment Support"},{"location":"core/risky-experiment/#who-is-eligible-to-run-risky-experiments","text":"To be able to create and run risky experiments you must first request permission from DETERLab. Please file a ticket with your project name, a description of your experiment and its risk. We may need to exchange a few emails with you before granting permission to use this feature. Attempts to create risky experiments without approval of DETER staff will fail.","title":"Who Is Eligible to Run Risky Experiments?"},{"location":"core/risky-experiment/#specifying-risk-and-connectivity-options","text":"This section explains how to specify risk and connectivity options and what effect this will have on your experiment. Note Many containment strategies require automated modification of the experiment's NS file. We can currently only parse \"simple\" NS files, i.e. those that specify each node's features separately rather than using \"foreach\" option. We expect to extend our parsing ability in the near future. parsable.ns\u200b is a sample file we can parse, and unparsable.ns\u200b is the same file in a version we cannot parse yet. Further, it is unclear yet how containment works with NS options such as traffic generation via NS file, use of templates, use of Planetlab nodes, etc.","title":"Specifying Risk and Connectivity Options"},{"location":"core/risky-experiment/#configuring-for-malware","text":"This section describes which options to check for malware. Experiment uses live malware If your experiment uses any type of malware such as DoS, worm code, exploit code, scanning code, etc. even if it is written by you, please check this box. You are not likely to experience any adverse effects if this box is checked. If you need no outside connectivity we may not take any special containment action, in addition to those already in place for all experiments. If you need outside connectivity we may log your traffic or filter it based on malware signatures to ensure no malware traffic escapes DETER. The type of containment will depend on your connectivity selections, and on type of malware you are using. Again, you are unlikely to experience any adverse effects from our actions. Malware self-propagates? Check this box if you use malware such as worms that can spread by itself, once launched, through a set of vulnerable nodes without human action. Also check this box if you use email or other types of viruses that spread when a human accesses them through a benign application (e.g. human opens an email, human opens an IM attachment with the virus, etc). Checking this box will increase DETER's monitoring and filtering of traffic generated by your experiment to the control net and the outside. Type of malware. This selection helps us customize our monitoring and filtering to a specific malware you are using. We expect this list to grow in the future. If no selection fits malware you are using, please select Unknown.","title":"Configuring for Malware"},{"location":"core/risky-experiment/#connectivity","text":"Experiment needs outside connectivity. Check this box if you require the ability to initiate conversations with the Internet from your experiment. Connectivity will be provided by adding a special type of node, called tunnel to your topology. We will add this node automatically. Its name - tunnel - becomes a keyword so no other nodes in your topology can be named tunnel if this box is selected. The tunnel node will be linked to a random, well-connected node in your topology. We will also automatically set up routing on your nodes so they know they reach outside via the tunnel node. You will be able to see the added code in the NS file. Do not change it, otherwise you are likely to lose connectivity. You can change other parts of your NS file at will, including changing a link that connects the tunnel node with the topology to reconnect it to another experimental node. Do not create additional links to the tunnel node, this likely won't work. If at some point you do not need connectivity anymore, visit the Modify Experiment page and uncheck this box. Note You will never get an unlimited outside connectivity. Instead we install a firewall on a tunnel node and open communication between your experiment and remote IPs you have specified on fine-grain basis. The rest of the form allows you to specify allowed communication patterns. If you don't know which nodes you will need to talk to at experiment creation time please file a ticket and explain to us your experiment design. We'll work together with you to find a safe way to support your experiment. Select type of communication with the outside. If you plan to communicate with the Internet for benign purposes (e.g., to get rpms, to contact public Web servers in a benign way, to let others talk to a Web server you have set up in DETER) you would select the benign option. If you plan to let malware generate traffic to the outside or you plan to attract malware from the outside select Malware-generated. If you use multiple traffic generators that fall into different categories, select Mixed. Type of communication you select will have direct influence on the level of our monitoring and filtering on tunnel nodes. Names and ports of experiment nodes that receive outside connections. If you plan to set up a server on some of your experimental nodes that will receive communication from the outside specify the nodes, destination ports and protocols here. Names of the nodes are names from your NS file. Only single protocols, not protocol ranges, can be specified. Supported protocols are tcp and udp. This communication is NAT-ed via the tunnel node, thus remote clients will need to contact you on a specific, high-numbered port. You will be notified of NAT specifics in an email once you swap your experiment in. This information can also be viewed on your experiment's Web page. For example, let a node n1 in your topology have IP address 1.2.3.4 and wants to run a public Web server. You would specify n1/80/tcp in the textbox for experiment nodes that receive outside connections. NAT may map this into 5.6.7.8 port 10001. Remote clients would then type \u200bhttp://5.6.7.8:10001 to access your Web server. If this is confusing please file a ticket for more information. IPs and ports of outside nodes that receive connections from the experiment. If you need to contact remote servers from your experimental nodes (e.g., to send some statistics to a server in your institution) specify the IPs, destination ports and protocols here. Same restrictions for ports (single numbers, not ranges) and protocols (tcp and udp) apply. Note that a lot of public servers have multiple IP addresses. If you want to contact such servers (e.g. Google) you will need to list all IP addresses to ensure that when you try to access them via their URL, the corresponding IP is on our list of allowed remote hosts. Otherwise you could list only one IP for such public servers and contact them using this IP address and not the URL (e.g., you could type 74.125.19.103 instead of www.google.com in your browser's address bar and this would guarantee that you always go to this one specific Google server).","title":"Connectivity"},{"location":"core/risky-experiment/#logging-on-tunnel-nodes","text":"For security reasons, all prohibited traffic on tunnel nodes is logged. This means that traffic you specifically allowed via our forms for experiment creation and modification will not be logged but any other traffic to/from tunnel nodes and any other transit traffic will be logged.","title":"Logging on Tunnel Nodes"},{"location":"core/risky-experiment/#accessing-tunnel-nodes","text":"For security reasons, users do not have sudoer access to tunnel nodes.","title":"Accessing Tunnel Nodes"},{"location":"core/risky-experiment/#rebooting-and-modifying-experiments-with-tunnel-nodes","text":"You can safely reboot tunnel nodes if you want - they should come back up with all the correct settings. If you reboot any other nodes in your experiment you will need to set up routing to the outside. You can do this by logging on the rebooted nodes and running /share/t1t2/set_route if you have sudo privileges on that node. If you run into problems please contact us and we will help restore the connectivity. The safest way to change something in your experiment is via Modify Experiment Web page. This should properly set up all the routes, firewall rules, etc. in your experiment. You will not be allowed to modify tunnel node's settings such as OS image and name but you can modify where this node links with your topology. If you need to add some start commands on experiment nodes (except the tunnel node) the best way is to copy the commands we have automatically inserted into your startup script, add commands you want, then replace the start-cmd options in the NS file with the name of your startup script. If you mess up startcmd options in the NS file routing on experiment nodes will not be set up properly. To amend the problem you can delete the entire \"tunnelcode\" section of the NS file, make sure that the connectivity box is checked and run Modify Experiment and the proper code for connectivity will be automatically inserted.","title":"Rebooting and Modifying Experiments with Tunnel Nodes"},{"location":"core/routing/","text":"Setting up IP routing between nodes \uf0c1 As DETERLab strives to make all aspects of the network controllable by the user, we do not attempt to impose any IP routing architecture or protocol by default. However, many users are more interested in end-to-end aspects and don't want to be bothered with setting up routes. For those users we provide an option to automatically set up routes on nodes. You can use the NS rtproto syntax in your NS file to enable routing: $ns rtproto protocolOption where the protocolOption is limited to one of Session , Static or Manual . Session routing provides fully automated routing support, and is implemented by enabling gated running of the OSPF protocol on all nodes in the experiment. This is not supported on Windows XP nodes. Static routing also provides automatic routing support, but rather than computing the routes dynamically, the routes are precomputed by a distributed route computation algorithm running in parallel on the experiment nodes. Manual routing allows you to explicitly specify per-node routing information in the NS file. To do this, use the Manual routing option to rtproto , followed by a list of routes using the add-route command: $node add-route $dst $nexthop where the dst can be either a node, a link, or a LAN. For example: $client add-route $server $router $client add-route [$ns link $server $router] $router $client add-route $serverlan $router Note that you would need a separate add-route command to establish a route for the reverse direction; thus allowing you to specify differing forward and reverse routes if so desired. These statements are converted into appropriate route(8) commands on your experimental nodes when they boot. In the above examples, the first form says to set up a manual route between $client and $server , using $router as the nexthop; $client and $router should be directly connected, and the interface on $server should be unambiguous; either directly connected to the router, or an edge node that has just a single interface. If the destination has multiple interfaces configured, and it is not connected directly to the nexthop, the interface that you are intending to route to is ambiguous. In the topology shown to the right, $nodeD has two interfaces configured. If you attempted to set up a route like this: $nodeA add-route $nodeD $nodeB you would receive an error since DETERLab staff would not easily be able to determine which of the two links on $nodeD you are referring to. Fortunately, there is an easy solution. Instead of a node, specify the link directly: $nodeA add-route [$ns link $nodeD $nodeC] $nodeB This tells us exactly which link you mean, enabling us to convert that information into a proper route command on $nodeA . The last form of the add-route command is used when adding a route to an entire LAN. It would be tedious and error prone to specify a route to each node in a LAN by hand. Instead, just route to the entire network: set clientlan [$ns make-lan \"$nodeE $nodeF $nodeG\" 100Mb 0ms] $nodeA add-route $clientlan $nodeB In general, it is still best practice to use either Session or Static routing for all but small, simple topologies. Explicitly setting up all the routes in even a moderately-sized experiment is extremely error prone. Consider this: a recently created experiment with 17 nodes and 10 subnets required 140 hand-created routes in the NS file . Two final, cautionary notes on routing: The default route must be set to use the control network interface. You might be tempted to set the default route on your nodes to reduce the number of explicit routes used. Please avoid this. That would prevent nodes from contacting the outside world, i.e., you. If you use your own routing daemon, you must avoid using the control network interface in the configuration. Since every node in the testbed is directly connected to the control network LAN, a naive routing daemon configuration will discover that any node is just one hop away, via the control network, from any other node and all inter-node traffic will be routed via that interface.","title":"Setting up IP routing between nodes"},{"location":"core/routing/#setting-up-ip-routing-between-nodes","text":"As DETERLab strives to make all aspects of the network controllable by the user, we do not attempt to impose any IP routing architecture or protocol by default. However, many users are more interested in end-to-end aspects and don't want to be bothered with setting up routes. For those users we provide an option to automatically set up routes on nodes. You can use the NS rtproto syntax in your NS file to enable routing: $ns rtproto protocolOption where the protocolOption is limited to one of Session , Static or Manual . Session routing provides fully automated routing support, and is implemented by enabling gated running of the OSPF protocol on all nodes in the experiment. This is not supported on Windows XP nodes. Static routing also provides automatic routing support, but rather than computing the routes dynamically, the routes are precomputed by a distributed route computation algorithm running in parallel on the experiment nodes. Manual routing allows you to explicitly specify per-node routing information in the NS file. To do this, use the Manual routing option to rtproto , followed by a list of routes using the add-route command: $node add-route $dst $nexthop where the dst can be either a node, a link, or a LAN. For example: $client add-route $server $router $client add-route [$ns link $server $router] $router $client add-route $serverlan $router Note that you would need a separate add-route command to establish a route for the reverse direction; thus allowing you to specify differing forward and reverse routes if so desired. These statements are converted into appropriate route(8) commands on your experimental nodes when they boot. In the above examples, the first form says to set up a manual route between $client and $server , using $router as the nexthop; $client and $router should be directly connected, and the interface on $server should be unambiguous; either directly connected to the router, or an edge node that has just a single interface. If the destination has multiple interfaces configured, and it is not connected directly to the nexthop, the interface that you are intending to route to is ambiguous. In the topology shown to the right, $nodeD has two interfaces configured. If you attempted to set up a route like this: $nodeA add-route $nodeD $nodeB you would receive an error since DETERLab staff would not easily be able to determine which of the two links on $nodeD you are referring to. Fortunately, there is an easy solution. Instead of a node, specify the link directly: $nodeA add-route [$ns link $nodeD $nodeC] $nodeB This tells us exactly which link you mean, enabling us to convert that information into a proper route command on $nodeA . The last form of the add-route command is used when adding a route to an entire LAN. It would be tedious and error prone to specify a route to each node in a LAN by hand. Instead, just route to the entire network: set clientlan [$ns make-lan \"$nodeE $nodeF $nodeG\" 100Mb 0ms] $nodeA add-route $clientlan $nodeB In general, it is still best practice to use either Session or Static routing for all but small, simple topologies. Explicitly setting up all the routes in even a moderately-sized experiment is extremely error prone. Consider this: a recently created experiment with 17 nodes and 10 subnets required 140 hand-created routes in the NS file . Two final, cautionary notes on routing: The default route must be set to use the control network interface. You might be tempted to set the default route on your nodes to reduce the number of explicit routes used. Please avoid this. That would prevent nodes from contacting the outside world, i.e., you. If you use your own routing daemon, you must avoid using the control network interface in the configuration. Since every node in the testbed is directly connected to the control network LAN, a naive routing daemon configuration will discover that any node is just one hop away, via the control network, from any other node and all inter-node traffic will be routed via that interface.","title":"Setting up IP routing between nodes"},{"location":"core/sample-topologies/","text":"Sample Topologies \uf0c1 The following are various topologies you can use to experiment with DETERLab Core. Toy topologies \uf0c1 LAN \uf0c1 set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 5 set lanstr \"\" for {set i 0} {$i < $NODES} {incr i} { set node($i) [$ns node] append lanstr \"$node($i) \" } # Change the BW and delay if you want set lan0 [$ns make-lan \"$lanstr\" 100Mb 0ms] $ns rtproto Static $ns run Ring \uf0c1 set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 5 set node(0) [$ns node] for {set i 1} {$i < $NODES} {incr i} { set node($i) [$ns node] set lastindex [expr $i-1] # Change BW and delay if you want set Link$i [$ns duplex-link $node($i) $node($lastindex) 100Mb 0ms DropTail] } set lastindex [expr $i-1] # Change BW and delay if you want set Link$i [$ns duplex-link $node(0) $node($lastindex) 100Mb 0ms DropTail] $ns rtproto Static $ns run Dumbbell \uf0c1 set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 10 set rem 0 set l 0 for {set i 0} {$i < 2} {incr i} { set node($rem) [$ns node] for {set j 1} {$j < $NODES/2} {incr j} { set index [expr $rem+$j] set node($index) [$ns node] # Change BW and delay if you want set Link$l [$ns duplex-link $node($rem) $node($index) 100Mb 0ms DropTail] set l [expr $l+1] } set rem [expr $rem+$NODES/2] } set rem [expr $NODES/2] # Change BW and delay if you want set Link$l [$ns duplex-link $node($rem) $node(0) 100Mb 0ms DropTail] $ns rtproto Static $ns run Tree \uf0c1 set ns [new Simulator] source tb_compat.tcl # Change fanout if you want but bear in mind that some of # our nodes have only 5 interfaces, so max # of experimental # interfaces (and fan out) is 4 set FANOUT 3 # Change depth if you want set DEPTH 3 set node(0) [$ns node] set lastj 0 set f $FANOUT set lastl 0 for {set i 0} {$i < $DEPTH} {incr i} { for {set j 1} {$j <# $f} {incr j} { set index [expr $lastj+$j] set node($index) [$ns node] set lastindex [expr ($index-1)/$FANOUT] # Change BW and delay if you want set Link$lastl [$ns duplex-link $node($index) $node($lastindex) 100Mb 0ms DropTail] set lastl [expr $lastl+1] } set f [expr $f*$FANOUT] set lastj [expr $lastj+$j-1] } $ns rtproto Static $ns run Real AS topologies \uf0c1 Because most of our PCs have up to 4 experimental interfaces, note that some of these topologies may have to be modified to have a fan out of at most 4. http://www.cs.purdue.edu/homes/fahmy/software/rf2ns/topo/","title":"Sample Topologies"},{"location":"core/sample-topologies/#sample-topologies","text":"The following are various topologies you can use to experiment with DETERLab Core.","title":"Sample Topologies"},{"location":"core/sample-topologies/#toy-topologies","text":"","title":"Toy topologies"},{"location":"core/sample-topologies/#lan","text":"set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 5 set lanstr \"\" for {set i 0} {$i < $NODES} {incr i} { set node($i) [$ns node] append lanstr \"$node($i) \" } # Change the BW and delay if you want set lan0 [$ns make-lan \"$lanstr\" 100Mb 0ms] $ns rtproto Static $ns run","title":"LAN"},{"location":"core/sample-topologies/#ring","text":"set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 5 set node(0) [$ns node] for {set i 1} {$i < $NODES} {incr i} { set node($i) [$ns node] set lastindex [expr $i-1] # Change BW and delay if you want set Link$i [$ns duplex-link $node($i) $node($lastindex) 100Mb 0ms DropTail] } set lastindex [expr $i-1] # Change BW and delay if you want set Link$i [$ns duplex-link $node(0) $node($lastindex) 100Mb 0ms DropTail] $ns rtproto Static $ns run","title":"Ring"},{"location":"core/sample-topologies/#dumbbell","text":"set ns [new Simulator] source tb_compat.tcl # Change this to a number of nodes you want set NODES 10 set rem 0 set l 0 for {set i 0} {$i < 2} {incr i} { set node($rem) [$ns node] for {set j 1} {$j < $NODES/2} {incr j} { set index [expr $rem+$j] set node($index) [$ns node] # Change BW and delay if you want set Link$l [$ns duplex-link $node($rem) $node($index) 100Mb 0ms DropTail] set l [expr $l+1] } set rem [expr $rem+$NODES/2] } set rem [expr $NODES/2] # Change BW and delay if you want set Link$l [$ns duplex-link $node($rem) $node(0) 100Mb 0ms DropTail] $ns rtproto Static $ns run","title":"Dumbbell"},{"location":"core/sample-topologies/#tree","text":"set ns [new Simulator] source tb_compat.tcl # Change fanout if you want but bear in mind that some of # our nodes have only 5 interfaces, so max # of experimental # interfaces (and fan out) is 4 set FANOUT 3 # Change depth if you want set DEPTH 3 set node(0) [$ns node] set lastj 0 set f $FANOUT set lastl 0 for {set i 0} {$i < $DEPTH} {incr i} { for {set j 1} {$j <# $f} {incr j} { set index [expr $lastj+$j] set node($index) [$ns node] set lastindex [expr ($index-1)/$FANOUT] # Change BW and delay if you want set Link$lastl [$ns duplex-link $node($index) $node($lastindex) 100Mb 0ms DropTail] set lastl [expr $lastl+1] } set f [expr $f*$FANOUT] set lastj [expr $lastj+$j-1] } $ns rtproto Static $ns run","title":"Tree"},{"location":"core/sample-topologies/#real-as-topologies","text":"Because most of our PCs have up to 4 experimental interfaces, note that some of these topologies may have to be modified to have a fan out of at most 4. http://www.cs.purdue.edu/homes/fahmy/software/rf2ns/topo/","title":"Real AS topologies"},{"location":"core/serial-console/","text":"Using the Serial Console \uf0c1 Determining which nodes to connect to \uf0c1 You can determine the nodes allocated to your experiment by looking at the Reserved Nodes table on the Show Experiment page on the web interface. The node names will be given in the first column of the table. Connecting to the Serial Console \uf0c1 Every node on the testbed has serial console access enabled. To connect to a node's serial console, you must first log into users.deterlab.net and use the /usr/testbed/bin/console command. To connect to a particular node, type /usr/testbed/bin/console node-name where node-name is a node allocated to your experiment. To disconnect from the console session, press Ctrl key and ']' key together, then type quit . Serial Console Logs \uf0c1 All console output from each node is saved in /var/log/tiplogs/node-name.run , where node-name is a node allocated to your experiment. This run file is created when nodes are first allocated to an experiment, and the Unix permissions of the run files permit only members of the project to view them. Warning When the experiment is swapped out, the run logs are removed. In order to preserve them, you must make a copy before swapping out your experiment. Console logs may be viewed through the web interface on the Show Experiment page by clicking on the icon in the Console column of the Reserved Nodes table.","title":"Using the Serial Console"},{"location":"core/serial-console/#using-the-serial-console","text":"","title":"Using the Serial Console"},{"location":"core/serial-console/#determining-which-nodes-to-connect-to","text":"You can determine the nodes allocated to your experiment by looking at the Reserved Nodes table on the Show Experiment page on the web interface. The node names will be given in the first column of the table.","title":"Determining which nodes to connect to"},{"location":"core/serial-console/#connecting-to-the-serial-console","text":"Every node on the testbed has serial console access enabled. To connect to a node's serial console, you must first log into users.deterlab.net and use the /usr/testbed/bin/console command. To connect to a particular node, type /usr/testbed/bin/console node-name where node-name is a node allocated to your experiment. To disconnect from the console session, press Ctrl key and ']' key together, then type quit .","title":"Connecting to the Serial Console"},{"location":"core/serial-console/#serial-console-logs","text":"All console output from each node is saved in /var/log/tiplogs/node-name.run , where node-name is a node allocated to your experiment. This run file is created when nodes are first allocated to an experiment, and the Unix permissions of the run files permit only members of the project to view them. Warning When the experiment is swapped out, the run logs are removed. In order to preserve them, you must make a copy before swapping out your experiment. Console logs may be viewed through the web interface on the Show Experiment page by clicking on the icon in the Console column of the Reserved Nodes table.","title":"Serial Console Logs"},{"location":"core/sharing/","text":"Note This page is updated to show the workflow with our new platform . Using Shared Materials \uf0c1 Sharing and Using Shared Materials \uf0c1 These functionalities are available by choosing My Deterlab and then the Sharing tab. Any user can share materials they think would be helpful to others, and any user can find shared materials. What can be shared \uf0c1 Currently, you can share teaching materials (lectures, homeworks, teacher manuals or class capture-the-flag exercises) and research materials (experiments, tools, datasets and HOWTOs). Any material can be shared either by uploading a ZIP file or by specifying a URL. ZIP files will be unzipped prior to moving them to our shared space. Requirements for Sharing \uf0c1 A material is uniquely identified by its title, type and username of the user who shared it. Multiple users can share versions of a material with the same title and type, or a single user can share multiple materials with the same title but different types. We assume that ZIP files are created by having a folder with materials you want to share and zipping it. This folder must have an index.html file inside, which contains at least 50 characters. When sharing something you need to specify its type (see here ), a few keywords that people can use to search for your material and an E-mail address of the person responsible for maintenance. Material Types \uf0c1 Here are some semi-formal definitions of the types of materials that can be shared. Lectures \uf0c1 A lecture can be a Word document, a set of slides, or a URL to an online content. You could share a whole lecture or a set of smaller modules covering some topic. Homework \uf0c1 A homework is a specification of an assignment that a student will see. Ideally it should follow the format similar to other homework assignments on site. Teacher Manuals \uf0c1 A teacher manual accompanies a homework or a CCTF. Ideally it should follow the format similar to other teacher manuals on site. Note Teacher manuals are only visible to heads of class projects on DETERLab and they can only be downloaded as ZIP files. CCTFs \uf0c1 CCTFs or Class Capture-the-Flag exercises are targeted exercises that pit two student teams against each other in attack/defense scenarios. These are ideal to assign to classes after they have completed a few homeworks with DETERLab. For more information about CCTFs see this paper . CCTFs are not currently supported on our new platform. We hope to support them starting in early 2024. Experiments \uf0c1 An experiment is a set of files (e.g., topology file, input data, output data, setup scripts, etc.) that enables someone else to recreate an experiment done by a user. This definition is intentionally open-ended. Share any files you believe are useful to others that seek to repeat or build upon your work. Datasets \uf0c1 A data set is a collection of data you want to share with others. Such data should be either related to your DETERLab use (e.g., it was used in an experiment by you that later produced results for a publication) or should be generated by your DETERLab experiment (e.g., traces of traffic collected in your experiment as you performed some attack). Warning Only share data that is not private! If in doubt, ask us. Tools \uf0c1 A tool is some application that is useful for experimentation, such as a traffic generator, an attack generator, etc. Ideally you would share both the source and the binary of your tool, and some test data. HOWTOs \uf0c1 A HOWTO is a small building block for an experiment. For example, it could be a recipe how to set up a DNS server, or how to perform a SYN flood attack. Finding Materials \uf0c1 You can find materials by searching for them under the Sharing tab. You can search by keyword (or leave empty to search for all materials), by type or both. Adopting Materials \uf0c1 Once you [#find find] materials you are interested in you can either download them as a ZIP file, or if you teach a class, you can adopt them directly into your class (visibility of adopted materials is set to all students). If you want to modify a material before adopting it to your class, download it as ZIP, apply changes locally and then upload it to your class. Modifying Materials Shared by You \uf0c1 If you want to modify a material you have shared previously simply re-share it using the same title and type. This will overwrite the previously-shared content.","title":"Shared Materials"},{"location":"core/sharing/#using-shared-materials","text":"","title":"Using Shared Materials"},{"location":"core/sharing/#sharing-and-using-shared-materials","text":"These functionalities are available by choosing My Deterlab and then the Sharing tab. Any user can share materials they think would be helpful to others, and any user can find shared materials.","title":"Sharing and Using Shared Materials"},{"location":"core/sharing/#what-can-be-shared","text":"Currently, you can share teaching materials (lectures, homeworks, teacher manuals or class capture-the-flag exercises) and research materials (experiments, tools, datasets and HOWTOs). Any material can be shared either by uploading a ZIP file or by specifying a URL. ZIP files will be unzipped prior to moving them to our shared space.","title":"What can be shared "},{"location":"core/sharing/#requirements-for-sharing","text":"A material is uniquely identified by its title, type and username of the user who shared it. Multiple users can share versions of a material with the same title and type, or a single user can share multiple materials with the same title but different types. We assume that ZIP files are created by having a folder with materials you want to share and zipping it. This folder must have an index.html file inside, which contains at least 50 characters. When sharing something you need to specify its type (see here ), a few keywords that people can use to search for your material and an E-mail address of the person responsible for maintenance.","title":"Requirements for Sharing"},{"location":"core/sharing/#material-types","text":"Here are some semi-formal definitions of the types of materials that can be shared.","title":"Material Types "},{"location":"core/sharing/#lectures","text":"A lecture can be a Word document, a set of slides, or a URL to an online content. You could share a whole lecture or a set of smaller modules covering some topic.","title":"Lectures"},{"location":"core/sharing/#homework","text":"A homework is a specification of an assignment that a student will see. Ideally it should follow the format similar to other homework assignments on site.","title":"Homework"},{"location":"core/sharing/#teacher-manuals","text":"A teacher manual accompanies a homework or a CCTF. Ideally it should follow the format similar to other teacher manuals on site. Note Teacher manuals are only visible to heads of class projects on DETERLab and they can only be downloaded as ZIP files.","title":"Teacher Manuals"},{"location":"core/sharing/#cctfs","text":"CCTFs or Class Capture-the-Flag exercises are targeted exercises that pit two student teams against each other in attack/defense scenarios. These are ideal to assign to classes after they have completed a few homeworks with DETERLab. For more information about CCTFs see this paper . CCTFs are not currently supported on our new platform. We hope to support them starting in early 2024.","title":"CCTFs"},{"location":"core/sharing/#experiments","text":"An experiment is a set of files (e.g., topology file, input data, output data, setup scripts, etc.) that enables someone else to recreate an experiment done by a user. This definition is intentionally open-ended. Share any files you believe are useful to others that seek to repeat or build upon your work.","title":"Experiments"},{"location":"core/sharing/#datasets","text":"A data set is a collection of data you want to share with others. Such data should be either related to your DETERLab use (e.g., it was used in an experiment by you that later produced results for a publication) or should be generated by your DETERLab experiment (e.g., traces of traffic collected in your experiment as you performed some attack). Warning Only share data that is not private! If in doubt, ask us.","title":"Datasets"},{"location":"core/sharing/#tools","text":"A tool is some application that is useful for experimentation, such as a traffic generator, an attack generator, etc. Ideally you would share both the source and the binary of your tool, and some test data.","title":"Tools"},{"location":"core/sharing/#howtos","text":"A HOWTO is a small building block for an experiment. For example, it could be a recipe how to set up a DNS server, or how to perform a SYN flood attack.","title":"HOWTOs"},{"location":"core/sharing/#finding-materials","text":"You can find materials by searching for them under the Sharing tab. You can search by keyword (or leave empty to search for all materials), by type or both.","title":"Finding Materials "},{"location":"core/sharing/#adopting-materials","text":"Once you [#find find] materials you are interested in you can either download them as a ZIP file, or if you teach a class, you can adopt them directly into your class (visibility of adopted materials is set to all students). If you want to modify a material before adopting it to your class, download it as ZIP, apply changes locally and then upload it to your class.","title":"Adopting Materials "},{"location":"core/sharing/#modifying-materials-shared-by-you","text":"If you want to modify a material you have shared previously simply re-share it using the same title and type. This will overwrite the previously-shared content.","title":"Modifying Materials Shared by You "},{"location":"core/swapin/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Start An Experiment (Swap-in) \uf0c1 To start an experiment you will need to ``swap it in'', which will allocate resources to it. Go to your dashboard by clicking the My DETERLab link on the top menu. In the Current Experiments table, click on the name of the experiment you want to start. In our example this will be basic-experiment If needed, modify default options for when the experiment should be swapped out. You can do so by clicking on Modify Settings on the left sidebar. On the left sidebar, click Swap Experiment In , then click Confirm . Usually the swap in process will take several minutes, depending on the size of your toology. You will receive an email notification when it is complete. While you are waiting, you can watch the swap in process displayed in your web browser. In some cases there may be errors during swap in, either because you have errors in your topology specification or because you have asked for resources the testbed does not currently have. The email message you receive will help you understand why the error occured. For more information about swapping in, and to understand various swapin options please read our swapping-in guide","title":"Swap in an experiment"},{"location":"core/swapin/#start-an-experiment-swap-in","text":"To start an experiment you will need to ``swap it in'', which will allocate resources to it. Go to your dashboard by clicking the My DETERLab link on the top menu. In the Current Experiments table, click on the name of the experiment you want to start. In our example this will be basic-experiment If needed, modify default options for when the experiment should be swapped out. You can do so by clicking on Modify Settings on the left sidebar. On the left sidebar, click Swap Experiment In , then click Confirm . Usually the swap in process will take several minutes, depending on the size of your toology. You will receive an email notification when it is complete. While you are waiting, you can watch the swap in process displayed in your web browser. In some cases there may be errors during swap in, either because you have errors in your topology specification or because you have asked for resources the testbed does not currently have. The email message you receive will help you understand why the error occured. For more information about swapping in, and to understand various swapin options please read our swapping-in guide","title":"Start An Experiment (Swap-in)"},{"location":"core/swapout/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Swap Out (Release Resources) \uf0c1 When you are done working with your nodes, it is best practice to save your work and swap out the experiment so that other users have access to the physical machines. Saving Your Work \uf0c1 There are two folders that are mounted via NFS on every node in your experiment and on users.deterlab.net . These are (1) your home directory, i.e., /users/YourUsername and (2) your project directory, i.e., /proj/YourProject . You can place the files you want to save into these directories. Everything else on experimental nodes is permanently lost when an experiment is swapped out. Note Remember: Make sure you save your work into your home or project directory before swapping out your experiment! Swap Out vs Terminate \uf0c1 When to Swap Out When you are done with your experiment for the time being, make sure you save your work into an appropriate location and then swap out your experiment. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you are taking a break from the work, but coming back later. To do this, click on the experiment's name in \"My Deterlab\" view, then select Swap Experiment Out from the left menu. This releases resources so that someone else can use them. When to Terminate When you are completely finished with your experiment and have no intention of running it again, click on the experiment's name in \"My Deterlab\" view, then selecte Terminate Experiment from the left menu. This will delete all traces of the experiment .","title":"Swap out or terminate"},{"location":"core/swapout/#swap-out-release-resources","text":"When you are done working with your nodes, it is best practice to save your work and swap out the experiment so that other users have access to the physical machines.","title":"Swap Out (Release Resources)"},{"location":"core/swapout/#saving-your-work","text":"There are two folders that are mounted via NFS on every node in your experiment and on users.deterlab.net . These are (1) your home directory, i.e., /users/YourUsername and (2) your project directory, i.e., /proj/YourProject . You can place the files you want to save into these directories. Everything else on experimental nodes is permanently lost when an experiment is swapped out. Note Remember: Make sure you save your work into your home or project directory before swapping out your experiment!","title":"Saving Your Work"},{"location":"core/swapout/#swap-out-vs-terminate","text":"When to Swap Out When you are done with your experiment for the time being, make sure you save your work into an appropriate location and then swap out your experiment. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you are taking a break from the work, but coming back later. To do this, click on the experiment's name in \"My Deterlab\" view, then select Swap Experiment Out from the left menu. This releases resources so that someone else can use them. When to Terminate When you are completely finished with your experiment and have no intention of running it again, click on the experiment's name in \"My Deterlab\" view, then selecte Terminate Experiment from the left menu. This will delete all traces of the experiment .","title":"Swap Out vs Terminate"},{"location":"core/swapping/","text":"Understanding Swapping (Node Use Policies) \uf0c1 What are DETERLab's use policies? \uf0c1 When you swap in an experiment please use it actively and swap it out promptly if you foresee that you will not use it for an hour or longer. There are a limited number of nodes available, and we depend on users to actively manage their experiments and return resources when they are not in use. In general, if an experiment is idle for several hours, the system will automatically swap it out, or E-mail the user about it. In some cases, an operator may manually swap out an epxeriment. Please see full details below. What is \"active use\"? \uf0c1 A node in an experiment that is being actively used will be doing something related to your experimentation goal. In almost all cases, someone will either be logged into it using it interactively, or some program will be running, sending and receiving packets, and performing the operations necessary to carry out the experiment. When is an experiment considered idle? \uf0c1 Your experiment will be considered idle if it has no measurable activity for a significant period of time (a few hours; the exact time is typically set at swapin time). We detect the following types of activity: Any network activity on the experimental network Substantial activity on the control network TTY/console activity on nodes High CPU activity on nodes Certain external events, such as rebooting a node with node_reboot If your experiment's activity falls outside these measured types of activity, or it seems that DETERLab is not assessing your idle time correctly, please be sure to let us know when you create your experiment, or you may be swapped out unexpectedly. ''It is considered abuse to generate artificial activity in order to prevent your experiment from being marked idle. Abusers' access to DETERLab will be revoked, and their actions will be reported to their project leader. Please do not do this. If you think you need special assistance for a deadline, demo or other reason, please contact us .'' What is \"swapping\"? \uf0c1 Swapping is the process of instantiating your experiment, i.e., allocating nodes, configuring links, etc. It also refers to the reverse process, in which nodes are released. These processes are called \"swapping in\" and \"swapping out\" respectively. What is an \"Idle-Swap\"? \uf0c1 An \"Idle-Swap\" is when DETERLab or its operators swap out your experiment because it was idle for too long. There are two ways that your experiment may be idle-swapped: automatic and manual. When Idle-Swap is enabled for your experiment and the experiment has been continuously idle for the time specified in its settings, DETERLab will then automatically swap it out. Class experiments have default idle-swap time set for 1 hour . When there is very high resource demand and the experiment has been idle a substantial time (e.g., a day) a DETERLab operator may swap it out manually. In this case we will typically make every effort to contact you and ask you to save state and swap out the nodes yourself. When you create your experiment, you may uncheck the \"Idle-Swap\" box, disabling the automatic idle-swapping of your experiment. If you do so, you must specify the reason, which will be reviewed by the operators. If your reason is judged unacceptable or insufficient, we will explain why, and your experiment will be marked idle-swappable. Valid reasons might be things such as: your experiment is actually active but our system fails to detect it as such you have extensive local state on the node, which is hard to preserve between swap ins your experiment uses a large number of nodes and you have a research/class deadline If an experiment is non-idle-swappable, our system will not automatically swap it out, and testbed administrators will attempt to contact you in the event a swapout becomes necessary. However, we expect you to be responsible for managing your experiment in a responsible way, a way that uses DETERLab's hardware resources efficiently. You may edit the swap settings (Idle-Swap, Max-Duration, and corresponding reasons and timeouts) using the Modify Settings menu item on the left sidebar in the Experiment view. Is there any data loss when an experiment is swapped out \uf0c1 Any system settings (e.g., installed applications, changes to configuration files) and any files not stored in your home directory ( /users/YourUsername ) or your project ( /proj/YourProject ) or group ( /groups/YourGroup ) directory, will be lost when your experiment is swapped out. You should make arrangements to store this state manually if it is important for you and to restore it manually when you swap in your experiment again. What is \"Max duration\"? \uf0c1 Each experiment may have a Maximum Duration, where an experimenter specifies the maximum amount of time that the experiment should stay swapped in. When that time is exceeded, the experiment is unconditionally swapped out even if it is not idle. Class experiments have default max duration set to 1 day.","title":"Understanding Swapping (Node Use Policies)"},{"location":"core/swapping/#understanding-swapping-node-use-policies","text":"","title":"Understanding Swapping (Node Use Policies)"},{"location":"core/swapping/#what-are-deterlabs-use-policies","text":"When you swap in an experiment please use it actively and swap it out promptly if you foresee that you will not use it for an hour or longer. There are a limited number of nodes available, and we depend on users to actively manage their experiments and return resources when they are not in use. In general, if an experiment is idle for several hours, the system will automatically swap it out, or E-mail the user about it. In some cases, an operator may manually swap out an epxeriment. Please see full details below.","title":"What are DETERLab's use policies?"},{"location":"core/swapping/#what-is-active-use","text":"A node in an experiment that is being actively used will be doing something related to your experimentation goal. In almost all cases, someone will either be logged into it using it interactively, or some program will be running, sending and receiving packets, and performing the operations necessary to carry out the experiment.","title":"What is \"active use\"?"},{"location":"core/swapping/#when-is-an-experiment-considered-idle","text":"Your experiment will be considered idle if it has no measurable activity for a significant period of time (a few hours; the exact time is typically set at swapin time). We detect the following types of activity: Any network activity on the experimental network Substantial activity on the control network TTY/console activity on nodes High CPU activity on nodes Certain external events, such as rebooting a node with node_reboot If your experiment's activity falls outside these measured types of activity, or it seems that DETERLab is not assessing your idle time correctly, please be sure to let us know when you create your experiment, or you may be swapped out unexpectedly. ''It is considered abuse to generate artificial activity in order to prevent your experiment from being marked idle. Abusers' access to DETERLab will be revoked, and their actions will be reported to their project leader. Please do not do this. If you think you need special assistance for a deadline, demo or other reason, please contact us .''","title":"When is an experiment considered idle?"},{"location":"core/swapping/#what-is-swapping","text":"Swapping is the process of instantiating your experiment, i.e., allocating nodes, configuring links, etc. It also refers to the reverse process, in which nodes are released. These processes are called \"swapping in\" and \"swapping out\" respectively.","title":"What is \"swapping\"? "},{"location":"core/swapping/#what-is-an-idle-swap","text":"An \"Idle-Swap\" is when DETERLab or its operators swap out your experiment because it was idle for too long. There are two ways that your experiment may be idle-swapped: automatic and manual. When Idle-Swap is enabled for your experiment and the experiment has been continuously idle for the time specified in its settings, DETERLab will then automatically swap it out. Class experiments have default idle-swap time set for 1 hour . When there is very high resource demand and the experiment has been idle a substantial time (e.g., a day) a DETERLab operator may swap it out manually. In this case we will typically make every effort to contact you and ask you to save state and swap out the nodes yourself. When you create your experiment, you may uncheck the \"Idle-Swap\" box, disabling the automatic idle-swapping of your experiment. If you do so, you must specify the reason, which will be reviewed by the operators. If your reason is judged unacceptable or insufficient, we will explain why, and your experiment will be marked idle-swappable. Valid reasons might be things such as: your experiment is actually active but our system fails to detect it as such you have extensive local state on the node, which is hard to preserve between swap ins your experiment uses a large number of nodes and you have a research/class deadline If an experiment is non-idle-swappable, our system will not automatically swap it out, and testbed administrators will attempt to contact you in the event a swapout becomes necessary. However, we expect you to be responsible for managing your experiment in a responsible way, a way that uses DETERLab's hardware resources efficiently. You may edit the swap settings (Idle-Swap, Max-Duration, and corresponding reasons and timeouts) using the Modify Settings menu item on the left sidebar in the Experiment view.","title":"What is an \"Idle-Swap\"? "},{"location":"core/swapping/#is-there-any-data-loss-when-an-experiment-is-swapped-out","text":"Any system settings (e.g., installed applications, changes to configuration files) and any files not stored in your home directory ( /users/YourUsername ) or your project ( /proj/YourProject ) or group ( /groups/YourGroup ) directory, will be lost when your experiment is swapped out. You should make arrangements to store this state manually if it is important for you and to restore it manually when you swap in your experiment again.","title":"Is there any data loss when an experiment is swapped out"},{"location":"core/swapping/#what-is-max-duration","text":"Each experiment may have a Maximum Duration, where an experimenter specifies the maximum amount of time that the experiment should stay swapped in. When that time is exceeded, the experiment is unconditionally swapped out even if it is not idle. Class experiments have default max duration set to 1 day.","title":"What is \"Max duration\"?"},{"location":"core/topology/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Create Experiment Topology \uf0c1 Part of DETERLab's power lies in its ability to assume many different topologies; the description of a such a topology is a necessary part of an experiment. Before you can start your experiment, you must model the elements of the experiment's network with a topology. Create your topology in a text file using any text editor you like (e.g., Notepad on Windows, vi, vim, emacs, nano or pico on Linux or TextEdit on Mac) **For this basic tutorial, use this NS file which includes a simple topology and save it locally on your device ** The rest of this section describes NS format and walks you through the different parts of the sample file. NS Format \uf0c1 DETERLab uses the \"NS\" (\"Network Simulator\") format to describe network topologies. You can see the list of supported commands . Basic Example \uf0c1 In our example, we are creating a test network which looks like the following: Figure 1: A is connected to B with a link, and B to C and D with a LAN. The red shapes on the nodes denote network interfaces. Each physical node has a limited number of interfaces (usually 3-4). Here's how to describe this topology: Step 1. Prologue All NS files start with a simple prologue, declaring a simulator and including a file that allows you to use the special tb- commands: # This is a simple ns script. Comments start with #. set ns [new Simulator] source tb_compat.tcl Step 2. Define the nodes in the topology set A [$ns node] set B [$ns node] set C [$ns node] set D [$ns node] A , B , C , D are names of the nodes in your topology. You can use any alphanumeric string, as long as it starts with a letter. Step 3. Define the link and the LAN that connect the nodes NS syntax permits you to specify the bandwidth, latency, and queue type. When you specify bandwidth that is not divisible by 10Mbps or latency that is different from 0, the testbed inserts another node on the link to emulate these conditions. Unless you absolutely need different settings, please specify 10 Mbps, 100 Mbps, 100 Mbps or 1000 Mbps bandwidth and 0 ms delay. set link0 [$ns duplex-link $A $B 100Mb 0ms DropTail] set lan0 [$ns make-lan \"$B $C $D\" 100Mb 0ms] The difference between a link and a LAN is how many nodes they can connect. A link can only connect two nodes. A LAN can connect any number of nodes (in our example 3 nodes). In addition to the standard NS syntax above, a number of extensions are available in DETERLab that allow you to better control your experiment. For example, you may specify what Operating System is booted on your nodes, the IP addresses to be assigned to network interfaces (red shapes in the picture), the hardware for each node, etc. Step 4. Enable routing In a topology like this, you will likely want to communicate between all the nodes, including nodes that aren't directly connected, like A and C . In order for that to happen, we must enable routing in our experiment, so B can route packets for the other nodes. The typical way to do this is with Static routing. $ns rtproto Static Step 5. Epilogue All NS files have the following epilogue. # Go! $ns run Advanced options \uf0c1 DETERLab has many advanced options, which you can use to customize your topology. You can specify a different hardware for each node You can specify which OS image the node should load. You can customize one of existing OS images to add your own configuration, files and applications, then create a custom OS image and use that image on your nodes. You can specify link loss, delay or limited bandwidth on your links You can assign any IP address to your nodes (as long as it is not from 198.162. . address range) You can specify routing between nodes You can install RPMs or tarballs during experiment swap in and specify commands to run automatically on each swap in. You can implement end-node traffic shaping . Please see full NS command reference .","title":"Create a topology"},{"location":"core/topology/#create-experiment-topology","text":"Part of DETERLab's power lies in its ability to assume many different topologies; the description of a such a topology is a necessary part of an experiment. Before you can start your experiment, you must model the elements of the experiment's network with a topology. Create your topology in a text file using any text editor you like (e.g., Notepad on Windows, vi, vim, emacs, nano or pico on Linux or TextEdit on Mac) **For this basic tutorial, use this NS file which includes a simple topology and save it locally on your device ** The rest of this section describes NS format and walks you through the different parts of the sample file.","title":"Create Experiment Topology"},{"location":"core/topology/#ns-format","text":"DETERLab uses the \"NS\" (\"Network Simulator\") format to describe network topologies. You can see the list of supported commands .","title":"NS Format"},{"location":"core/topology/#basic-example","text":"In our example, we are creating a test network which looks like the following: Figure 1: A is connected to B with a link, and B to C and D with a LAN. The red shapes on the nodes denote network interfaces. Each physical node has a limited number of interfaces (usually 3-4). Here's how to describe this topology: Step 1. Prologue All NS files start with a simple prologue, declaring a simulator and including a file that allows you to use the special tb- commands: # This is a simple ns script. Comments start with #. set ns [new Simulator] source tb_compat.tcl Step 2. Define the nodes in the topology set A [$ns node] set B [$ns node] set C [$ns node] set D [$ns node] A , B , C , D are names of the nodes in your topology. You can use any alphanumeric string, as long as it starts with a letter. Step 3. Define the link and the LAN that connect the nodes NS syntax permits you to specify the bandwidth, latency, and queue type. When you specify bandwidth that is not divisible by 10Mbps or latency that is different from 0, the testbed inserts another node on the link to emulate these conditions. Unless you absolutely need different settings, please specify 10 Mbps, 100 Mbps, 100 Mbps or 1000 Mbps bandwidth and 0 ms delay. set link0 [$ns duplex-link $A $B 100Mb 0ms DropTail] set lan0 [$ns make-lan \"$B $C $D\" 100Mb 0ms] The difference between a link and a LAN is how many nodes they can connect. A link can only connect two nodes. A LAN can connect any number of nodes (in our example 3 nodes). In addition to the standard NS syntax above, a number of extensions are available in DETERLab that allow you to better control your experiment. For example, you may specify what Operating System is booted on your nodes, the IP addresses to be assigned to network interfaces (red shapes in the picture), the hardware for each node, etc. Step 4. Enable routing In a topology like this, you will likely want to communicate between all the nodes, including nodes that aren't directly connected, like A and C . In order for that to happen, we must enable routing in our experiment, so B can route packets for the other nodes. The typical way to do this is with Static routing. $ns rtproto Static Step 5. Epilogue All NS files have the following epilogue. # Go! $ns run","title":"Basic Example"},{"location":"core/topology/#advanced-options","text":"DETERLab has many advanced options, which you can use to customize your topology. You can specify a different hardware for each node You can specify which OS image the node should load. You can customize one of existing OS images to add your own configuration, files and applications, then create a custom OS image and use that image on your nodes. You can specify link loss, delay or limited bandwidth on your links You can assign any IP address to your nodes (as long as it is not from 198.162. . address range) You can specify routing between nodes You can install RPMs or tarballs during experiment swap in and specify commands to run automatically on each swap in. You can implement end-node traffic shaping . Please see full NS command reference .","title":"Advanced options"},{"location":"core/traffic/","text":"Core Guide \uf0c1 In this tutorial we begin with a small 3-5 node experiment, so that you will become familiar with NS syntax and the practical aspects of DETERLab operation. Usually, you will want to incorporate another system such as the MAGI Orchestrator for more fully fleshed out experiments. But this is a good starting point for those new to DETERLab. Note If you are a student, go to the http://education.deterlab.net site for classroom-specific instructions. Node Use Policy \uf0c1 Please make sure to read our guidelines for using nodes in DETERLab . These guidelines help keep DETERLab an effective environment for all users. DETERLab Environment \uf0c1 Your experiment is made up of one or more machines on the internal DETERLab network, which is behind a firewall. users.deterlab.net (or users for short) is the \"control server\" for DETERLab. From users , you can contact all your nodes, reboot them, connect to their serial ports, etc. Each user has a home directory on this server and you may SSH into it with your username and password for your DETERLab account. myboss.isi.deterlab.net ( or boss for short) is the main testbed server that runs DETERLab. Users are not allowed to log directly into it. Basic Tutorial \uf0c1 Getting Started \uf0c1 Work in DETERLab is organized by experiments within projects . Each project is created and managed by a leader - usually the Principal Investigator (PI) of a research project or the instructor of a class on cybersecurity. The project leader then invites members to join by providing them with the project name and sending them the link to the 'Join a Project' page. Before you can take the following tutorial, you need an active account in a project in DETERLab. See How to Register to make sure if you're qualified, and then follow the directions to create a project or ask to join an existing project - if you go through either process for the first time, your account is created as a result. If you already have an account, proceed to the next step. Step 1: Design the topology \uf0c1 Part of DETERLab's power lies in its ability to assume many different topologies; the description of a such a topology is a necessary part of an experiment. Before you can start your experiment, you must model the elements of the experiment's network with a topology. For this basic tutorial, use this NS file which includes a simple topology and save it to a directory called basicExp in your local directory on users.deterlab.net . The rest of this section describes NS format and walks you through the different parts of the sample file. NS Format \uf0c1 DETERLab uses the \"NS\" (\"Network Simulator\") format to describe network topologies. This is substantially the same Tcl-based format used by ns-2 . Since DETERLab offers emulation, rather than simulation, these files are interpreted in a somewhat different manner than ns-2. Therefore, some ns-2 functionality may work differently than you expect, or may not be implemented at all. Please look for warnings of the form: *** WARNING: Unsupported NS Statement! Link type BAZ, using DropTail! If you feel there is useful functionality missing, please let us know . Also, some testbed-specific syntax has been added, which, with the inclusion of the compatibility module tb_compat.tcl , will be ignored by the NS simulator. This allows the same NS file to work on both DETERLab and ns-2, most of the time. NS Example \uf0c1 In our example, we are creating a test network which looks like the following: Figure 1: A is connected to B, and B to C and D with a LAN. Here's how to describe this topology: Declare a simulator and include a file that allows you to use the special tb- commands. First off, all NS files start with a simple prologue, declaring a simulator and including a file that allows you to use the special tb- commands: # This is a simple ns script. Comments start with #. set ns [new Simulator] source tb_compat.tcl Define the 4 nodes in the topology. set nodeA [$ns node] set nodeB [$ns node] set nodeC [$ns node] set nodeD [$ns node] nodeA and so on are the virtual names ( vnames ) of the nodes in your topology. When your experiment is swapped in (has allocated resources), they will be assigned to physical node names like pc45 , probably different ones each time. NOTE: Avoid vnames that clash with the physical node names in the testbed.** Define the link and the LAN that connect the nodes. NS syntax permits you to specify the bandwidth, latency, and queue type. Note that since NS can't impose artificial losses like DETERLab can, we use a separate tb- command to add loss on a link. For our example, we will define a full speed LAN between B, C, and D, and a shaped link from node A to B. set link0 [$ns duplex-link $nodeB $nodeA 30Mb 50ms DropTail] tb-set-link-loss $link0 0.01 set lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms] In addition to the standard NS syntax above, a number of extensions are available in DETERLab that allow you to better control your experiment. For example, you may specify what Operating System is booted on your nodes. For the versions of FreeBSD, Linux, and Windows we currently support, please refer to the Operating System Images page. Click List ImageIDs in the DETERLab web interface Interaction pane to see the current list of DETERLab-supplied operating systems. By default, our most recent Linux image is selected. tb-set-node-os $nodeA FBSD11-STD tb-set-node-os $nodeC Ubuntu1604-STD tb-set-node-os $nodeC WINXP-UPDATE Enable routing. In a topology like this, you will likely want to communicate between all the nodes, including nodes that aren't directly connected, like A and C . In order for that to happen, we must enable routing in our experiment, so B can route packets for the other nodes. The typical way to do this is with Static routing. (Other options are detailed in the Routing section below ). $ns rtproto Static End with an epilogue that instructs the simulator to start. # Go! $ns run Step 2: Create a new experiment \uf0c1 For this tutorial, we will use the web interface to create a new experiment. You could also use the DETERLab Shell Commands . Log into DETERLab with your account credentials (see How to Register ). Click the Experimentation menu item, then click Begin an Experiment . Click Select Project and choose your project. This is also know as your project name or Project ID (PID). This is used as an argument in many commands. Most people will be a member of just one project, and will not have a choice. If you are a member of multiple projects, be sure to select the correct project from the menu. In this example, we will refer to the project as DeterTest . Leave the Group field set to Default Group unless otherwise instructed. Enter the Name field with an easily identifiable name for the experiment. The Name should be a single word (no spaces) identifier. For this tutorial, use basic-experiment . This is also known as your experiment name or Experiment ID (EID) and is used as an argument in many commands. Enter the Description field with a brief description of the experiment. In the Your NS File field, enter the local path to the basic.ns file you downloaded. This file will be uploaded through your browser when you choose \"Submit.\" The rest of the settings depend on the goals of your experiment. In this case, you may simply set the Idle Swap field to 1 h and leave the rest of the settings for Swapping , Linktest Option , and BatchMode at their default for now. Check the Swap In Immediately box to start your lab now. If you did not check this box, you would follow the directions for [starting an experiment] to allocate resources later. Click Submit . After submission, DETERLab will begin processing your request. This process can take several minutes, depending on how large your topology is, and what other features (such as delay nodes and bandwidth limits) you are using. While you are waiting, you may watch the swap in process displayed in your web browser. Assuming all goes well, you will receive an email message indicating success or failure, and if successful, a listing of the nodes and IP address that were allocated to your experiment. For the NS file in this example, you should receive a listing that looks similar to this: Experiment: DeterTest/basic-experiment State: swapped Virtual Node Info: ID Type OS Qualified Name --------------- ------------ --------------- -------------------- nodeA pc FBSD11-STD nodeA.basic-experiment.DeterTest.isi.deterlab.net nodeB pc nodeB.basic-experiment.DeterTest.isi.deterlab.net nodeC pc Ubuntu1604-STD nodeC.basic-experiment.DeterTest.isi.deterlab.net nodeD pc nodeD.basic-experiment.DeterTest.isi.deterlab.net Virtual Lan/Link Info: ID Member/Proto IP/Mask Delay BW (Kbs) Loss Rate --------------- --------------- --------------- --------- --------- --------- lan0 nodeB:1 10.1.2.4 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 lan0 nodeC:0 10.1.2.3 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 lan0 nodeD:0 10.1.2.2 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 link0 nodeA:0 10.1.1.3 25.00 30000 0.00501256 ethernet 255.255.255.0 25.00 30000 0.00501256 link0 nodeB:0 10.1.1.2 25.00 30000 0.00501256 ethernet 255.255.255.0 25.00 30000 0.00501256 Virtual Queue Info: ID Member Q Limit Type weight/min_th/max_th/linterm --------------- --------------- ---------- ------- ---------------------------- lan0 nodeB:1 100 slots Tail 0/0/0/0 lan0 nodeC:0 100 slots Tail 0/0/0/0 lan0 nodeD:0 100 slots Tail 0/0/0/0 link0 nodeA:0 100 slots Tail 0/0/0/0 link0 nodeB:0 100 slots Tail 0/0/0/0 Event Groups: Group Name Members --------------- --------------------------------------------------------------- link0-tracemon link0-nodeB-tracemon,link0-nodeA-tracemon __all_lans lan0,link0 __all_tracemon link0-nodeB-tracemon,link0-nodeA-tracemon,lan0-nodeD-tracemon,lan0-nodeC-tracemon,lan0-nodeB-tracemon lan0-tracemon lan0-nodeB-tracemon,lan0-nodeC-tracemon,lan0-nodeD-tracemon Here is a breakdown of the results: * A single delay node was allocated and inserted into the link between nodeA and nodeB . This link is invisible from your perspective, except for the fact that it adds latency, error, or reduced bandwidth. However, the information for the delay links are included so that you can modify the delay parameters after the experiment has been created (Note that you cannot convert a non-shaped link into a shaped link; you can only modify the traffic shaping parameters of a link that is already being shaped). [[BR]] * Delays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. A delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other. [[BR]] * Each link in the Virtual Lan/Link section has its delay, etc., split between two entries. One is for traffic coming into the link from the node, and the other is for traffic leaving the link to the node. In the case of links, the four entries often get optimized to two entries in a Physical Lan/Link section. [[BR]] * The names in the Qualified Name column refer to the control network interfaces for each of your allocated nodes. These names are added to the DETERLab nameserver map on the fly, and are immediately available for you to use so that you do not have to worry about the actual physical node names that were chosen. In the names listed above, DeterTest is the name of the project that you chose to work in, and basic-experiment is the name of the experiment that you provided on the Begin an Experiment page. [[BR]] * Please don't use the Qualified Name from within nodes in your experiment, since it will contact them over the control network, bypassing the link shaping we configured. Starting an experiment (Swap-in) \uf0c1 If you want to go back to an existing experiment to start it and swap-in (allocate resources): Go to your dashboard by clicking the My DETERLab link in the top menu. In the Current Experiments table, click on the EID (Experiment ID) of the experiment you want to start. In the left sidebar, click Swap Experiment In , then click Confirm . The swap in process will take 5 to 10 minutes; you will receive an email notification when it is complete. While you are waiting, you can watch the swap in process displayed in your web browser. Step 3: Access nodes in your lab environment \uf0c1 To access your experimental nodes, you'll need to first SSH into users.deterlab.net using your DETERLab username and password. Once you log in to users , you'll need to SSH again to your actual experimental nodes. Since your nodes addresses may change every time you swap them in, it's best to SSH to the permanent network names of the nodes. As we mentioned in the previous step, the Qualified Names are included in the output after the experiment is swapped in. Here is another way to find them after swap-in: a. Navigate to the experiment you just created in the web interface . This location is usually called the experiment page . * If you just swapped in your experiment, the quickest way to find your node names is to click on the experiment name in the table under Swap Control . * You can also get there by clicking My DETERLab in the top navigation. Your experiment is listed as \"active\" in the State column. Click on the experiment's name in the EID column to display the experiment page.. b. Click on the Details tab . * Your nodes' network names are listed under the heading Qualified Name . For example, node1.basic-experiment.DeterTest.isi.deterlab.net . * You should familiarize yourself with the information available on this page, but for now we just need to know the long DNS qualified name(s) node(s) you just swapped in. * If you are curious, you should also look at the Settings (generic info), Visualization , and NS File tabs. (The topology mapplet may be disabled for some labs, so these last two may not be visible). c. SSH from users to your experimental nodes by running a command with the following syntax : ssh node1.ExperimentName.ProjectName.isi.deterlab.net * You will not need to re-authenticate. * You may need to wait a few more minutes. Once DETERLab is finished setting up the experiment, the nodes still need a minute or two to boot and complete their configuration. If you get a message about \"server configuration\" when you try to log in, wait a few minutes and try again. d. If you need to create new users on your experimental nodes, you may log in as them by running the following from the experimental node: ssh newuser@node1.basicExp.ProjectName.isi.deterlab.net or ssh newuser@localhost Step 4: View results and modify the experiment \uf0c1 You can visualize the experiment by going to your experiment page (from My DETERLab, click the EID link for your experiment) and clicking the Visualization tab. From this page you can also change the NS file by clicking on the NS File tab or modify parameters by clicking Modify Traffic Shaping in the left sidebar. An alternative method is to log into users.deterlab.net and use the delay_config program. This program requires that you know the symbolic names of the individual links. This information is available on the experiment page. Step 5: Configure and run your experiment. \uf0c1 Once you have all link modifications to your liking, you now need to install any additional tools you need (tools not included in the OS images you chose in Step 1), configure your tools and coordinate these tools to create events in your experiment. For simple experiments, installation, configuration and triggering events can be done by hand or through small scripts. To accomplish this, log into your machines (see Step 3), perform the OS-specific steps needed to install and configure your tools, and run these tools by hand or through scripts, such as shell scripts or remote scripts such as Fabric-based scripts http://www.fabfile.org . For more complicated experiments, you may need more automated ways to install and configure tools as well as coordinate events within your experiment. For fine-grained control over events and event triggers, see the MAGI Orchestrator . A large part of many experiments is traffic generation: the generation and modulation of packets on experiment links. Tools for such generation include the MAGI Orchestrator and LegoTG , as well as many other possibilities . Step 6: Save your work and swap-out (release resources) \uf0c1 When you are done working with your nodes, it is best practice to save your work and swap out the experiment so that other users have access to the physical machines. Saving and securing your files on DETERLab \uf0c1 Every user on DETERLab has a home directory on users.deterlab.net which is mounted via NFS to experimental nodes. This means that anything you place in your home directory on one experimental node (or the users machine) is visible in your home directory on your other experimental nodes. Your home directory is private and will not be overwritten, so you may save your work in that directory. However, everything else on experimental nodes is permanently lost when an experiment is swapped out. Remember: Make sure you save your work in your home directory before swapping out your experiment! Another place you may save your files would be /proj/YourProject . This directory is also NFS-mounted to all experimental nodes, so the same rules apply about writing to it a lot, as for your home directory. It is shared by all members of your project/class. Again, on DETERLab, files ARE NOT SAVED between swap-ins. Additionally, experiments may be forcibly swapped out after a certain number of idle hours (or some maximum amount of time). You must manually save copies of any files you want to keep in your home directory. Any files left elsewhere on the experimental nodes will be erased and lost forever. This means that if you want to store progress for a lab and come back to it later, you will need to put it in your home directory before swapping out the experiment. Swap Out vs Terminate \uf0c1 When to Swap Out When you are done with your experiment for the time being, make sure you save your work into an appropriate location and then swap out your experiment. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you are taking a break from the work, but coming back later. To do this, click Swap Experiment Out link on the experiment page. This allows the resources to be de-allocated so that someone else can use them. When to Terminate When you are completely finished with your experiment and have no intention of running it again, use the Terminate Experiment link in the sidebar of the experiment page. Be careful: termination will erase the experiment and you won't be able to swap it back in without recreating it. DETERLab will then tear down your experiment, and send you an email message when the process is complete. At this point you are allowed to reuse the experiment name (say, if you wanted to create a similar experiment with different parameters). Terminating says \"I won't need this experiment ever again.\" Just remember to Swap In/Out, and never \"Terminate\" unless you're sure you're completely done with the experiment. If you do end up terminating an experiment, you can always go back and recreate it. Scheduling experiment swapout/termination \uf0c1 If you expect that your experiment should run for a set period of time, but you will not be around to terminate or swap the experiment out, then you should use the scheduled swapout/termination feature. This allows you to specify a maximum running time in your NS file so that you will not hold scarce resources when you are offline. To schedule a swapout or termination in your NS file: $ns at 2000.0 \"$ns terminate\" or $ns at 2000.0 \"$ns swapout\" This will cause your experiment to either be terminated or swapped out after 2000 seconds of wallclock time. Why can't I log in to DETERLab? \uf0c1 DETERLab has an automatic blacklist mechanism. If you enter the wrong username and password combination too many times, your account will no longer be accessible from your current IP address. If you think that this has happened to you, try logging in from another address (if you know how), or create an issue (see Getting Help ), which will relay the request to the testbed-ops group that this specific blacklist entry should be erased. Installing RPMs automatically \uf0c1 The DETERLab NS extension tb-set-node-rpms allows you to specify a (space-separated) list of RPMs to install on each of your nodes when it boots: tb-set-node-rpms $nodeA /proj/myproj/rpms/silly-freebsd.rpm tb-set-node-rpms $nodeB /proj/myproj/rpms/silly-linux.rpm tb-set-node-rpms $nodeC /proj/myproj/rpms/silly-windows.rpm The above NS code says to install the silly-freebsd.rpm file on nodeA , the silly-linux.rpm on nodeB , and the silly-windows.rpm on nodeC . RPMs are installed as root, and must reside in either the project's /proj directory, or if the experiment has been created in a subgroup, in the /groups directory. You may not place your RPMs in your home directory. Installing TAR files automatically \uf0c1 The DETERLab NS extension tb-set-node-tarfiles allows you to specify a set of tarfiles to install on each of your nodes when it boots. While similar to the tb-set-node-rpms command, the format of this command is slightly different in that you must specify a directory in which to unpack the tar file. This avoids problems with having to specify absolute pathnames in your tarfile, which many modern tar programs balk at. tb-set-node-tarfiles $nodeA /usr/site /proj/projectName/tarfiles/silly.tar.gz The above NS code says to install the silly.tar.gz tar file on nodeA from the working directory /usr/site when the node first boots. The tarfile must reside in either the project's /proj directory, or if the experiment has been created in a subgroup, in the /groups directory. You may not place your tarfiles in your home directory. You may specify as many tarfiles as you wish, as long as each one is preceded by the directory it should be unpacked in, all separated by spaces. Starting your application automatically \uf0c1 You may start your application automatically when your nodes boot for the first time (when an experiment is started or swapped in) by using the tb-set-node-startcmd NS extension. The argument is a command string (pathname of a script or program, plus arguments) that is run as the UID of the experiment creator, after the node has reached multiuser mode. The command is invoked using /bin/csh , and the working directory is undefined (your script should cd to the directory you need). You can specify the same program for each node, or a different program. For example: tb-set-node-startcmd $nodeA \"/proj/projectName/runme.nodeA\" tb-set-node-startcmd $nodeB \"/proj/projectName/runme.nodeB\" will run /proj/projectName/runme.nodeA on nodeA and /proj/projectName/runme.nodeB on nodeB. The programs must reside on the node's local filesystem, or in a directory that can be reached via NFS. This is either the project's /proj directory, in the /groups directory if the experiment has been created in a subgroup, or a project member's home directory in /users . If you need to see the output of your command, be sure to redirect the output into a file. You may place the file on the local node, or in one of the NFS mounted directories mentioned above. For example: tb-set-node-startcmd $nodeB \"/proj/myproj/runme >& /tmp/foo.log\" Note that the syntax and function of /bin/csh differs from other shells (including bash), specifically in redirection syntax. Be sure to use csh syntax or your start command will fail silently. The exit value of the start command is reported back to the Web Interface, and is made available to you via the experiment page. There is a listing for all of the nodes in the experiment, and the exit value is recorded in this listing. The special symbol none indicates that the node is still running the start command. Notifying the start program when all other nodes have started \uf0c1 It is often necessary for your start program to determine when all of the other nodes in the experiment have started, and are ready to proceed. Sometimes called a barrier , this allows programs to wait at a specific point, and then all proceed at once. DETERLab provides a simple form of this mechanism using a synchronization server that runs on a node of your choice. Specify the node in your NS file: tb-set-sync-server $nodeB When nodeB boots, the synchronization server will automatically start. Your software can then synchronize using the emulab-sync program that is installed on your nodes. For example, your node start command might look like this: #!/bin/sh if [ \"$1\" = \"master\" ]; then /usr/testbed/bin/emulab-sync -i 4 else /usr/testbed/bin/emulab-sync fi /usr/site/bin/dosilly In this example, there are five nodes in the experiment, one of which must be configured to operate as the master, initializing the barrier to the number of clients (four in the above example) that are expected to rendezvous at the barrier. The master will by default wait for all of the clients to reach the barrier. Each client of the barrier also waits until all of the clients have reached the barrier (and of course, until the master initializes the barrier to the proper count). Any number of clients may be specified (any subset of nodes in your experiment can wait). If the master does not need to wait for the clients, you may use the async option which releases the master immediately: /usr/testbed/bin/emulab-sync -a -i 4 You may also specify the name of the barrier. /usr/testbed/bin/emulab-sync -a -i 4 -n mybarrier This allows multiple barriers to be in use at the same time. Scripts on nodeA and nodeB can be waiting on a barrier named \"foo\" while (other) scripts on nodeA and nodeC can be waiting on a barrier named \"bar.\" You may reuse an existing barrier (including the default barrier) once it has been released (all clients arrived and woken up). Setting up IP routing between nodes \uf0c1 As DETER strives to make all aspects of the network controllable by the user, we do not attempt to impose any IP routing architecture or protocol by default. However, many users are more interested in end-to-end aspects and don't want to be bothered with setting up routes. For those users we provide an option to automatically set up routes on nodes which run one of our provided FreeBSD, Linux or Windows XP disk images. You can use the NS rtproto syntax in your NS file to enable routing: $ns rtproto protocolOption where the protocolOption is limited to one of Session , Static , Static-old , or Manual . Session routing provides fully automated routing support, and is implemented by enabling gated running of the OSPF protocol on all nodes in the experiment. This is not supported on Windows XP nodes. Static routing also provides automatic routing support, but rather than computing the routes dynamically, the routes are precomputed by a distributed route computation algorithm running in parallel on the experiment nodes. Static-old specifies use of the older centralized route computation algorithm, precomputing the nodes when the experiment is created, and then loading them onto each node when it boots. Manual routing allows you to explicitly specify per-node routing information in the NS file. To do this, use the Manual routing option to rtproto , followed by a list of routes using the add-route command: $node add-route $dst $nexthop where the dst can be either a node, a link, or a LAN. For example: $client add-route $server $router $client add-route [$ns link $server $router] $router $client add-route $serverlan $router Note that you would need a separate add-route command to establish a route for the reverse direction; thus allowing you to specify differing forward and reverse routes if so desired. These statements are converted into appropriate route(8) commands on your experimental nodes when they boot. In the above examples, the first form says to set up a manual route between $client and $server , using $router as the nexthop; $client and $router should be directly connected, and the interface on $server should be unambiguous; either directly connected to the router, or an edge node that has just a single interface. If the destination has multiple interfaces configured, and it is not connected directly to the nexthop, the interface that you are intending to route to is ambiguous. In the topology shown to the right, $nodeD has two interfaces configured. If you attempted to set up a route like this: $nodeA add-route $nodeD $nodeB you would receive an error since DETERLab staff would not easily be able to determine which of the two links on $nodeD you are referring to. Fortunately, there is an easy solution. Instead of a node, specify the link directly: $nodeA add-route [$ns link $nodeD $nodeC] $nodeB This tells us exactly which link you mean, enabling us to convert that information into a proper route command on $nodeA . The last form of the add-route command is used when adding a route to an entire LAN. It would be tedious and error prone to specify a route to each node in a LAN by hand. Instead, just route to the entire network: set clientlan [$ns make-lan \"$nodeE $nodeF $nodeG\" 100Mb 0ms] $nodeA add-route $clientlan $nodeB In general, it is still best practice to use either Session or Static routing for all but small, simple topologies. Explicitly setting up all the routes in even a moderately-sized experiment is extremely error prone. Consider this: a recently created experiment with 17 nodes and 10 subnets required 140 hand-created routes in the NS file . Two final, cautionary notes on routing: * The default route must be set to use the control network interface. You might be tempted to set the default route on your nodes to reduce the number of explicit routes used. Please avoid this. That would prevent nodes from contacting the outside world, i.e., you. * If you use your own routing daemon, you must avoid using the control network interface in the configuration. Since every node in the testbed is directly connected to the control network LAN, a naive routing daemon configuration will discover that any node is just one hop away, via the control network, from any other node and all inter-node traffic will be routed via that interface.","title":"Core Guide"},{"location":"core/traffic/#core-guide","text":"In this tutorial we begin with a small 3-5 node experiment, so that you will become familiar with NS syntax and the practical aspects of DETERLab operation. Usually, you will want to incorporate another system such as the MAGI Orchestrator for more fully fleshed out experiments. But this is a good starting point for those new to DETERLab. Note If you are a student, go to the http://education.deterlab.net site for classroom-specific instructions.","title":"Core Guide"},{"location":"core/traffic/#node-use-policy","text":"Please make sure to read our guidelines for using nodes in DETERLab . These guidelines help keep DETERLab an effective environment for all users.","title":"Node Use Policy"},{"location":"core/traffic/#deterlab-environment","text":"Your experiment is made up of one or more machines on the internal DETERLab network, which is behind a firewall. users.deterlab.net (or users for short) is the \"control server\" for DETERLab. From users , you can contact all your nodes, reboot them, connect to their serial ports, etc. Each user has a home directory on this server and you may SSH into it with your username and password for your DETERLab account. myboss.isi.deterlab.net ( or boss for short) is the main testbed server that runs DETERLab. Users are not allowed to log directly into it.","title":"DETERLab Environment"},{"location":"core/traffic/#basic-tutorial","text":"","title":"Basic Tutorial"},{"location":"core/traffic/#getting-started","text":"Work in DETERLab is organized by experiments within projects . Each project is created and managed by a leader - usually the Principal Investigator (PI) of a research project or the instructor of a class on cybersecurity. The project leader then invites members to join by providing them with the project name and sending them the link to the 'Join a Project' page. Before you can take the following tutorial, you need an active account in a project in DETERLab. See How to Register to make sure if you're qualified, and then follow the directions to create a project or ask to join an existing project - if you go through either process for the first time, your account is created as a result. If you already have an account, proceed to the next step.","title":"Getting Started"},{"location":"core/traffic/#step-1-design-the-topology","text":"Part of DETERLab's power lies in its ability to assume many different topologies; the description of a such a topology is a necessary part of an experiment. Before you can start your experiment, you must model the elements of the experiment's network with a topology. For this basic tutorial, use this NS file which includes a simple topology and save it to a directory called basicExp in your local directory on users.deterlab.net . The rest of this section describes NS format and walks you through the different parts of the sample file.","title":"Step 1: Design the topology"},{"location":"core/traffic/#ns-format","text":"DETERLab uses the \"NS\" (\"Network Simulator\") format to describe network topologies. This is substantially the same Tcl-based format used by ns-2 . Since DETERLab offers emulation, rather than simulation, these files are interpreted in a somewhat different manner than ns-2. Therefore, some ns-2 functionality may work differently than you expect, or may not be implemented at all. Please look for warnings of the form: *** WARNING: Unsupported NS Statement! Link type BAZ, using DropTail! If you feel there is useful functionality missing, please let us know . Also, some testbed-specific syntax has been added, which, with the inclusion of the compatibility module tb_compat.tcl , will be ignored by the NS simulator. This allows the same NS file to work on both DETERLab and ns-2, most of the time.","title":"NS Format"},{"location":"core/traffic/#ns-example","text":"In our example, we are creating a test network which looks like the following: Figure 1: A is connected to B, and B to C and D with a LAN. Here's how to describe this topology: Declare a simulator and include a file that allows you to use the special tb- commands. First off, all NS files start with a simple prologue, declaring a simulator and including a file that allows you to use the special tb- commands: # This is a simple ns script. Comments start with #. set ns [new Simulator] source tb_compat.tcl Define the 4 nodes in the topology. set nodeA [$ns node] set nodeB [$ns node] set nodeC [$ns node] set nodeD [$ns node] nodeA and so on are the virtual names ( vnames ) of the nodes in your topology. When your experiment is swapped in (has allocated resources), they will be assigned to physical node names like pc45 , probably different ones each time. NOTE: Avoid vnames that clash with the physical node names in the testbed.** Define the link and the LAN that connect the nodes. NS syntax permits you to specify the bandwidth, latency, and queue type. Note that since NS can't impose artificial losses like DETERLab can, we use a separate tb- command to add loss on a link. For our example, we will define a full speed LAN between B, C, and D, and a shaped link from node A to B. set link0 [$ns duplex-link $nodeB $nodeA 30Mb 50ms DropTail] tb-set-link-loss $link0 0.01 set lan0 [$ns make-lan \"$nodeD $nodeC $nodeB \" 100Mb 0ms] In addition to the standard NS syntax above, a number of extensions are available in DETERLab that allow you to better control your experiment. For example, you may specify what Operating System is booted on your nodes. For the versions of FreeBSD, Linux, and Windows we currently support, please refer to the Operating System Images page. Click List ImageIDs in the DETERLab web interface Interaction pane to see the current list of DETERLab-supplied operating systems. By default, our most recent Linux image is selected. tb-set-node-os $nodeA FBSD11-STD tb-set-node-os $nodeC Ubuntu1604-STD tb-set-node-os $nodeC WINXP-UPDATE Enable routing. In a topology like this, you will likely want to communicate between all the nodes, including nodes that aren't directly connected, like A and C . In order for that to happen, we must enable routing in our experiment, so B can route packets for the other nodes. The typical way to do this is with Static routing. (Other options are detailed in the Routing section below ). $ns rtproto Static End with an epilogue that instructs the simulator to start. # Go! $ns run","title":"NS Example"},{"location":"core/traffic/#step-2-create-a-new-experiment","text":"For this tutorial, we will use the web interface to create a new experiment. You could also use the DETERLab Shell Commands . Log into DETERLab with your account credentials (see How to Register ). Click the Experimentation menu item, then click Begin an Experiment . Click Select Project and choose your project. This is also know as your project name or Project ID (PID). This is used as an argument in many commands. Most people will be a member of just one project, and will not have a choice. If you are a member of multiple projects, be sure to select the correct project from the menu. In this example, we will refer to the project as DeterTest . Leave the Group field set to Default Group unless otherwise instructed. Enter the Name field with an easily identifiable name for the experiment. The Name should be a single word (no spaces) identifier. For this tutorial, use basic-experiment . This is also known as your experiment name or Experiment ID (EID) and is used as an argument in many commands. Enter the Description field with a brief description of the experiment. In the Your NS File field, enter the local path to the basic.ns file you downloaded. This file will be uploaded through your browser when you choose \"Submit.\" The rest of the settings depend on the goals of your experiment. In this case, you may simply set the Idle Swap field to 1 h and leave the rest of the settings for Swapping , Linktest Option , and BatchMode at their default for now. Check the Swap In Immediately box to start your lab now. If you did not check this box, you would follow the directions for [starting an experiment] to allocate resources later. Click Submit . After submission, DETERLab will begin processing your request. This process can take several minutes, depending on how large your topology is, and what other features (such as delay nodes and bandwidth limits) you are using. While you are waiting, you may watch the swap in process displayed in your web browser. Assuming all goes well, you will receive an email message indicating success or failure, and if successful, a listing of the nodes and IP address that were allocated to your experiment. For the NS file in this example, you should receive a listing that looks similar to this: Experiment: DeterTest/basic-experiment State: swapped Virtual Node Info: ID Type OS Qualified Name --------------- ------------ --------------- -------------------- nodeA pc FBSD11-STD nodeA.basic-experiment.DeterTest.isi.deterlab.net nodeB pc nodeB.basic-experiment.DeterTest.isi.deterlab.net nodeC pc Ubuntu1604-STD nodeC.basic-experiment.DeterTest.isi.deterlab.net nodeD pc nodeD.basic-experiment.DeterTest.isi.deterlab.net Virtual Lan/Link Info: ID Member/Proto IP/Mask Delay BW (Kbs) Loss Rate --------------- --------------- --------------- --------- --------- --------- lan0 nodeB:1 10.1.2.4 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 lan0 nodeC:0 10.1.2.3 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 lan0 nodeD:0 10.1.2.2 0.00 100000 0.00000000 ethernet 255.255.255.0 0.00 100000 0.00000000 link0 nodeA:0 10.1.1.3 25.00 30000 0.00501256 ethernet 255.255.255.0 25.00 30000 0.00501256 link0 nodeB:0 10.1.1.2 25.00 30000 0.00501256 ethernet 255.255.255.0 25.00 30000 0.00501256 Virtual Queue Info: ID Member Q Limit Type weight/min_th/max_th/linterm --------------- --------------- ---------- ------- ---------------------------- lan0 nodeB:1 100 slots Tail 0/0/0/0 lan0 nodeC:0 100 slots Tail 0/0/0/0 lan0 nodeD:0 100 slots Tail 0/0/0/0 link0 nodeA:0 100 slots Tail 0/0/0/0 link0 nodeB:0 100 slots Tail 0/0/0/0 Event Groups: Group Name Members --------------- --------------------------------------------------------------- link0-tracemon link0-nodeB-tracemon,link0-nodeA-tracemon __all_lans lan0,link0 __all_tracemon link0-nodeB-tracemon,link0-nodeA-tracemon,lan0-nodeD-tracemon,lan0-nodeC-tracemon,lan0-nodeB-tracemon lan0-tracemon lan0-nodeB-tracemon,lan0-nodeC-tracemon,lan0-nodeD-tracemon Here is a breakdown of the results: * A single delay node was allocated and inserted into the link between nodeA and nodeB . This link is invisible from your perspective, except for the fact that it adds latency, error, or reduced bandwidth. However, the information for the delay links are included so that you can modify the delay parameters after the experiment has been created (Note that you cannot convert a non-shaped link into a shaped link; you can only modify the traffic shaping parameters of a link that is already being shaped). [[BR]] * Delays of less than 2ms (per trip) are too small to be accurately modeled at this time, and will be silently ignored. A delay of 0ms can be used to indicate that you do not want added delay; the two interfaces will be \"directly\" connected to each other. [[BR]] * Each link in the Virtual Lan/Link section has its delay, etc., split between two entries. One is for traffic coming into the link from the node, and the other is for traffic leaving the link to the node. In the case of links, the four entries often get optimized to two entries in a Physical Lan/Link section. [[BR]] * The names in the Qualified Name column refer to the control network interfaces for each of your allocated nodes. These names are added to the DETERLab nameserver map on the fly, and are immediately available for you to use so that you do not have to worry about the actual physical node names that were chosen. In the names listed above, DeterTest is the name of the project that you chose to work in, and basic-experiment is the name of the experiment that you provided on the Begin an Experiment page. [[BR]] * Please don't use the Qualified Name from within nodes in your experiment, since it will contact them over the control network, bypassing the link shaping we configured.","title":"Step 2: Create a new experiment"},{"location":"core/traffic/#starting-an-experiment-swap-in","text":"If you want to go back to an existing experiment to start it and swap-in (allocate resources): Go to your dashboard by clicking the My DETERLab link in the top menu. In the Current Experiments table, click on the EID (Experiment ID) of the experiment you want to start. In the left sidebar, click Swap Experiment In , then click Confirm . The swap in process will take 5 to 10 minutes; you will receive an email notification when it is complete. While you are waiting, you can watch the swap in process displayed in your web browser.","title":"Starting an experiment (Swap-in)"},{"location":"core/traffic/#step-3-access-nodes-in-your-lab-environment","text":"To access your experimental nodes, you'll need to first SSH into users.deterlab.net using your DETERLab username and password. Once you log in to users , you'll need to SSH again to your actual experimental nodes. Since your nodes addresses may change every time you swap them in, it's best to SSH to the permanent network names of the nodes. As we mentioned in the previous step, the Qualified Names are included in the output after the experiment is swapped in. Here is another way to find them after swap-in: a. Navigate to the experiment you just created in the web interface . This location is usually called the experiment page . * If you just swapped in your experiment, the quickest way to find your node names is to click on the experiment name in the table under Swap Control . * You can also get there by clicking My DETERLab in the top navigation. Your experiment is listed as \"active\" in the State column. Click on the experiment's name in the EID column to display the experiment page.. b. Click on the Details tab . * Your nodes' network names are listed under the heading Qualified Name . For example, node1.basic-experiment.DeterTest.isi.deterlab.net . * You should familiarize yourself with the information available on this page, but for now we just need to know the long DNS qualified name(s) node(s) you just swapped in. * If you are curious, you should also look at the Settings (generic info), Visualization , and NS File tabs. (The topology mapplet may be disabled for some labs, so these last two may not be visible). c. SSH from users to your experimental nodes by running a command with the following syntax : ssh node1.ExperimentName.ProjectName.isi.deterlab.net * You will not need to re-authenticate. * You may need to wait a few more minutes. Once DETERLab is finished setting up the experiment, the nodes still need a minute or two to boot and complete their configuration. If you get a message about \"server configuration\" when you try to log in, wait a few minutes and try again. d. If you need to create new users on your experimental nodes, you may log in as them by running the following from the experimental node: ssh newuser@node1.basicExp.ProjectName.isi.deterlab.net or ssh newuser@localhost","title":"Step 3: Access nodes in your lab environment"},{"location":"core/traffic/#step-4-view-results-and-modify-the-experiment","text":"You can visualize the experiment by going to your experiment page (from My DETERLab, click the EID link for your experiment) and clicking the Visualization tab. From this page you can also change the NS file by clicking on the NS File tab or modify parameters by clicking Modify Traffic Shaping in the left sidebar. An alternative method is to log into users.deterlab.net and use the delay_config program. This program requires that you know the symbolic names of the individual links. This information is available on the experiment page.","title":"Step 4: View results and modify the experiment"},{"location":"core/traffic/#step-5-configure-and-run-your-experiment","text":"Once you have all link modifications to your liking, you now need to install any additional tools you need (tools not included in the OS images you chose in Step 1), configure your tools and coordinate these tools to create events in your experiment. For simple experiments, installation, configuration and triggering events can be done by hand or through small scripts. To accomplish this, log into your machines (see Step 3), perform the OS-specific steps needed to install and configure your tools, and run these tools by hand or through scripts, such as shell scripts or remote scripts such as Fabric-based scripts http://www.fabfile.org . For more complicated experiments, you may need more automated ways to install and configure tools as well as coordinate events within your experiment. For fine-grained control over events and event triggers, see the MAGI Orchestrator . A large part of many experiments is traffic generation: the generation and modulation of packets on experiment links. Tools for such generation include the MAGI Orchestrator and LegoTG , as well as many other possibilities .","title":"Step 5: Configure and run your experiment."},{"location":"core/traffic/#step-6-save-your-work-and-swap-out-release-resources","text":"When you are done working with your nodes, it is best practice to save your work and swap out the experiment so that other users have access to the physical machines.","title":"Step 6: Save your work and swap-out (release resources)"},{"location":"core/traffic/#saving-and-securing-your-files-on-deterlab","text":"Every user on DETERLab has a home directory on users.deterlab.net which is mounted via NFS to experimental nodes. This means that anything you place in your home directory on one experimental node (or the users machine) is visible in your home directory on your other experimental nodes. Your home directory is private and will not be overwritten, so you may save your work in that directory. However, everything else on experimental nodes is permanently lost when an experiment is swapped out. Remember: Make sure you save your work in your home directory before swapping out your experiment! Another place you may save your files would be /proj/YourProject . This directory is also NFS-mounted to all experimental nodes, so the same rules apply about writing to it a lot, as for your home directory. It is shared by all members of your project/class. Again, on DETERLab, files ARE NOT SAVED between swap-ins. Additionally, experiments may be forcibly swapped out after a certain number of idle hours (or some maximum amount of time). You must manually save copies of any files you want to keep in your home directory. Any files left elsewhere on the experimental nodes will be erased and lost forever. This means that if you want to store progress for a lab and come back to it later, you will need to put it in your home directory before swapping out the experiment.","title":"Saving and securing your files on DETERLab"},{"location":"core/traffic/#swap-out-vs-terminate","text":"When to Swap Out When you are done with your experiment for the time being, make sure you save your work into an appropriate location and then swap out your experiment. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you are taking a break from the work, but coming back later. To do this, click Swap Experiment Out link on the experiment page. This allows the resources to be de-allocated so that someone else can use them. When to Terminate When you are completely finished with your experiment and have no intention of running it again, use the Terminate Experiment link in the sidebar of the experiment page. Be careful: termination will erase the experiment and you won't be able to swap it back in without recreating it. DETERLab will then tear down your experiment, and send you an email message when the process is complete. At this point you are allowed to reuse the experiment name (say, if you wanted to create a similar experiment with different parameters). Terminating says \"I won't need this experiment ever again.\" Just remember to Swap In/Out, and never \"Terminate\" unless you're sure you're completely done with the experiment. If you do end up terminating an experiment, you can always go back and recreate it.","title":"Swap Out vs Terminate"},{"location":"core/traffic/#scheduling-experiment-swapouttermination","text":"If you expect that your experiment should run for a set period of time, but you will not be around to terminate or swap the experiment out, then you should use the scheduled swapout/termination feature. This allows you to specify a maximum running time in your NS file so that you will not hold scarce resources when you are offline. To schedule a swapout or termination in your NS file: $ns at 2000.0 \"$ns terminate\" or $ns at 2000.0 \"$ns swapout\" This will cause your experiment to either be terminated or swapped out after 2000 seconds of wallclock time.","title":"Scheduling experiment swapout/termination"},{"location":"core/traffic/#why-cant-i-log-in-to-deterlab","text":"DETERLab has an automatic blacklist mechanism. If you enter the wrong username and password combination too many times, your account will no longer be accessible from your current IP address. If you think that this has happened to you, try logging in from another address (if you know how), or create an issue (see Getting Help ), which will relay the request to the testbed-ops group that this specific blacklist entry should be erased.","title":"Why can't I log in to DETERLab?"},{"location":"core/traffic/#installing-rpms-automatically","text":"The DETERLab NS extension tb-set-node-rpms allows you to specify a (space-separated) list of RPMs to install on each of your nodes when it boots: tb-set-node-rpms $nodeA /proj/myproj/rpms/silly-freebsd.rpm tb-set-node-rpms $nodeB /proj/myproj/rpms/silly-linux.rpm tb-set-node-rpms $nodeC /proj/myproj/rpms/silly-windows.rpm The above NS code says to install the silly-freebsd.rpm file on nodeA , the silly-linux.rpm on nodeB , and the silly-windows.rpm on nodeC . RPMs are installed as root, and must reside in either the project's /proj directory, or if the experiment has been created in a subgroup, in the /groups directory. You may not place your RPMs in your home directory.","title":"Installing RPMs automatically"},{"location":"core/traffic/#installing-tar-files-automatically","text":"The DETERLab NS extension tb-set-node-tarfiles allows you to specify a set of tarfiles to install on each of your nodes when it boots. While similar to the tb-set-node-rpms command, the format of this command is slightly different in that you must specify a directory in which to unpack the tar file. This avoids problems with having to specify absolute pathnames in your tarfile, which many modern tar programs balk at. tb-set-node-tarfiles $nodeA /usr/site /proj/projectName/tarfiles/silly.tar.gz The above NS code says to install the silly.tar.gz tar file on nodeA from the working directory /usr/site when the node first boots. The tarfile must reside in either the project's /proj directory, or if the experiment has been created in a subgroup, in the /groups directory. You may not place your tarfiles in your home directory. You may specify as many tarfiles as you wish, as long as each one is preceded by the directory it should be unpacked in, all separated by spaces.","title":"Installing TAR files automatically"},{"location":"core/traffic/#starting-your-application-automatically","text":"You may start your application automatically when your nodes boot for the first time (when an experiment is started or swapped in) by using the tb-set-node-startcmd NS extension. The argument is a command string (pathname of a script or program, plus arguments) that is run as the UID of the experiment creator, after the node has reached multiuser mode. The command is invoked using /bin/csh , and the working directory is undefined (your script should cd to the directory you need). You can specify the same program for each node, or a different program. For example: tb-set-node-startcmd $nodeA \"/proj/projectName/runme.nodeA\" tb-set-node-startcmd $nodeB \"/proj/projectName/runme.nodeB\" will run /proj/projectName/runme.nodeA on nodeA and /proj/projectName/runme.nodeB on nodeB. The programs must reside on the node's local filesystem, or in a directory that can be reached via NFS. This is either the project's /proj directory, in the /groups directory if the experiment has been created in a subgroup, or a project member's home directory in /users . If you need to see the output of your command, be sure to redirect the output into a file. You may place the file on the local node, or in one of the NFS mounted directories mentioned above. For example: tb-set-node-startcmd $nodeB \"/proj/myproj/runme >& /tmp/foo.log\" Note that the syntax and function of /bin/csh differs from other shells (including bash), specifically in redirection syntax. Be sure to use csh syntax or your start command will fail silently. The exit value of the start command is reported back to the Web Interface, and is made available to you via the experiment page. There is a listing for all of the nodes in the experiment, and the exit value is recorded in this listing. The special symbol none indicates that the node is still running the start command.","title":"Starting your application automatically"},{"location":"core/traffic/#notifying-the-start-program-when-all-other-nodes-have-started","text":"It is often necessary for your start program to determine when all of the other nodes in the experiment have started, and are ready to proceed. Sometimes called a barrier , this allows programs to wait at a specific point, and then all proceed at once. DETERLab provides a simple form of this mechanism using a synchronization server that runs on a node of your choice. Specify the node in your NS file: tb-set-sync-server $nodeB When nodeB boots, the synchronization server will automatically start. Your software can then synchronize using the emulab-sync program that is installed on your nodes. For example, your node start command might look like this: #!/bin/sh if [ \"$1\" = \"master\" ]; then /usr/testbed/bin/emulab-sync -i 4 else /usr/testbed/bin/emulab-sync fi /usr/site/bin/dosilly In this example, there are five nodes in the experiment, one of which must be configured to operate as the master, initializing the barrier to the number of clients (four in the above example) that are expected to rendezvous at the barrier. The master will by default wait for all of the clients to reach the barrier. Each client of the barrier also waits until all of the clients have reached the barrier (and of course, until the master initializes the barrier to the proper count). Any number of clients may be specified (any subset of nodes in your experiment can wait). If the master does not need to wait for the clients, you may use the async option which releases the master immediately: /usr/testbed/bin/emulab-sync -a -i 4 You may also specify the name of the barrier. /usr/testbed/bin/emulab-sync -a -i 4 -n mybarrier This allows multiple barriers to be in use at the same time. Scripts on nodeA and nodeB can be waiting on a barrier named \"foo\" while (other) scripts on nodeA and nodeC can be waiting on a barrier named \"bar.\" You may reuse an existing barrier (including the default barrier) once it has been released (all clients arrived and woken up).","title":"Notifying the start program when all other nodes have started"},{"location":"core/traffic/#setting-up-ip-routing-between-nodes","text":"As DETER strives to make all aspects of the network controllable by the user, we do not attempt to impose any IP routing architecture or protocol by default. However, many users are more interested in end-to-end aspects and don't want to be bothered with setting up routes. For those users we provide an option to automatically set up routes on nodes which run one of our provided FreeBSD, Linux or Windows XP disk images. You can use the NS rtproto syntax in your NS file to enable routing: $ns rtproto protocolOption where the protocolOption is limited to one of Session , Static , Static-old , or Manual . Session routing provides fully automated routing support, and is implemented by enabling gated running of the OSPF protocol on all nodes in the experiment. This is not supported on Windows XP nodes. Static routing also provides automatic routing support, but rather than computing the routes dynamically, the routes are precomputed by a distributed route computation algorithm running in parallel on the experiment nodes. Static-old specifies use of the older centralized route computation algorithm, precomputing the nodes when the experiment is created, and then loading them onto each node when it boots. Manual routing allows you to explicitly specify per-node routing information in the NS file. To do this, use the Manual routing option to rtproto , followed by a list of routes using the add-route command: $node add-route $dst $nexthop where the dst can be either a node, a link, or a LAN. For example: $client add-route $server $router $client add-route [$ns link $server $router] $router $client add-route $serverlan $router Note that you would need a separate add-route command to establish a route for the reverse direction; thus allowing you to specify differing forward and reverse routes if so desired. These statements are converted into appropriate route(8) commands on your experimental nodes when they boot. In the above examples, the first form says to set up a manual route between $client and $server , using $router as the nexthop; $client and $router should be directly connected, and the interface on $server should be unambiguous; either directly connected to the router, or an edge node that has just a single interface. If the destination has multiple interfaces configured, and it is not connected directly to the nexthop, the interface that you are intending to route to is ambiguous. In the topology shown to the right, $nodeD has two interfaces configured. If you attempted to set up a route like this: $nodeA add-route $nodeD $nodeB you would receive an error since DETERLab staff would not easily be able to determine which of the two links on $nodeD you are referring to. Fortunately, there is an easy solution. Instead of a node, specify the link directly: $nodeA add-route [$ns link $nodeD $nodeC] $nodeB This tells us exactly which link you mean, enabling us to convert that information into a proper route command on $nodeA . The last form of the add-route command is used when adding a route to an entire LAN. It would be tedious and error prone to specify a route to each node in a LAN by hand. Instead, just route to the entire network: set clientlan [$ns make-lan \"$nodeE $nodeF $nodeG\" 100Mb 0ms] $nodeA add-route $clientlan $nodeB In general, it is still best practice to use either Session or Static routing for all but small, simple topologies. Explicitly setting up all the routes in even a moderately-sized experiment is extremely error prone. Consider this: a recently created experiment with 17 nodes and 10 subnets required 140 hand-created routes in the NS file . Two final, cautionary notes on routing: * The default route must be set to use the control network interface. You might be tempted to set the default route on your nodes to reduce the number of explicit routes used. Please avoid this. That would prevent nodes from contacting the outside world, i.e., you. * If you use your own routing daemon, you must avoid using the control network interface in the configuration. Since every node in the testbed is directly connected to the control network LAN, a naive routing daemon configuration will discover that any node is just one hop away, via the control network, from any other node and all inter-node traffic will be routed via that interface.","title":" Setting up IP routing between nodes"},{"location":"core/user-guidelines/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. User Do's and Don'ts \uf0c1 Preserving our Control Network \uf0c1 DON'T use control network unless absolutely necessary. This means: DON'T use full node names such as ping node1.YourExperiment.YourProject when communicating between nodes in your experiment DO use short names such as ping node1 . This ensures that traffic goes over experimental network. DON'T generate traffic to 192.168.x.x network. DO use addresses of experimental interfaces. These can be from any IPv4 address range, depending on your NS file, but are often from the 10.10.x.x address range. Preserving our File System \uf0c1 DON'T store large files (e.g. uncompressed kernel source) in your home or project directory unless you need them in multiple experiment instances. DO store these files locally on a node, e.g., in /tmp folder. If you need more disk space on a Linux or FreeBSD node you can mount more to /mnt/local by doing sudo mkdir /mnt/local sudo /usr/local/etc/emulab/mkextrafs /mnt/local user=`whoami` sudo chown $user /mnt/local Remember to transfer the files to your home directory before you swap out to save them. DON'T transfer large (>500 MB) files frequently between your home or project directories and a local directory on your experimental machine. If you need to regularly save and read large files that persist between experiment instances create a ticket and we will help you use our ZFS storage. DON'T perform large (> 500 MB) or frequent (< 10 s) reads/writes on your experimental nodes into your home or project directory DO perform these reads/writes on a local disk ( /tmp or /mnt/local on experimental machines) DON'T compile software or kernels in your home directory DO compile them on a local disk ( /tmp or /mnt/local on experimental machines) Preserving CPU Cycles on users \uf0c1 DON'T compile large files or run CPU intensive jobs on users.deterlab.net . DO allocate experimental nodes, store files locally and compile/run jobs there.","title":"User DOs and DON'Ts"},{"location":"core/user-guidelines/#user-dos-and-donts","text":"","title":"User Do's and Don'ts"},{"location":"core/user-guidelines/#preserving-our-control-network","text":"DON'T use control network unless absolutely necessary. This means: DON'T use full node names such as ping node1.YourExperiment.YourProject when communicating between nodes in your experiment DO use short names such as ping node1 . This ensures that traffic goes over experimental network. DON'T generate traffic to 192.168.x.x network. DO use addresses of experimental interfaces. These can be from any IPv4 address range, depending on your NS file, but are often from the 10.10.x.x address range.","title":"Preserving our Control Network"},{"location":"core/user-guidelines/#preserving-our-file-system","text":"DON'T store large files (e.g. uncompressed kernel source) in your home or project directory unless you need them in multiple experiment instances. DO store these files locally on a node, e.g., in /tmp folder. If you need more disk space on a Linux or FreeBSD node you can mount more to /mnt/local by doing sudo mkdir /mnt/local sudo /usr/local/etc/emulab/mkextrafs /mnt/local user=`whoami` sudo chown $user /mnt/local Remember to transfer the files to your home directory before you swap out to save them. DON'T transfer large (>500 MB) files frequently between your home or project directories and a local directory on your experimental machine. If you need to regularly save and read large files that persist between experiment instances create a ticket and we will help you use our ZFS storage. DON'T perform large (> 500 MB) or frequent (< 10 s) reads/writes on your experimental nodes into your home or project directory DO perform these reads/writes on a local disk ( /tmp or /mnt/local on experimental machines) DON'T compile software or kernels in your home directory DO compile them on a local disk ( /tmp or /mnt/local on experimental machines)","title":"Preserving our File System"},{"location":"core/user-guidelines/#preserving-cpu-cycles-on-users","text":"DON'T compile large files or run CPU intensive jobs on users.deterlab.net . DO allocate experimental nodes, store files locally and compile/run jobs there.","title":"Preserving CPU Cycles on users"},{"location":"core/using-nodes/","text":"Using Your Nodes \uf0c1 Know your DETER servers \uf0c1 Here are the most important things to know. www.isi.deterlab.net is the primary web interface for the testbed. users.deterlab.net is the host through which the testbed nodes are accessed and it is primary file server. scratch is the local package mirror for CentOS, Ubuntu, and FreeBSD. Accessing your nodes \uf0c1 Modes of use \uf0c1 There are several ways in which you can access your nodes: As you start developing your experiment, you may want to SSH to your experiment and configure nodes or generate traffic manually. See our guidelines for interacting with your nodes. As your work progresses you may want to develop scripts (e.g., using Bash or Python or MAGI or Ansible) to automate running of your experiments We have developed three toolkits to help you with experiment design and automation . DEW - distributed experiment workflows can be used to design your experiment in a human-readable format and generate NS file and bash scripts. We provide more guidance on this direction in DEW YouTube channel as well as in documentation on DEW Web site. Second, if you use image Ubuntu-DEW on your nodes, all the commands you type and snippets of their outputs will be saved in your project directory. You can use the tool flight_log , which is automatically installed in that image, to remind yourself of the commands you ran in the past and to select those you want to include in a Bash script. The script will be automatically generated for you. More information about this direction is in DEW YouTube channel . Third, you can use our MAGI orchestrator to create scripts that will be more robust and readable than Bash scripts.","title":"Using Your Nodes"},{"location":"core/using-nodes/#using-your-nodes","text":"","title":"Using Your Nodes"},{"location":"core/using-nodes/#know-your-deter-servers","text":"Here are the most important things to know. www.isi.deterlab.net is the primary web interface for the testbed. users.deterlab.net is the host through which the testbed nodes are accessed and it is primary file server. scratch is the local package mirror for CentOS, Ubuntu, and FreeBSD.","title":"Know your DETER servers"},{"location":"core/using-nodes/#accessing-your-nodes","text":"","title":"Accessing your nodes"},{"location":"core/using-nodes/#modes-of-use","text":"There are several ways in which you can access your nodes: As you start developing your experiment, you may want to SSH to your experiment and configure nodes or generate traffic manually. See our guidelines for interacting with your nodes. As your work progresses you may want to develop scripts (e.g., using Bash or Python or MAGI or Ansible) to automate running of your experiments We have developed three toolkits to help you with experiment design and automation . DEW - distributed experiment workflows can be used to design your experiment in a human-readable format and generate NS file and bash scripts. We provide more guidance on this direction in DEW YouTube channel as well as in documentation on DEW Web site. Second, if you use image Ubuntu-DEW on your nodes, all the commands you type and snippets of their outputs will be saved in your project directory. You can use the tool flight_log , which is automatically installed in that image, to remind yourself of the commands you ran in the past and to select those you want to include in a Bash script. The script will be automatically generated for you. More information about this direction is in DEW YouTube channel . Third, you can use our MAGI orchestrator to create scripts that will be more robust and readable than Bash scripts.","title":"Modes of use"},{"location":"core/windows/","text":"Windows XP \uf0c1 Microsoft Windows XP is supported as one of the operating system types for experiment nodes in DETER. As much as possible, we have left Windows XP \"stock\". Some Windows services are shut down: Messenger, SSDP Discovery Service, Universal Plug and Play Device Host, and Remote Registry. Other setting changes are described under Network config and Routing below. Before booting the node at swap-in time, DETER loads a fresh image of Windows XP onto the experiment nodes in parallel, using our frisbee service. DETER software automatically configures each Windows XP node, providing the expected experiment user environment including: user accounts and DETER SSH keys; remote home, project, and shared directories; and network connections. The Cygwin GNU environment is provided, including Bash and TCSH shells, the C/C++, Perl and Python programming languages, and several editors including Emacs, vim, nano and ed. Cygwin handles both Unix and Windows-style command paths, as described below . The DETERLab web interface manages a separate Windows password in the user profile, as well as making login connections to the experiment nodes. Remote Desktop Protocol service supports Windows Desktop logins from the user's workstation screen to the experiment node. SSH and Serial Console command-line connections are also supported. Windows XP installations are more hardware dependent than Linux or FreeBSD. At present, this Windows XP image runs on only pc3000 class machines. Differences from FreeBSD and Linux \uf0c1 The biggest difference of course, is that this is Windows , with Cygwin layered on top, and DETERLab management services added. In particular, this is Windows XP (NT 5.1), with various levels of service packs and updates (see below .) File Sharing \uf0c1 The second-biggest difference is that shared directories are provided not by the NFS (Network File System) protocol, but instead by the SMB (Server Message Block) protocol, otherwise known as Windows File Sharing. The \"Client for Microsoft Networks\" software contacts the SMB server, in this case Samba running on the file server known as Fs (an alias for Users .) The SMB protocol authenticates using a plain-text user name and password, encrypted as they go across the network. (These Windows Shares are then accessed by UNC paths under Cygwin mounts, [#SMB_mounts described below].) In Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with backslashes , e.g. \\\\fs\\share or \\\\fs\\<username> . User and project shares are marked \"not browsable\", so just \\\\fs shows only share . If you want to serve files from one of your experiment nodes to others, see the section on [#netbt_command The netbt command ]. Windows Passwords \uf0c1 A separate Windows password is kept for use only with experiment nodes running Windows. It is presented behind-the-scenes to rdesktop for RDP logins by our Web interface under Unix, and for the Samba mount of shared directories like your home directory under an SSH login, so you don't have to type it in those cases. You will have to type it each time if you use the Microsoft RDC (Remote Desktop Connector) client program from a Windows machine. The default Windows password is randomly generated. It's easy to change it to something easier to remember. To see or edit your Windows password, log in to DETERLab, and click Manage User Profile and then Edit Profile under User Options . You will see Windows Password fields in addition to the regular DETERLab Password fields. When you change your Windows password, you will also have to re-type it as a check. The new Windows password should propagate to the Samba server on Fs instantly, so you can swap in an experiment and log in to its Windows nodes with the new password. If you have already swapped-in experiment nodes and changed your Windows password, the account information including passwords will be updated at the next DETERLab watchdog daemon isalive interval. This should be in 3 to 6 minutes. Experiment setup for Windows nodes \uf0c1 All you have to do is put a line specifying a WINXP OS image in your experiment NS file, like this: tb-set-node-os $node WINXP-UPDATE The Windows XP images are not specific to a particular hardware type. (See the Change Log for more information.) You may explicitly specify the hardware type to run on if you wish, for example: tb-set-hardware $node pc3000 Since the bandwidth of the connection between the ISI and Berkeley portions of the testbed is constrained, it is best to run Windows nodes only on the ISI side of the testbed. This can be accomplished by creating a custom hardware type with tb-make-soft-vtype my_custom_node_type {pc3060 pc3000 pc2133 pc2133x} and then specifying that your Windows nodes must only swap-in on this custom type using tb-set-hardware $node my_custom_node_type See the note below on using the tb-set-node-failure-action command for experiments with a large number of Windows nodes. This can save a swap-in with a large number of Windows nodes, or prevent a single node boot failure on a swapmod from swapping-out the whole experiment. If you use these commands: tb-set-node-startcmd , tb-set-node-tarfiles , or tb-set-node-rpms you should read the sections on Permissions and Windows GUI programs below. The only available Windows image currently is: WINXP-UPDATE - The most recent Windows XP-SP3+. It is updated periodically from Windows Update, typically after a Microsoft \"Patch Tuesday\", the second Tuesday of each month. All critical and security fixes are installed, up through the date we pull the image file. (See the date created field on the individual WINXP Image IDs ). Note The Windows Firewall is disabled by default (as it will inform you repeatedly!) Network config \uf0c1 Some default Windows networking features are disabled. NetBT (!NetBios over TCP) ( NetbiosOptions=2 ) and DNS auto-registration ( DisableDynamicUpdate=1 ) are disabled to allow network idle detection by the slothd service. TCP/IP address autoconfiguration is disabled ( IPAutoconfigurationEnabled=0 ) so that un-switched interfaces like the sixth NICs on the pc3000's don't get bogus Microsoft class B network 169.254.0.0 addresses assigned. The Windows ipconfig /all command only shows the configuration information for the enabled network interfaces. There will always be one enabled control net interface on the 192.168.0.0/22 network. The others are disabled if not used in your experiment. (See file /var/emulab/boot/ipconfig-cache for a full listing from boot time, including the interfaces that were later disabled.) If you specified links or LANs in your experiment network topology, other interfaces will be enabled, with an IP address, subnet mask, and gateway that you can specify in the NS file. Notice that the Windows names of the interfaces start with Local Area Connection and have a number appended. You can't count on what this number is, since it depends on the order the NIC's are probed as Windows boots. Note Often, we have seen ipconfig report an IP address and mask of 0.0.0.0 , while the TCP/IP properties dialog boxes and the netsh command show the proper values. Our startup scripts disable and re-enable the network interface in an attempt to reset this. Sometimes it doesn't work, and another reboot is done in an attempt to get the network up. Routing \uf0c1 Full-blown router nodes cannot run Windows, i.e. rtproto Session is not supported. However, basic routing between connected network components of your experiment topology works. The Windows command to see the routing tables is route print . The IPEnableRouter=1 registry key is set on multi-homed hosts in the experiment network, before they are rebooted to change the hostname. rtproto Static is supported in all recent WINXP images, but not in WINXP-02-16 (2005) or before. rtproto Static-old or rtproto Manual will work in any image. There is more information on routing in the Routing section of the Core Guide . Windows nodes boot twice \uf0c1 Notice that Windows reboots an extra time after being loaded onto a node during swap-in. It must reboot after changing the node name to set up the network stack properly. Be patient, Windows XP doesn't boot quickly. With hardware-independent , ( sysprep'ed ) images, the first boot is actually running Mini-Setup as well, setting up device drivers and so on. It's best not to log in to the nodes until the experiment is fully swapped-in. (You may be able to log in briefly between the first two reboots; if you see the wrong pcXXX name, you'll know that a reboot is imminent.) You can know that the swap-in process is finished by any of these methods: Waiting until you get the \"experiment swapped in\" email from DETERLab. Checking the node status on the experiment status page in DETERLab. (You must refresh the page to see node status change.) Watching the realtime swap-in log to monitor its progress. Note Sometimes Windows XP fails to do the second reboot. One reason is transient race conditions in the Windows startup, for example in the network stack when there are multiple network interface devices being initialized at the same time. We make a strong effort to recover from this, but if the recovery code fails, by default it results in a swap-in or swapmod failure. At boot time, the startup service on Windows XP runs the /usr/local/etc/emulab/rc/rc.bootsetup script, logging output to /var/log/bootsetup.log . If you're having swap-in problems and rc.bootsetup doesn't finish sending ISUP to DETERLab within 10 minutes, the node will be rebooted. After a couple of reboot cycles without a ISUP , DETERLab gives up on the node. You can cause these boot-time problems to be nonfatal by adding this line to your ns file for each Windows node : tb-set-node-failure-action $node \"nonfatal\" (where $node is replaced with the node variable, of course.) DETERLab will still complain if it doesn't get the ISUP signal at the end of rc.bootsetup, but the swap-in or swapmod will proceed and allow you to figure out what's happening. Then you will probably have to manually reboot the failed Windows node to make it available to your experiment. If you try to login to a node after swap-in to diagnose the problem and your Windows password isn't honored, use this command on Ops to remotely reboot the node: node_reboot pcxxx If you are able to log in but your remote home directory isn't mounted, this is another symptom of a partial set-up. You have the additional option of executing this command on the node itself: /sbin/reboot This gives Windows another chance to get it right. Login connections to Windows \uf0c1 You can manually start up the SSH or RDP client programs to connect and log in to nodes in your experiment, or use the console command on Ops. You will have to type your Windows Password when logging in, except for SSH when you have ssh-agent keys loaded. Or you can set up your browser to automatically connect in one click from the DETERLab web interface and pop up a connection window. Once you start swapping in an experiment, the Experiment Information page contains a table of the physical node ID and logical node name, status, and connection buttons. The captions at the top of the button columns link to pages explaining how to set up up mime-types in your browser to make the buttons work, from FreeBSD, Linux, and Windows workstations: SSH (setup) - The SSH connection button gives a Bash or TCSH shell, as usual. Your DETERLab SSH keys are installed on the node in a /sshkeys subdirectory. Console - The serial console is supported for Cygwin shell logins using the agetty and sysvinit packages. This is the only way in when network connections are closed down! You can also monitor the Frisbee loading and booting of the Windows image on the console. RDP - The RDP button starts up a Remote Desktop Protocol connection, giving a Windows Desktop login from the user's workstation screen to the experiment node. The rdesktop client software is used from Linux and Unix client workstations. A Microsoft RDC (Remote Desktop Connector) client program is included in Windows XP, and may be installed onto other versions of Windows as well. It has the feature that you can make it full-screen without (too much) confusion, since it hangs a little tab at the top of the screen to switch back. Unfortunately, we have no way to present your DETERLab Windows password to RDC, so you'll have to type it on each login. Note If you import dot-files into DETERLab that replace the system execution search path rather than add to it, you will have a problem running Windows system commands in shells. Fix this by adding /cygdrive/c/WINDOWS/system32 and /cygdrive/c/WINDOWS to your $PATH in ~/.cshrc and either ~/.bash_profile or ~/.profile . Don't worry about your home directory dot-files being shared among Windows, FreeBSD, and Linux nodes; non-existent directories in the $PATH are ignored by shells. When new DETERLab user accounts are created, the default CSH and Bash dotfiles are copied from the FreeBSD /usr/share/skel . They replace the whole $PATH rather than add to it. Then we append a DETERLab-specific part that takes care of the path, conditionally adding the Windows directories on Cygwin. Note The Windows ping program has completely different option arguments from the Linux and FreeBSD ones, and they differ widely from each other. There is a ping package in Cygwin that is a port of the 4.3bsd ping. Its options are close to a common subset of the Linux and FreeBSD options, so it will be included in future WINXP images: ping [ -dfqrv ] host [ packetsize [count [ preload]]] You can load it yourself now using Cygwin Setup . Note There are no Cygwin ports of some other useful networking commands, such as traceroute and ifconfig -a . The Windows system equivalents are tracert and ipconfig /all . RDP details \uf0c1 Here are some fine points and hints for RDP logins to remote Windows desktops: Microsoft allows only one desktop login at a time to Windows XP , although this is the same Citrix Hydra technology that supports many concurrent logins to Terminal Server or Server 2003. The Fast User Switching option to XP is turned on, so a second RDP connection disconnects a previous one rather than killing it. Similarly, just closing your RDP client window disconnects your Windows Login session rather than killing it. You can reconnect later on without losing anything. SSH doesn't count as a desktop, so you can SSH in and use this command: qwinsta (Query WINdows STAtion) to show existing winstation sessions and their session ID's, and this one to reset (kill) a session by ID: rwinsta . We rename My Computer to show the PCxxx physical node name, but it doesn't appear on the Windows XP desktop by default. The XP user interface incorporates \"My Computer\" into the upper-right quadrant of the \"Start\" menu by default, and removes it from the desktop. You can go back to the \"classic\" user interface of Windows 2000, including showing \"My Computer\". Right-click on the background of the Taskbar which contains the \"Start\" button at the left, and choose \"Properties\". Select the \"Start Menu\" tab, click the \"Classic Start menu\" radio-button, and click \"OK\". Alternatively, you can force \"My Computer\" to appear on your XP desktop by right-clicking on the desktop background and choosing \"Properties\". Select the \"Desktop\" tab and click \"Customize Desktop...\" to get the \"Desktop Items\" dialog. Turn on the \"My Computer\" checkbox, then click \"OK\" twice. There are several Desktop icons (i.e. \"shortcuts\") installed by default in the XP images: Computer Management, Bash and TCSH shells, and NtEmacs . You will notice two flavors of Bash and TCSH icons on the desktop, labeled rxvt and Cygwin . The rxvt shells run in windows with X -like cut-and-paste mouse clicks: Left-click starts a selection, Right-click extends it, and middle-click pastes. These are the ones to use if you're connecting from an X workstation. Note The default colors used in Bash and rxvt don't work well in 4-bit color mode under RDP. Make sure you update your rdp-mime.pl to get the rdesktop -a 16 argument for 16-bit color. Or, you can over-ride the rxvt defaults by putting lines in your ~/.Xdefaults file like this: rxvt*background: steelblue The Cygwin shells run in a Windows Terminal window, just as the Windows cmd.exe does. These are the ones to use if you're connecting from a Windows workstation. Quick-edit mode is on by default, so you can cut-and-paste freely between your local workstation desktop and your remote RDP desktops. In a Windows Terminal window on your RDP remote desktop, the quick-edit cut-and-paste mouse clicks are: Left-drag the mouse to mark a rectangle of text, highlighting it. Type Enter or right-click the mouse when text is highlighted , to copy* the selected text to the clipboard. ( Escape* cancels the selection without copying it.) Right-click the mouse with nothing selected to paste the contents of the clipboard. On the first login by a user , Windows creates the user's Windows profile directory under C:\\Documents and Settings , and creates the registry key (folder) for persistent settings for that user. We arrange that early in the user's login process, a user HOME environment variable value is set in the user's registry. Otherwise Emacs wouldn't know how to find your .emacs setup file in your remotely mounted home directory. User \"root\" is special, and has a local home directory under /home . /home is a Cygwin symbolic link to C:\\Documents and Settings . The Windows XP Start menu has no Shutdown button under RDP. Instead, it is labeled Disconnect and only closes the RDP client window, leaving the login session and the node running. If you simply close the window, or the RDP client network connection is lost, you are also disconnected rather than logged out. When you reconnect, it comes right back, just as it was. To restart the computer, run /sbin/reboot , or use the \"Shut Down\" menu of Task Manager . One way to start Task Manager is to right-click on the background of the Taskbar at the bottom of the screen and select \"Task Manager\". The netbt command \uf0c1 The NetBT (Netbios over TCP) protocol is used to announce shared directories (folders) from one Windows machine to others. (See the Name and Session services in http://en.wikipedia.org/wiki/Netbios .) The SMB (Server Message Block) protocol is used to actually serve files. (See http://en.wikipedia.org/wiki/Server_Message_Block .) In DETERLab, we normally disable NetBT on experiment nodes, because it chatters and messes up slothd network idle detection, and is not needed for the usual SMB mounts of /users , /proj , and /share dirs, which are served from a Samba service on fs . However, NetBT does have to be enabled on the experiment nodes if you want to make Windows file shares between them. The netbt script sets the registry keys on the Windows network interface objects. Run it on the server nodes (the ones containing directories which you want to share) and reboot them afterwards to activate. There is an optional -r argument to reboot the node. Usage: netbt [-r] off|on If you use netbt to turn on NetBT, it persists across reboots. No reboot is necessary if you use Network Connections in the Control Panel to turn on NetBT. It takes effect immediately, but is turned off at reboot unless you do netbt on afterward as well. Right-click Local Area Connection (or the name of another connection, if appropriate), click Properties, click Internet Protocol (TCP/IP), and then click the Properties button. On the Internet Protocol (TCP/IP) Properties page, click the Advanced button, and click the WINS tab. Select Enable or Disable NetBIOS over TCP/IP. ipconfig /all reports \"NetBIOS over Tcpip . . . : Disabled\" on interfaces where NetBT is disabled, and says nothing where NetBT is enabled. To start sharing a directory, on the node, use the net share command, or turn on network sharing on the Sharing tab of the Properties of a directory (folder.) On XP-SP2 or above, when you first do this, the \"Network sharing and security\" subdialog says: As a security measure, Windows has disabled remote access to this computer. However, you can enable remote access and safely share files by running the _Network_Setup_Wizard_. _If_you_understand_the_security_risks_but_want_to_share_ _files_without_running_the_wizard,_click_here._\" Skip the wizard and click the latter (\"I understand\") link. Then click \"Just enable file sharing\", and \"OK\". Then you finally get the click-box to \"Share this folder on the network\". The machine names for UNC paths sharing are the same as in shell prompts: pcXXX , where XXX is the machine number. These will show up in My Network Places / Entire Network / Microsoft Windows Network / DETER once you have used them. IP numbers can also be used in UNC paths, giving you a way to share files across experiment network links rather than the control network. There is an DETER-generated LMHOSTS file, to provide the usual node aliases within an experiment, but it is currently ignored even though \"Enable LMHOSTS lookup\" is turned on in the TCP/IP WINS settings. Try nbtstat -c and nbtstat -R to experiment with this. (See the Microsoft doc for nbtstat . Making Custom Windows OS Images \uf0c1 Making custom Windows images is similar to doing it on the other DETER operating systems , except that you must do a little more work to run the prepare script as user root since there are no su or sudo commands on Windows. This is optional on the other OS types, but on Windows, proper TCP/IP network setup depends on prepare being run. Log in to the node where you want to save a custom image. Give the shell command to change the root password. Pick a password string you can remember, typing it twice as prompted: % passwd root Enter the new password (minimum of 5, maximum of 8 characters). Please use a combination of upper and lower case letters and numbers. New password: Re-enter new password: This works because you are part of the Windows Administrators group . Otherwise you would have to already know the root password to change it. Note If you change the root password and reboot Windows before running prepare below, the root password will not match the definitions of the DETER Windows services (daemons) that run as root, so they will not start up. Log out all sessions by users other than root , because prepare will be unable to remove their login profile directories if they are logged in. (See QWINSTA .) Log in to the node as user root through the Console or SSH, using the password you set above, then run the prepare command. (It will print \"Must be root to run this script!\" and do nothing if not run as root.) /usr/local/etc/emulab/prepare If run without option arguments, prepare will ask for the root password you want to use in your new image, prompting twice as the passwd command did above. It needs this to redefine the DETER Windows services (daemons) that run as root. It doesn't need to be the same as the root password you logged in with, since it sets the root password to be sure. The Administrator password is changed as well, since the Sysprep option needs that (below.) You can give the -p option to specify the root password on the command line: /usr/local/etc/emulab/prepare -p myRootPwd The -n option says not to change the passwords at all, and the DETER Windows services are not redefined. /usr/local/etc/emulab/prepare -n The -s option is used to make hardware-independent images using the Windows Sysprep deploy tool. If you use it with the -n option instead of giving a password, it assumes that you separately blank the Administrator password, or edit your Administrator password into the [GuiUnattended]AdminPassword entry of the sysprep.inf file. /usr/local/etc/emulab/prepare -s -p myRootPwd Note This must be done from a login on the serial console , because Sysprep shuts down the network. prepare -s refuses to run from an SSH or RDP login. Note Currently, hardware-independent images must be made on a pc850, and will then run on the pc600, pc3000, and pc3000w as well. There is an unresolved boot-time problem going the other direction, from the pc3000 to a pc850 or pc600. Windows normally casts some aspects of the NT image into concrete at the first boot after installation, including the specific boot disk driver to be used by the NT loader (IDE, SCSI, or SATA.) Sysprep is used by PC hardware manufacturers as they make XP installation disks with their own drivers installed. The Sysprep option to run an unattended Mini-Setup at first boot instead of the normal \"Out Of the Box Experience\" is used in some large corporate roll-outs. We do both. The DETER /share/windows/sysprep directory contains several versions of the XP deploy tools matched to the XP service pack level, appropriate device driver directories, and a draft sysprep.inf file to direct the automated install process. Mini-setup needs to reboot after setting up device drivers. XP also needs to [#Boots_twice reboot] after changing the host name. We combine the two by using a Cmdlines.txt script to run rc.firstboot -mini to set the host name at the end of Mini-Setup . Thus we only pay the extra time to set up device drivers and so on from scratch, about two minutes, rather than adding a third hardware and XP reboot cycle. Note As you create your Image Descriptor, set the reboot wait-time to 360 rather than 240 so that swap-ins don't time out. Then log out and create your custom image . Note Windows XP is too big to fit in the partitioning scheme used by FreeBSD and Linux, so it's necessary when making a Windows custom image to specify Partition 1 , and click Whole Disk Image. When you're testing your custom image, it's a good idea to set the tb-set-node-failure-action to \"nonfatal\" in the ns file so you get a chance to examine an image that hasn't completed the set-up process. See the note below for other useful ideas. Cygwin \uf0c1 Cygwin is GNU + Cygnus + Windows , providing Linux-like functionality at the API, command-line, and package installation levels. Cygwin documentation \uf0c1 Cygwin is well documented. Here are some links to get you started: Users guide Cygwin highlights Cygwin-added utilities FAQ API compatibility and Cygwin functions Cygwin packages \uf0c1 A number of optional Cygwin packages are installed in the image due to our building and running the DETER client software, plus some editors for convenience. These packages are currently agetty, bison, cvs, cygrunsrv, ed, file, flex, gcc, gdb, inetutils, make, minires-devel, more, nano, openssh, openssl-devel, patch, perl, perl-libwin32, psmisc, python, rpm, rsync, shutdown, sysvinit, tcsh, vim, wget, and zip. The Cygwin command cygcheck -c lists the packages that are installed, and their current version number and status. Package-specific notes and/or documentation for installed packages are in /usr{,/share}/doc/Cygwin/*.README and /usr/share/doc/*/README files. The Cygwin package site lists the available pre-compiled packages and provides a search engine. If you want to install more Cygwin pre-compiled packages, run the graphical installer: C:/Software/Cygwin/setup.exe The Cygwin command cygcheck -l package-name lists the contents of an installed package, which may help you to make a tarfile or rpm from a package you have installed. You can then cause it to be installed automatically by DETER into all of the nodes of your experiment. See the [Tutorial#TARBALLS Tutorial] for more information about installing RPM's and tarballs . Watch out for post-install scripts in: /etc/postinstall/package-name.sh{,.done} Many packages not in the Cygwin package site have also been ported to Cygwin already. Download the sources to an experiment node and try ./configure make make install as usual. SMB mounts and Samba \uf0c1 User home directories and other shared directories are served by fs , another alias for Ops/Users, via the SMB protocol (Server Message Block, also known as Windows File Sharing) with the Windows Client connecting to the Samba server. UNC paths with leading double-slashes and a server name, e.g. //fs , are used to access the SMB Shares under Cygwin. DETER then uses the Cygwin mount command to make them appear on the usual Unix paths for the DETER shared directories: /users/<username> , /proj/<pid> , /group/<pid>/<gid> , and /share . The Cygwin mount command lists what you could access on the Samba server, with the UNC path in the first column. Unix file permissions may further limit your access on the Samba server. Log in to Ops to investigate. /share/windows contains Windows software. See /share/windows/README.bin for descriptions of binary packages available for installation. In Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with backslashes , e.g. \\\\fs\\share or \\\\fs\\<username> . User and project shares are marked \"not browsable\", so just \\\\fs shows only share . Windows limitation: There is only one protection mask for everything in a whole mount/share under SMB. It's set in the \"share properties\" on the server (Samba config file in this case) so chmod will do you no good across SMB. Cygwin limitation: There is a hard-coded limit of 30 mount points in Cygwin. Cygwin uses 4 of them, and DETER uses another 3 or 4. So some of your /users mounts will fail on Windows startup if you have more than 23 or 24 members in your project, unless they are grouped into smaller subgroups. Cygwin arcana \uf0c1 File paths Cygwin accepts either flavor of slashes in paths, Unix/POSIX-style forward-slashes, or Windows-style back-slashes. In Unix shell commands, backslashes need to be quoted. Single-quotes work best. Doubling each backslash also works. This must also be done inside double-quotes. Examples: '\\single\\quoted' , \"\\\\double\\\\quoted\" , \\\\un\\\\quoted . (The difference between double and single quotes is that $variable references and back-quoted command execution are expanded in double-quotes.) When you invoke Windows (as opposed to Cygwin) commands, for example net use , they will know nothing about Unix-style paths in their arguments. The cygpath utility is an aid to converting paths between the Unix and Windows conventions. cygpath -w converts its arguments to Windows format, and cygpath -u converts its arguments to Unix format, e.g. $ cygpath -w /cygdrive/c/WINDOWS c:\\WINDOWS $ cygpath -u 'c:\\WINDOWS' /cygdrive/c/WINDOWS Mount points Cygwin mount points are shown by the mount and df commands. Note that there is a hard-coded limit of 30 mount points in Cygwin. Attempts to use the Cygwin mount command after that will fail. See the discussion of mount points and UNC //machine paths to SMB shares above . Another special case is the Unix root , \" / \". It's mounted to C:\\cygwin in the Windows filesystem. Drive letter mounts Cygwin knows about drive letter prefixes like C: \u00c2 , which are equivalent to /cygdrive/<drive-letter> \u00c2 . However, /cygdrive , like /dev , isn't a real directory, so you can't ls it. Some Windows software requires drive-letter mounts to be created for its use. You can use the Windows net use command to associate drive letters with UNC paths to SMB shares, e.g. net use W: '\\\\fs\\share\\windows' You can use the Windows subst command to associate drive letters with local paths, e.g. subst T: 'C:\\Temp' Filename completion in Cygwin shells with <Tab> doesn't work following a drive-letter prefix, but it works normally after a /cygdrive/ prefix. Also, filename completion is case-sensitive, although the underlying Windows is case-insensitive, so a filename in the wrong case is still opened properly. NTSEC Cygwin is running in NTSEC (NT Security) mode, so /etc/passwd and /etc/group contain Windows SID's as user and group ID's. Your Windows UID is the computer SID with a user number appended, something like S-1-5-21-2000478354-436374069-1060284298-1334 . Cygwin commands, such as id , ls -ln , and chown/chgrp , use the numeric suffix as the uid, e.g. 1334 . This is different from your normal DETER Unix user ID number, and the Samba server takes care of the difference. The id command reports your user id and group memberships. Note that all users are in group None on XP. Contrary to the name, this is a group that contains all users . It was named Everybody on Windows 2000, which was a better name. setuid There is no direct equivalent of the Unix setuid programs under Windows, and hence no su or sudo commands. The Windows equivalent to running a Unix command as root is membership in the Windows Administrators group. DETER project members who have either local_root or group_root privileges are put in group wheel , another alias for Administrators . Project members with user privileges are not members of the wheel group. You can ssh a command to the node as the target user, as long as you arrange for the proper authentication. For C/C++ code, there is a setuid() function in the Cygwin library, which \"impersonates\" the user if proper setup is done first. root There is not normally a Windows account named root . root on XP is just another user who is a member of the Administrators group, see below. We create a root account as part of the DETER setup to own installed software, and to run services and Unix scripts that check that they're running with root privileges. You can log in as root via RDP, ssh , or the serial console if you change the root password as described in the custom Windows OS images section. The root user does not have any Samba privileges to access Samba shared mounts, including the /proj , /groups , and /users . Administrators group All users are members of the Windows Administrators group. (The DETER non-local-root user property is not implemented on Windows.) Membership in the Windows Administrators group is very different from being root on Unix, and is also different from being logged in as Administrator. Administrators group membership on Windows only means you can set the ownership, group, and permissions on any file using the Cygwin chown , chgrp , chmod , or their Windows equivalents. Until you have done that, you can be completely locked out by read, write, or execute/open permissions of the directory or files. Another subtlety is that the group called None on XP is what used to be named Everybody on Windows 2000. All users are automatically in group None , so in practice setting group None permissions is no different from setting public access permissions. Permissions Cygwin does a pretty good job of mapping Unix user-group-other file permissions to Windows NT security ACLs. On Windows, unlike Unix, file and directory permissions can lock out root, Administrator, or SYSTEM user access. Many Unix scripts don't bother with permissions if they're running as root, and hence need modification to run on Cygwin. This creates a potential problem with the tb-set-node-tarfiles and tb-set-node-rpms commands. The tb-set-node-tarfiles page says \"Notes: 1. ... the files are installed as root\". So you can easily install files that your login doesn't have permission to access. The solution is to chmod the files before making the tarball or rpm file to grant appropriate access permissions. Executables Cygwin tries to treat .exe files the same as executable files without the .exe suffix, but with execute permissions turned on. (See the Cygwin Users Guide .) This breaks down in Makefile actions and scripts, where rm , ls -l , and install commands may need an explicit .exe added. Windows GUI programs You cannot run Windows GUI (Graphical User Interface) programs under ssh, on the serial console, or by tb-set-node-startcmd. There is no user login graphics context until you log in via RDP. However, you can use a startcmd to set a Windows registry key that causes a GUI program to be run automatically for all users when they log in to the node via RDP , if that's what you want. The program can be one that is installed by tb-set-node-tarfiles. You can pick any regkey name you want and put it in the Run registry folder. It's good not to step on the ones already there, so choose a name specific to your program. Put the following in your startcmd script: regtool -s set /HKLM/SOFTWARE/Microsoft/Windows/CurrentVersion/Run/mypgm 'C:\\mypgm\\mypgm.exe' (where mypgm is the name of your program, of course.) Notice that the value string is single-quoted with C: and backslashes. Windows interprets this regkey, and wants its flavor of file path. NtEmacs \uf0c1 We don't include the Cygwin X server in our XP images to keep the bulk and complexity down. So NtEmacs 21.3 is provided instead of the Cygwin X Emacs. NtEmacs \"frames\" are windows on the Windows Desktop, e.g. ^X-5-2 makes another one. The /usr/local/bin/emacs executable is a symlink to /cygdrive/c/emacs-21.3/bin/runemacs.exe , which starts up an Emacs on the desktop. This only works under RDP, since SSH logins have a null desktop. There is also a /usr/local/bin/emacs-exe executable, a symlink to /cygdrive/c/emacs-21.3/bin/emacs.exe , which is only useful as an Emacs compiler. It could be used to run Emacs in an SSH or Serial Console login window with the -nw (no windows) flag, except that it exits with emacs: standard input is not a tty . Another thing not to try is running emacs-exe -nw in a Bash or TCSH shell on the RDP desktop. It crashed Windows XP when I tried it. Can drag-and-drop files from Windows Explorer to !NtEmacs windows. cygwin-mount.el in c:/emacs-21.3/site-lisp/ makes Cygwin mounts visible within !NtEmacs . It doesn't do Cygwin symlinks yet. Options - See ~root/.emacs mouse-wheel-mode CUA-mode option ( ^C copy / ^X cut on selection, ^V paste, ^Z undo). Ctrl and Alt key mappings, etc.","title":"Windows XP"},{"location":"core/windows/#windows-xp","text":"Microsoft Windows XP is supported as one of the operating system types for experiment nodes in DETER. As much as possible, we have left Windows XP \"stock\". Some Windows services are shut down: Messenger, SSDP Discovery Service, Universal Plug and Play Device Host, and Remote Registry. Other setting changes are described under Network config and Routing below. Before booting the node at swap-in time, DETER loads a fresh image of Windows XP onto the experiment nodes in parallel, using our frisbee service. DETER software automatically configures each Windows XP node, providing the expected experiment user environment including: user accounts and DETER SSH keys; remote home, project, and shared directories; and network connections. The Cygwin GNU environment is provided, including Bash and TCSH shells, the C/C++, Perl and Python programming languages, and several editors including Emacs, vim, nano and ed. Cygwin handles both Unix and Windows-style command paths, as described below . The DETERLab web interface manages a separate Windows password in the user profile, as well as making login connections to the experiment nodes. Remote Desktop Protocol service supports Windows Desktop logins from the user's workstation screen to the experiment node. SSH and Serial Console command-line connections are also supported. Windows XP installations are more hardware dependent than Linux or FreeBSD. At present, this Windows XP image runs on only pc3000 class machines.","title":"Windows XP"},{"location":"core/windows/#differences-from-freebsd-and-linux","text":"The biggest difference of course, is that this is Windows , with Cygwin layered on top, and DETERLab management services added. In particular, this is Windows XP (NT 5.1), with various levels of service packs and updates (see below .)","title":"Differences from FreeBSD and Linux"},{"location":"core/windows/#file-sharing","text":"The second-biggest difference is that shared directories are provided not by the NFS (Network File System) protocol, but instead by the SMB (Server Message Block) protocol, otherwise known as Windows File Sharing. The \"Client for Microsoft Networks\" software contacts the SMB server, in this case Samba running on the file server known as Fs (an alias for Users .) The SMB protocol authenticates using a plain-text user name and password, encrypted as they go across the network. (These Windows Shares are then accessed by UNC paths under Cygwin mounts, [#SMB_mounts described below].) In Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with backslashes , e.g. \\\\fs\\share or \\\\fs\\<username> . User and project shares are marked \"not browsable\", so just \\\\fs shows only share . If you want to serve files from one of your experiment nodes to others, see the section on [#netbt_command The netbt command ].","title":"File Sharing"},{"location":"core/windows/#windows-passwords","text":"A separate Windows password is kept for use only with experiment nodes running Windows. It is presented behind-the-scenes to rdesktop for RDP logins by our Web interface under Unix, and for the Samba mount of shared directories like your home directory under an SSH login, so you don't have to type it in those cases. You will have to type it each time if you use the Microsoft RDC (Remote Desktop Connector) client program from a Windows machine. The default Windows password is randomly generated. It's easy to change it to something easier to remember. To see or edit your Windows password, log in to DETERLab, and click Manage User Profile and then Edit Profile under User Options . You will see Windows Password fields in addition to the regular DETERLab Password fields. When you change your Windows password, you will also have to re-type it as a check. The new Windows password should propagate to the Samba server on Fs instantly, so you can swap in an experiment and log in to its Windows nodes with the new password. If you have already swapped-in experiment nodes and changed your Windows password, the account information including passwords will be updated at the next DETERLab watchdog daemon isalive interval. This should be in 3 to 6 minutes.","title":"Windows Passwords"},{"location":"core/windows/#experiment-setup-for-windows-nodes","text":"All you have to do is put a line specifying a WINXP OS image in your experiment NS file, like this: tb-set-node-os $node WINXP-UPDATE The Windows XP images are not specific to a particular hardware type. (See the Change Log for more information.) You may explicitly specify the hardware type to run on if you wish, for example: tb-set-hardware $node pc3000 Since the bandwidth of the connection between the ISI and Berkeley portions of the testbed is constrained, it is best to run Windows nodes only on the ISI side of the testbed. This can be accomplished by creating a custom hardware type with tb-make-soft-vtype my_custom_node_type {pc3060 pc3000 pc2133 pc2133x} and then specifying that your Windows nodes must only swap-in on this custom type using tb-set-hardware $node my_custom_node_type See the note below on using the tb-set-node-failure-action command for experiments with a large number of Windows nodes. This can save a swap-in with a large number of Windows nodes, or prevent a single node boot failure on a swapmod from swapping-out the whole experiment. If you use these commands: tb-set-node-startcmd , tb-set-node-tarfiles , or tb-set-node-rpms you should read the sections on Permissions and Windows GUI programs below. The only available Windows image currently is: WINXP-UPDATE - The most recent Windows XP-SP3+. It is updated periodically from Windows Update, typically after a Microsoft \"Patch Tuesday\", the second Tuesday of each month. All critical and security fixes are installed, up through the date we pull the image file. (See the date created field on the individual WINXP Image IDs ). Note The Windows Firewall is disabled by default (as it will inform you repeatedly!)","title":"Experiment setup for Windows nodes"},{"location":"core/windows/#network-config","text":"Some default Windows networking features are disabled. NetBT (!NetBios over TCP) ( NetbiosOptions=2 ) and DNS auto-registration ( DisableDynamicUpdate=1 ) are disabled to allow network idle detection by the slothd service. TCP/IP address autoconfiguration is disabled ( IPAutoconfigurationEnabled=0 ) so that un-switched interfaces like the sixth NICs on the pc3000's don't get bogus Microsoft class B network 169.254.0.0 addresses assigned. The Windows ipconfig /all command only shows the configuration information for the enabled network interfaces. There will always be one enabled control net interface on the 192.168.0.0/22 network. The others are disabled if not used in your experiment. (See file /var/emulab/boot/ipconfig-cache for a full listing from boot time, including the interfaces that were later disabled.) If you specified links or LANs in your experiment network topology, other interfaces will be enabled, with an IP address, subnet mask, and gateway that you can specify in the NS file. Notice that the Windows names of the interfaces start with Local Area Connection and have a number appended. You can't count on what this number is, since it depends on the order the NIC's are probed as Windows boots. Note Often, we have seen ipconfig report an IP address and mask of 0.0.0.0 , while the TCP/IP properties dialog boxes and the netsh command show the proper values. Our startup scripts disable and re-enable the network interface in an attempt to reset this. Sometimes it doesn't work, and another reboot is done in an attempt to get the network up.","title":"Network config"},{"location":"core/windows/#routing","text":"Full-blown router nodes cannot run Windows, i.e. rtproto Session is not supported. However, basic routing between connected network components of your experiment topology works. The Windows command to see the routing tables is route print . The IPEnableRouter=1 registry key is set on multi-homed hosts in the experiment network, before they are rebooted to change the hostname. rtproto Static is supported in all recent WINXP images, but not in WINXP-02-16 (2005) or before. rtproto Static-old or rtproto Manual will work in any image. There is more information on routing in the Routing section of the Core Guide .","title":"Routing"},{"location":"core/windows/#windows-nodes-boot-twice","text":"Notice that Windows reboots an extra time after being loaded onto a node during swap-in. It must reboot after changing the node name to set up the network stack properly. Be patient, Windows XP doesn't boot quickly. With hardware-independent , ( sysprep'ed ) images, the first boot is actually running Mini-Setup as well, setting up device drivers and so on. It's best not to log in to the nodes until the experiment is fully swapped-in. (You may be able to log in briefly between the first two reboots; if you see the wrong pcXXX name, you'll know that a reboot is imminent.) You can know that the swap-in process is finished by any of these methods: Waiting until you get the \"experiment swapped in\" email from DETERLab. Checking the node status on the experiment status page in DETERLab. (You must refresh the page to see node status change.) Watching the realtime swap-in log to monitor its progress. Note Sometimes Windows XP fails to do the second reboot. One reason is transient race conditions in the Windows startup, for example in the network stack when there are multiple network interface devices being initialized at the same time. We make a strong effort to recover from this, but if the recovery code fails, by default it results in a swap-in or swapmod failure. At boot time, the startup service on Windows XP runs the /usr/local/etc/emulab/rc/rc.bootsetup script, logging output to /var/log/bootsetup.log . If you're having swap-in problems and rc.bootsetup doesn't finish sending ISUP to DETERLab within 10 minutes, the node will be rebooted. After a couple of reboot cycles without a ISUP , DETERLab gives up on the node. You can cause these boot-time problems to be nonfatal by adding this line to your ns file for each Windows node : tb-set-node-failure-action $node \"nonfatal\" (where $node is replaced with the node variable, of course.) DETERLab will still complain if it doesn't get the ISUP signal at the end of rc.bootsetup, but the swap-in or swapmod will proceed and allow you to figure out what's happening. Then you will probably have to manually reboot the failed Windows node to make it available to your experiment. If you try to login to a node after swap-in to diagnose the problem and your Windows password isn't honored, use this command on Ops to remotely reboot the node: node_reboot pcxxx If you are able to log in but your remote home directory isn't mounted, this is another symptom of a partial set-up. You have the additional option of executing this command on the node itself: /sbin/reboot This gives Windows another chance to get it right.","title":"Windows nodes boot twice"},{"location":"core/windows/#login-connections-to-windows","text":"You can manually start up the SSH or RDP client programs to connect and log in to nodes in your experiment, or use the console command on Ops. You will have to type your Windows Password when logging in, except for SSH when you have ssh-agent keys loaded. Or you can set up your browser to automatically connect in one click from the DETERLab web interface and pop up a connection window. Once you start swapping in an experiment, the Experiment Information page contains a table of the physical node ID and logical node name, status, and connection buttons. The captions at the top of the button columns link to pages explaining how to set up up mime-types in your browser to make the buttons work, from FreeBSD, Linux, and Windows workstations: SSH (setup) - The SSH connection button gives a Bash or TCSH shell, as usual. Your DETERLab SSH keys are installed on the node in a /sshkeys subdirectory. Console - The serial console is supported for Cygwin shell logins using the agetty and sysvinit packages. This is the only way in when network connections are closed down! You can also monitor the Frisbee loading and booting of the Windows image on the console. RDP - The RDP button starts up a Remote Desktop Protocol connection, giving a Windows Desktop login from the user's workstation screen to the experiment node. The rdesktop client software is used from Linux and Unix client workstations. A Microsoft RDC (Remote Desktop Connector) client program is included in Windows XP, and may be installed onto other versions of Windows as well. It has the feature that you can make it full-screen without (too much) confusion, since it hangs a little tab at the top of the screen to switch back. Unfortunately, we have no way to present your DETERLab Windows password to RDC, so you'll have to type it on each login. Note If you import dot-files into DETERLab that replace the system execution search path rather than add to it, you will have a problem running Windows system commands in shells. Fix this by adding /cygdrive/c/WINDOWS/system32 and /cygdrive/c/WINDOWS to your $PATH in ~/.cshrc and either ~/.bash_profile or ~/.profile . Don't worry about your home directory dot-files being shared among Windows, FreeBSD, and Linux nodes; non-existent directories in the $PATH are ignored by shells. When new DETERLab user accounts are created, the default CSH and Bash dotfiles are copied from the FreeBSD /usr/share/skel . They replace the whole $PATH rather than add to it. Then we append a DETERLab-specific part that takes care of the path, conditionally adding the Windows directories on Cygwin. Note The Windows ping program has completely different option arguments from the Linux and FreeBSD ones, and they differ widely from each other. There is a ping package in Cygwin that is a port of the 4.3bsd ping. Its options are close to a common subset of the Linux and FreeBSD options, so it will be included in future WINXP images: ping [ -dfqrv ] host [ packetsize [count [ preload]]] You can load it yourself now using Cygwin Setup . Note There are no Cygwin ports of some other useful networking commands, such as traceroute and ifconfig -a . The Windows system equivalents are tracert and ipconfig /all .","title":"Login connections to Windows"},{"location":"core/windows/#rdp-details","text":"Here are some fine points and hints for RDP logins to remote Windows desktops: Microsoft allows only one desktop login at a time to Windows XP , although this is the same Citrix Hydra technology that supports many concurrent logins to Terminal Server or Server 2003. The Fast User Switching option to XP is turned on, so a second RDP connection disconnects a previous one rather than killing it. Similarly, just closing your RDP client window disconnects your Windows Login session rather than killing it. You can reconnect later on without losing anything. SSH doesn't count as a desktop, so you can SSH in and use this command: qwinsta (Query WINdows STAtion) to show existing winstation sessions and their session ID's, and this one to reset (kill) a session by ID: rwinsta . We rename My Computer to show the PCxxx physical node name, but it doesn't appear on the Windows XP desktop by default. The XP user interface incorporates \"My Computer\" into the upper-right quadrant of the \"Start\" menu by default, and removes it from the desktop. You can go back to the \"classic\" user interface of Windows 2000, including showing \"My Computer\". Right-click on the background of the Taskbar which contains the \"Start\" button at the left, and choose \"Properties\". Select the \"Start Menu\" tab, click the \"Classic Start menu\" radio-button, and click \"OK\". Alternatively, you can force \"My Computer\" to appear on your XP desktop by right-clicking on the desktop background and choosing \"Properties\". Select the \"Desktop\" tab and click \"Customize Desktop...\" to get the \"Desktop Items\" dialog. Turn on the \"My Computer\" checkbox, then click \"OK\" twice. There are several Desktop icons (i.e. \"shortcuts\") installed by default in the XP images: Computer Management, Bash and TCSH shells, and NtEmacs . You will notice two flavors of Bash and TCSH icons on the desktop, labeled rxvt and Cygwin . The rxvt shells run in windows with X -like cut-and-paste mouse clicks: Left-click starts a selection, Right-click extends it, and middle-click pastes. These are the ones to use if you're connecting from an X workstation. Note The default colors used in Bash and rxvt don't work well in 4-bit color mode under RDP. Make sure you update your rdp-mime.pl to get the rdesktop -a 16 argument for 16-bit color. Or, you can over-ride the rxvt defaults by putting lines in your ~/.Xdefaults file like this: rxvt*background: steelblue The Cygwin shells run in a Windows Terminal window, just as the Windows cmd.exe does. These are the ones to use if you're connecting from a Windows workstation. Quick-edit mode is on by default, so you can cut-and-paste freely between your local workstation desktop and your remote RDP desktops. In a Windows Terminal window on your RDP remote desktop, the quick-edit cut-and-paste mouse clicks are: Left-drag the mouse to mark a rectangle of text, highlighting it. Type Enter or right-click the mouse when text is highlighted , to copy* the selected text to the clipboard. ( Escape* cancels the selection without copying it.) Right-click the mouse with nothing selected to paste the contents of the clipboard. On the first login by a user , Windows creates the user's Windows profile directory under C:\\Documents and Settings , and creates the registry key (folder) for persistent settings for that user. We arrange that early in the user's login process, a user HOME environment variable value is set in the user's registry. Otherwise Emacs wouldn't know how to find your .emacs setup file in your remotely mounted home directory. User \"root\" is special, and has a local home directory under /home . /home is a Cygwin symbolic link to C:\\Documents and Settings . The Windows XP Start menu has no Shutdown button under RDP. Instead, it is labeled Disconnect and only closes the RDP client window, leaving the login session and the node running. If you simply close the window, or the RDP client network connection is lost, you are also disconnected rather than logged out. When you reconnect, it comes right back, just as it was. To restart the computer, run /sbin/reboot , or use the \"Shut Down\" menu of Task Manager . One way to start Task Manager is to right-click on the background of the Taskbar at the bottom of the screen and select \"Task Manager\".","title":"RDP details"},{"location":"core/windows/#the-netbt-command","text":"The NetBT (Netbios over TCP) protocol is used to announce shared directories (folders) from one Windows machine to others. (See the Name and Session services in http://en.wikipedia.org/wiki/Netbios .) The SMB (Server Message Block) protocol is used to actually serve files. (See http://en.wikipedia.org/wiki/Server_Message_Block .) In DETERLab, we normally disable NetBT on experiment nodes, because it chatters and messes up slothd network idle detection, and is not needed for the usual SMB mounts of /users , /proj , and /share dirs, which are served from a Samba service on fs . However, NetBT does have to be enabled on the experiment nodes if you want to make Windows file shares between them. The netbt script sets the registry keys on the Windows network interface objects. Run it on the server nodes (the ones containing directories which you want to share) and reboot them afterwards to activate. There is an optional -r argument to reboot the node. Usage: netbt [-r] off|on If you use netbt to turn on NetBT, it persists across reboots. No reboot is necessary if you use Network Connections in the Control Panel to turn on NetBT. It takes effect immediately, but is turned off at reboot unless you do netbt on afterward as well. Right-click Local Area Connection (or the name of another connection, if appropriate), click Properties, click Internet Protocol (TCP/IP), and then click the Properties button. On the Internet Protocol (TCP/IP) Properties page, click the Advanced button, and click the WINS tab. Select Enable or Disable NetBIOS over TCP/IP. ipconfig /all reports \"NetBIOS over Tcpip . . . : Disabled\" on interfaces where NetBT is disabled, and says nothing where NetBT is enabled. To start sharing a directory, on the node, use the net share command, or turn on network sharing on the Sharing tab of the Properties of a directory (folder.) On XP-SP2 or above, when you first do this, the \"Network sharing and security\" subdialog says: As a security measure, Windows has disabled remote access to this computer. However, you can enable remote access and safely share files by running the _Network_Setup_Wizard_. _If_you_understand_the_security_risks_but_want_to_share_ _files_without_running_the_wizard,_click_here._\" Skip the wizard and click the latter (\"I understand\") link. Then click \"Just enable file sharing\", and \"OK\". Then you finally get the click-box to \"Share this folder on the network\". The machine names for UNC paths sharing are the same as in shell prompts: pcXXX , where XXX is the machine number. These will show up in My Network Places / Entire Network / Microsoft Windows Network / DETER once you have used them. IP numbers can also be used in UNC paths, giving you a way to share files across experiment network links rather than the control network. There is an DETER-generated LMHOSTS file, to provide the usual node aliases within an experiment, but it is currently ignored even though \"Enable LMHOSTS lookup\" is turned on in the TCP/IP WINS settings. Try nbtstat -c and nbtstat -R to experiment with this. (See the Microsoft doc for nbtstat .","title":"The netbt command"},{"location":"core/windows/#making-custom-windows-os-images","text":"Making custom Windows images is similar to doing it on the other DETER operating systems , except that you must do a little more work to run the prepare script as user root since there are no su or sudo commands on Windows. This is optional on the other OS types, but on Windows, proper TCP/IP network setup depends on prepare being run. Log in to the node where you want to save a custom image. Give the shell command to change the root password. Pick a password string you can remember, typing it twice as prompted: % passwd root Enter the new password (minimum of 5, maximum of 8 characters). Please use a combination of upper and lower case letters and numbers. New password: Re-enter new password: This works because you are part of the Windows Administrators group . Otherwise you would have to already know the root password to change it. Note If you change the root password and reboot Windows before running prepare below, the root password will not match the definitions of the DETER Windows services (daemons) that run as root, so they will not start up. Log out all sessions by users other than root , because prepare will be unable to remove their login profile directories if they are logged in. (See QWINSTA .) Log in to the node as user root through the Console or SSH, using the password you set above, then run the prepare command. (It will print \"Must be root to run this script!\" and do nothing if not run as root.) /usr/local/etc/emulab/prepare If run without option arguments, prepare will ask for the root password you want to use in your new image, prompting twice as the passwd command did above. It needs this to redefine the DETER Windows services (daemons) that run as root. It doesn't need to be the same as the root password you logged in with, since it sets the root password to be sure. The Administrator password is changed as well, since the Sysprep option needs that (below.) You can give the -p option to specify the root password on the command line: /usr/local/etc/emulab/prepare -p myRootPwd The -n option says not to change the passwords at all, and the DETER Windows services are not redefined. /usr/local/etc/emulab/prepare -n The -s option is used to make hardware-independent images using the Windows Sysprep deploy tool. If you use it with the -n option instead of giving a password, it assumes that you separately blank the Administrator password, or edit your Administrator password into the [GuiUnattended]AdminPassword entry of the sysprep.inf file. /usr/local/etc/emulab/prepare -s -p myRootPwd Note This must be done from a login on the serial console , because Sysprep shuts down the network. prepare -s refuses to run from an SSH or RDP login. Note Currently, hardware-independent images must be made on a pc850, and will then run on the pc600, pc3000, and pc3000w as well. There is an unresolved boot-time problem going the other direction, from the pc3000 to a pc850 or pc600. Windows normally casts some aspects of the NT image into concrete at the first boot after installation, including the specific boot disk driver to be used by the NT loader (IDE, SCSI, or SATA.) Sysprep is used by PC hardware manufacturers as they make XP installation disks with their own drivers installed. The Sysprep option to run an unattended Mini-Setup at first boot instead of the normal \"Out Of the Box Experience\" is used in some large corporate roll-outs. We do both. The DETER /share/windows/sysprep directory contains several versions of the XP deploy tools matched to the XP service pack level, appropriate device driver directories, and a draft sysprep.inf file to direct the automated install process. Mini-setup needs to reboot after setting up device drivers. XP also needs to [#Boots_twice reboot] after changing the host name. We combine the two by using a Cmdlines.txt script to run rc.firstboot -mini to set the host name at the end of Mini-Setup . Thus we only pay the extra time to set up device drivers and so on from scratch, about two minutes, rather than adding a third hardware and XP reboot cycle. Note As you create your Image Descriptor, set the reboot wait-time to 360 rather than 240 so that swap-ins don't time out. Then log out and create your custom image . Note Windows XP is too big to fit in the partitioning scheme used by FreeBSD and Linux, so it's necessary when making a Windows custom image to specify Partition 1 , and click Whole Disk Image. When you're testing your custom image, it's a good idea to set the tb-set-node-failure-action to \"nonfatal\" in the ns file so you get a chance to examine an image that hasn't completed the set-up process. See the note below for other useful ideas.","title":"Making Custom Windows OS Images"},{"location":"core/windows/#cygwin","text":"Cygwin is GNU + Cygnus + Windows , providing Linux-like functionality at the API, command-line, and package installation levels.","title":"Cygwin"},{"location":"core/windows/#cygwin-documentation","text":"Cygwin is well documented. Here are some links to get you started: Users guide Cygwin highlights Cygwin-added utilities FAQ API compatibility and Cygwin functions","title":"Cygwin documentation"},{"location":"core/windows/#cygwin-packages","text":"A number of optional Cygwin packages are installed in the image due to our building and running the DETER client software, plus some editors for convenience. These packages are currently agetty, bison, cvs, cygrunsrv, ed, file, flex, gcc, gdb, inetutils, make, minires-devel, more, nano, openssh, openssl-devel, patch, perl, perl-libwin32, psmisc, python, rpm, rsync, shutdown, sysvinit, tcsh, vim, wget, and zip. The Cygwin command cygcheck -c lists the packages that are installed, and their current version number and status. Package-specific notes and/or documentation for installed packages are in /usr{,/share}/doc/Cygwin/*.README and /usr/share/doc/*/README files. The Cygwin package site lists the available pre-compiled packages and provides a search engine. If you want to install more Cygwin pre-compiled packages, run the graphical installer: C:/Software/Cygwin/setup.exe The Cygwin command cygcheck -l package-name lists the contents of an installed package, which may help you to make a tarfile or rpm from a package you have installed. You can then cause it to be installed automatically by DETER into all of the nodes of your experiment. See the [Tutorial#TARBALLS Tutorial] for more information about installing RPM's and tarballs . Watch out for post-install scripts in: /etc/postinstall/package-name.sh{,.done} Many packages not in the Cygwin package site have also been ported to Cygwin already. Download the sources to an experiment node and try ./configure make make install as usual.","title":"Cygwin packages"},{"location":"core/windows/#smb-mounts-and-samba","text":"User home directories and other shared directories are served by fs , another alias for Ops/Users, via the SMB protocol (Server Message Block, also known as Windows File Sharing) with the Windows Client connecting to the Samba server. UNC paths with leading double-slashes and a server name, e.g. //fs , are used to access the SMB Shares under Cygwin. DETER then uses the Cygwin mount command to make them appear on the usual Unix paths for the DETER shared directories: /users/<username> , /proj/<pid> , /group/<pid>/<gid> , and /share . The Cygwin mount command lists what you could access on the Samba server, with the UNC path in the first column. Unix file permissions may further limit your access on the Samba server. Log in to Ops to investigate. /share/windows contains Windows software. See /share/windows/README.bin for descriptions of binary packages available for installation. In Windows GUI programs, you can just type the UNC path into the Address bar or a file-open dialog with backslashes , e.g. \\\\fs\\share or \\\\fs\\<username> . User and project shares are marked \"not browsable\", so just \\\\fs shows only share . Windows limitation: There is only one protection mask for everything in a whole mount/share under SMB. It's set in the \"share properties\" on the server (Samba config file in this case) so chmod will do you no good across SMB. Cygwin limitation: There is a hard-coded limit of 30 mount points in Cygwin. Cygwin uses 4 of them, and DETER uses another 3 or 4. So some of your /users mounts will fail on Windows startup if you have more than 23 or 24 members in your project, unless they are grouped into smaller subgroups.","title":"SMB mounts and Samba"},{"location":"core/windows/#cygwin-arcana","text":"File paths Cygwin accepts either flavor of slashes in paths, Unix/POSIX-style forward-slashes, or Windows-style back-slashes. In Unix shell commands, backslashes need to be quoted. Single-quotes work best. Doubling each backslash also works. This must also be done inside double-quotes. Examples: '\\single\\quoted' , \"\\\\double\\\\quoted\" , \\\\un\\\\quoted . (The difference between double and single quotes is that $variable references and back-quoted command execution are expanded in double-quotes.) When you invoke Windows (as opposed to Cygwin) commands, for example net use , they will know nothing about Unix-style paths in their arguments. The cygpath utility is an aid to converting paths between the Unix and Windows conventions. cygpath -w converts its arguments to Windows format, and cygpath -u converts its arguments to Unix format, e.g. $ cygpath -w /cygdrive/c/WINDOWS c:\\WINDOWS $ cygpath -u 'c:\\WINDOWS' /cygdrive/c/WINDOWS Mount points Cygwin mount points are shown by the mount and df commands. Note that there is a hard-coded limit of 30 mount points in Cygwin. Attempts to use the Cygwin mount command after that will fail. See the discussion of mount points and UNC //machine paths to SMB shares above . Another special case is the Unix root , \" / \". It's mounted to C:\\cygwin in the Windows filesystem. Drive letter mounts Cygwin knows about drive letter prefixes like C: \u00c2 , which are equivalent to /cygdrive/<drive-letter> \u00c2 . However, /cygdrive , like /dev , isn't a real directory, so you can't ls it. Some Windows software requires drive-letter mounts to be created for its use. You can use the Windows net use command to associate drive letters with UNC paths to SMB shares, e.g. net use W: '\\\\fs\\share\\windows' You can use the Windows subst command to associate drive letters with local paths, e.g. subst T: 'C:\\Temp' Filename completion in Cygwin shells with <Tab> doesn't work following a drive-letter prefix, but it works normally after a /cygdrive/ prefix. Also, filename completion is case-sensitive, although the underlying Windows is case-insensitive, so a filename in the wrong case is still opened properly. NTSEC Cygwin is running in NTSEC (NT Security) mode, so /etc/passwd and /etc/group contain Windows SID's as user and group ID's. Your Windows UID is the computer SID with a user number appended, something like S-1-5-21-2000478354-436374069-1060284298-1334 . Cygwin commands, such as id , ls -ln , and chown/chgrp , use the numeric suffix as the uid, e.g. 1334 . This is different from your normal DETER Unix user ID number, and the Samba server takes care of the difference. The id command reports your user id and group memberships. Note that all users are in group None on XP. Contrary to the name, this is a group that contains all users . It was named Everybody on Windows 2000, which was a better name. setuid There is no direct equivalent of the Unix setuid programs under Windows, and hence no su or sudo commands. The Windows equivalent to running a Unix command as root is membership in the Windows Administrators group. DETER project members who have either local_root or group_root privileges are put in group wheel , another alias for Administrators . Project members with user privileges are not members of the wheel group. You can ssh a command to the node as the target user, as long as you arrange for the proper authentication. For C/C++ code, there is a setuid() function in the Cygwin library, which \"impersonates\" the user if proper setup is done first. root There is not normally a Windows account named root . root on XP is just another user who is a member of the Administrators group, see below. We create a root account as part of the DETER setup to own installed software, and to run services and Unix scripts that check that they're running with root privileges. You can log in as root via RDP, ssh , or the serial console if you change the root password as described in the custom Windows OS images section. The root user does not have any Samba privileges to access Samba shared mounts, including the /proj , /groups , and /users . Administrators group All users are members of the Windows Administrators group. (The DETER non-local-root user property is not implemented on Windows.) Membership in the Windows Administrators group is very different from being root on Unix, and is also different from being logged in as Administrator. Administrators group membership on Windows only means you can set the ownership, group, and permissions on any file using the Cygwin chown , chgrp , chmod , or their Windows equivalents. Until you have done that, you can be completely locked out by read, write, or execute/open permissions of the directory or files. Another subtlety is that the group called None on XP is what used to be named Everybody on Windows 2000. All users are automatically in group None , so in practice setting group None permissions is no different from setting public access permissions. Permissions Cygwin does a pretty good job of mapping Unix user-group-other file permissions to Windows NT security ACLs. On Windows, unlike Unix, file and directory permissions can lock out root, Administrator, or SYSTEM user access. Many Unix scripts don't bother with permissions if they're running as root, and hence need modification to run on Cygwin. This creates a potential problem with the tb-set-node-tarfiles and tb-set-node-rpms commands. The tb-set-node-tarfiles page says \"Notes: 1. ... the files are installed as root\". So you can easily install files that your login doesn't have permission to access. The solution is to chmod the files before making the tarball or rpm file to grant appropriate access permissions. Executables Cygwin tries to treat .exe files the same as executable files without the .exe suffix, but with execute permissions turned on. (See the Cygwin Users Guide .) This breaks down in Makefile actions and scripts, where rm , ls -l , and install commands may need an explicit .exe added. Windows GUI programs You cannot run Windows GUI (Graphical User Interface) programs under ssh, on the serial console, or by tb-set-node-startcmd. There is no user login graphics context until you log in via RDP. However, you can use a startcmd to set a Windows registry key that causes a GUI program to be run automatically for all users when they log in to the node via RDP , if that's what you want. The program can be one that is installed by tb-set-node-tarfiles. You can pick any regkey name you want and put it in the Run registry folder. It's good not to step on the ones already there, so choose a name specific to your program. Put the following in your startcmd script: regtool -s set /HKLM/SOFTWARE/Microsoft/Windows/CurrentVersion/Run/mypgm 'C:\\mypgm\\mypgm.exe' (where mypgm is the name of your program, of course.) Notice that the value string is single-quoted with C: and backslashes. Windows interprets this regkey, and wants its flavor of file path.","title":"Cygwin arcana"},{"location":"core/windows/#ntemacs","text":"We don't include the Cygwin X server in our XP images to keep the bulk and complexity down. So NtEmacs 21.3 is provided instead of the Cygwin X Emacs. NtEmacs \"frames\" are windows on the Windows Desktop, e.g. ^X-5-2 makes another one. The /usr/local/bin/emacs executable is a symlink to /cygdrive/c/emacs-21.3/bin/runemacs.exe , which starts up an Emacs on the desktop. This only works under RDP, since SSH logins have a null desktop. There is also a /usr/local/bin/emacs-exe executable, a symlink to /cygdrive/c/emacs-21.3/bin/emacs.exe , which is only useful as an Emacs compiler. It could be used to run Emacs in an SSH or Serial Console login window with the -nw (no windows) flag, except that it exits with emacs: standard input is not a tty . Another thing not to try is running emacs-exe -nw in a Bash or TCSH shell on the RDP desktop. It crashed Windows XP when I tried it. Can drag-and-drop files from Windows Explorer to !NtEmacs windows. cygwin-mount.el in c:/emacs-21.3/site-lisp/ makes Cygwin mounts visible within !NtEmacs . It doesn't do Cygwin symlinks yet. Options - See ~root/.emacs mouse-wheel-mode CUA-mode option ( ^C copy / ^X cut on selection, ^V paste, ^Z undo). Ctrl and Alt key mappings, etc.","title":"NtEmacs"},{"location":"dash/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Human User Emulation using DASH \uf0c1 DASH (Deterlab Agents Simulating Humans) are intended to simulate human behavior in a variety of situations in Deterlab and other environments, where group decision-making is mediated by computers. For example, they have been used to model observed behavior in: Managing passwords on multiple accounts Launching a coordinated SQL injection attack Making inferences and decisions while controlling a power plant. In situations like these, human behavior might differ from the optimal, as may be defined by decision-theoretic measures or accepted best practice, and these differences impact the behavior of systems under test. This may happen because the typical user of a system has an incorrect or incomplete model of the system\u2019s behavior or of its security, or because humans inevitably make mistakes, particularly when their attention is taken with other tasks. DASH agents model this behavior using a dual-process cognitive architecture. The rational behavior module contains sub-modules for reactive planning and for projection using mental models. The instinctive behavior module models instinctive reactions and other reasoning that humans are typically not aware of The combination of these two modules can account for effects of cognitive load, time pressure, or fatigue on human performance, which have been documented in many different domains. The combination can also duplicate some well-known human biases in reasoning, such as confirmation bias. The DASH platform includes support for teams of agents that communicate with each other and a GUI to control agent parameters and view the state of both modules as the agent executes. It can be time consuming to program agent behaviors, and a typical cyber security test scenario may include a number of relatively standard agents, with some alterations to a few key players. Therefore DASH provides a library of agents that can be used in a range of situations and extended as required, covering end user behavior as well as attackers and defenders, though the latter are currently more limited. Please see DASH documentation here and download DASH from https://github.com/isi-usc-edu/dash DASH can simulate traffic stemming from user actions, or it can emulate it by sending real traffic in a DeterLab experiment. Documentation showing how to use DASH in emulation is here . For comments or questions about DASH and to obtain a copy for research purposes, please contact Jim Blythe at blythe@isi.edu .","title":"Human user emulation"},{"location":"dash/#human-user-emulation-using-dash","text":"DASH (Deterlab Agents Simulating Humans) are intended to simulate human behavior in a variety of situations in Deterlab and other environments, where group decision-making is mediated by computers. For example, they have been used to model observed behavior in: Managing passwords on multiple accounts Launching a coordinated SQL injection attack Making inferences and decisions while controlling a power plant. In situations like these, human behavior might differ from the optimal, as may be defined by decision-theoretic measures or accepted best practice, and these differences impact the behavior of systems under test. This may happen because the typical user of a system has an incorrect or incomplete model of the system\u2019s behavior or of its security, or because humans inevitably make mistakes, particularly when their attention is taken with other tasks. DASH agents model this behavior using a dual-process cognitive architecture. The rational behavior module contains sub-modules for reactive planning and for projection using mental models. The instinctive behavior module models instinctive reactions and other reasoning that humans are typically not aware of The combination of these two modules can account for effects of cognitive load, time pressure, or fatigue on human performance, which have been documented in many different domains. The combination can also duplicate some well-known human biases in reasoning, such as confirmation bias. The DASH platform includes support for teams of agents that communicate with each other and a GUI to control agent parameters and view the state of both modules as the agent executes. It can be time consuming to program agent behaviors, and a typical cyber security test scenario may include a number of relatively standard agents, with some alterations to a few key players. Therefore DASH provides a library of agents that can be used in a range of situations and extended as required, covering end user behavior as well as attackers and defenders, though the latter are currently more limited. Please see DASH documentation here and download DASH from https://github.com/isi-usc-edu/dash DASH can simulate traffic stemming from user actions, or it can emulate it by sending real traffic in a DeterLab experiment. Documentation showing how to use DASH in emulation is here . For comments or questions about DASH and to obtain a copy for research purposes, please contact Jim Blythe at blythe@isi.edu .","title":"Human User Emulation using DASH"},{"location":"dsc/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Network Emulation Using GraphGen \uf0c1 Sometimes researchers need to emulate very large and complex topologies connecting enclaves of nodes. The core of such topology may contain hundreds of nodes and it may be impractical to emulate it using physical machines. This guide shows how to replace the core of such topology with a single Click router, which faithfully emulates bandwidth and delay on inter-router links. Core principles - introduces core principles of network emulation in Deterlab and a topology model Tutorial - step-by-step tutorial to create experiments with emulated networks","title":"Nework Emulation"},{"location":"dsc/#network-emulation-using-graphgen","text":"Sometimes researchers need to emulate very large and complex topologies connecting enclaves of nodes. The core of such topology may contain hundreds of nodes and it may be impractical to emulate it using physical machines. This guide shows how to replace the core of such topology with a single Click router, which faithfully emulates bandwidth and delay on inter-router links. Core principles - introduces core principles of network emulation in Deterlab and a topology model Tutorial - step-by-step tutorial to create experiments with emulated networks","title":"Network Emulation Using GraphGen"},{"location":"dsc/core/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Core Principles of Network Emulation on Deterlab \uf0c1 To use network emulation, you need to represent your topology as a list of edges. Some edges may have attributes, such as bandwidth and delay associated with them. To specify nodes that will not be modeled on a Click router (enclave nodes, usually at the edges of topology) preface their names with the letter \"e\", like \"e1\". Nodes that will be modeled on a Click router are denoted by numbers (e.g., 1 2 represents an edge between node 1 and node 2). Attributes for an edge are specified in a json format (attribute: value) following the edge. For example, the following edge-list file represents a topology of two enclaves (one node each) and two core nodes with some edges between them (no attributes): # This is a simple edge list e1 1 e2 2 1 2 The resulting topology is illustrated in the figure below: Here is another edge list example, which represents a topology with three enclaves and three core nodes. Some edges have defined attributes (in json format) like bandwidth and delay. # A more complex edge list e1 1 e2 2 e3 3 {\"bw\": \"10Mbps\", \"delay\": \"20ms\"} 1 2 1 3 2 3 {\"bw\": \"1Mbps\", \"delay\": \"2ms\"} The resulting topology is illustrated in the figure below: Now please proceed to the tutorial to see how to generate NS files and Click templates from these basic topology models, and validate them on Deterlab.","title":"Core"},{"location":"dsc/core/#core-principles-of-network-emulation-on-deterlab","text":"To use network emulation, you need to represent your topology as a list of edges. Some edges may have attributes, such as bandwidth and delay associated with them. To specify nodes that will not be modeled on a Click router (enclave nodes, usually at the edges of topology) preface their names with the letter \"e\", like \"e1\". Nodes that will be modeled on a Click router are denoted by numbers (e.g., 1 2 represents an edge between node 1 and node 2). Attributes for an edge are specified in a json format (attribute: value) following the edge. For example, the following edge-list file represents a topology of two enclaves (one node each) and two core nodes with some edges between them (no attributes): # This is a simple edge list e1 1 e2 2 1 2 The resulting topology is illustrated in the figure below: Here is another edge list example, which represents a topology with three enclaves and three core nodes. Some edges have defined attributes (in json format) like bandwidth and delay. # A more complex edge list e1 1 e2 2 e3 3 {\"bw\": \"10Mbps\", \"delay\": \"20ms\"} 1 2 1 3 2 3 {\"bw\": \"1Mbps\", \"delay\": \"2ms\"} The resulting topology is illustrated in the figure below: Now please proceed to the tutorial to see how to generate NS files and Click templates from these basic topology models, and validate them on Deterlab.","title":"Core Principles of Network Emulation on Deterlab"},{"location":"dsc/dev/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Network Emulation: Developer Documentation \uf0c1 The program graphGen is an automatic generator for Click templates based on an edge-list format. The repository for this project implements 'Continuous Integration' with TravisCI, so in the file travis.yml you can find the installation details and requirements, as well as contribution guidelines. graph_gen.py This is the main file that is ran to convert edge-list to NS file. It takes one required argument - input graph file to convert. There are many optional arguments. Use -h flag to see all the optional arguments. ns_gen.py Generates an NS file associated with the given graph file. The generated file will have \"enclaves\" with ct, crypto, traffic and server nodes. click_gen.py Generates a click template. update_click_config.py update_routes.py Unit Tests \uf0c1 The tests subdirectory has several unit tests that can be ran using run_test.py. Examples: run_tests.py - run default set of tests run_tests.py MyTestSuite - run suite 'MyTestSuite' run_tests.py MyTestCase.testSomething - run MyTestCase.testSomething run_tests.py MyTestCase - run all 'test*' test methods in MyTestCase syntax_check.sh In order to run this program, first grab_nagelfar.sh needs to clone nagelfar repository.","title":"Dev"},{"location":"dsc/dev/#network-emulation-developer-documentation","text":"The program graphGen is an automatic generator for Click templates based on an edge-list format. The repository for this project implements 'Continuous Integration' with TravisCI, so in the file travis.yml you can find the installation details and requirements, as well as contribution guidelines. graph_gen.py This is the main file that is ran to convert edge-list to NS file. It takes one required argument - input graph file to convert. There are many optional arguments. Use -h flag to see all the optional arguments. ns_gen.py Generates an NS file associated with the given graph file. The generated file will have \"enclaves\" with ct, crypto, traffic and server nodes. click_gen.py Generates a click template. update_click_config.py update_routes.py","title":"Network Emulation: Developer Documentation"},{"location":"dsc/dev/#unit-tests","text":"The tests subdirectory has several unit tests that can be ran using run_test.py. Examples: run_tests.py - run default set of tests run_tests.py MyTestSuite - run suite 'MyTestSuite' run_tests.py MyTestCase.testSomething - run MyTestCase.testSomething run_tests.py MyTestCase - run all 'test*' test methods in MyTestCase syntax_check.sh In order to run this program, first grab_nagelfar.sh needs to clone nagelfar repository.","title":"Unit Tests"},{"location":"dsc/tutorial/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Network Emulation Tutorial \uf0c1 In this tutorial we will show you how to create an experiment with emulated network. Step 1: Clone the Repository \uf0c1 First, you need to clone the repository that contains scripts that convert edge-list representation of the topology into scripts and NS file that can run on Deterlab. You can do this on your device or on users.deterlab.net. git clone https://github.com/ISIEdgeLab/graphGen.git cd graphGen Step 2: Create Edge-list Representation of Your Topology \uf0c1 We will use the basic example with two enclaves and two nodes that was described previously. You can find this example in the cloned repository, in the file /tests/inputs/basic.xnet : e1 1 e2 2 1 2 Step 3: Convert Edge-list to NS Representation \uf0c1 The program graph_gen.py has many optional arguments to use (check developer documentation if you want to learn more). From the base folder where you cloned the git repository type the following: cd graphGen python graphgen/graph_gen.py tests/inputs/basic.xnet -n tutorial.ns --click-hardware MicroCloud --disable-crypto-nodes --num-servers 0 --num-clients 0 --CT-OS Ubuntu-STD --ct-hardware pc3000 Note Sometimes the above command cannot be pasted properly due to double dashes in options. Ensure that the command you are executing has the correct options and arguments. Be aware that nodes you can specify in the --click-hardware directive at the time are MicroCloud, dl380g3 and sm. You can specify any node type for --ct-hardware option. After running graphGen framework, you'll have two new files: vrouter.template and tutorial.ns . Step 4: Deploy Your Experiment on Deterlab \uf0c1 Copy your vrouter.template and tutorial.ns files to DETERLab, if they are not there already. Then run: bash /share/click/exp_scripts/saveresources.sh tutorial.ns to minimize the number of physical nodes you will need for deployment. On users.deterlab.net create a folder with the same name as the NS file ('tutorial' in this example) you want to create, inside the directory /proj/YourProject/templates . Copy vrouter.template file there. mkdir /proj/YourProject/templates/tutorial cp vrouter.template /proj/YourProject/templates/tutorial Next, use the tutorial.ns file to create a new DETERLab experiment and swap it in. To see how the network looks like, you can click 'Visualization; tab and the network's diagram should look like this: Be aware that the DETER topology hides the emulated nodes within vrouter. The vrouter node emulates your specified core topology, which actually looks like this. Step 5: Verify Setup \uf0c1 Swap in your experiment (we will assume that the experiment name is YourExperiment ) and check if the components are running correctly. Check Click Is Running \uf0c1 Check if click is actually running on vrouter node. ssh vrouter.YourExperiment.YourProject ps axuw | grep click If Click isn't running, the problem may be in the coremask argument for DPDK when Click is being started. We use DPDK to emulate your desired topology on Click. The coremask parameter is always mandatory for DPDK applications. Each bit of the mask corresponds to the number of cores on the machine. You can check how many cores your vrouter node has by doing the following on the node: lscpu and look at the CPU(s) line in the output. Then use the mask with that many bits set to 1 (-c parameter). For example, if the vrouter node has 8 cores we would start the Click router by typing click --dpdk -c 0xff -n 4 -- -u /click /tmp/vrouter.click >/tmp/click.log 2>&1 < /dev/null & Check Liveness and Loss \uf0c1 You can use ping command to check liveness and packet loss. To install ping, type the following on ct1 node: sudo apt-get install -y ping Then check that you can ping node ct2 from ct1: ssh ct1.YourExperiment.YourProject ping ct2 You should see an output similar to below: PING ct2-elink2 (10.2.2.1) 56(84) bytes of data. 64 bytes from ct2-elink2 (10.2.2.1): icmp_seq=1 ttl=62 time=4.68 ms 64 bytes from ct2-elink2 (10.2.2.1): icmp_seq=2 ttl=62 time=2.22 ms 64 bytes from ct2-elink2 (10.2.2.1): icmp_seq=3 ttl=62 time=2.12 ms 64 bytes from ct2-elink2 (10.2.2.1): icmp_seq=4 ttl=62 time=2.11 ms --- ct2-elink2 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3004ms rtt min/avg/max/mdev = 2.119/2.786/4.681/1.095 ms This indicates 0% loss and 2ms delay, which is what we expect based on the desired core topology. Checking Bandwidth \uf0c1 You can use iperf command to check bandwidth between your nodes. To install iperf, type the following on ct1 and ct2 nodes: sudo apt-get install -y iperf Then run on ct1 node: iperf -s commnad and on ct2 node: iperf -c ct1 Let the measurement run for a few seconds. You should see the output similar to the one below: ------------------------------------------------------------ Client connecting to ct1, TCP port 5001 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 3] local 10.2.2.1 port 47962 connected with 10.1.2.1 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0-10.0 sec 1.03 GBytes 887 Mbits/sec This output indicates that the bandwidth is around 1Gbps (887Mbps). Finally, you should check if the routing level is working correctly with a command like traceroute. To check that you have reachability from one enclave to the other, you can install traceroute on ct1 and run it sudo apt install -y traceroute traceroute ct2 You should see three hops (two intermediate nodes) between ct1 and ct2 in traceroute's output. Different available models \uf0c1 If you want to explore different models, you can take a look at /share/click/templates , where you'll find many models with different number of enclaves, node types, number of nodes and other options. Moreover, many of these models have their ns file, enclave.routes file, vrouter.template, among others. Each folder has an image file, illustrating the topology.","title":"Tutorial"},{"location":"dsc/tutorial/#network-emulation-tutorial","text":"In this tutorial we will show you how to create an experiment with emulated network.","title":"Network Emulation Tutorial"},{"location":"dsc/tutorial/#step-1-clone-the-repository","text":"First, you need to clone the repository that contains scripts that convert edge-list representation of the topology into scripts and NS file that can run on Deterlab. You can do this on your device or on users.deterlab.net. git clone https://github.com/ISIEdgeLab/graphGen.git cd graphGen","title":"Step 1: Clone the Repository"},{"location":"dsc/tutorial/#step-2-create-edge-list-representation-of-your-topology","text":"We will use the basic example with two enclaves and two nodes that was described previously. You can find this example in the cloned repository, in the file /tests/inputs/basic.xnet : e1 1 e2 2 1 2","title":"Step 2: Create Edge-list Representation of Your Topology"},{"location":"dsc/tutorial/#step-3-convert-edge-list-to-ns-representation","text":"The program graph_gen.py has many optional arguments to use (check developer documentation if you want to learn more). From the base folder where you cloned the git repository type the following: cd graphGen python graphgen/graph_gen.py tests/inputs/basic.xnet -n tutorial.ns --click-hardware MicroCloud --disable-crypto-nodes --num-servers 0 --num-clients 0 --CT-OS Ubuntu-STD --ct-hardware pc3000 Note Sometimes the above command cannot be pasted properly due to double dashes in options. Ensure that the command you are executing has the correct options and arguments. Be aware that nodes you can specify in the --click-hardware directive at the time are MicroCloud, dl380g3 and sm. You can specify any node type for --ct-hardware option. After running graphGen framework, you'll have two new files: vrouter.template and tutorial.ns .","title":"Step 3: Convert Edge-list to NS Representation"},{"location":"dsc/tutorial/#step-4-deploy-your-experiment-on-deterlab","text":"Copy your vrouter.template and tutorial.ns files to DETERLab, if they are not there already. Then run: bash /share/click/exp_scripts/saveresources.sh tutorial.ns to minimize the number of physical nodes you will need for deployment. On users.deterlab.net create a folder with the same name as the NS file ('tutorial' in this example) you want to create, inside the directory /proj/YourProject/templates . Copy vrouter.template file there. mkdir /proj/YourProject/templates/tutorial cp vrouter.template /proj/YourProject/templates/tutorial Next, use the tutorial.ns file to create a new DETERLab experiment and swap it in. To see how the network looks like, you can click 'Visualization; tab and the network's diagram should look like this: Be aware that the DETER topology hides the emulated nodes within vrouter. The vrouter node emulates your specified core topology, which actually looks like this.","title":"Step 4: Deploy Your Experiment on Deterlab"},{"location":"dsc/tutorial/#step-5-verify-setup","text":"Swap in your experiment (we will assume that the experiment name is YourExperiment ) and check if the components are running correctly.","title":"Step 5: Verify Setup"},{"location":"dsc/tutorial/#check-click-is-running","text":"Check if click is actually running on vrouter node. ssh vrouter.YourExperiment.YourProject ps axuw | grep click If Click isn't running, the problem may be in the coremask argument for DPDK when Click is being started. We use DPDK to emulate your desired topology on Click. The coremask parameter is always mandatory for DPDK applications. Each bit of the mask corresponds to the number of cores on the machine. You can check how many cores your vrouter node has by doing the following on the node: lscpu and look at the CPU(s) line in the output. Then use the mask with that many bits set to 1 (-c parameter). For example, if the vrouter node has 8 cores we would start the Click router by typing click --dpdk -c 0xff -n 4 -- -u /click /tmp/vrouter.click >/tmp/click.log 2>&1 < /dev/null &","title":"Check Click Is Running"},{"location":"dsc/tutorial/#check-liveness-and-loss","text":"You can use ping command to check liveness and packet loss. To install ping, type the following on ct1 node: sudo apt-get install -y ping Then check that you can ping node ct2 from ct1: ssh ct1.YourExperiment.YourProject ping ct2 You should see an output similar to below: PING ct2-elink2 (10.2.2.1) 56(84) bytes of data. 64 bytes from ct2-elink2 (10.2.2.1): icmp_seq=1 ttl=62 time=4.68 ms 64 bytes from ct2-elink2 (10.2.2.1): icmp_seq=2 ttl=62 time=2.22 ms 64 bytes from ct2-elink2 (10.2.2.1): icmp_seq=3 ttl=62 time=2.12 ms 64 bytes from ct2-elink2 (10.2.2.1): icmp_seq=4 ttl=62 time=2.11 ms --- ct2-elink2 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3004ms rtt min/avg/max/mdev = 2.119/2.786/4.681/1.095 ms This indicates 0% loss and 2ms delay, which is what we expect based on the desired core topology.","title":"Check Liveness and Loss"},{"location":"dsc/tutorial/#checking-bandwidth","text":"You can use iperf command to check bandwidth between your nodes. To install iperf, type the following on ct1 and ct2 nodes: sudo apt-get install -y iperf Then run on ct1 node: iperf -s commnad and on ct2 node: iperf -c ct1 Let the measurement run for a few seconds. You should see the output similar to the one below: ------------------------------------------------------------ Client connecting to ct1, TCP port 5001 TCP window size: 85.0 KByte (default) ------------------------------------------------------------ [ 3] local 10.2.2.1 port 47962 connected with 10.1.2.1 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0-10.0 sec 1.03 GBytes 887 Mbits/sec This output indicates that the bandwidth is around 1Gbps (887Mbps). Finally, you should check if the routing level is working correctly with a command like traceroute. To check that you have reachability from one enclave to the other, you can install traceroute on ct1 and run it sudo apt install -y traceroute traceroute ct2 You should see three hops (two intermediate nodes) between ct1 and ct2 in traceroute's output.","title":"Checking Bandwidth"},{"location":"dsc/tutorial/#different-available-models","text":"If you want to explore different models, you can take a look at /share/click/templates , where you'll find many models with different number of enclaves, node types, number of nodes and other options. Moreover, many of these models have their ns file, enclave.routes file, vrouter.template, among others. Each folder has an image file, illustrating the topology.","title":"Different available models"},{"location":"education/","text":"Note This page is updated to show the workflow with our new platform . Education Materials Overview \uf0c1 DETERLab is dedicated to supporting cyber security education. Since its inception, DETERLab has been extensively used by classes all over the world. You can view up to date statistics on educational use of DETERLab here . DETERLab offers excellent support for teaching. Instructors can: Benefit from a large collection of publicly available teaching materials Automatically create student accounts Upload class materials Assign homework/projects to students Track student progress on assignments Download assignments for grading Help students directly with many issues, without involving DETERLab staff Students benefit from using DETERLab, too. They develop practical skills in cybersecurity, networking, operating system administration, and coding. These skills make a big difference in job search! Note Students will access education materials (e.g., homework assignments) on our old platform but complete their assignments on our new platform . Their usernames and passwords are the same on both platforms.","title":"Education Materials Overview"},{"location":"education/#education-materials-overview","text":"DETERLab is dedicated to supporting cyber security education. Since its inception, DETERLab has been extensively used by classes all over the world. You can view up to date statistics on educational use of DETERLab here . DETERLab offers excellent support for teaching. Instructors can: Benefit from a large collection of publicly available teaching materials Automatically create student accounts Upload class materials Assign homework/projects to students Track student progress on assignments Download assignments for grading Help students directly with many issues, without involving DETERLab staff Students benefit from using DETERLab, too. They develop practical skills in cybersecurity, networking, operating system administration, and coding. These skills make a big difference in job search! Note Students will access education materials (e.g., homework assignments) on our old platform but complete their assignments on our new platform . Their usernames and passwords are the same on both platforms.","title":"Education Materials Overview"},{"location":"education/class-support/","text":"Note This page is updated to show the workflow with our new platform . Class Support \uf0c1 Classes use DETERLab differently than researchers do. Both groups can use DETERLab's full range of tools and resources, but we limit the amount of accidental sharing students do and assign them accounts that we can reuse. In order to do that we need your help in keeping track of who is using DETERLab for your classes and broadly what resources you will be using. The details are below. If you run into a problem when using DETERLab in your classes please let us know. If you come up with a better solution than the ones we described below we'd really like to hear about it! Course Setup - What we need from you to set up a DETERLab course. Course Wrap-Up - Actions to take at the end of your course Course Hand-Off - How to have a different instructor re-use the same course at your institution Managing Your Course - How add/remove students, unfreeze accounts, add assignments, etc. Access Control - How we enforce students' individual work","title":"Class Support"},{"location":"education/class-support/#class-support","text":"Classes use DETERLab differently than researchers do. Both groups can use DETERLab's full range of tools and resources, but we limit the amount of accidental sharing students do and assign them accounts that we can reuse. In order to do that we need your help in keeping track of who is using DETERLab for your classes and broadly what resources you will be using. The details are below. If you run into a problem when using DETERLab in your classes please let us know. If you come up with a better solution than the ones we described below we'd really like to hear about it! Course Setup - What we need from you to set up a DETERLab course. Course Wrap-Up - Actions to take at the end of your course Course Hand-Off - How to have a different instructor re-use the same course at your institution Managing Your Course - How add/remove students, unfreeze accounts, add assignments, etc. Access Control - How we enforce students' individual work","title":"Class Support"},{"location":"education/connect-with-teachers/","text":"Connect With Other Teachers \uf0c1 If you are teaching a class with DETERLab, we will automatically subscribe you to our education mailing list . You may also ask to be subscribed. This list is for teachers and TAs only. Teachers must request that their TAs get subscribed. You can use this list to discuss issues related to your class and DETERLab. We use this list to facilitate scheduling and resolve conflicts between assignment deadlines and to plan resource usage.","title":"Connect With Other Teachers"},{"location":"education/connect-with-teachers/#connect-with-other-teachers","text":"If you are teaching a class with DETERLab, we will automatically subscribe you to our education mailing list . You may also ask to be subscribed. This list is for teachers and TAs only. Teachers must request that their TAs get subscribed. You can use this list to discuss issues related to your class and DETERLab. We use this list to facilitate scheduling and resolve conflicts between assignment deadlines and to plan resource usage.","title":"Connect With Other Teachers"},{"location":"education/course-setup/","text":"Note This page is updated to show the workflow with our new platform . Course Setup \uf0c1 Since we apply different access control ( see about this topic ) for educational projects, we really need to hear from you if you're planning to run a class on DETERLab. This is the procedure you should follow: Actions at the Start/End of a Class \uf0c1 Start a class project \uf0c1 If you don't already have a project for the given class, start a new project on DETERLab by selecting Experimentation->Start New Project once you log into DETERLab or click here . Tell us in the description that this is a class project. Note Only do this if you have never taught a given class. For each new semester that you teach for the same class, recycle your current class project (see here ). If you already have a research project on DETERLab, do not reuse your research project for your class. Start a new project and categorize it as \"class\" in the project application. Wait for your project to be approved. It should take a few days and you should receive an automated email message once it is approved. Set up your class \uf0c1 If you don't already have a class project see above. Log in to DETERLab, click the My DETERLab link, find the Teaching tab and click on your class. Then select Setup Class from the left menu. You will not be able to enroll students until you complete this step. You will be able to populate your class with materials. Input the end date for your class and the estimated number of students In our new workflow you must set up your Merge password in your user profile, so you and your students can use our new platform for experimentation. You will do this only once. Please see illustrations . In our new workflow you must select Migrate Class from the left menu to create your class on our new platform. You only need to do this once per class project. Create accounts for students and TAs \uf0c1 DO NOT ask students or TAs to open accounts themselves. Follow the steps below. Note If this is a repeat offering of the class, make sure to recycle all accounts first (see how ). Materials will remain in the class so you can reuse them in the current offering if you like. Remember to change the visibility of materials manually (see how in Manage materials ). To create accounts: Copy and paste your students' (or TA's) emails, one per line. Account creation takes up to a minute per student. When accounts are created, the system will automatically email the students/TAs so make sure to alert them to the fact that you are signing them up for a DETERLab account. Students and TAs will get an automatically chosen username and password in email. Note If you, your students or TAs later desire to use DETERLab for research, they will need to open a research account on our new platform . Class accounts are only for class use . Course Wrap-up \uf0c1 Student account locking and removal \uf0c1 On the day that the end-of class is reached: All student accounts will be locked and instructors will be notified by email. This means that your students will no longer be able to log into DETERLab. Two weeks after the class end date: We will email the instructor reminding them that student accounts are about to be recycled. If the instructor wishes to delay this, they can update the end-of-class date. Four weeks after the class end date: All student accounts will be wiped (files removed, email aliases removed, new SSL certificates generated). Incompletes \uf0c1 The instructor can preserve accounts for students who were granted incomplete grades in the class by doing the following: In My DETERLab view, find the Teaching tab and click on Manage students or TAs from the left menu. Select the students you want to grant \"incompletes\" for. Select Grant Incomplete from the drop box below the student list. These accounts will not be wiped. Note If the incomplete is granted before the end of class, the account will not be locked. If the incomplete is granted after the class ends but before it is is wiped, the account will be unlocked, but the student will need to set the password again in the manner they did at the beginning of the semester. Once the student has completed the work the teacher should recycle the student's account ( see how ). Course hand-off to another instructor \uf0c1 Some classes within the same institution may be taught by different instructors each time. To hand your class off to another instructor: Create a TA account for the new instructor Either you or the new instructor should file a ticket asking Testbed Ops to complete the hand-off. Please follow these steps even if a hand-off is temporary. Note These instructions do not apply to instructors from different institutions looking to adopt each other's material in a course they teach. For that, look at our guidelines for sharing . Managing a Class \uf0c1 Students \uf0c1 You and your TA both can manage your course with minimal involvement of DETERLab operations. You can create accounts for students and TAs, recycle accounts for the next semester, delete accounts for students who dropped the class, and follow student's usage of DETERLab. Additionally, you can do the following actions to help students that have problems during your class: Lookup and reset student passwords \uf0c1 If a student forgets their password: From the left menu choose Manage students or TAs . Look up student's password in the last column of the table. If you wish you can select the student and choose Reset password from the select box below the student list. The student will receive an automated email with their new password. Unfreeze student accounts \uf0c1 If a student makes too many failed login attempts, their account will be automatically frozen. To unfreeze it: From the left menu choose Manage students or TAs . Select the student and choose Unfreeze Web access from the select box below the student list. The students will receive an automated email with instructions for choosing a new password. Reset SSH keys \uf0c1 If a student mangles or deletes their .ssh directory, any attempt to swap in an experiment will fail with the error message \"event system failed to start\". To reset their SSH keys: From the left menu choose Manage students or TAs . Select the student and choose Reset SSH keys from the select box below the student list. Students will receive an automated email with further instructions. Login as a student in the web interface \uf0c1 From the left menu choose Manage students or TAs Click on the glasses icon next to the student. Recycle an account \uf0c1 From the left menu choose Manage students or TAs . Select the appropriate students and choose Recycle from the select box below the student list. Warning This will wipe out the student's password and SSH keys and all the items in the student's home directory. Materials \uf0c1 The following functions occur within DETERLab's web interface (go to My DETERLab -> Teaching ). Adding new materials \uf0c1 A \"material\" is a useful link for your students, required reading, a set of lecture slides, a homework assignment, etc. There are two ways to add a new material to your class. 1. Adopt a shared material \uf0c1 See instructions about sharing . 2. Upload a material or specify a URL \uf0c1 From the left menu choose Add Materials to Class and follow the direction to upload a file or use a URL. For upload, we only support adding of ZIP files that are automatically unzipped after upload. You can zip and upload a single file (e.g., a Word document) or place many materials in a folder, create index.html to point to them and zip and upload the entire folder. For URLs, we only support those that start with http . If your URL starts with https , use http instead - servers automatically rewrite these URLs to use https . From the selection menu, choose the closest type of the material. The visibility setting determines who can see the materials: only the instructor/TA, a group of students or all students. Managing materials \uf0c1 Click on Manage Materials from the left menu. You can change visibility of materials or delete them. Note Deleting a material also deletes all assignments based on this material and any submissions for these assignments. Assignments \uf0c1 These functions occur within DETERLab's web interface (go to My DETERLab -> Teaching ). Assign to Students \uf0c1 To assign something to students: Add it to your class via Add Materials . Choose Assign to Students from the left menu. Select materials you want to assign from the list and choose if you want to assign them to all students, a group of students, or individual students. You must set the due date for the assignment. Managing Assignments \uf0c1 To make changes to assignments: From the left menu choose Manage Assignments . To delete an assignment, select those you want to delete and click Modify (see warning below). To change the due date, input a new date and click Modify . If you modify the due date, the system will automatically modify the limits for your class. Warning Deleting an assignment also deletes all submissions for it. Monitoring progress \uf0c1 You can monitor progress on an assignment by clicking on Teaching tab, then Manage Assignments on left sidebar, then View progress button next to the given assignment. You will be able to see which students have started work, and which have submitted their answers. Downloading submissions \uf0c1 To download student submissions click on Teaching tab, then Manage Assignments on left sidebar, then Download submissions button next to the given assignment. You can download submissions many times. New submissions will be added to the folder with the old ones. Access Control \uf0c1 Students do not have access to each other's files or experiments. Currently teachers and TAs can access students' experiments using username and password for a student (shown in Manage Students or TAs view).","title":"Course Setup"},{"location":"education/course-setup/#course-setup","text":"Since we apply different access control ( see about this topic ) for educational projects, we really need to hear from you if you're planning to run a class on DETERLab. This is the procedure you should follow:","title":"Course Setup"},{"location":"education/course-setup/#actions-at-the-startend-of-a-class","text":"","title":"Actions at the Start/End of a Class"},{"location":"education/course-setup/#start-a-class-project","text":"If you don't already have a project for the given class, start a new project on DETERLab by selecting Experimentation->Start New Project once you log into DETERLab or click here . Tell us in the description that this is a class project. Note Only do this if you have never taught a given class. For each new semester that you teach for the same class, recycle your current class project (see here ). If you already have a research project on DETERLab, do not reuse your research project for your class. Start a new project and categorize it as \"class\" in the project application. Wait for your project to be approved. It should take a few days and you should receive an automated email message once it is approved.","title":"Start a class project"},{"location":"education/course-setup/#set-up-your-class","text":"If you don't already have a class project see above. Log in to DETERLab, click the My DETERLab link, find the Teaching tab and click on your class. Then select Setup Class from the left menu. You will not be able to enroll students until you complete this step. You will be able to populate your class with materials. Input the end date for your class and the estimated number of students In our new workflow you must set up your Merge password in your user profile, so you and your students can use our new platform for experimentation. You will do this only once. Please see illustrations . In our new workflow you must select Migrate Class from the left menu to create your class on our new platform. You only need to do this once per class project.","title":"Set up your class"},{"location":"education/course-setup/#create-accounts-for-students-and-tas","text":"DO NOT ask students or TAs to open accounts themselves. Follow the steps below. Note If this is a repeat offering of the class, make sure to recycle all accounts first (see how ). Materials will remain in the class so you can reuse them in the current offering if you like. Remember to change the visibility of materials manually (see how in Manage materials ). To create accounts: Copy and paste your students' (or TA's) emails, one per line. Account creation takes up to a minute per student. When accounts are created, the system will automatically email the students/TAs so make sure to alert them to the fact that you are signing them up for a DETERLab account. Students and TAs will get an automatically chosen username and password in email. Note If you, your students or TAs later desire to use DETERLab for research, they will need to open a research account on our new platform . Class accounts are only for class use .","title":"Create accounts for students and TAs"},{"location":"education/course-setup/#course-wrap-up","text":"","title":"Course Wrap-up"},{"location":"education/course-setup/#student-account-locking-and-removal","text":"On the day that the end-of class is reached: All student accounts will be locked and instructors will be notified by email. This means that your students will no longer be able to log into DETERLab. Two weeks after the class end date: We will email the instructor reminding them that student accounts are about to be recycled. If the instructor wishes to delay this, they can update the end-of-class date. Four weeks after the class end date: All student accounts will be wiped (files removed, email aliases removed, new SSL certificates generated).","title":"Student account locking and removal"},{"location":"education/course-setup/#incompletes","text":"The instructor can preserve accounts for students who were granted incomplete grades in the class by doing the following: In My DETERLab view, find the Teaching tab and click on Manage students or TAs from the left menu. Select the students you want to grant \"incompletes\" for. Select Grant Incomplete from the drop box below the student list. These accounts will not be wiped. Note If the incomplete is granted before the end of class, the account will not be locked. If the incomplete is granted after the class ends but before it is is wiped, the account will be unlocked, but the student will need to set the password again in the manner they did at the beginning of the semester. Once the student has completed the work the teacher should recycle the student's account ( see how ).","title":"Incompletes"},{"location":"education/course-setup/#course-hand-off-to-another-instructor","text":"Some classes within the same institution may be taught by different instructors each time. To hand your class off to another instructor: Create a TA account for the new instructor Either you or the new instructor should file a ticket asking Testbed Ops to complete the hand-off. Please follow these steps even if a hand-off is temporary. Note These instructions do not apply to instructors from different institutions looking to adopt each other's material in a course they teach. For that, look at our guidelines for sharing .","title":"Course hand-off to another instructor"},{"location":"education/course-setup/#managing-a-class","text":"","title":"Managing a Class"},{"location":"education/course-setup/#students","text":"You and your TA both can manage your course with minimal involvement of DETERLab operations. You can create accounts for students and TAs, recycle accounts for the next semester, delete accounts for students who dropped the class, and follow student's usage of DETERLab. Additionally, you can do the following actions to help students that have problems during your class:","title":"Students"},{"location":"education/course-setup/#lookup-and-reset-student-passwords","text":"If a student forgets their password: From the left menu choose Manage students or TAs . Look up student's password in the last column of the table. If you wish you can select the student and choose Reset password from the select box below the student list. The student will receive an automated email with their new password.","title":"Lookup and reset student passwords"},{"location":"education/course-setup/#unfreeze-student-accounts","text":"If a student makes too many failed login attempts, their account will be automatically frozen. To unfreeze it: From the left menu choose Manage students or TAs . Select the student and choose Unfreeze Web access from the select box below the student list. The students will receive an automated email with instructions for choosing a new password.","title":"Unfreeze student accounts"},{"location":"education/course-setup/#reset-ssh-keys","text":"If a student mangles or deletes their .ssh directory, any attempt to swap in an experiment will fail with the error message \"event system failed to start\". To reset their SSH keys: From the left menu choose Manage students or TAs . Select the student and choose Reset SSH keys from the select box below the student list. Students will receive an automated email with further instructions.","title":"Reset SSH keys"},{"location":"education/course-setup/#login-as-a-student-in-the-web-interface","text":"From the left menu choose Manage students or TAs Click on the glasses icon next to the student.","title":"Login as a student in the web interface"},{"location":"education/course-setup/#recycle-an-account","text":"From the left menu choose Manage students or TAs . Select the appropriate students and choose Recycle from the select box below the student list. Warning This will wipe out the student's password and SSH keys and all the items in the student's home directory.","title":"Recycle an account"},{"location":"education/course-setup/#materials","text":"The following functions occur within DETERLab's web interface (go to My DETERLab -> Teaching ).","title":"Materials"},{"location":"education/course-setup/#adding-new-materials","text":"A \"material\" is a useful link for your students, required reading, a set of lecture slides, a homework assignment, etc. There are two ways to add a new material to your class.","title":"Adding new materials"},{"location":"education/course-setup/#1-adopt-a-shared-material","text":"See instructions about sharing .","title":"1. Adopt a shared material"},{"location":"education/course-setup/#2-upload-a-material-or-specify-a-url","text":"From the left menu choose Add Materials to Class and follow the direction to upload a file or use a URL. For upload, we only support adding of ZIP files that are automatically unzipped after upload. You can zip and upload a single file (e.g., a Word document) or place many materials in a folder, create index.html to point to them and zip and upload the entire folder. For URLs, we only support those that start with http . If your URL starts with https , use http instead - servers automatically rewrite these URLs to use https . From the selection menu, choose the closest type of the material. The visibility setting determines who can see the materials: only the instructor/TA, a group of students or all students.","title":"2. Upload a material or specify a URL"},{"location":"education/course-setup/#managing-materials","text":"Click on Manage Materials from the left menu. You can change visibility of materials or delete them. Note Deleting a material also deletes all assignments based on this material and any submissions for these assignments.","title":"Managing materials"},{"location":"education/course-setup/#assignments","text":"These functions occur within DETERLab's web interface (go to My DETERLab -> Teaching ).","title":"Assignments"},{"location":"education/course-setup/#assign-to-students","text":"To assign something to students: Add it to your class via Add Materials . Choose Assign to Students from the left menu. Select materials you want to assign from the list and choose if you want to assign them to all students, a group of students, or individual students. You must set the due date for the assignment.","title":"Assign to Students"},{"location":"education/course-setup/#managing-assignments","text":"To make changes to assignments: From the left menu choose Manage Assignments . To delete an assignment, select those you want to delete and click Modify (see warning below). To change the due date, input a new date and click Modify . If you modify the due date, the system will automatically modify the limits for your class. Warning Deleting an assignment also deletes all submissions for it.","title":"Managing Assignments"},{"location":"education/course-setup/#monitoring-progress","text":"You can monitor progress on an assignment by clicking on Teaching tab, then Manage Assignments on left sidebar, then View progress button next to the given assignment. You will be able to see which students have started work, and which have submitted their answers.","title":"Monitoring progress"},{"location":"education/course-setup/#downloading-submissions","text":"To download student submissions click on Teaching tab, then Manage Assignments on left sidebar, then Download submissions button next to the given assignment. You can download submissions many times. New submissions will be added to the folder with the old ones.","title":"Downloading submissions"},{"location":"education/course-setup/#access-control","text":"Students do not have access to each other's files or experiments. Currently teachers and TAs can access students' experiments using username and password for a student (shown in Manage Students or TAs view).","title":"Access Control"},{"location":"education/good-teaching-practices/","text":"Good Teaching Practices \uf0c1 Ask your students to contact a TA or you first with problems, then the TA or you can contact us with any new issues. Some teachers have even offered to take points off if a student contacts DETER Ops directly. Make it clear to students that they are using a shared testbed, and should not leave their work for the last day before the deadline. Ask students to promptly swap out experiments if they will not use them for at least an hour. They should save their work in their home directory before swapping out. Ask students to set the idle swap out period to 1 hour . They should swap out manually whenever they are done with a chunk of work, but this setting will catch the cases when they forget to do so. Contact us promptly if you run into any issues with experiment creation, lack of resources, etc. Communicate to us any small issues that frustrate you so we can improve our handling of classes. Please make sure to explain to students the difference between using the control and the experimental network (see here ), and make sure your assignment instructions minimize use of control network whenever possible.","title":"Good Teaching Practices"},{"location":"education/good-teaching-practices/#good-teaching-practices","text":"Ask your students to contact a TA or you first with problems, then the TA or you can contact us with any new issues. Some teachers have even offered to take points off if a student contacts DETER Ops directly. Make it clear to students that they are using a shared testbed, and should not leave their work for the last day before the deadline. Ask students to promptly swap out experiments if they will not use them for at least an hour. They should save their work in their home directory before swapping out. Ask students to set the idle swap out period to 1 hour . They should swap out manually whenever they are done with a chunk of work, but this setting will catch the cases when they forget to do so. Contact us promptly if you run into any issues with experiment creation, lack of resources, etc. Communicate to us any small issues that frustrate you so we can improve our handling of classes. Please make sure to explain to students the difference between using the control and the experimental network (see here ), and make sure your assignment instructions minimize use of control network whenever possible.","title":"Good Teaching Practices"},{"location":"education/guidelines-for-students/","text":"Guidelines for Students \uf0c1 Please visit our new guidelines here","title":"Guidelines for Students"},{"location":"education/guidelines-for-students/#guidelines-for-students","text":"Please visit our new guidelines here","title":"Guidelines for Students"},{"location":"education/guidelines-for-teachers/","text":"Guidelines for Teachers \uf0c1 Welcome to the DETERLab's site for teacher support in education. The DETERLab testbed has been used in many security classes to demonstrate and complement concepts taught in class. Such practices enhance student learning and promote interest in the material. This page provides guidelines to teachers on DETERLab's support for class activities that differs from our support to research users. It further provides guidelines on using our DETERLab's UI to find, adopt and contribute teaching materials. DETER's support for classes - Read this if you plan to teach a class with DETERLab testbed. Good practices - Learn how to maximize the benefits of using DETERLab testbed in class. Sharing teaching materials - Read this if you have materials to contribute, or would like to find materials to adopt. Connect with other teachers - Find ways to connect with other teachers that use the DETERLab testbed.","title":"Guidelines for Teachers"},{"location":"education/guidelines-for-teachers/#guidelines-for-teachers","text":"Welcome to the DETERLab's site for teacher support in education. The DETERLab testbed has been used in many security classes to demonstrate and complement concepts taught in class. Such practices enhance student learning and promote interest in the material. This page provides guidelines to teachers on DETERLab's support for class activities that differs from our support to research users. It further provides guidelines on using our DETERLab's UI to find, adopt and contribute teaching materials. DETER's support for classes - Read this if you plan to teach a class with DETERLab testbed. Good practices - Learn how to maximize the benefits of using DETERLab testbed in class. Sharing teaching materials - Read this if you have materials to contribute, or would like to find materials to adopt. Connect with other teachers - Find ways to connect with other teachers that use the DETERLab testbed.","title":"Guidelines for Teachers"},{"location":"education/migrating-materials/","text":"Migrating Your Materials \uf0c1 These instructions assume that you had a mix of shared materials and your own private materials in your class. Migrating Shared Materials \uf0c1 If you have used shared materials in your class, they have all been updated. You simply need to delete them from your class and re-adopt them. Migrating Private Materials \uf0c1 To migrate private materials, you should apply the following steps. Create Topology File \uf0c1 These instructions can be followed for each private exercise If you have a custom NS file in your instructions, on old DeterLab you can translate it into new topology format. On users.deterlab.net run perl /usr/local/bin/ns2model.pl <projname> <expname> <path-to-ns-file> . The new topology file will be printed out. You can copy the topology onto your clipboard and include it in your instructions to students. Then ask your students to follow this guide to start an experiment with that topology (or in Merge speak \"model\"). You can also take additional steps to enable students to run experiments with the given topology from their XDC by using startexp . The following steps cover this second use case. Create Protected Project \uf0c1 These instructions should be followed only once per class (not per class offering) Log into our new platform and click on Projects. Create a new project in your organization (usually your organization will be your class name). You only have to do this once. Make the project protected: Click on XDC on the left menu, then create an XDC in your new project. Please wait around 5 minutes for XDC to be fully initialized. Click on XDC on the left menu, then click on Jupyter and then on Terminal app. Only the first time you access this XDC, type the text below in the Terminal, replacing yourusername and yourpassword with your actual information. su - yourusername echo \"PATH=$PATH:/share\" > .profile echo yourpassword > pass.txt exit su - yourusername Every other time you access the XDC look at the command line prompt. If the prompt looks like # , type su - yourusername Add Students to Project \uf0c1 These instructions should be followed once per class offering Type the command below, replacing projname and org with your actual information. You need to do this only once per semester. addstudents projname org This will add all your current students to your new project. If new students enroll into your class you can repeat this action without any ill consequences. Populating Private Lab \uf0c1 Create a folder structure that looks like illustrated in the figure below. Red text denotes names of files or folders that should not be changed. Files are denoted by F and folders by D label. The topology for the lab is in the file with extension \".model\" and the same name as your lab name (in our example that is \"firstlab\"). Our example assumes you had three-node topology with node names X, Y and Z. Nodes X and Y needed some set up for your lab and node Z did not. All node names are listed in nodes file. Only those nodes that need setup have a separate folder with install script inside. That script should have a+rx permissions. You can see sample install scripts in the illustration. If there are any files or folders that should be copied onto individual nodes at setup time, please place them into the corresponding nodes' folders and they will be copied when students run runlab . In the illustration, file file1 will be copied onto /tmp/X folder on node X and folder folder1 will be copied into /tmp/Y/folder1 on node Y . If you previously used start-cmd to set up your nodes, then in each install file you would copy-paste the contents of the start-cmd script you originally had. You will also need to modify these contents in the following way. For each command that uses a file from a user's home directory, a project directory or /share directory, you will need to copy that file into the given subfolder in /organization, and modify the command to look for it in the node's local /tmp folder under the folder with the node's name. For example if node a previously ran a start-cmd directive that was doing somecmd /share/somefile you would copy somefile from old DeterLab into /organizations/labname/a/somefile on your XDC. See our scp instructions for how to copy files. You can also see some step-by-step examples in this lab exercise . Once you copy the file, you would also change the command from somecmd /share/somefile to somecmd /tmp/a/somefile . In the illustration this is done to copy contents of folder1 into /var/www/html on node Y . How Can Students Run Exercises \uf0c1 Your students can run shared exercises by typing startexp labname as described in guidelines for students Your students can run your private exercises by typing startexp labname -o projname or they can follow this guide to start an experiment with the topology you give them in student materials.","title":"Migrating Your Materials"},{"location":"education/migrating-materials/#migrating-your-materials","text":"These instructions assume that you had a mix of shared materials and your own private materials in your class.","title":"Migrating Your Materials"},{"location":"education/migrating-materials/#migrating-shared-materials","text":"If you have used shared materials in your class, they have all been updated. You simply need to delete them from your class and re-adopt them.","title":"Migrating Shared Materials"},{"location":"education/migrating-materials/#migrating-private-materials","text":"To migrate private materials, you should apply the following steps.","title":"Migrating Private Materials"},{"location":"education/migrating-materials/#create-topology-file","text":"These instructions can be followed for each private exercise If you have a custom NS file in your instructions, on old DeterLab you can translate it into new topology format. On users.deterlab.net run perl /usr/local/bin/ns2model.pl <projname> <expname> <path-to-ns-file> . The new topology file will be printed out. You can copy the topology onto your clipboard and include it in your instructions to students. Then ask your students to follow this guide to start an experiment with that topology (or in Merge speak \"model\"). You can also take additional steps to enable students to run experiments with the given topology from their XDC by using startexp . The following steps cover this second use case.","title":"Create Topology File"},{"location":"education/migrating-materials/#create-protected-project","text":"These instructions should be followed only once per class (not per class offering) Log into our new platform and click on Projects. Create a new project in your organization (usually your organization will be your class name). You only have to do this once. Make the project protected: Click on XDC on the left menu, then create an XDC in your new project. Please wait around 5 minutes for XDC to be fully initialized. Click on XDC on the left menu, then click on Jupyter and then on Terminal app. Only the first time you access this XDC, type the text below in the Terminal, replacing yourusername and yourpassword with your actual information. su - yourusername echo \"PATH=$PATH:/share\" > .profile echo yourpassword > pass.txt exit su - yourusername Every other time you access the XDC look at the command line prompt. If the prompt looks like # , type su - yourusername","title":"Create Protected Project"},{"location":"education/migrating-materials/#add-students-to-project","text":"These instructions should be followed once per class offering Type the command below, replacing projname and org with your actual information. You need to do this only once per semester. addstudents projname org This will add all your current students to your new project. If new students enroll into your class you can repeat this action without any ill consequences.","title":"Add Students to Project"},{"location":"education/migrating-materials/#populating-private-lab","text":"Create a folder structure that looks like illustrated in the figure below. Red text denotes names of files or folders that should not be changed. Files are denoted by F and folders by D label. The topology for the lab is in the file with extension \".model\" and the same name as your lab name (in our example that is \"firstlab\"). Our example assumes you had three-node topology with node names X, Y and Z. Nodes X and Y needed some set up for your lab and node Z did not. All node names are listed in nodes file. Only those nodes that need setup have a separate folder with install script inside. That script should have a+rx permissions. You can see sample install scripts in the illustration. If there are any files or folders that should be copied onto individual nodes at setup time, please place them into the corresponding nodes' folders and they will be copied when students run runlab . In the illustration, file file1 will be copied onto /tmp/X folder on node X and folder folder1 will be copied into /tmp/Y/folder1 on node Y . If you previously used start-cmd to set up your nodes, then in each install file you would copy-paste the contents of the start-cmd script you originally had. You will also need to modify these contents in the following way. For each command that uses a file from a user's home directory, a project directory or /share directory, you will need to copy that file into the given subfolder in /organization, and modify the command to look for it in the node's local /tmp folder under the folder with the node's name. For example if node a previously ran a start-cmd directive that was doing somecmd /share/somefile you would copy somefile from old DeterLab into /organizations/labname/a/somefile on your XDC. See our scp instructions for how to copy files. You can also see some step-by-step examples in this lab exercise . Once you copy the file, you would also change the command from somecmd /share/somefile to somecmd /tmp/a/somefile . In the illustration this is done to copy contents of folder1 into /var/www/html on node Y .","title":"Populating Private Lab"},{"location":"education/migrating-materials/#how-can-students-run-exercises","text":"Your students can run shared exercises by typing startexp labname as described in guidelines for students Your students can run your private exercises by typing startexp labname -o projname or they can follow this guide to start an experiment with the topology you give them in student materials.","title":"How Can Students Run Exercises"},{"location":"education/public-materials/","text":"Public Materials \uf0c1 Go to this section of the DETERLab testbed to search for educational materials that have been made available to the public: https://www.isi.deterlab.net/sharedpublic.php","title":"Public Materials"},{"location":"education/public-materials/#public-materials","text":"Go to this section of the DETERLab testbed to search for educational materials that have been made available to the public: https://www.isi.deterlab.net/sharedpublic.php","title":"Public Materials"},{"location":"education/student-intro/","text":"Student Introduction to DETERLab \uf0c1 Contributors: Peter A. H. Peterson, UCLA. pahp@... David Morgan, USC. davidmor@... What is DETERLab? \uf0c1 DETERLab is a security and education-enhanced version of Emulab. Funded by the National Science Foundation and the Department of Homeland Security, DETERLab is hosted by USC/ISI and UC Berkeley. \"USC/ISI's DETERLab (cyber DEfense Technology Experimental Research Laboratory) is a state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. DETERLab is a shared testbed providing a platform for research in cyber security and serving a broad user community, including academia, industry, and government. To date, DETERLab-based projects have included behavior analysis and defensive technologies including DDoS attacks, worm and botnet attacks, encryption, pattern detection, and intrusion-tolerant storage protocols. [ 1 ].\" DETERLab (like Emulab) offers user accounts with assorted permissions associated with different experiment groups. Each group can have its own preconfigured experimental environments running on Linux, BSD, Windows, or other operating systems. Users running DETERLab experiments have full control of real hardware and networks running preconfigured software packages. These features make it an ideal platform for computer science and especially computer security education. Many instructors have designed class exercises (homework assignments, project assignments, in-class demos, etc.) consisting of a lab manual, software, data, network configurations, and machines from DETER's pool. This allows each student to run her own experiments on dedicated hardware. How does it work? \uf0c1 The software running DETERLab will load operating system images (low level disk copies) onto to free nodes in the testbed, and then reconfigure programmable switches to create VLANs with the newly-imaged nodes connected according to the topology specified by the experiment creator. After the system is fully imaged and configured, DETERLab will execute specified scripts, unpack tarballs, and/or install rpm files according to the experiment's configuration. The end result is a live network of real machines, accessible via the Internet. How do I get a DETERLab login? \uf0c1 Your instructor will request an account for you. Simply send your preferred email address to your instructor. Once the testbed ops set up your account, you will receive an email with your username and password at the address you supplied. Within one week, use those credentials to log in. Edit your profile as follows: a. Choose \"Profile\" tab. b. Choose \"Edit profile\" menu option. c. Replace any default contents in the two fields shown with your actual name and working phone number d. Change your password! e. Click \"Submit\" Using DETERLab \uf0c1 How do I start an exercise? \uf0c1 Before you can perform the tasks described in your exercise assignment, you will, in many cases, need to create an experiment in DETERLab to work on. This will be your environment to use whenever you need it. To create a new experiment: Log into DETERLab with your account. Under the \"Experimentation\" menu at the top of the page, click \"Begin an Experiment\". Select your Class Project name from the \"Select Project\" dropdown. (Throughout this document, we'll assume your class project name is YourProject) Leave the \"Group\" dropdown set to Default unless otherwise instructed. In the \"Name\" field, enter a name of the format username-exercisename. (Example: jstudent-exploits). Enter a brief description in the \"Description\" field. In the \"Your NS File\" field, follow the instructions in the \"Setup\" section of your exercise manual. Set the \"Idle Swap\" field to 1 h. Leave the rest of the settings for \"Swapping,\" \"Linktest Option,\" and \"BatchMode\" alone (unless otherwise instructed). If you would like to start your lab now, check the \"Swap In Immediately\" box and move to the next section. Otherwise, do not check this box. Click \"Submit\"! How do I work on my exercise? \uf0c1 Log into DETERLab with your DETERLab account (or contact your instructor if you need an account). Click on the \"My DETERLab\" link on the left hand menu. In the \"Current Experiments\" table, click on the name of the experiment you want. Under the \"Experiment Options\" menu on the left margin, click \"Swap Experiment In\", then click \"Confirm\". The swap in process will take 5 to 10 minutes. While you're waiting, you can watch the swap in process displayed in your web browser. Or, you can watch your email box for a message letting you know that it has finished swapping in. When the experiment has finished swapping in, you can perform the tasks in your exercise manual. How do I access my experiment? \uf0c1 Your experiment is made up of one or more machines on the internal DETERLab network, which is behind a firewall. To access your experimental nodes, you'll need to first SSH to users.deterlab.net. If you don't know how to use SSH, see our tutorial. users.deterlab.net (or users for short) is the \"control server\" for DETERLab. From users you can contact all your nodes, reboot them, connect to their serial ports, etc. Once you log in to users, you'll need to SSH again to your actual experimental nodes. Since your nodes' addresses may change every time you swap them in, it's best to SSH to the permanent network names of the nodes. Here's how to figure out what their names are: Once your experiment has swapped in: Navigate to the experiment you just installed. If you just swapped in your experiment, the quickest way to find your node names is to click on the experiment name in the table under \"Swap Control.\" However, you can also get there by clicking \"My DETERLab\". Your experiment is listed as \"active\" in the \"State\" column. Click on the experiment's name in the \"EID\" column. Once you can see your experiment's page, click on the \"Details\" tab in the main content panel. Your nodes' network names are listed under the heading \"Qualified Name.\" For example, node1.YourExperiment.YourProject.isi.deterlab.net. You should familiarize yourself with the information available on this page, but for now we just need to know the long DNS qualified name(s) node(s) you just swapped in. If you are curious, you should also look at the \"Settings\" (generic info), \"Visualization,\" and \"NS File.\" (The topology mapplet may be disabled for some labs, so these last two may not be visible). Now that you are logged in to users.deterlab.net, your nodes are swapped in, and you know their network name(s), you can SSH from users to your experimental nodes by executing: ssh node1.YourExperiment.YourProject.isi.deterlab.net. You will not need to re-authenticate. You may need to wait a few more minutes. Once DETERLab is finished setting up the experiment, the nodes still need a minute or two to boot and complete their configuration. If you get a message about \"server configuration\" when you try to log in, wait a few minutes and try again. If a lab instructs you to create new users on your experimental nodes, you can log in as them by running ssh newuser@node1.YourExperiment.YourProject.isi.deterlab.net or ssh newuser@localhost from the experimental node. Congratulations! Your lab environment is now set up, and you can get to work at the tasks in your lab manual. Make sure you read the \"Things to keep in mind\" section below! Some labs benefit from Port Forwarding. Port Forwarding is a technique that can allow you to access your experimental nodes directly from your desktop computer. This is especially useful for accessing web applications running on your experimental nodes. See our ssh tutorial for more information. Finally, when you are done working with your nodes, you should save your work and swap out the experiment so that someone else can use the physical machines. Things to keep in mind \uf0c1 Carefully read the evolving version of this documentXXX. Saving and securing your files on DETERLab \uf0c1 Every user on DETERLab has a home directory on users.deterlab.net which is mounted via NFS (Network File System) to experimental nodes. This means that anything you place in your home directory on one experimental node (or the users machine) is visible in your home directory on your other experimental nodes. Your home directory is private, so you may save your work in that directory. However, everything else on experimental nodes is permanently lost when an experiment is swapped out. Make sure you save your work in your home directory before swapping out your experiment''' Another place to save your files would be /proj/YourProject. This directory is also NFS-mounted to all experimental nodes so same rules apply about writing to it a lot, as for your home directory. It is shared by all members of your project/class. Again, on DETERLab, files ARE NOT SAVED between swap-ins. Additionally, class experiments may be forcibly swapped out after a certain number of idle hours (or some maximum amount of time). You must manually save copies of any files you want to keep in your home directory. Any files left elsewhere on the experimental nodes will be erased and lost forever. This means that if you want to store progress for a lab and come back to it later, you will need to put it in your home directory before swapping out the experiment. Swap out -- DON'T \"terminate\"! \uf0c1 When you are done with your experiment for the time being, please make sure you save your work into an appropriate location and then swap out your experiment. To do this, use the \"Swap Experiment Out\" link in the \"Experiment Options\" panel. (This is the same place that used to have a \"Swap Experiment In\" link.) This allows the resources to be deallocated so that someone else can use them. Do not use the potentially misleading \"Terminate Experiment\" link unless you are completely finished with your exercise. Termination will erase the experiment and you won't be able to swap it back in without recreating it. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you're taking a break from the work, but coming back later. Terminating says \"I won't need this experiment again, ever.\" This may be confusing, especially since \"Swap Out\" seems to imply that it saves your progress (it doesn't, as described above). Just remember to Swap In/Out, and never \"Terminate\" unless you're sure you're completely done with the experiment. If you do end up terminating an experiment, you can always go back and recreate it. Submitting your work to your instructor \uf0c1 Each exercise manual has a section entitled \"Submission Instructions,\" and your instructor may have given you additional instructions for submission. Follow the instructions in that section, and submit your work to your instructor. Unless otherwise instructed, it's a good idea to include: Your name Your preferred email address Your student ID (if applicable) Your DETERLab username Your experiment's name (e.g., jstudent-exploits) Frequently Asked Questions \uf0c1 Please check the following list of questions for answers. If you do not find an answer to your question here or elsewhere, please email your instructor or TA. Do not email testbed ops unless specifically instructed to do so by your instructor. Why can't I log in to DETERLab? \uf0c1 DETERLab has an automatic blacklist mechanism. If you enter the wrong username and password combination too many times, your account will no longer be accessible from your current IP address. If you think that this has happened to you, you can try logging in from another address (if you know how), or you can email your instructor or TA and specify your IP address. They will relay the request to the testbed ops that this specific blacklist entry should be erased. If you have questions you think should be added to this FAQ, or other information you think should be added to this document, please contact usXXX.","title":"Student Introduction to DETERLab"},{"location":"education/student-intro/#student-introduction-to-deterlab","text":"Contributors: Peter A. H. Peterson, UCLA. pahp@... David Morgan, USC. davidmor@...","title":"Student Introduction to DETERLab"},{"location":"education/student-intro/#what-is-deterlab","text":"DETERLab is a security and education-enhanced version of Emulab. Funded by the National Science Foundation and the Department of Homeland Security, DETERLab is hosted by USC/ISI and UC Berkeley. \"USC/ISI's DETERLab (cyber DEfense Technology Experimental Research Laboratory) is a state-of-the-art scientific computing facility for cyber-security researchers engaged in research, development, discovery, experimentation, and testing of innovative cyber-security technology. DETERLab is a shared testbed providing a platform for research in cyber security and serving a broad user community, including academia, industry, and government. To date, DETERLab-based projects have included behavior analysis and defensive technologies including DDoS attacks, worm and botnet attacks, encryption, pattern detection, and intrusion-tolerant storage protocols. [ 1 ].\" DETERLab (like Emulab) offers user accounts with assorted permissions associated with different experiment groups. Each group can have its own preconfigured experimental environments running on Linux, BSD, Windows, or other operating systems. Users running DETERLab experiments have full control of real hardware and networks running preconfigured software packages. These features make it an ideal platform for computer science and especially computer security education. Many instructors have designed class exercises (homework assignments, project assignments, in-class demos, etc.) consisting of a lab manual, software, data, network configurations, and machines from DETER's pool. This allows each student to run her own experiments on dedicated hardware.","title":"What is DETERLab?"},{"location":"education/student-intro/#how-does-it-work","text":"The software running DETERLab will load operating system images (low level disk copies) onto to free nodes in the testbed, and then reconfigure programmable switches to create VLANs with the newly-imaged nodes connected according to the topology specified by the experiment creator. After the system is fully imaged and configured, DETERLab will execute specified scripts, unpack tarballs, and/or install rpm files according to the experiment's configuration. The end result is a live network of real machines, accessible via the Internet.","title":"How does it work?"},{"location":"education/student-intro/#how-do-i-get-a-deterlab-login","text":"Your instructor will request an account for you. Simply send your preferred email address to your instructor. Once the testbed ops set up your account, you will receive an email with your username and password at the address you supplied. Within one week, use those credentials to log in. Edit your profile as follows: a. Choose \"Profile\" tab. b. Choose \"Edit profile\" menu option. c. Replace any default contents in the two fields shown with your actual name and working phone number d. Change your password! e. Click \"Submit\"","title":"How do I get a DETERLab login?"},{"location":"education/student-intro/#using-deterlab","text":"","title":"Using DETERLab"},{"location":"education/student-intro/#how-do-i-start-an-exercise","text":"Before you can perform the tasks described in your exercise assignment, you will, in many cases, need to create an experiment in DETERLab to work on. This will be your environment to use whenever you need it. To create a new experiment: Log into DETERLab with your account. Under the \"Experimentation\" menu at the top of the page, click \"Begin an Experiment\". Select your Class Project name from the \"Select Project\" dropdown. (Throughout this document, we'll assume your class project name is YourProject) Leave the \"Group\" dropdown set to Default unless otherwise instructed. In the \"Name\" field, enter a name of the format username-exercisename. (Example: jstudent-exploits). Enter a brief description in the \"Description\" field. In the \"Your NS File\" field, follow the instructions in the \"Setup\" section of your exercise manual. Set the \"Idle Swap\" field to 1 h. Leave the rest of the settings for \"Swapping,\" \"Linktest Option,\" and \"BatchMode\" alone (unless otherwise instructed). If you would like to start your lab now, check the \"Swap In Immediately\" box and move to the next section. Otherwise, do not check this box. Click \"Submit\"!","title":"How do I start an exercise?"},{"location":"education/student-intro/#how-do-i-work-on-my-exercise","text":"Log into DETERLab with your DETERLab account (or contact your instructor if you need an account). Click on the \"My DETERLab\" link on the left hand menu. In the \"Current Experiments\" table, click on the name of the experiment you want. Under the \"Experiment Options\" menu on the left margin, click \"Swap Experiment In\", then click \"Confirm\". The swap in process will take 5 to 10 minutes. While you're waiting, you can watch the swap in process displayed in your web browser. Or, you can watch your email box for a message letting you know that it has finished swapping in. When the experiment has finished swapping in, you can perform the tasks in your exercise manual.","title":"How do I work on my exercise?"},{"location":"education/student-intro/#how-do-i-access-my-experiment","text":"Your experiment is made up of one or more machines on the internal DETERLab network, which is behind a firewall. To access your experimental nodes, you'll need to first SSH to users.deterlab.net. If you don't know how to use SSH, see our tutorial. users.deterlab.net (or users for short) is the \"control server\" for DETERLab. From users you can contact all your nodes, reboot them, connect to their serial ports, etc. Once you log in to users, you'll need to SSH again to your actual experimental nodes. Since your nodes' addresses may change every time you swap them in, it's best to SSH to the permanent network names of the nodes. Here's how to figure out what their names are: Once your experiment has swapped in: Navigate to the experiment you just installed. If you just swapped in your experiment, the quickest way to find your node names is to click on the experiment name in the table under \"Swap Control.\" However, you can also get there by clicking \"My DETERLab\". Your experiment is listed as \"active\" in the \"State\" column. Click on the experiment's name in the \"EID\" column. Once you can see your experiment's page, click on the \"Details\" tab in the main content panel. Your nodes' network names are listed under the heading \"Qualified Name.\" For example, node1.YourExperiment.YourProject.isi.deterlab.net. You should familiarize yourself with the information available on this page, but for now we just need to know the long DNS qualified name(s) node(s) you just swapped in. If you are curious, you should also look at the \"Settings\" (generic info), \"Visualization,\" and \"NS File.\" (The topology mapplet may be disabled for some labs, so these last two may not be visible). Now that you are logged in to users.deterlab.net, your nodes are swapped in, and you know their network name(s), you can SSH from users to your experimental nodes by executing: ssh node1.YourExperiment.YourProject.isi.deterlab.net. You will not need to re-authenticate. You may need to wait a few more minutes. Once DETERLab is finished setting up the experiment, the nodes still need a minute or two to boot and complete their configuration. If you get a message about \"server configuration\" when you try to log in, wait a few minutes and try again. If a lab instructs you to create new users on your experimental nodes, you can log in as them by running ssh newuser@node1.YourExperiment.YourProject.isi.deterlab.net or ssh newuser@localhost from the experimental node. Congratulations! Your lab environment is now set up, and you can get to work at the tasks in your lab manual. Make sure you read the \"Things to keep in mind\" section below! Some labs benefit from Port Forwarding. Port Forwarding is a technique that can allow you to access your experimental nodes directly from your desktop computer. This is especially useful for accessing web applications running on your experimental nodes. See our ssh tutorial for more information. Finally, when you are done working with your nodes, you should save your work and swap out the experiment so that someone else can use the physical machines.","title":"How do I access my experiment?"},{"location":"education/student-intro/#things-to-keep-in-mind","text":"Carefully read the evolving version of this documentXXX.","title":"Things to keep in mind"},{"location":"education/student-intro/#saving-and-securing-your-files-on-deterlab","text":"Every user on DETERLab has a home directory on users.deterlab.net which is mounted via NFS (Network File System) to experimental nodes. This means that anything you place in your home directory on one experimental node (or the users machine) is visible in your home directory on your other experimental nodes. Your home directory is private, so you may save your work in that directory. However, everything else on experimental nodes is permanently lost when an experiment is swapped out. Make sure you save your work in your home directory before swapping out your experiment''' Another place to save your files would be /proj/YourProject. This directory is also NFS-mounted to all experimental nodes so same rules apply about writing to it a lot, as for your home directory. It is shared by all members of your project/class. Again, on DETERLab, files ARE NOT SAVED between swap-ins. Additionally, class experiments may be forcibly swapped out after a certain number of idle hours (or some maximum amount of time). You must manually save copies of any files you want to keep in your home directory. Any files left elsewhere on the experimental nodes will be erased and lost forever. This means that if you want to store progress for a lab and come back to it later, you will need to put it in your home directory before swapping out the experiment.","title":"Saving and securing your files on DETERLab"},{"location":"education/student-intro/#swap-out-dont-terminate","text":"When you are done with your experiment for the time being, please make sure you save your work into an appropriate location and then swap out your experiment. To do this, use the \"Swap Experiment Out\" link in the \"Experiment Options\" panel. (This is the same place that used to have a \"Swap Experiment In\" link.) This allows the resources to be deallocated so that someone else can use them. Do not use the potentially misleading \"Terminate Experiment\" link unless you are completely finished with your exercise. Termination will erase the experiment and you won't be able to swap it back in without recreating it. Swapping out is the equivalent of temporarily stopping the experiment and relinquishing the testbed resources. Swapping out is what you want to do when you're taking a break from the work, but coming back later. Terminating says \"I won't need this experiment again, ever.\" This may be confusing, especially since \"Swap Out\" seems to imply that it saves your progress (it doesn't, as described above). Just remember to Swap In/Out, and never \"Terminate\" unless you're sure you're completely done with the experiment. If you do end up terminating an experiment, you can always go back and recreate it.","title":"Swap out -- DON'T \"terminate\"!"},{"location":"education/student-intro/#submitting-your-work-to-your-instructor","text":"Each exercise manual has a section entitled \"Submission Instructions,\" and your instructor may have given you additional instructions for submission. Follow the instructions in that section, and submit your work to your instructor. Unless otherwise instructed, it's a good idea to include: Your name Your preferred email address Your student ID (if applicable) Your DETERLab username Your experiment's name (e.g., jstudent-exploits)","title":"Submitting your work to your instructor"},{"location":"education/student-intro/#frequently-asked-questions","text":"Please check the following list of questions for answers. If you do not find an answer to your question here or elsewhere, please email your instructor or TA. Do not email testbed ops unless specifically instructed to do so by your instructor.","title":"Frequently Asked Questions"},{"location":"education/student-intro/#why-cant-i-log-in-to-deterlab","text":"DETERLab has an automatic blacklist mechanism. If you enter the wrong username and password combination too many times, your account will no longer be accessible from your current IP address. If you think that this has happened to you, you can try logging in from another address (if you know how), or you can email your instructor or TA and specify your IP address. They will relay the request to the testbed ops that this specific blacklist entry should be erased. If you have questions you think should be added to this FAQ, or other information you think should be added to this document, please contact usXXX.","title":"Why can't I log in to DETERLab?"},{"location":"education/submit-work/","text":"Submitting Your Work \uf0c1 A student can submit their work by doing the following: Click on My Classes tab in My Deterlab view Click on Choose file button next to your assignment then click on Submit button You can resubmit your work as many times as you like. Your later submissions overwrite the previous ones. Currently, there is no indication that the system shows to the student for a successful submission. Your instructor or TA can check if the system has noted your submission.","title":"Submitting Your Work"},{"location":"education/submit-work/#submitting-your-work","text":"A student can submit their work by doing the following: Click on My Classes tab in My Deterlab view Click on Choose file button next to your assignment then click on Submit button You can resubmit your work as many times as you like. Your later submissions overwrite the previous ones. Currently, there is no indication that the system shows to the student for a successful submission. Your instructor or TA can check if the system has noted your submission.","title":"Submitting Your Work"},{"location":"orchestrator/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. MAGI Orchestrator \uf0c1 The MAGI Orchestrator is a DETERLab capability that allows you to automate and manage the procedures of a DETERLab experiment. Docs in this section include: Orchestrator Quickstart Orchestrator Guide Orchestrator Case Studies MAGI Configuration MAGI System Organization MAGI Library MAGI Tools Writing Your Own MAGI Agents Configuring a MAGI Agent at Runtime Orchestrator Data Management MAGI Development Codebase MAGI Desktop Orchestrator Reference","title":"Orchestrator"},{"location":"orchestrator/#magi-orchestrator","text":"The MAGI Orchestrator is a DETERLab capability that allows you to automate and manage the procedures of a DETERLab experiment. Docs in this section include: Orchestrator Quickstart Orchestrator Guide Orchestrator Case Studies MAGI Configuration MAGI System Organization MAGI Library MAGI Tools Writing Your Own MAGI Agents Configuring a MAGI Agent at Runtime Orchestrator Data Management MAGI Development Codebase MAGI Desktop Orchestrator Reference","title":"MAGI Orchestrator"},{"location":"orchestrator/agent-configuration/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Configuring a MAGI Agent at Runtime \uf0c1 In Writing MAGI Agents , you saw how to create a basic agent. The sample agent created a single file on a test node. This document will explain how to use configuration in the AAL file to configure an agent at runtime. Setting Agent Configuration \uf0c1 This document will expand the sample code of our FileCreator example. For reference, here is the agent code: from magi.util.agent import DispatchAgent, agentmethod from magi.util.processAgent import initializeProcessAgent # The FileCreator agent implementation, derived from DispatchAgent. class FileCreator(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) self.filename = '/tmp/newfile' # A single method which creates the file named by self.filename. # (The @agentmethod() decorator is not required, but is encouraged. # it does nothing of substance now, but may in the future.) @agentmethod() def createFile(self, msg): **Create a file on the host.** # open and immediately close the file to create it. open(self.filename, 'w').close() # the getAgent() method must be defined somewhere for all agents. # The Magi daemon invokes this mehod to get a reference to an # agent. It uses this reference to run and interact with an agent # instance. def getAgent(): agent = FileCreator() return agent # In case the agent is run as a separate process, we need to # create an instance of the agent, initialize the required # parameters based on the received arguments, and then call the # run method defined in DispatchAgent. if __name__ \"__main__\": agent = FileCreator() initializeProcessAgent(agent, sys.argv) agent.run() If we reset the self.filename variable in the agent before invoking createFile in the AAL, we can change the file that is created. The base class DispatchAgent itself is derived from a class that will let us do this. The Agent class implements two methods: setConfiguration - Sets the passed parameters as class instance variables. confirmConfiguration - This method is meant to be re-implemented in your agent if you need confirm the variables set are valid for your agent. To set the self.filename variable in the FileCreator Agents, we modify the AAL to include a call to the Agent method setConfiguration , passing in a list of key-value pairs. (In the following example, it is a single key-value pair.) - type: event agent: myFileCreators method: setConfiguration args: filename: /tmp/myCreatedFile Note that you do not specify self when referencing an Agent variable. We make sure to place this event in the AAL event stream prior to the createFile event. The complete AAL file is: streamstarts: [main] groups: myFileCreatorGroup: [NODES] agents: myFileCreators: group: myFileCreatorGroup # (note: the \"PATH\" argument is the agent directory. The # directory must contain an IDL and agent implementation. It must # also contain a *__init__.py* file, which is required # for it to be considered as a valid python package.) path: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /tmp/myCreatedFile - type: event agent: myFileCreators method: createFile args: {} Now when we run the Agent again (possibly using agentTool to restart the Magi daemons), we see the following events: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/tmp/myCreatedFile ... ) --> myFileCreatorGroup stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ ssh myNode.myExperiment.myGroup ls -l /tmp/myCreatedFile -rw-r--r-- 1 root root 0 Mar 5 13:55 /tmp/myCreatedFile $ And we see that our specified file, /tmp/myCreatedFile was created. Confirming Valid Configuration \uf0c1 This works well, but the input to the Agent is free-form. What if the user gives invalid input, like the wrong type or data that is not in a valid range? This is where the Agent confirmConfiguration method comes into play. confirmConfiguration should be written for any Agent that wants to validate its state. It gets invoked in the AAL file after the user invokes setConfiguration . Note: The concept of an Agent confirming user input will change in future releases of MAGI. The Orchestrator (or other MAGI/Montage components) will use the interface specification in the Agent\u2019s IDL file to ensure the input to the agent is valid. Suppose our sample agent wanted to allow the user to create a file in only the /local directory on the host machine. The confirmConfiguration method that does this is: def confirmConfiguration(self): **Make sure the user input is a string value and starts with \"/local\".** if not isinstance(self.filename, (str, unicode)): return False if not self.filename.startswith('/local'): return False return True When we add this method to our sample Agent, and run the experiment with the existing AAL file, which contains configuration that does not start with /local , the Orchestrator gives us an error while executing the event stream: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/tmp/myCreatedFile ... ) --> myFileCreatorGroup stream unknown : exit : method setConfiguration returned False on agent unknown in group unknown and on node(s): moat. $ The Orchestrator exited with an error, as it should. If we now modify the AAL file to include a valid configuration, the Orchestrator succeeds. The updated AAL fragment is: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile When we run the Orchestrator with the modified AAL, it succeeds as the agent configuration is now valid: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/local/myGreatFile ... ) --> myFileCreatorGroup stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ And the \u201cvalid\u201d file has been created on the machine: $ ssh myNode.myExperiment.myGroup ls -l /local total 4 drwxrwxr-x 2 glawler Deter 4096 Mar 5 08:35 logs -rw-r--r-- 1 root root 0 Mar 5 14:33 myGreatFile $ Triggers and Event Stream Sequence Points \uf0c1 If you run the AAL and Agent code above, you may see that it does not actually work. One small needed detail has been left out of the AAL file. Normally the Orchestrator will run through the events in the AAL as fast is it can. If we used the event streams in the AAL file as it now stands: eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile - type: event agent: myFileCreators method: createFile args: {} The Orchestrator will send two messages to the Agents in rapid succession: the setConfiguration and createFile event messages. If the setConfiguration call returns False , which it will given invalid input, the Orchestrator will not receive the message because would have sent the messages and exited. Therefore, we need a way to tell the Orchestrator to wait for a response from setConfiguration before continuing. This is done by inserting a small pause, using a trigger which times out after 3 seconds: # Wait 3 seconds for a response to setConfiguration # timeout value is in milliseconds. - type: trigger triggers: [{timeout: 3000}] If we insert this trigger between setConfiguration and createFile , the Orchestrator will receive the error message from the agent and exit on error. The full AAL file now is: streamstarts: [main] groups: myFileCreatorGroup: [witch, moat] agents: myFileCreators: group: myFileCreatorGroup path: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile - type: trigger triggers: [{timeout: 3000}] - type: event agent: myFileCreators method: createFile args: {} But how do we know that waiting for 3 seconds is a long enough time to wait? Wouldn\u2019t it be better if we could tell the Orchestrator to wait for a response from the agent before continuing? We can do this using a named trigger. We add a trigger statement to the setConfiguration event clause and modify the trigger to wait for that event before continuing to process the event stream: - type: event agent: myFileCreators trigger: configDone method: setConfiguration args: filename: /local/myGreatFile # Wait for the event \"configDone\" from all fileCreator agents. - type: trigger triggers: [{event: configDone, agent: myFileCreators}] Now when setConfiguration is called on the Agent, the daemon will send a trigger with the event configDone after the method has returned. With this modified trigger, the Orchestrator will wait for the trigger event configDone before processing the next event in the event stream. Here is the Orchestrator output now. Note that setConfiguration now \u201cfires\u201d a trigger (sends a trigger) and the Orchestrator waits until the trigger is resolved before moving on. $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/local/myGreatFile ... ) --> myFileCreatorGroup (fires trigger: configDone) stream main : done : trigger configDone complete. stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ For reference, the new agent implementation, AAL file, and IDL, file can be downloaded as a tar file here: FileCreator-withconfig.tbz .","title":"Agent configuration"},{"location":"orchestrator/agent-configuration/#configuring-a-magi-agent-at-runtime","text":"In Writing MAGI Agents , you saw how to create a basic agent. The sample agent created a single file on a test node. This document will explain how to use configuration in the AAL file to configure an agent at runtime.","title":"Configuring a MAGI Agent at Runtime"},{"location":"orchestrator/agent-configuration/#setting-agent-configuration","text":"This document will expand the sample code of our FileCreator example. For reference, here is the agent code: from magi.util.agent import DispatchAgent, agentmethod from magi.util.processAgent import initializeProcessAgent # The FileCreator agent implementation, derived from DispatchAgent. class FileCreator(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) self.filename = '/tmp/newfile' # A single method which creates the file named by self.filename. # (The @agentmethod() decorator is not required, but is encouraged. # it does nothing of substance now, but may in the future.) @agentmethod() def createFile(self, msg): **Create a file on the host.** # open and immediately close the file to create it. open(self.filename, 'w').close() # the getAgent() method must be defined somewhere for all agents. # The Magi daemon invokes this mehod to get a reference to an # agent. It uses this reference to run and interact with an agent # instance. def getAgent(): agent = FileCreator() return agent # In case the agent is run as a separate process, we need to # create an instance of the agent, initialize the required # parameters based on the received arguments, and then call the # run method defined in DispatchAgent. if __name__ \"__main__\": agent = FileCreator() initializeProcessAgent(agent, sys.argv) agent.run() If we reset the self.filename variable in the agent before invoking createFile in the AAL, we can change the file that is created. The base class DispatchAgent itself is derived from a class that will let us do this. The Agent class implements two methods: setConfiguration - Sets the passed parameters as class instance variables. confirmConfiguration - This method is meant to be re-implemented in your agent if you need confirm the variables set are valid for your agent. To set the self.filename variable in the FileCreator Agents, we modify the AAL to include a call to the Agent method setConfiguration , passing in a list of key-value pairs. (In the following example, it is a single key-value pair.) - type: event agent: myFileCreators method: setConfiguration args: filename: /tmp/myCreatedFile Note that you do not specify self when referencing an Agent variable. We make sure to place this event in the AAL event stream prior to the createFile event. The complete AAL file is: streamstarts: [main] groups: myFileCreatorGroup: [NODES] agents: myFileCreators: group: myFileCreatorGroup # (note: the \"PATH\" argument is the agent directory. The # directory must contain an IDL and agent implementation. It must # also contain a *__init__.py* file, which is required # for it to be considered as a valid python package.) path: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /tmp/myCreatedFile - type: event agent: myFileCreators method: createFile args: {} Now when we run the Agent again (possibly using agentTool to restart the Magi daemons), we see the following events: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/tmp/myCreatedFile ... ) --> myFileCreatorGroup stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ ssh myNode.myExperiment.myGroup ls -l /tmp/myCreatedFile -rw-r--r-- 1 root root 0 Mar 5 13:55 /tmp/myCreatedFile $ And we see that our specified file, /tmp/myCreatedFile was created.","title":"Setting Agent Configuration"},{"location":"orchestrator/agent-configuration/#confirming-valid-configuration","text":"This works well, but the input to the Agent is free-form. What if the user gives invalid input, like the wrong type or data that is not in a valid range? This is where the Agent confirmConfiguration method comes into play. confirmConfiguration should be written for any Agent that wants to validate its state. It gets invoked in the AAL file after the user invokes setConfiguration . Note: The concept of an Agent confirming user input will change in future releases of MAGI. The Orchestrator (or other MAGI/Montage components) will use the interface specification in the Agent\u2019s IDL file to ensure the input to the agent is valid. Suppose our sample agent wanted to allow the user to create a file in only the /local directory on the host machine. The confirmConfiguration method that does this is: def confirmConfiguration(self): **Make sure the user input is a string value and starts with \"/local\".** if not isinstance(self.filename, (str, unicode)): return False if not self.filename.startswith('/local'): return False return True When we add this method to our sample Agent, and run the experiment with the existing AAL file, which contains configuration that does not start with /local , the Orchestrator gives us an error while executing the event stream: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/tmp/myCreatedFile ... ) --> myFileCreatorGroup stream unknown : exit : method setConfiguration returned False on agent unknown in group unknown and on node(s): moat. $ The Orchestrator exited with an error, as it should. If we now modify the AAL file to include a valid configuration, the Orchestrator succeeds. The updated AAL fragment is: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile When we run the Orchestrator with the modified AAL, it succeeds as the agent configuration is now valid: $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/local/myGreatFile ... ) --> myFileCreatorGroup stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ And the \u201cvalid\u201d file has been created on the machine: $ ssh myNode.myExperiment.myGroup ls -l /local total 4 drwxrwxr-x 2 glawler Deter 4096 Mar 5 08:35 logs -rw-r--r-- 1 root root 0 Mar 5 14:33 myGreatFile $","title":"Confirming Valid Configuration"},{"location":"orchestrator/agent-configuration/#triggers-and-event-stream-sequence-points","text":"If you run the AAL and Agent code above, you may see that it does not actually work. One small needed detail has been left out of the AAL file. Normally the Orchestrator will run through the events in the AAL as fast is it can. If we used the event streams in the AAL file as it now stands: eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile - type: event agent: myFileCreators method: createFile args: {} The Orchestrator will send two messages to the Agents in rapid succession: the setConfiguration and createFile event messages. If the setConfiguration call returns False , which it will given invalid input, the Orchestrator will not receive the message because would have sent the messages and exited. Therefore, we need a way to tell the Orchestrator to wait for a response from setConfiguration before continuing. This is done by inserting a small pause, using a trigger which times out after 3 seconds: # Wait 3 seconds for a response to setConfiguration # timeout value is in milliseconds. - type: trigger triggers: [{timeout: 3000}] If we insert this trigger between setConfiguration and createFile , the Orchestrator will receive the error message from the agent and exit on error. The full AAL file now is: streamstarts: [main] groups: myFileCreatorGroup: [witch, moat] agents: myFileCreators: group: myFileCreatorGroup path: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: setConfiguration args: filename: /local/myGreatFile - type: trigger triggers: [{timeout: 3000}] - type: event agent: myFileCreators method: createFile args: {} But how do we know that waiting for 3 seconds is a long enough time to wait? Wouldn\u2019t it be better if we could tell the Orchestrator to wait for a response from the agent before continuing? We can do this using a named trigger. We add a trigger statement to the setConfiguration event clause and modify the trigger to wait for that event before continuing to process the event stream: - type: event agent: myFileCreators trigger: configDone method: setConfiguration args: filename: /local/myGreatFile # Wait for the event \"configDone\" from all fileCreator agents. - type: trigger triggers: [{event: configDone, agent: myFileCreators}] Now when setConfiguration is called on the Agent, the daemon will send a trigger with the event configDone after the method has returned. With this modified trigger, the Orchestrator will wait for the trigger event configDone before processing the next event in the event stream. Here is the Orchestrator output now. Note that setConfiguration now \u201cfires\u201d a trigger (sends a trigger) and the Orchestrator waits until the trigger is resolved before moving on. $ magi_orchestrator.py --project $project --experiment $experiment --events ./myEvents.aal -o run.log -v stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : setConfiguration(['/local/myGreatFile ... ) --> myFileCreatorGroup (fires trigger: configDone) stream main : done : trigger configDone complete. stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. $ For reference, the new agent implementation, AAL file, and IDL, file can be downloaded as a tar file here: FileCreator-withconfig.tbz .","title":"Triggers and Event Stream Sequence Points"},{"location":"orchestrator/agent-library/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. MAGI Agent Library \uf0c1 Every node has a daemon running with a series of Agents on it. These Agents are each executed in their own thread or process. They are provided with a defined interface with which to send and receive messages to other objects in the experiment. The MAGI daemon will route messages to the agent based on the routing information in the message. The daemon supports group-based, name-based, and \u201cdock\u201d-based routing. (A dock is like a port for a traditional daemon; an agent listens on a dock.) Once a message is delivered to an agent, the format of the message data is then up to the agent itself. Most agents will not need to parse messages directly, however, because the MAGI Agent Library supports a number of useful abstractions implemented in base classes from which Agent authors can derive. These are described in detail below. Agent Execution Models \uf0c1 There are two execution models supported by the daemon for Agents: Thread-based - A thread-based Agent is loaded and runs in the process space of the daemon. The daemon communicates with a thread-based agent directly. Process-based - A process-based Agent is started as a separate process. The daemon communicates with it via standard interprocess communication techniques: a pipe or a socket. Here is a list outlining the differences between the execution models. ** Threads** ** Pro** : Lightweight ** Pro** : Messages passed as objects without need for serialization ** Con** : Must be written in Python ** Con** : Must be aware of other threads when it comes to file descriptors or other shared memory ** Process** (Pipe or Socket) ** Pro** : Agents may be written in languages other than Python. ** Pro** : May kill off agent individually from the shell ** Con** : Heavier weight if invoking a new interpreter for each Agent for scripted languages ** Con** : Message transceiver is more complex, in particular if a library for the language has not been written. Note As of now, only Python is supported. We are working on adding support for other languages. Interface Description Language (IDL) \uf0c1 Agent authors must write an IDL that matches the interface exported by their agent. This IDL is used by MAGI to validate the interface of the agent (and in the future to generate GUIs for agent execution and configuration.) The IDL should specify: * agent execution model (thread or process); * any variables exposed by the agent and their types, ranges, or enumerated values; * any public methods and the method arguments and their types; * \u201chelp\u201d strings for each method and agent variable which explain their purpose; * any Agent library from which they derive. This may seem like a lot to specify, but the Agent Library supplies IDL for base Agents -- so in practice much of the IDL specification will be supplied to the Agent author. The IDL format and keywords are given in a table below. (TBD - Coming soon) Agent Library \uf0c1 In this section we describe the Agent Library and give brief examples for usage. Classes are organized from the bottom up, that is, starting with the class from which the others derive. Note When using the Orchestrator to run your experiment, the Orchestrator will, by default, handle a return value of False from an Agent method as a reason to unload all Agents, break down communication groups and exit. Thus your Agent may stop an experiment by returning False . Agent \uf0c1 This is the base Agent class. It implements a setConfiguration method. If derived from, the user may call setConfiguration to set any self variables in your class. Agent also implements an empty confirmConfiguration method that is called once the self variables are set. You may implement your own confirmConfiguration if you need to make sure the user has set your internal variables to match any constraints you may want to impose. Returning False from this method will signal to the Orchestrator that something is wrong and the Orchestrator should handle this as an error. The default implementation of confirmConfiguration simply returns True . The method signature for confirmConfiguration() is def confirmConfiguration(self): It takes no arguments. In your confirmConfiguration method, you should confirm that your agent internal variables are the correct type and in the expected range. In the following example, imagine an agent has a variable that is an integer and the range of the value must be between 1 and 10. An agent can use the Agent class to implement this as so: from magi.util.agent import Agent class myAgent(Agent): def __init__(self): self.value = None def confirmConfiguration(): if not isinstance(self.value, int): return False if not 1 <= self.value <= 10: return False return True If the variable self.value is not an integer or is not between 1 and 10, confirmConfiguration returns False . If running this agent with the Orchestrator, the False value will get returned to the Orchestrator which will unload all agents, destroy all group communications, then exit. Thus your agent may cause the experiment to stop and be reset when it is not given the correct inputs. Note In the future, this functionality of enforcing correct input will be handled outside of the agent code. The IDL associated with the agent already specifies correct input and the Orchestrator (or other Montage/MAGI front end tool) will enforce proper input. All classes in the AgentLibrary inherit from Agent . The Agent documentation can seen here . DispatchAgent \uf0c1 The DispatchAgent implements the simple remote procedure call (RPC) mechanism used by the Orchestrator (and available through the MAGI python API). This allows any method in a class derived from DispatchAgent to be invoked in an AAL file (or by a MagiMessage if using the MAGI python interface directly). You almost always want to derive your agent from DispatchAgent . The DispatchAgent code simply loops, parsing incoming messages, looking for an event message. When it finds one, it attempts to call the method specified in the message with the arguments given in the message, thus implementing a basic RPC functionality in your agent. The first argument to your RPC-enabled method is the received message. It is accompanied by the optional named-parameters, sent as part of the MagiMessage . The Agent Library exports a function decorator for DispatchAgent -callable methods named agentmethod . It is not currently used for anything, but it is suggested that agent developers use it anyway. The DispatchAgent reads incoming messages and invokes the required method synchronously, i.e., it waits for a method call to return before reading the next message. Here is a simple example: from magi.util.agent import DispatchAgent, agentmethod def myAgent(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) @agentmethod() def doAction(self, msg): pass Given the agent myAgent above and the AAL fragment below, the method doAction will be called on all test nodes associated with myAgentGroup . eventstreams: myStream: - type: event agent: myAgentGroup method: doAction args: { } The DispatchAgent documentation may seen here . NonBlockingDispatchAgent \uf0c1 The NonBlockingDispatchAgent is similar to DispatchAgent . The only difference is that NonBlockingDispatchAgent invokes the methods asynchronously , i.e., it forks a new thread for each method call and does not wait for the call to return. It invokes the required method and moves on to read the next message. ReportingDispatchAgent \uf0c1 You will note that the DispatchAgent only allows an outside source to send commands to the agent. There is no communication backwards. The ReportingDispatchAgent base class has a slightly different run loop. Rather than blocking forever on incoming messages, it will also call its own method, periodic , to allow other operations to occur. The call to periodic will return the amount of time in seconds (as a float) that it will wait until calling periodic again. The periodic function therefore controls how often it is called. The first call will happen as soon as the run is called. The method signature of the periodic method is: def periodic(self, now): If periodic is not implemented in the subclass, an exception is raised. This example code writes the current time to a file once a second. Note the explicit use of the Agent class to set the file name. import os.path from magi.util.agent import ReportingDispatchAgent, agentmethod class myTimeTracker(ReportingDispatchAgent): def __init__(self): ReportingDispatchAgent.__init__(self) self.filename = None def confirmConfiguration(self): if not os.path.exists(self.filename): return False def periodic(self, now): with open(self.filename, 'a') as fd: fd.write('%f\\n' % now) # call again one second from now return 1.0 The ReportingDispatchAgent documentation may seen here. http://montage.deterlab.net/backend/python/util.html#magi.util.agent.ReportingDispatchAgent SharedServer \uf0c1 The SharedServer class inherits from DispatchAgent and expects the subclass to implement the methods runserver and terminateserver to start or stop a local server process. The SharedServer class takes care of multiple agents requesting use of the server and only calls runserver or terminateserver when required. This ensures that there is ever only one instance of the server running at once on a given host. A canonical example of this would be a web server running a single instance of Apache. The methods runserver and stopserver take no arguments. Below is an example of a simple agent that starts and stops Apache on the local host. If there are other agents running on the machine that require Apache to be running, they may inherit from SharedServer as well, thus ensuring that there is only ever one instance of Apache running. from subprocess import check_call, CalledProcessError from magi.util.agent import SharedServer class ApacheServerAgent(SharedServer): def __init__(self): SharedServer.__init__(self) def runserver(self): try: check_call('apachectl start'.split()) except CalledProcessError: return False return True def stopserver(self): try: check_call('apachectl stop'.split()) except CalledProcessError: return False return True The SharedServer documentation may seen here . TrafficClientAgent \uf0c1 TrafficClientAgent models an agent that periodically generates traffic. It must implement the getCmd method, returning a string to execute on the commandline to generate traffic. For example, the getCmd could return a curl or wget command to generate client-side HTML traffic. The signature of getCmd is: def getCmd(self, destination) Where destination is a server host name from which the agent should request traffic. The TrafficClientAgent class implements the following event-callable methods: startClient() and stopClient() . Neither method takes any arguments. These methods may be invoked from an AAL and start and stop the client respectively. The base class contains a number of variables which control how often getCmd is called and which servers should be contacted: * servers : A list of server hostnames * interval : A distribution variable Note A distribution variable is any valid python expression that returns a float. It may be as simple as an integer, \u201c1\u201d, or an actual distribution function. The Agent Library provides minmax , gamma , pareto , and expo in the distributions module. Thus a valid value for the TrafficClientAgent interval value could be minmax(1,10) , which returns a value between 1 and 10 inclusive. The signatures of these distributions are: minmax(min, max) gamma(alpha, rate, cap = None) pareto(alpha, scale = 1.0, cap = None) expo(lambd, scale = 1.0, cap = None) Below is a sample TrafficClientAgent which implements a simple HTTP client-side traffic agent. It assumes the destinations have been set correctly (via the Agent setConfiguration method) and there are web servers already running there. from magi.util.agent import TrafficClientAgent class mySimpleHTTPClient(TrafficClientAgent): def __init__(self): TrafficClientAgent.__init__(self) def getCmd(self, destination): cmd = 'curl -s -o /dev/null http://%s/index.html' % destination return cmd When this agent is used with the following AAL clauses, the servers ** server_1 and ** server_2 are used as HTTP traffic generation servers and traffic is generated once an interval where the interval ranges randomly between 5 and 10 seconds, inclusive. The first event sets the agent\u2019s internal configuration. The second event starts the traffic generation. eventstreams: myStream: - type: event agent: myHTTPClients method: setConfiguration args: interval: 'minmax(5, 10)' servers: ['server_1', 'server_2'] - type: event agent: myHTTPClients method: startClient args: { } The TrafficClientAgent documentation may seen here. http://montage.deterlab.net/backend/python/util.html#magi.util.agent.TrafficClientAgent ProbabilisticTrafficClientAgent \uf0c1 ProbabilisticTrafficClientAgent provides the same service as TrafficAgent , but getCmd is called only when the configured probability function evaluates to a non-zero value. ConnectedTrafficClientAgent \uf0c1 ConnectedTrafficClientAgent is a base for an agent that controls a set of agents that have standing connections to, and traffic between, a set of servers. connect() and disconnect() are called periodically when a given client should connect or disconnect to a given server. generateTraffic() is called when the given client should generate traffic between itself and the server it is connected to. The sequence of calls is: [period], connect(), [period], generateTraffic(), [period], generateTraffic(), ..., disconnect() This sequence may be repeated. Derived classes should implement connect() , disconnect() , and generateTraffic() . Agent Load and Execution Chain (for threaded agents) \uf0c1 (TBD - Coming soon)","title":"Agent library"},{"location":"orchestrator/agent-library/#magi-agent-library","text":"Every node has a daemon running with a series of Agents on it. These Agents are each executed in their own thread or process. They are provided with a defined interface with which to send and receive messages to other objects in the experiment. The MAGI daemon will route messages to the agent based on the routing information in the message. The daemon supports group-based, name-based, and \u201cdock\u201d-based routing. (A dock is like a port for a traditional daemon; an agent listens on a dock.) Once a message is delivered to an agent, the format of the message data is then up to the agent itself. Most agents will not need to parse messages directly, however, because the MAGI Agent Library supports a number of useful abstractions implemented in base classes from which Agent authors can derive. These are described in detail below.","title":"MAGI Agent Library"},{"location":"orchestrator/agent-library/#agent-execution-models","text":"There are two execution models supported by the daemon for Agents: Thread-based - A thread-based Agent is loaded and runs in the process space of the daemon. The daemon communicates with a thread-based agent directly. Process-based - A process-based Agent is started as a separate process. The daemon communicates with it via standard interprocess communication techniques: a pipe or a socket. Here is a list outlining the differences between the execution models. ** Threads** ** Pro** : Lightweight ** Pro** : Messages passed as objects without need for serialization ** Con** : Must be written in Python ** Con** : Must be aware of other threads when it comes to file descriptors or other shared memory ** Process** (Pipe or Socket) ** Pro** : Agents may be written in languages other than Python. ** Pro** : May kill off agent individually from the shell ** Con** : Heavier weight if invoking a new interpreter for each Agent for scripted languages ** Con** : Message transceiver is more complex, in particular if a library for the language has not been written. Note As of now, only Python is supported. We are working on adding support for other languages.","title":"Agent Execution Models"},{"location":"orchestrator/agent-library/#interface-description-language-idl","text":"Agent authors must write an IDL that matches the interface exported by their agent. This IDL is used by MAGI to validate the interface of the agent (and in the future to generate GUIs for agent execution and configuration.) The IDL should specify: * agent execution model (thread or process); * any variables exposed by the agent and their types, ranges, or enumerated values; * any public methods and the method arguments and their types; * \u201chelp\u201d strings for each method and agent variable which explain their purpose; * any Agent library from which they derive. This may seem like a lot to specify, but the Agent Library supplies IDL for base Agents -- so in practice much of the IDL specification will be supplied to the Agent author. The IDL format and keywords are given in a table below. (TBD - Coming soon)","title":"Interface Description Language (IDL)"},{"location":"orchestrator/agent-library/#agent-library","text":"In this section we describe the Agent Library and give brief examples for usage. Classes are organized from the bottom up, that is, starting with the class from which the others derive. Note When using the Orchestrator to run your experiment, the Orchestrator will, by default, handle a return value of False from an Agent method as a reason to unload all Agents, break down communication groups and exit. Thus your Agent may stop an experiment by returning False .","title":"Agent Library"},{"location":"orchestrator/agent-library/#agent","text":"This is the base Agent class. It implements a setConfiguration method. If derived from, the user may call setConfiguration to set any self variables in your class. Agent also implements an empty confirmConfiguration method that is called once the self variables are set. You may implement your own confirmConfiguration if you need to make sure the user has set your internal variables to match any constraints you may want to impose. Returning False from this method will signal to the Orchestrator that something is wrong and the Orchestrator should handle this as an error. The default implementation of confirmConfiguration simply returns True . The method signature for confirmConfiguration() is def confirmConfiguration(self): It takes no arguments. In your confirmConfiguration method, you should confirm that your agent internal variables are the correct type and in the expected range. In the following example, imagine an agent has a variable that is an integer and the range of the value must be between 1 and 10. An agent can use the Agent class to implement this as so: from magi.util.agent import Agent class myAgent(Agent): def __init__(self): self.value = None def confirmConfiguration(): if not isinstance(self.value, int): return False if not 1 <= self.value <= 10: return False return True If the variable self.value is not an integer or is not between 1 and 10, confirmConfiguration returns False . If running this agent with the Orchestrator, the False value will get returned to the Orchestrator which will unload all agents, destroy all group communications, then exit. Thus your agent may cause the experiment to stop and be reset when it is not given the correct inputs. Note In the future, this functionality of enforcing correct input will be handled outside of the agent code. The IDL associated with the agent already specifies correct input and the Orchestrator (or other Montage/MAGI front end tool) will enforce proper input. All classes in the AgentLibrary inherit from Agent . The Agent documentation can seen here .","title":"Agent"},{"location":"orchestrator/agent-library/#dispatchagent","text":"The DispatchAgent implements the simple remote procedure call (RPC) mechanism used by the Orchestrator (and available through the MAGI python API). This allows any method in a class derived from DispatchAgent to be invoked in an AAL file (or by a MagiMessage if using the MAGI python interface directly). You almost always want to derive your agent from DispatchAgent . The DispatchAgent code simply loops, parsing incoming messages, looking for an event message. When it finds one, it attempts to call the method specified in the message with the arguments given in the message, thus implementing a basic RPC functionality in your agent. The first argument to your RPC-enabled method is the received message. It is accompanied by the optional named-parameters, sent as part of the MagiMessage . The Agent Library exports a function decorator for DispatchAgent -callable methods named agentmethod . It is not currently used for anything, but it is suggested that agent developers use it anyway. The DispatchAgent reads incoming messages and invokes the required method synchronously, i.e., it waits for a method call to return before reading the next message. Here is a simple example: from magi.util.agent import DispatchAgent, agentmethod def myAgent(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) @agentmethod() def doAction(self, msg): pass Given the agent myAgent above and the AAL fragment below, the method doAction will be called on all test nodes associated with myAgentGroup . eventstreams: myStream: - type: event agent: myAgentGroup method: doAction args: { } The DispatchAgent documentation may seen here .","title":"DispatchAgent"},{"location":"orchestrator/agent-library/#nonblockingdispatchagent","text":"The NonBlockingDispatchAgent is similar to DispatchAgent . The only difference is that NonBlockingDispatchAgent invokes the methods asynchronously , i.e., it forks a new thread for each method call and does not wait for the call to return. It invokes the required method and moves on to read the next message.","title":"NonBlockingDispatchAgent"},{"location":"orchestrator/agent-library/#reportingdispatchagent","text":"You will note that the DispatchAgent only allows an outside source to send commands to the agent. There is no communication backwards. The ReportingDispatchAgent base class has a slightly different run loop. Rather than blocking forever on incoming messages, it will also call its own method, periodic , to allow other operations to occur. The call to periodic will return the amount of time in seconds (as a float) that it will wait until calling periodic again. The periodic function therefore controls how often it is called. The first call will happen as soon as the run is called. The method signature of the periodic method is: def periodic(self, now): If periodic is not implemented in the subclass, an exception is raised. This example code writes the current time to a file once a second. Note the explicit use of the Agent class to set the file name. import os.path from magi.util.agent import ReportingDispatchAgent, agentmethod class myTimeTracker(ReportingDispatchAgent): def __init__(self): ReportingDispatchAgent.__init__(self) self.filename = None def confirmConfiguration(self): if not os.path.exists(self.filename): return False def periodic(self, now): with open(self.filename, 'a') as fd: fd.write('%f\\n' % now) # call again one second from now return 1.0 The ReportingDispatchAgent documentation may seen here. http://montage.deterlab.net/backend/python/util.html#magi.util.agent.ReportingDispatchAgent","title":"ReportingDispatchAgent"},{"location":"orchestrator/agent-library/#sharedserver","text":"The SharedServer class inherits from DispatchAgent and expects the subclass to implement the methods runserver and terminateserver to start or stop a local server process. The SharedServer class takes care of multiple agents requesting use of the server and only calls runserver or terminateserver when required. This ensures that there is ever only one instance of the server running at once on a given host. A canonical example of this would be a web server running a single instance of Apache. The methods runserver and stopserver take no arguments. Below is an example of a simple agent that starts and stops Apache on the local host. If there are other agents running on the machine that require Apache to be running, they may inherit from SharedServer as well, thus ensuring that there is only ever one instance of Apache running. from subprocess import check_call, CalledProcessError from magi.util.agent import SharedServer class ApacheServerAgent(SharedServer): def __init__(self): SharedServer.__init__(self) def runserver(self): try: check_call('apachectl start'.split()) except CalledProcessError: return False return True def stopserver(self): try: check_call('apachectl stop'.split()) except CalledProcessError: return False return True The SharedServer documentation may seen here .","title":"SharedServer"},{"location":"orchestrator/agent-library/#trafficclientagent","text":"TrafficClientAgent models an agent that periodically generates traffic. It must implement the getCmd method, returning a string to execute on the commandline to generate traffic. For example, the getCmd could return a curl or wget command to generate client-side HTML traffic. The signature of getCmd is: def getCmd(self, destination) Where destination is a server host name from which the agent should request traffic. The TrafficClientAgent class implements the following event-callable methods: startClient() and stopClient() . Neither method takes any arguments. These methods may be invoked from an AAL and start and stop the client respectively. The base class contains a number of variables which control how often getCmd is called and which servers should be contacted: * servers : A list of server hostnames * interval : A distribution variable Note A distribution variable is any valid python expression that returns a float. It may be as simple as an integer, \u201c1\u201d, or an actual distribution function. The Agent Library provides minmax , gamma , pareto , and expo in the distributions module. Thus a valid value for the TrafficClientAgent interval value could be minmax(1,10) , which returns a value between 1 and 10 inclusive. The signatures of these distributions are: minmax(min, max) gamma(alpha, rate, cap = None) pareto(alpha, scale = 1.0, cap = None) expo(lambd, scale = 1.0, cap = None) Below is a sample TrafficClientAgent which implements a simple HTTP client-side traffic agent. It assumes the destinations have been set correctly (via the Agent setConfiguration method) and there are web servers already running there. from magi.util.agent import TrafficClientAgent class mySimpleHTTPClient(TrafficClientAgent): def __init__(self): TrafficClientAgent.__init__(self) def getCmd(self, destination): cmd = 'curl -s -o /dev/null http://%s/index.html' % destination return cmd When this agent is used with the following AAL clauses, the servers ** server_1 and ** server_2 are used as HTTP traffic generation servers and traffic is generated once an interval where the interval ranges randomly between 5 and 10 seconds, inclusive. The first event sets the agent\u2019s internal configuration. The second event starts the traffic generation. eventstreams: myStream: - type: event agent: myHTTPClients method: setConfiguration args: interval: 'minmax(5, 10)' servers: ['server_1', 'server_2'] - type: event agent: myHTTPClients method: startClient args: { } The TrafficClientAgent documentation may seen here. http://montage.deterlab.net/backend/python/util.html#magi.util.agent.TrafficClientAgent","title":"TrafficClientAgent"},{"location":"orchestrator/agent-library/#probabilistictrafficclientagent","text":"ProbabilisticTrafficClientAgent provides the same service as TrafficAgent , but getCmd is called only when the configured probability function evaluates to a non-zero value.","title":"ProbabilisticTrafficClientAgent"},{"location":"orchestrator/agent-library/#connectedtrafficclientagent","text":"ConnectedTrafficClientAgent is a base for an agent that controls a set of agents that have standing connections to, and traffic between, a set of servers. connect() and disconnect() are called periodically when a given client should connect or disconnect to a given server. generateTraffic() is called when the given client should generate traffic between itself and the server it is connected to. The sequence of calls is: [period], connect(), [period], generateTraffic(), [period], generateTraffic(), ..., disconnect() This sequence may be repeated. Derived classes should implement connect() , disconnect() , and generateTraffic() .","title":"ConnectedTrafficClientAgent"},{"location":"orchestrator/agent-library/#agent-load-and-execution-chain-for-threaded-agents","text":"(TBD - Coming soon)","title":"Agent Load and Execution Chain (for threaded agents)"},{"location":"orchestrator/data-management/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Orchestrator Data Management \uf0c1 Data Management is a very important aspect of experimentation, which is why the data management layer is a very important aspect of the Orchestrator's framework. The following are some of the important terms that are used in context of Orchestrator's data management layer. Sensor : Orchestrator agent that senses information and needs to store it. Collector : Database server that can be used to store data. Shard : In case of a distributed database setup, the data is partitioned and stored in multiple database servers. This concept of partitioning data is known as \"sharding\", and each of the database servers is known as a \"shard\". Orchestrator's data management layer is highly configurable, with experimenters having the ability to setup a centralized or a distributed database, and also configure, at the node level, where sensors collect data. In case of a distributed/shared database setup, Orchestrator sets up a global database server. This server gives a holistic view of the database. Orchestrator data management uses MongoDB at its base. Data Manager Configuration \uf0c1 The data management layer configuration is part of the Orchestrator\u2019s experiment level and node level configuration files. As mentioned earlier, Orchestrator\u2019s data management layer is highly configurable. More information about the same in available at DBDL: Configure the Orchestrator Data Management Layer . Orchestrator\u2019s data management layer enables an experimenter to do the following. Sense and Collect \uf0c1 The following are the steps an agent developer should follow to populate Orchestrator\u2019s database Import the database management utility from magi.util import database Initialize a database collection passing it a unique name. We suggest using the agent name. Each agent implementation that extends from one of the predefined agents, like the DispatchAgent, has a variable \u201cname\u201d that stores the agent name. self.collection = database.getCollection(self.name) Insert data. Each record can be inserted as a dictionary of key-value pairs. self.collection.insert({\u201ckey1\u201d : \u201cvalue1\u201d, \u201ckey2\u201d: \u201cvalue2\u201d}) Note The db management utility inserts three other entries per record: host: <node\u2019s hostname> created: <record creation time> agent: <agent name> Query and Analyze \uf0c1 In case of a distributed database setup, a user can connect to the mongo db server running on the global server node to get an experiment-wide view. However, in case of an unsharded setup, a user would have to connect to the appropriate collector based on the sensor-collector mapping to fetch data stored by a particular sensor. Orchestrator, by default, sets up an non-distributed database, with all the sensors collecting at the same collector. > mongo node-1.myExperiment.myProject:27018 mongo> use magi switched to db magi mongo> db.experiment_data.find() { \"agent\" : \"user_agent\", \"host\" : \"node-1\", \"created\" : 1409075736.646182, \"key1\" : \"value1\", \"key2\" : \"value2\" } { \"agent\" : \"user_agent\", \"host\" : \"node-2\", \"created\" : 1409075737.514683, \"key3\" : \"value3\", \"key4\" : \"value4\" } In case of a distributed setup, the configuration file would have information about a global server host. An experimenter can connect to the global server to get an experiment wide view of the database, or connect to individual collectors to get their local view. And, for more advanced queries, you can refer the Mongo documentation available at http://docs.mongodb.org/manual/tutorial/query-documents/ .","title":"Data management"},{"location":"orchestrator/data-management/#orchestrator-data-management","text":"Data Management is a very important aspect of experimentation, which is why the data management layer is a very important aspect of the Orchestrator's framework. The following are some of the important terms that are used in context of Orchestrator's data management layer. Sensor : Orchestrator agent that senses information and needs to store it. Collector : Database server that can be used to store data. Shard : In case of a distributed database setup, the data is partitioned and stored in multiple database servers. This concept of partitioning data is known as \"sharding\", and each of the database servers is known as a \"shard\". Orchestrator's data management layer is highly configurable, with experimenters having the ability to setup a centralized or a distributed database, and also configure, at the node level, where sensors collect data. In case of a distributed/shared database setup, Orchestrator sets up a global database server. This server gives a holistic view of the database. Orchestrator data management uses MongoDB at its base.","title":"Orchestrator Data Management"},{"location":"orchestrator/data-management/#data-manager-configuration","text":"The data management layer configuration is part of the Orchestrator\u2019s experiment level and node level configuration files. As mentioned earlier, Orchestrator\u2019s data management layer is highly configurable. More information about the same in available at DBDL: Configure the Orchestrator Data Management Layer . Orchestrator\u2019s data management layer enables an experimenter to do the following.","title":"Data Manager Configuration"},{"location":"orchestrator/data-management/#sense-and-collect","text":"The following are the steps an agent developer should follow to populate Orchestrator\u2019s database Import the database management utility from magi.util import database Initialize a database collection passing it a unique name. We suggest using the agent name. Each agent implementation that extends from one of the predefined agents, like the DispatchAgent, has a variable \u201cname\u201d that stores the agent name. self.collection = database.getCollection(self.name) Insert data. Each record can be inserted as a dictionary of key-value pairs. self.collection.insert({\u201ckey1\u201d : \u201cvalue1\u201d, \u201ckey2\u201d: \u201cvalue2\u201d}) Note The db management utility inserts three other entries per record: host: <node\u2019s hostname> created: <record creation time> agent: <agent name>","title":"Sense and Collect"},{"location":"orchestrator/data-management/#query-and-analyze","text":"In case of a distributed database setup, a user can connect to the mongo db server running on the global server node to get an experiment-wide view. However, in case of an unsharded setup, a user would have to connect to the appropriate collector based on the sensor-collector mapping to fetch data stored by a particular sensor. Orchestrator, by default, sets up an non-distributed database, with all the sensors collecting at the same collector. > mongo node-1.myExperiment.myProject:27018 mongo> use magi switched to db magi mongo> db.experiment_data.find() { \"agent\" : \"user_agent\", \"host\" : \"node-1\", \"created\" : 1409075736.646182, \"key1\" : \"value1\", \"key2\" : \"value2\" } { \"agent\" : \"user_agent\", \"host\" : \"node-2\", \"created\" : 1409075737.514683, \"key3\" : \"value3\", \"key4\" : \"value4\" } In case of a distributed setup, the configuration file would have information about a global server host. An experimenter can connect to the global server to get an experiment wide view of the database, or connect to individual collectors to get their local view. And, for more advanced queries, you can refer the Mongo documentation available at http://docs.mongodb.org/manual/tutorial/query-documents/ .","title":"Query and Analyze"},{"location":"orchestrator/feedback/","text":"Feedback Case Study \uf0c1 Introduction \uf0c1 This case study demonstrates our system\u2019s ability to do real-time feedback from the experiment using triggers. We show how information from the experiment may be used to extend the Orchestrator autonomy and deterministically control dynamic experiments. This is an example where the active state of an experiment is a driving input to the control. The data management layer too plays an important role in enabling the flow of information. In this case study we show how, in a semi-controllable environment, the amount of traffic on a given link may be controlled using feedback from the experiment itself . The traffic on one of the links in the experiment must be maintained within a certain range; for this example, the range was 100-105 MB. We assume that the uncontrollable traffic on the link would not exceed the required maximum. This experiment is set up with the following characteristics: the monitored link has some noise (uncontrollable traffic) flowing through it. The noise has been artificially generated. We set the noise generating clients to pull randomly changing amount of traffic from the servers, in order to enact the noise. The solution is not dependent on the experiment topology. We deploy a traffic monitor agent on one of the end nodes of the link to be monitored. The traffic monitoring agent continuously monitors the traffic on the link. We also deploy a traffic generator agent (control client) that coordinates with the traffic monitor agent in real time and generates exactly the amount of traffic that would help maintain the total traffic on the link within the required limits. To demonstrate this case study, we set up an experiment topology similar to the one seen in the following figure, with 50 noise generating clients and 10 servers. We also tested a scaled up version of the experiment with 300 noise generating agents and 100 servers. However, due to the resource constraints on the testbed, we recommend you try this case with the smaller topology first. The scaling up of the experiment may be achieved with very simple modifications. We use two kinds of agents: A server agent (located on the server nodes) that runs an apache server and serves random data of requested size. A client agent (located on the client nodes) that periodically requests data from a randomally chosen server agent. The size of data that a client agent requests is configurable. The initial configuration is set to ~1.2 MB for each of the 50 noise generating clients, and ~40 MB for the control client, adding up to a total of ~100 MB. Now, to synthetically generate uncontrollable traffic, the size of data requested by the noise generating clients is changed randomly. We set the probability of change as: 80% - no change, 10% - increment, and 10% - reduction. The amount by which the fetch size is changed is calculated randomly. Then, to add control to this experiment, specifically on the traffic on the monitored link, we add a traffic monitor and a traffic generator (control client). We deploy a traffic monitor agent on the client-side end node of the link to be monitored. This agent continuously monitors all the traffic flowing through the node. Further, we attach a node (control client) to the traffic generator, and deploy a client agent on it. The Orchestrator, based on the real time traffic feedback, sends change load messages to the control client agent, in order to maintain the total traffic on the monitored link within the set limits. Event Streams This example has six events streams; the server stream, the noise stream, the noise modify stream, the control client stream, the control stream and the duration stream. Mapping to the Topology \uf0c1 The groups directive in the AAL file allows mapping a agent behavior to one or more nodes. #!php groups: server_group: &slist [s-0, s-1, s-2, s-3, s-4, s-5, s-6, s-7, s-8, s-9] noise_group: [uc-0, uc-1, uc-2, uc-3, uc-4, uc-5, uc-6, uc-7, uc-8, uc-9, uc-10, uc-11, uc-12, uc-13, uc-14, uc-15, uc-16, uc-17, uc-18, uc-19, uc-20, uc-21, uc-22, uc-23, uc-24, uc-25, uc-26, uc-27, uc-28, uc-29, uc-30, uc-31, uc-32, uc-33, uc-34, uc-35, uc-36, uc-37, uc-38, uc-39, uc-40, uc-41, uc-42, uc-43, uc-44, uc-45, uc-46, uc-47, uc-48, uc-49] sensor_group: [rc] client_group: [c-0] In this example, we observe that there are four groups server_group, noise_group, sensor_group and client_group. The server_group consists of 10 servers. The noise_group consists of 50 noise generating clients. The sensor_group consists of the lone sensor node and the client_group consists of the controlling client. Additionally,we use yaml pointers to annotate the server_group as \u201cslist\u201d. The slist annotation is used to refer to the list of servers for configuring the client agents in the section below. Configuring the Agents There are four types of agents, server_agent, noise_agent, monitor_agent, and client_agent. Each of the server agent is used to start a web server serving garbage data of requested size. The noise agents are used to create the noise of the network. Each of the noise agent periodically fetches data of a randomally changing size from a ramdonly chosen server. server_agent: group: server_group path: /share/magi/modules/apache/apache.tar.gz execargs: [] noise_agent: group: noise_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '2', sizes: '120000'} monitor_agent: group: sensor_group path: /proj/montage/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} client_agent: group: client_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '2', sizes: '4000000'} Server Stream \uf0c1 The server event stream consists of three states. The start state which generates a trigger, called serverStarted, once all the server agents are activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the noise stream and the client event stream. Once the trigger is received, it enters the stop state, where the server is deactivated or terminated. Noise Stream and Control Client Stream The noise and client event streams consists of five states. First, the agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all the agent nodes. The streams then synchronize with the server stream by waiting for the serverStarted trigger from the server nodes. Once they receive the trigger the agents are activated in the start state. Each agent fetches web pages from one of the listed servers. Next, the streams wait for the monitorStopped trigger from the monitorstream. Once the trigger is received the clients are instructed to stop fetching data. On termination, the agents sends a noiseStopped/clientStopped trigger that allows the server stream to synchronize and terminate the servers, which is done only after all the http client agents have terminated. Noise Modify Stream The noise modify stream starts once the noise generating agents start. It continously instructs the noise generating agents to randomly modify the amount of noise being generated. This is done to create an uncontrolled noise generation behaviour. Control Stream The control stream starts once the control client agent has started. It configures the traffic monitoring agent and instucts it to start monitoring. Once the monitoring starts, this stream continously monitors the amount of traffic flowing on the monitored link, and based on the traffic information, instructs the control client to modify the amount of traffic it is pulling, in order to maintain the total traffic on the monitored link within the required limits. Duration Stream The duration stream manages the time duration for which the experiment needs to run. Its starts once the monitor agent has started. The stream then waits for \u2206t before instructing the monitor agent stop and terminating all of the agents. Running the Experiment \uf0c1 Step 1: Set up your environment \uf0c1 Set up your environment. Assuming your experiment is named myExp, your DETER project is myProj, and the AAL file is called procedure.aal. PROJ=myExp EXP=myProj AAL=procedure.aal Step 2: Set Up Containerized Experiment \uf0c1 As this experiment requires a lot of nodes, we should try and use containerized nodes. Create a containerized version of the experiment using this network description file: casestudy_feedback.tcl NS=fb_topology.tcl > /share/containers/containerize.py $PROJ $EXP $NS --packing=8 --openvz-diskspace 15G --pnode-type MicroCloud,pc2133 Step 3: Swap in your Experiment \uf0c1 Swap in the newly created experiment. Step 4: Run Orchestrator \uf0c1 Once the experiment is swapped in, run the Orchestrator, giving it this AAL: casestudy_feedback.aal , the experiment name and project name. > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseFeedback\u201d: The Orchestrator enacts an internally defined stream called initialization that is responsible for establishing all the groups and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , the initialization stream is complete. Now all of the six above mentioned streams start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The noiseStream and the controlClientStream on receiving the trigger serverStarted from the server_group, send the startClient event to the noise_group and the control_group, respectively. All memebers of both the groups start http clients and fire noiseStarted and controlClientStarted triggers. The noiseModifyStream on receiving the noiseStarted trigger joins a loop that sends a changeTraffic event to the noise_group, every two seconds. The control stream on receiving the controlClientStarted trigger sends a startCollection event to the monitor_group. The lone member of the monitor_group starts monitoring the interfaces on the node, and fires a monitorStarted trigger. The control stream then, joins a loop that sends a sense event to the monitor_group, every two seconds, and based on the return value in the response trigger intfSensed, sends a increaseTraffic or a reduceTraffic event to the control_group, if required. The duration stream after receiving the monitorStarted trigger, waits for 5 minutes. On completion, it sends a stopCollection event to the monitor_group. The monitor agent stop monitoring and sends back a monitorStopped trigger. Once the noiseStream and the controlClientStream recieve the monitorStopped trigger, they send out the stopClient event to their respective members. The http clients are stopped on all the members, and the noiseStopped and the controlClientStopped triggers are sent back to the orchestrator. The serverStream, on receiving the noiseStopped and the controlClientStopped triggers, sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the durationStream. On receiving the serverStopped trigger, the durationStream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment. Procedure: casestudy_feedback.aal Topology: casestudy_feedback.tcl Archive Logs: casestudy_feedback.tar.gz Orchestration: casestudy_feedback.orch.log Visualizing Experiment Results \uf0c1 Offline: A traffic plot may be generated using the MAGI Graph Creation Tool . You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject","title":"Feedback Case Study"},{"location":"orchestrator/feedback/#feedback-case-study","text":"","title":"Feedback Case Study"},{"location":"orchestrator/feedback/#introduction","text":"This case study demonstrates our system\u2019s ability to do real-time feedback from the experiment using triggers. We show how information from the experiment may be used to extend the Orchestrator autonomy and deterministically control dynamic experiments. This is an example where the active state of an experiment is a driving input to the control. The data management layer too plays an important role in enabling the flow of information. In this case study we show how, in a semi-controllable environment, the amount of traffic on a given link may be controlled using feedback from the experiment itself . The traffic on one of the links in the experiment must be maintained within a certain range; for this example, the range was 100-105 MB. We assume that the uncontrollable traffic on the link would not exceed the required maximum. This experiment is set up with the following characteristics: the monitored link has some noise (uncontrollable traffic) flowing through it. The noise has been artificially generated. We set the noise generating clients to pull randomly changing amount of traffic from the servers, in order to enact the noise. The solution is not dependent on the experiment topology. We deploy a traffic monitor agent on one of the end nodes of the link to be monitored. The traffic monitoring agent continuously monitors the traffic on the link. We also deploy a traffic generator agent (control client) that coordinates with the traffic monitor agent in real time and generates exactly the amount of traffic that would help maintain the total traffic on the link within the required limits. To demonstrate this case study, we set up an experiment topology similar to the one seen in the following figure, with 50 noise generating clients and 10 servers. We also tested a scaled up version of the experiment with 300 noise generating agents and 100 servers. However, due to the resource constraints on the testbed, we recommend you try this case with the smaller topology first. The scaling up of the experiment may be achieved with very simple modifications. We use two kinds of agents: A server agent (located on the server nodes) that runs an apache server and serves random data of requested size. A client agent (located on the client nodes) that periodically requests data from a randomally chosen server agent. The size of data that a client agent requests is configurable. The initial configuration is set to ~1.2 MB for each of the 50 noise generating clients, and ~40 MB for the control client, adding up to a total of ~100 MB. Now, to synthetically generate uncontrollable traffic, the size of data requested by the noise generating clients is changed randomly. We set the probability of change as: 80% - no change, 10% - increment, and 10% - reduction. The amount by which the fetch size is changed is calculated randomly. Then, to add control to this experiment, specifically on the traffic on the monitored link, we add a traffic monitor and a traffic generator (control client). We deploy a traffic monitor agent on the client-side end node of the link to be monitored. This agent continuously monitors all the traffic flowing through the node. Further, we attach a node (control client) to the traffic generator, and deploy a client agent on it. The Orchestrator, based on the real time traffic feedback, sends change load messages to the control client agent, in order to maintain the total traffic on the monitored link within the set limits. Event Streams This example has six events streams; the server stream, the noise stream, the noise modify stream, the control client stream, the control stream and the duration stream.","title":"Introduction"},{"location":"orchestrator/feedback/#mapping-to-the-topology","text":"The groups directive in the AAL file allows mapping a agent behavior to one or more nodes. #!php groups: server_group: &slist [s-0, s-1, s-2, s-3, s-4, s-5, s-6, s-7, s-8, s-9] noise_group: [uc-0, uc-1, uc-2, uc-3, uc-4, uc-5, uc-6, uc-7, uc-8, uc-9, uc-10, uc-11, uc-12, uc-13, uc-14, uc-15, uc-16, uc-17, uc-18, uc-19, uc-20, uc-21, uc-22, uc-23, uc-24, uc-25, uc-26, uc-27, uc-28, uc-29, uc-30, uc-31, uc-32, uc-33, uc-34, uc-35, uc-36, uc-37, uc-38, uc-39, uc-40, uc-41, uc-42, uc-43, uc-44, uc-45, uc-46, uc-47, uc-48, uc-49] sensor_group: [rc] client_group: [c-0] In this example, we observe that there are four groups server_group, noise_group, sensor_group and client_group. The server_group consists of 10 servers. The noise_group consists of 50 noise generating clients. The sensor_group consists of the lone sensor node and the client_group consists of the controlling client. Additionally,we use yaml pointers to annotate the server_group as \u201cslist\u201d. The slist annotation is used to refer to the list of servers for configuring the client agents in the section below. Configuring the Agents There are four types of agents, server_agent, noise_agent, monitor_agent, and client_agent. Each of the server agent is used to start a web server serving garbage data of requested size. The noise agents are used to create the noise of the network. Each of the noise agent periodically fetches data of a randomally changing size from a ramdonly chosen server. server_agent: group: server_group path: /share/magi/modules/apache/apache.tar.gz execargs: [] noise_agent: group: noise_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '2', sizes: '120000'} monitor_agent: group: sensor_group path: /proj/montage/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} client_agent: group: client_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '2', sizes: '4000000'}","title":"Mapping to the Topology"},{"location":"orchestrator/feedback/#server-stream","text":"The server event stream consists of three states. The start state which generates a trigger, called serverStarted, once all the server agents are activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the noise stream and the client event stream. Once the trigger is received, it enters the stop state, where the server is deactivated or terminated. Noise Stream and Control Client Stream The noise and client event streams consists of five states. First, the agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all the agent nodes. The streams then synchronize with the server stream by waiting for the serverStarted trigger from the server nodes. Once they receive the trigger the agents are activated in the start state. Each agent fetches web pages from one of the listed servers. Next, the streams wait for the monitorStopped trigger from the monitorstream. Once the trigger is received the clients are instructed to stop fetching data. On termination, the agents sends a noiseStopped/clientStopped trigger that allows the server stream to synchronize and terminate the servers, which is done only after all the http client agents have terminated. Noise Modify Stream The noise modify stream starts once the noise generating agents start. It continously instructs the noise generating agents to randomly modify the amount of noise being generated. This is done to create an uncontrolled noise generation behaviour. Control Stream The control stream starts once the control client agent has started. It configures the traffic monitoring agent and instucts it to start monitoring. Once the monitoring starts, this stream continously monitors the amount of traffic flowing on the monitored link, and based on the traffic information, instructs the control client to modify the amount of traffic it is pulling, in order to maintain the total traffic on the monitored link within the required limits. Duration Stream The duration stream manages the time duration for which the experiment needs to run. Its starts once the monitor agent has started. The stream then waits for \u2206t before instructing the monitor agent stop and terminating all of the agents.","title":"Server Stream"},{"location":"orchestrator/feedback/#running-the-experiment","text":"","title":"Running the Experiment"},{"location":"orchestrator/feedback/#step-1-set-up-your-environment","text":"Set up your environment. Assuming your experiment is named myExp, your DETER project is myProj, and the AAL file is called procedure.aal. PROJ=myExp EXP=myProj AAL=procedure.aal","title":"Step 1: Set up your environment"},{"location":"orchestrator/feedback/#step-2-set-up-containerized-experiment","text":"As this experiment requires a lot of nodes, we should try and use containerized nodes. Create a containerized version of the experiment using this network description file: casestudy_feedback.tcl NS=fb_topology.tcl > /share/containers/containerize.py $PROJ $EXP $NS --packing=8 --openvz-diskspace 15G --pnode-type MicroCloud,pc2133","title":"Step 2: Set Up Containerized Experiment"},{"location":"orchestrator/feedback/#step-3-swap-in-your-experiment","text":"Swap in the newly created experiment.","title":"Step 3: Swap in your Experiment"},{"location":"orchestrator/feedback/#step-4-run-orchestrator","text":"Once the experiment is swapped in, run the Orchestrator, giving it this AAL: casestudy_feedback.aal , the experiment name and project name. > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseFeedback\u201d: The Orchestrator enacts an internally defined stream called initialization that is responsible for establishing all the groups and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , the initialization stream is complete. Now all of the six above mentioned streams start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The noiseStream and the controlClientStream on receiving the trigger serverStarted from the server_group, send the startClient event to the noise_group and the control_group, respectively. All memebers of both the groups start http clients and fire noiseStarted and controlClientStarted triggers. The noiseModifyStream on receiving the noiseStarted trigger joins a loop that sends a changeTraffic event to the noise_group, every two seconds. The control stream on receiving the controlClientStarted trigger sends a startCollection event to the monitor_group. The lone member of the monitor_group starts monitoring the interfaces on the node, and fires a monitorStarted trigger. The control stream then, joins a loop that sends a sense event to the monitor_group, every two seconds, and based on the return value in the response trigger intfSensed, sends a increaseTraffic or a reduceTraffic event to the control_group, if required. The duration stream after receiving the monitorStarted trigger, waits for 5 minutes. On completion, it sends a stopCollection event to the monitor_group. The monitor agent stop monitoring and sends back a monitorStopped trigger. Once the noiseStream and the controlClientStream recieve the monitorStopped trigger, they send out the stopClient event to their respective members. The http clients are stopped on all the members, and the noiseStopped and the controlClientStopped triggers are sent back to the orchestrator. The serverStream, on receiving the noiseStopped and the controlClientStopped triggers, sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the durationStream. On receiving the serverStopped trigger, the durationStream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment. Procedure: casestudy_feedback.aal Topology: casestudy_feedback.tcl Archive Logs: casestudy_feedback.tar.gz Orchestration: casestudy_feedback.orch.log","title":"Step 4: Run Orchestrator"},{"location":"orchestrator/feedback/#visualizing-experiment-results","text":"Offline: A traffic plot may be generated using the MAGI Graph Creation Tool . You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject","title":"Visualizing Experiment Results"},{"location":"orchestrator/flooder/","text":"Flooder Case Study \uf0c1 In this experiment we demonstrate how one can setup a flooding agent and a victim server. We demonstrate three aspects of MAGI: Specifying multiple event streams, Synchronizing with triggers, and A special target called exit to unload agents. Event Streams \uf0c1 This experiment has three streams: the flooder stream , the server stream , and the cleanup stream . The co-ordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers or time-based triggers. The triggers are indicated as wait states in gray. The group formation and loading the agents, which is also automated by the orchestrator tool, is not illustrated above. Server Stream \uf0c1 The server event stream consists of three states. The start state which generates a trigger, called serverStarted, once the server agent is activated on the experiment nodes. It then enters the wait state and stays there for a period \u2206t and terminates the server agent in stop state. The AAL description is below: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ { timeout: 180000 } ] - type: event agent: server_agent method: stopServer trigger: serverStopped args: {} Flooder Stream \uf0c1 The flooder stream consists of five states. First, the flooder agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The flooder stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger the flooder agent is activated in the start state. Next, the flooder stream waits for a period of \u2206t before repeating the start and stop events one more time while waiting for a period of \u2206t in between. Finally, the flooder stream terminates the flooder agent in the stop state. The AAL description is as follows: flooderstream: - type: event agent: flooder_agent method: setConfiguration args: dst: '10.1.1.3' proto: 'udp' length: 'minmax(64, 1024)' ratetype: 'flat' highrate: '5000' lowrate: '5000' sport: 'minmax(1024, 4096)' dport: 'minmax(1024, 4096)' - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: flooder_agent method: startFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: stopFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: startFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: stopFlood args: {} Cleanup Stream \uf0c1 The last event stream, the cleanup stream consists of two states. First, it waits for the server to stop and then it enters the exit state. The exit state unload and tears down all the comminucation mechanisms between the agents. The exit state is entered by the key target is used to transfer control to a reserved state internal to the orchestrator. It causes the orchestrator to send agent unload and disband group messages to all the experiment node and then it exits the orchestrator. cleanupstream: - type: trigger triggers: [ {event: serverStopped, target: exit} ] Running the experiment \uf0c1 Swap in the experiment using the network description file given below. Set up your environment. Assuming your experiment is named myExp, your DETER project is myProj, and the AAL file is called procedure.aal. $ export PROJ=myExp $ export EXP=myProj $ export AAL=procedure.aal Once the experiment is swapped in, run the orchestrator, giving it the AAL above. The orchestrator needs an AAL file, and the experiment and project name. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseClientServer\u201d. $ /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The output will be as follows: stream groupBuildS... : sent : (16:17:14) joinGroup flooder_group --> __ALL__ stream groupBuildS... : sent : (16:17:14) joinGroup server_group --> __ALL__ stream groupBuildS... : trig : (16:17:15) trigger completed: GroupBuildDone: {'group': 'flooder_group'} stream groupBuildS... : trig : (16:17:15) trigger completed: GroupBuildDone: {'group': 'server_group'} stream groupBuildS... : sent : (16:17:15) groupPing(['flooder_group']) --> flooder_group stream groupBuildS... : sent : (16:17:15) groupPing(['server_group']) --> server_group stream groupBuildS... : trig : (16:17:15) trigger completed: GroupPong: {'group': 'flooder_group'} stream groupBuildS... : trig : (16:17:15) trigger completed: GroupPong: {'group': 'server_group'} stream groupBuildS... : DONE : (16:17:15) complete. stream loadAgentSt... : sent : (16:17:15) loadAgent flooder_agent --> flooder_group stream loadAgentSt... : sent : (16:17:15) loadAgent server_agent --> server_group stream loadAgentSt... : trig : (16:17:27) trigger completed: AgentLoadDone: {'agent': 'flooder_agent'} stream loadAgentSt... : trig : (16:18:03) trigger completed: AgentLoadDone: {'agent': 'server_agent'} stream loadAgentSt... : DONE : (16:18:03) complete. 04-24 16:18:03.371 magi.orchestrator.orchestrator INFO Running Event Streams stream flooderstream : sent : (16:18:03) setConfiguration(['udp', '10.1.1.3', ... ) --> flooder_group stream serverstream : sent : (16:18:03) startServer(None) --> server_group (fires trigger: serverStarted) stream flooderstream : trig : (16:18:05) trigger completed: serverStarted: {'retVal': True} stream flooderstream : sent : (16:18:05) startFlood(None) --> flooder_group 04-24 16:18:05.624 magi.orchestrator.orchestrator CRITICAL Got a runtime exception from an agent. Jumping to exit target. stream unknown : exit : (16:18:05) Run-time exception in agent <magi.modules.flooder_agent_code.flooder.flooder_agent object at 0x7f47e2243490> on node(s) floodernode in method _execute_child, line 1343, in file subprocess.py. Error: [Errno 2] No such file or directory 04-24 16:18:05.625 magi.orchestrator.orchestrator INFO Running Exit Streams stream unloadAgent... : sent : (16:18:05) unloadAgent flooder_agent --> flooder_group stream unloadAgent... : sent : (16:18:05) unloadAgent server_agent --> server_group stream unloadAgent... : trig : (16:18:06) trigger completed: AgentUnloadDone: {'agent': 'flooder_agent'} stream unloadAgent... : trig : (16:18:06) trigger completed: AgentUnloadDone: {'agent': 'server_agent'} stream unloadAgent... : DONE : (16:18:06) complete. stream groupLeaveS... : sent : (16:18:06) leaveGroup flooder_group --> __ALL__ stream groupLeaveS... : sent : (16:18:06) leaveGroup server_group --> __ALL__ stream groupLeaveS... : trig : (16:18:06) trigger completed: GroupTeardownDone: {'group': 'flooder_group'} stream groupLeaveS... : trig : (16:18:06) trigger completed: GroupTeardownDone: {'group': 'server_group'} stream groupLeaveS... : DONE : (16:18:06) complete. The orchestration tool runs an internally defined stream called initilization that is responsible for establishing the server_group and the flooder_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone, The initialization stream is complete. Now the serverstream, flooderstream and the cleanupstream start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The flooderstream on receiving the trigger serverStarted from the server_group, sends the startFlood event to the flooder_group. One minute later, the clientstream sends the trigger stopFlood to the flooder_group. This sequence of events repeats one more time before sending the final stopFlood trigger to the flooder_group, terminating the flooder_agent. Once the serverstream finishes the wait period, it sends out stopServer on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream. The procedure and topology file that were used for the casestudy are attached below. Procedure : casestudy_flooder.aal Topology : casestudy_flooder.tcl Archived logs : casestudy_flooder.tar.gz Visualizing Experiment Results \uf0c1 In order to visulaize the traffic on the network, we modify the above mentioned procedure to add another stream called \u201cmonitor\u201d. This stream deploys a packet sensor agent on the server node to measure the traffic on the link in the experiment. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [servernode] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: serverStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the below mentioned tools to create a traffic plot. In order to populate the traffic data, re-orchestrate the experiment using the updated procedure. The updated procedure file and the corresponding logs are attached below. A plot of the traffic on the link containing the flooder and the victim server can be generated by the MAGI Graph Creation Tool . $ export GRAPHCONF=graph.conf $ /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o traffic_plot.png Procedure : flooder_monitor.aal Archived logs : casestudy_flooder_monitor.tar.gz Graph Config : graph.conf","title":"Flooder Case Study"},{"location":"orchestrator/flooder/#flooder-case-study","text":"In this experiment we demonstrate how one can setup a flooding agent and a victim server. We demonstrate three aspects of MAGI: Specifying multiple event streams, Synchronizing with triggers, and A special target called exit to unload agents.","title":"Flooder Case Study"},{"location":"orchestrator/flooder/#event-streams","text":"This experiment has three streams: the flooder stream , the server stream , and the cleanup stream . The co-ordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers or time-based triggers. The triggers are indicated as wait states in gray. The group formation and loading the agents, which is also automated by the orchestrator tool, is not illustrated above.","title":"Event Streams"},{"location":"orchestrator/flooder/#server-stream","text":"The server event stream consists of three states. The start state which generates a trigger, called serverStarted, once the server agent is activated on the experiment nodes. It then enters the wait state and stays there for a period \u2206t and terminates the server agent in stop state. The AAL description is below: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ { timeout: 180000 } ] - type: event agent: server_agent method: stopServer trigger: serverStopped args: {}","title":"Server Stream"},{"location":"orchestrator/flooder/#flooder-stream","text":"The flooder stream consists of five states. First, the flooder agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The flooder stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger the flooder agent is activated in the start state. Next, the flooder stream waits for a period of \u2206t before repeating the start and stop events one more time while waiting for a period of \u2206t in between. Finally, the flooder stream terminates the flooder agent in the stop state. The AAL description is as follows: flooderstream: - type: event agent: flooder_agent method: setConfiguration args: dst: '10.1.1.3' proto: 'udp' length: 'minmax(64, 1024)' ratetype: 'flat' highrate: '5000' lowrate: '5000' sport: 'minmax(1024, 4096)' dport: 'minmax(1024, 4096)' - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: flooder_agent method: startFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: stopFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: startFlood args: {} - type: trigger triggers: [ { timeout: 60000 } ] - type: event agent: flooder_agent method: stopFlood args: {}","title":"Flooder Stream"},{"location":"orchestrator/flooder/#cleanup-stream","text":"The last event stream, the cleanup stream consists of two states. First, it waits for the server to stop and then it enters the exit state. The exit state unload and tears down all the comminucation mechanisms between the agents. The exit state is entered by the key target is used to transfer control to a reserved state internal to the orchestrator. It causes the orchestrator to send agent unload and disband group messages to all the experiment node and then it exits the orchestrator. cleanupstream: - type: trigger triggers: [ {event: serverStopped, target: exit} ]","title":"Cleanup Stream"},{"location":"orchestrator/flooder/#running-the-experiment","text":"Swap in the experiment using the network description file given below. Set up your environment. Assuming your experiment is named myExp, your DETER project is myProj, and the AAL file is called procedure.aal. $ export PROJ=myExp $ export EXP=myProj $ export AAL=procedure.aal Once the experiment is swapped in, run the orchestrator, giving it the AAL above. The orchestrator needs an AAL file, and the experiment and project name. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseClientServer\u201d. $ /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The output will be as follows: stream groupBuildS... : sent : (16:17:14) joinGroup flooder_group --> __ALL__ stream groupBuildS... : sent : (16:17:14) joinGroup server_group --> __ALL__ stream groupBuildS... : trig : (16:17:15) trigger completed: GroupBuildDone: {'group': 'flooder_group'} stream groupBuildS... : trig : (16:17:15) trigger completed: GroupBuildDone: {'group': 'server_group'} stream groupBuildS... : sent : (16:17:15) groupPing(['flooder_group']) --> flooder_group stream groupBuildS... : sent : (16:17:15) groupPing(['server_group']) --> server_group stream groupBuildS... : trig : (16:17:15) trigger completed: GroupPong: {'group': 'flooder_group'} stream groupBuildS... : trig : (16:17:15) trigger completed: GroupPong: {'group': 'server_group'} stream groupBuildS... : DONE : (16:17:15) complete. stream loadAgentSt... : sent : (16:17:15) loadAgent flooder_agent --> flooder_group stream loadAgentSt... : sent : (16:17:15) loadAgent server_agent --> server_group stream loadAgentSt... : trig : (16:17:27) trigger completed: AgentLoadDone: {'agent': 'flooder_agent'} stream loadAgentSt... : trig : (16:18:03) trigger completed: AgentLoadDone: {'agent': 'server_agent'} stream loadAgentSt... : DONE : (16:18:03) complete. 04-24 16:18:03.371 magi.orchestrator.orchestrator INFO Running Event Streams stream flooderstream : sent : (16:18:03) setConfiguration(['udp', '10.1.1.3', ... ) --> flooder_group stream serverstream : sent : (16:18:03) startServer(None) --> server_group (fires trigger: serverStarted) stream flooderstream : trig : (16:18:05) trigger completed: serverStarted: {'retVal': True} stream flooderstream : sent : (16:18:05) startFlood(None) --> flooder_group 04-24 16:18:05.624 magi.orchestrator.orchestrator CRITICAL Got a runtime exception from an agent. Jumping to exit target. stream unknown : exit : (16:18:05) Run-time exception in agent <magi.modules.flooder_agent_code.flooder.flooder_agent object at 0x7f47e2243490> on node(s) floodernode in method _execute_child, line 1343, in file subprocess.py. Error: [Errno 2] No such file or directory 04-24 16:18:05.625 magi.orchestrator.orchestrator INFO Running Exit Streams stream unloadAgent... : sent : (16:18:05) unloadAgent flooder_agent --> flooder_group stream unloadAgent... : sent : (16:18:05) unloadAgent server_agent --> server_group stream unloadAgent... : trig : (16:18:06) trigger completed: AgentUnloadDone: {'agent': 'flooder_agent'} stream unloadAgent... : trig : (16:18:06) trigger completed: AgentUnloadDone: {'agent': 'server_agent'} stream unloadAgent... : DONE : (16:18:06) complete. stream groupLeaveS... : sent : (16:18:06) leaveGroup flooder_group --> __ALL__ stream groupLeaveS... : sent : (16:18:06) leaveGroup server_group --> __ALL__ stream groupLeaveS... : trig : (16:18:06) trigger completed: GroupTeardownDone: {'group': 'flooder_group'} stream groupLeaveS... : trig : (16:18:06) trigger completed: GroupTeardownDone: {'group': 'server_group'} stream groupLeaveS... : DONE : (16:18:06) complete. The orchestration tool runs an internally defined stream called initilization that is responsible for establishing the server_group and the flooder_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone, The initialization stream is complete. Now the serverstream, flooderstream and the cleanupstream start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The flooderstream on receiving the trigger serverStarted from the server_group, sends the startFlood event to the flooder_group. One minute later, the clientstream sends the trigger stopFlood to the flooder_group. This sequence of events repeats one more time before sending the final stopFlood trigger to the flooder_group, terminating the flooder_agent. Once the serverstream finishes the wait period, it sends out stopServer on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream. The procedure and topology file that were used for the casestudy are attached below. Procedure : casestudy_flooder.aal Topology : casestudy_flooder.tcl Archived logs : casestudy_flooder.tar.gz","title":"Running the experiment"},{"location":"orchestrator/flooder/#visualizing-experiment-results","text":"In order to visulaize the traffic on the network, we modify the above mentioned procedure to add another stream called \u201cmonitor\u201d. This stream deploys a packet sensor agent on the server node to measure the traffic on the link in the experiment. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [servernode] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: serverStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the below mentioned tools to create a traffic plot. In order to populate the traffic data, re-orchestrate the experiment using the updated procedure. The updated procedure file and the corresponding logs are attached below. A plot of the traffic on the link containing the flooder and the victim server can be generated by the MAGI Graph Creation Tool . $ export GRAPHCONF=graph.conf $ /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o traffic_plot.png Procedure : flooder_monitor.aal Archived logs : casestudy_flooder_monitor.tar.gz Graph Config : graph.conf","title":"Visualizing Experiment Results"},{"location":"orchestrator/magi-desktop/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. MAGI Desktop \uf0c1 Step 1: Install the following dependencies. \uf0c1 The source code for all of these is available on Deter Ops ( users.deterlab.net ) under /share/magi/tarfiles/ : python-yaml python-pydot python-networkx py-matplotlib Mongo DB python-pymongo Yaml C Library (required for c-based agents, otherwise too helps improve performance) Mongo C Library (required only for c-based agents) All python packages can be installed using pip. Install pip, if not already available. curl -O https://bootstrap.pypa.io/get-pip.py python get-pip.py pip install pyyaml pip install pydot pip install networkx pip install matplotlib pip install pymongo Step 2: Download and install MAGI \uf0c1 $ git clone https://github.com/deter-project/magi.git $ cd magi $ sudo python setup.py install Step 3: Run a simple two node MAGI setup. \uf0c1 $ tools/magi_desktop_bootstrap.py -n node1,node2 # This should start two MAGI daemon processes with names node1 and node2 # The script creates sample config files and uses them to run MAGI daemons # by default the config and logs files should be available under /tmp/<node_name> Step 4: Check if both MAGI daemons are running. \uf0c1 $ magi_status.py -b node1 -n node1,node2 # This should at the end say \"Received reply back from all the required nodes\"","title":"Magi desktop"},{"location":"orchestrator/magi-desktop/#magi-desktop","text":"","title":"MAGI Desktop"},{"location":"orchestrator/magi-desktop/#step-1-install-the-following-dependencies","text":"The source code for all of these is available on Deter Ops ( users.deterlab.net ) under /share/magi/tarfiles/ : python-yaml python-pydot python-networkx py-matplotlib Mongo DB python-pymongo Yaml C Library (required for c-based agents, otherwise too helps improve performance) Mongo C Library (required only for c-based agents) All python packages can be installed using pip. Install pip, if not already available. curl -O https://bootstrap.pypa.io/get-pip.py python get-pip.py pip install pyyaml pip install pydot pip install networkx pip install matplotlib pip install pymongo","title":"Step 1: Install the following dependencies."},{"location":"orchestrator/magi-desktop/#step-2-download-and-install-magi","text":"$ git clone https://github.com/deter-project/magi.git $ cd magi $ sudo python setup.py install","title":"Step 2: Download and install MAGI"},{"location":"orchestrator/magi-desktop/#step-3-run-a-simple-two-node-magi-setup","text":"$ tools/magi_desktop_bootstrap.py -n node1,node2 # This should start two MAGI daemon processes with names node1 and node2 # The script creates sample config files and uses them to run MAGI daemons # by default the config and logs files should be available under /tmp/<node_name>","title":"Step 3: Run a simple two node MAGI setup."},{"location":"orchestrator/magi-desktop/#step-4-check-if-both-magi-daemons-are-running","text":"$ magi_status.py -b node1 -n node1,node2 # This should at the end say \"Received reply back from all the required nodes\"","title":"Step 4: Check if both MAGI daemons are running."},{"location":"orchestrator/magi-dev/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. MAGI Development Codebase \uf0c1 The MAGI codebase is maintained in three branches. Current/Released : Stable released version Development : Stable with added features after the last release Test : Unstable To be able to work with the development branch, you will need to change the bootstrap command. Instead of: sudo python /share/magi/current/magi_bootstrap.py Use: sudo python /share/magi/dev/magi_bootstrap.py This will install MAGI from the development code base. Also, if you run MAGI tools from the Deter Ops ( users.deterlab.net ) machine, then make the tools point to the development code base: export PYTHONPATH=/share/magi/dev_src /share/magi/dev/magi_orchestrator.py -c bridgeNode -f eventsFile However, in case you use one of the experiment nodes to run MAGI tools, you can use them the same way as while running the current version of MAGI.","title":"Magi dev"},{"location":"orchestrator/magi-dev/#magi-development-codebase","text":"The MAGI codebase is maintained in three branches. Current/Released : Stable released version Development : Stable with added features after the last release Test : Unstable To be able to work with the development branch, you will need to change the bootstrap command. Instead of: sudo python /share/magi/current/magi_bootstrap.py Use: sudo python /share/magi/dev/magi_bootstrap.py This will install MAGI from the development code base. Also, if you run MAGI tools from the Deter Ops ( users.deterlab.net ) machine, then make the tools point to the development code base: export PYTHONPATH=/share/magi/dev_src /share/magi/dev/magi_orchestrator.py -c bridgeNode -f eventsFile However, in case you use one of the experiment nodes to run MAGI tools, you can use them the same way as while running the current version of MAGI.","title":"MAGI Development Codebase"},{"location":"orchestrator/magi-tools/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. MAGI Tools \uf0c1 The following tools are available for MAGI experiments in DETERLab: magi_status.py magi_graph.py magi_status.py : Check status of a MAGI experiment, reboot MAGI daemon, download logs \uf0c1 Use magi_status.py to: check MAGI\u2019s status on experiment nodes reboot MAGI daemon process download logs from experiment nodes This tool is run for one experiment at a time. The user needs to provide the project name and the experiment name to the tool. This tool, by default, works for all of the nodes corresponding to the given experiment. However, it can be made to work with a restricted set of nodes, either by directly providing the set of interested nodes, or by providing an AAL (experiment procedure) file to fetch the set of desired nodes. This tool by default only informs if the MAGI daemon process on a node is reachable or not. Specific options can be used to fetch group membership details and information about active agents. If you want to reboot the MAGI daemon, magi_status.py first reboots MAGI daemon processes on the experiment nodes, and then fetches their status. If the tool is asked to download logs, it just does that, and does not fetch the status. Usage: magi_status.py [options] Script to get the status of MAGI daemon processes on experiment nodes, to reboot them if required, and to download logs. Options: -h, --help show this help message and exit -p PROJECT, --project=PROJECT Project name -e EXPERIMENT, --experiment=EXPERIMENT Experiment name -n NODES, --nodes=NODES Comma-separated list of the nodes to reboot MAGI daemon -a AAL, --aal=AAL The yaml-based procedure file to extract the list of nodes -l, --logs Fetch logs. The -o/--logoutdir option is applicable only when fetching logs. -o LOGOUTDIR, --logoutdir=LOGOUTDIR Store logs under the given directory. Default: /tmp -g, --groupmembership Fetch group membership detail -i, --agentinfo Fetch loaded agent information -t TIMEOUT, --timeout=TIMEOUT Number of seconds to wait to receive the status reply from the nodes on the overlay -r, --reboot Reboot nodes. The following options are applicable only when rebooting. -d DISTPATH, --distpath=DISTPATH Location of the distribution -U, --noupdate Do not update the system before installing MAGI -N, --noinstall Do not install MAGI and the supporting libraries magi_graph.py : Create graphs for a MAGI experiment \uf0c1 magi_graph.py is a graph generator for experiments executed on DETERLab using MAGI. The tool fetches the required data using MAGI\u2019s data management layer and generates a graph in PNG format. This tool may be executed from either the DETER Ops machine or a remote computer with access to internet. The data to be plotted and other graph features are configurable. The various commandline options are as follows: Usage: magi_graph.py [options] Plots the graph for an experiment based on parameters provided. Experiment Configuration File OR Project and Experiment Name needs to be provided to be able to connect to the experiment. Need to provide build a graph specific configuration for plotting. Options: -h, --help show this help message and exit -e EXPERIMENT, --experiment=EXPERIMENT Experiment name -p PROJECT, --project=PROJECT Project name -x EXPERIMENTCONFIG, --experimentConfig=EXPERIMENTCONFIG Experiment configuration file -c CONFIG, --config=CONFIG Graph configuration file -a AGENT, --agent=AGENT Agent name. This is used to fetch available database fields -l AAL, --aal=AAL AAL (experiment procedure) file. This is also used to fetch available database fields -o OUTPUT, --output=OUTPUT Output graph file. Default: graph.png -t, --tunnel Tell the tool to tunnel request through Deter Ops (users.deterlab.net). -u USERNAME, --username=USERNAME Username for creating tunnel. Required only if different from current shell username. This tool expects the user to provide a configuration file. The format of the configuration file needs to be similar to the sample configuration file provided below. graph: type: line xLabel: Time(sec) yLabel: Bytes title: Traffic plot db: agent: monitor_agent filter: host: servernode peerNode: clientnode trafficDirection: out xValue: created yValue: bytes The configuration is divided into two parts a) Graph options and b) Database options. Graph options are used to configure the type of graph and the various labels. The database options help the tool fetch the data to be plotted. Each record stored in the database using MAGI\u2019s database layer has the following three fields along with any other that an agent populates. agent: Agent Name host: Node of which the agent is hosted created: Timestamp of when the record is created In the above mentioned example, data populated by the agent named \u201cmonitor_agent\u201d hosted on the node named \u201cservernode\u201d will be fetched. The data would further be filtered on the configured values of peerNode and trafficDirection, which are agent specific fields. Among the fetched data, values corresponding to the fields, created and bytes, will be plotted correspoding to the x and the y axis, respectively.","title":"Magi tools"},{"location":"orchestrator/magi-tools/#magi-tools","text":"The following tools are available for MAGI experiments in DETERLab: magi_status.py magi_graph.py","title":"MAGI Tools"},{"location":"orchestrator/magi-tools/#magi_statuspy-check-status-of-a-magi-experiment-reboot-magi-daemon-download-logs","text":"Use magi_status.py to: check MAGI\u2019s status on experiment nodes reboot MAGI daemon process download logs from experiment nodes This tool is run for one experiment at a time. The user needs to provide the project name and the experiment name to the tool. This tool, by default, works for all of the nodes corresponding to the given experiment. However, it can be made to work with a restricted set of nodes, either by directly providing the set of interested nodes, or by providing an AAL (experiment procedure) file to fetch the set of desired nodes. This tool by default only informs if the MAGI daemon process on a node is reachable or not. Specific options can be used to fetch group membership details and information about active agents. If you want to reboot the MAGI daemon, magi_status.py first reboots MAGI daemon processes on the experiment nodes, and then fetches their status. If the tool is asked to download logs, it just does that, and does not fetch the status. Usage: magi_status.py [options] Script to get the status of MAGI daemon processes on experiment nodes, to reboot them if required, and to download logs. Options: -h, --help show this help message and exit -p PROJECT, --project=PROJECT Project name -e EXPERIMENT, --experiment=EXPERIMENT Experiment name -n NODES, --nodes=NODES Comma-separated list of the nodes to reboot MAGI daemon -a AAL, --aal=AAL The yaml-based procedure file to extract the list of nodes -l, --logs Fetch logs. The -o/--logoutdir option is applicable only when fetching logs. -o LOGOUTDIR, --logoutdir=LOGOUTDIR Store logs under the given directory. Default: /tmp -g, --groupmembership Fetch group membership detail -i, --agentinfo Fetch loaded agent information -t TIMEOUT, --timeout=TIMEOUT Number of seconds to wait to receive the status reply from the nodes on the overlay -r, --reboot Reboot nodes. The following options are applicable only when rebooting. -d DISTPATH, --distpath=DISTPATH Location of the distribution -U, --noupdate Do not update the system before installing MAGI -N, --noinstall Do not install MAGI and the supporting libraries","title":"magi_status.py: Check status of a MAGI experiment, reboot MAGI daemon, download logs"},{"location":"orchestrator/magi-tools/#magi_graphpy-create-graphs-for-a-magi-experiment","text":"magi_graph.py is a graph generator for experiments executed on DETERLab using MAGI. The tool fetches the required data using MAGI\u2019s data management layer and generates a graph in PNG format. This tool may be executed from either the DETER Ops machine or a remote computer with access to internet. The data to be plotted and other graph features are configurable. The various commandline options are as follows: Usage: magi_graph.py [options] Plots the graph for an experiment based on parameters provided. Experiment Configuration File OR Project and Experiment Name needs to be provided to be able to connect to the experiment. Need to provide build a graph specific configuration for plotting. Options: -h, --help show this help message and exit -e EXPERIMENT, --experiment=EXPERIMENT Experiment name -p PROJECT, --project=PROJECT Project name -x EXPERIMENTCONFIG, --experimentConfig=EXPERIMENTCONFIG Experiment configuration file -c CONFIG, --config=CONFIG Graph configuration file -a AGENT, --agent=AGENT Agent name. This is used to fetch available database fields -l AAL, --aal=AAL AAL (experiment procedure) file. This is also used to fetch available database fields -o OUTPUT, --output=OUTPUT Output graph file. Default: graph.png -t, --tunnel Tell the tool to tunnel request through Deter Ops (users.deterlab.net). -u USERNAME, --username=USERNAME Username for creating tunnel. Required only if different from current shell username. This tool expects the user to provide a configuration file. The format of the configuration file needs to be similar to the sample configuration file provided below. graph: type: line xLabel: Time(sec) yLabel: Bytes title: Traffic plot db: agent: monitor_agent filter: host: servernode peerNode: clientnode trafficDirection: out xValue: created yValue: bytes The configuration is divided into two parts a) Graph options and b) Database options. Graph options are used to configure the type of graph and the various labels. The database options help the tool fetch the data to be plotted. Each record stored in the database using MAGI\u2019s database layer has the following three fields along with any other that an agent populates. agent: Agent Name host: Node of which the agent is hosted created: Timestamp of when the record is created In the above mentioned example, data populated by the agent named \u201cmonitor_agent\u201d hosted on the node named \u201cservernode\u201d will be fetched. The data would further be filtered on the configured values of peerNode and trafficDirection, which are agent specific fields. Among the fetched data, values corresponding to the fields, created and bytes, will be plotted correspoding to the x and the y axis, respectively.","title":"magi_graph.py: Create graphs for a MAGI experiment"},{"location":"orchestrator/orchestrator-case-studies/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Orchestrator Case Studies \uf0c1 This section includes more detailed descriptions of how to conduct an experiment in DETERLab using MAGI Orchestrator. Each case study also includes a complete archive with logs and data files. Before you try out the examples below, we recommend reading the MAGI Orchestrator Guide . Simple Client Server \uf0c1 Scaled Client Server \uf0c1 Feedback \uf0c1","title":"Orchestrator case studies"},{"location":"orchestrator/orchestrator-case-studies/#orchestrator-case-studies","text":"This section includes more detailed descriptions of how to conduct an experiment in DETERLab using MAGI Orchestrator. Each case study also includes a complete archive with logs and data files. Before you try out the examples below, we recommend reading the MAGI Orchestrator Guide .","title":"Orchestrator Case Studies"},{"location":"orchestrator/orchestrator-case-studies/#simple-client-server","text":"","title":"Simple Client Server"},{"location":"orchestrator/orchestrator-case-studies/#scaled-client-server","text":"","title":"Scaled Client Server"},{"location":"orchestrator/orchestrator-case-studies/#feedback","text":"","title":"Feedback"},{"location":"orchestrator/orchestrator-config/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. MAGI Configuration \uf0c1 Experiment Configuration \uf0c1 The MAGI experiment-wide configuration ( experiment.conf ) is a YAML-based configuration file. A customized experiment.conf can be provided to the MAGI bootstrap process. magi_bootstrap.py --expconf /path/to/experiment.conf It is optional for a user to provide one. In case it is not provided, the bootstrap process creates a default configuration file. Also, in cases where a user needs to customize a part of the configuration, the user can provide an experiment configuration file with only the parameters that need to be customized. The MAGI configuration validation process would update the user provided configuration to fill in the missing configuration. The experiment wide configuration file consists of three sections. MesDL : Messaging overlay configuration DBDL : Data layer configuration ExpDL : Other experiment information MesDL: Configure the MAGI Control Overlay \uf0c1 The MAGI bootstrap process establishes a networking overlay over which it communicates with the experiment nodes. The control overlay provides a control plane across heterogeneous resources, such as containers, federation, and specialized hardware, on the DETERLab testbed. The control overlay provides a network to reliably propagate discrete control events across the experiment. Hence, it is critical to establish a robust and scalable overlay to ensure the experiments are orchestrated correctly. MAGI provides multi-point to multi-point group communication. A \u201cgroup\u201d has a set of nodes as members. Each group has a logical name. Any member of the group can send a message to any other member of the group. By default, one MAGI overlay is established for the experiment and all the control messages are passed over it. However, it may be necessary to establish two or more overlays based on the experiment topology embedding, control and data management requirements. The required overlay structure can be explicitly specified during the experiment bootstrap process. The overlay configuration, Messaging Description Language (MesDL), is a configuration of the required overlays and bridges in the experiment. The MesDL section of experiment.conf defines all the overlays and bridges for the experiment. The MAGI \u201coverlay\u201d is built on top of the control network on the DETER testbed. Nodes in the overlay can be thought of as being connected by virtual point-to-point links. The overlay provides a way to communicate with all the MAGI-enabled nodes across the testbed boundaries and even over the internet. \u201cbridges\u201d act as meeting places for all the members of the overlay it serves. For example: # Establish two bridges on node3. # Typically one is inward facing towards the experiment and one # is outward facing towards the experimenter to use with magi tools # magi tools can now connect to the experiment overlay through node3, # port 18808 bridges: - { TCPServer: node3, port: 18808 } - { TCPServer: node3, port: 28808 } # Establish two overlays for the experiment. overlay: # Members of this overlay rendezvous at node3. All experiment nodes # are part of this overlay. - { type: TCPTransport, members: [ '__ALL__' ], server: node3, port: 28808 } # Members of this overlay rendezvous at node2. # node4 and node7 are members of this overlay. - { type: TCPTransport, members: [ 'node4', 'node7' ], server: node2, port: 48808 } In the absence of the MesDL, MAGI creates a single overlay with all the experiment nodes as members. MAGI establishes a bridge node based on the following. Note MAGI establishes one of the nodes in the experiment as a bridge node. MAGI selects the bridge node based on the following rules: If there is a node named \u201ccontrol\u201d, it uses that node as the bridge node. If not, it establishes the node with the lowest alphanumeric node name as the bridge node. MAGI expects the selected bridge node to be MAGI-enabled. However, if that is not the case, the user must provide a custom MesDL. For example, here is the MesDL section of experiment.conf for the Client-Server tutorial mesdl: bridges: - {server: clientnode.clientserver.montage, type: TCPServer, port: 18808} - {server: clientnode.clientserver.montage, type: TCPServer, port: 28808} overlay: - members: [__ALL__] port: 28808 server: clientnode.clientserver.montage type: TCPTransport DBDL: Configure the MAGI Data Management Layer \uf0c1 The database configuration contains the following parameters: dbdl: #whether the database management layer is enabled or not isDBEnabled: true #whether the database is sharded or not isDBSharded: false # mapping of sensor/data producer nodes to data collector nodes # __DEFAULT__ maps to the default collector node # if the user does not provide one, the first node in the alpha-numerically # sorted list of collectors is selected as the default collector # In case some of the sensor nodes are not mapped to a collector node, # MAGI would map them to the default collector node sensorToCollectorMap: {node1: node1, node2: node1, __DEFAULT__: node3} # the port at which the collectors listen collectorPort: 27018 #if sharded, where does the global server run globalServerHost: node-1 globalServerPort: 27017 By default, MAGI setups an unsharded setup with a centralized collector. All the sensors collect data at the same collector. However, an experimenter can choose to configure the database whichever way. All the collector nodes need to be MAGI-enabled. The data manager configuration needs to have a collector mapping for each of the MAGI-enabled experiment nodes. If one is not provided, any sensors on a node that does not have a mapped collector, would end up collecting at the default collector. The Data Management section has more information about MAGI\u2019s data management layer. Note The ports are not configurable, they are only for informational purpose. ExpDL: Other common experiment wide configuration \uf0c1 The ExpDL part of the experiment configuration file contains common experiment-wide configuration that is used by the various MAGI tools. Most of the configuration in this section is automatically generated. A user can override the default AAL file location and the default directories for config files, log files, database files and temporary files. expdl: aal: /proj/montage/exp/clientserver/procedure.aal nodePaths: {config: /var/log/magi, db: /var/lib/mongodb, logs: /var/log/magi, temp: /tmp} distributionPath: /share/magi/dev experimentName: clientserver projectName: montage nodeList: [clientnode, servernode] testbedPaths: {experimentDir: /proj/montage/exp/clientserver} Node Configuration \uf0c1 The MAGI daemon process runs using a node-specific configuration ( node.conf ). The experiment wide configuration ( experiment.conf ) is converted into a node specific configuration as part of the bootstrap process. The node configuration file contains some additional configuration apart from all the node specific configuration from the experiment wide configuration. A customized node.conf can be provided as an input to the MAGI bootstrap script, or to the MAGI daemon startup script, directly. Similar to the experiment.conf , a user can provide an incomplete configuration file, containing only customized parameters. The MAGI configuration validation process would fill in the missing configuration. transports: - {address: 0.0.0.0, class: TCPServer, port: 18808} - {address: 0.0.0.0, class: TCPServer, port: 28808} database: isDBEnabled: true configHost: clientnode sensorToCollectorMap: {clientnode: clientnode, servernode: servernode} localInfo: configDir: /var/log/magi logDir: /var/log/magi tempDir: /tmp dbDir: /var/lib/mongodb architecture: 32bit controlif: eth0 controlip: 172.16.111.95 distribution: Ubuntu 10.04 (lucid) nodename: clientnode hostname: clientnode.clientserver.montage.isi.deterlab.net interfaceInfo: eth1: expif: eth1 expip: 10.0.0.1 expmac: 00:00:00:00:00:01 linkname: link peernodes: [servernode] software: - {dir: /share/magi/dev/Linux-Ubuntu10.04-i686, type: rpmfile} - {dir: /share/magi/dev/Linux-Ubuntu10.04-i686, type: archive} - {type: apt} - {dir: /share/magi/dev/source, type: source} - {dir: /tmp/src, type: source} Bootstrap Process \uf0c1 The magi_bootstrap.py is used to configure the overlay and start MAGI on the experiment nodes. The magi_bootstrap.py tool is typically called when the node starts as follows: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\" The magi bootstrap script can be used to install, configure, and start MAGI. The various command line options are as follows. Usage: magi_bootstrap.py [options] Bootstrap script that can be used to install, configure, and start MAGI Options: -h, --help show this help message and exit -p RPATH, --path=RPATH Location of the distribution -U, --noupdate Do not update the system before installing Magi -N, --noinstall Do not install magi and the supporting libraries -v, --verbose Include debugging information -e EXPCONF, --expconf=EXPCONF Path to the experiment wide configuration file -c NODECONF, --nodeconf=NODECONF Path to the node specific configuration file. Cannot use along with -f (see below) -n NODEDIR, --nodedir=NODEDIR Directory to put MAGI daemon specific files -f, --force Recreate node configuration file, even if present. Cannot use along with -c (see above) -D, --nodataman Do not install and setup data manager -o LOGFILE, --logfile=LOGFILE Log file. Default: /tmp/magi_bootstrap.log","title":"Orchestrator config"},{"location":"orchestrator/orchestrator-config/#magi-configuration","text":"","title":"MAGI Configuration"},{"location":"orchestrator/orchestrator-config/#experiment-configuration","text":"The MAGI experiment-wide configuration ( experiment.conf ) is a YAML-based configuration file. A customized experiment.conf can be provided to the MAGI bootstrap process. magi_bootstrap.py --expconf /path/to/experiment.conf It is optional for a user to provide one. In case it is not provided, the bootstrap process creates a default configuration file. Also, in cases where a user needs to customize a part of the configuration, the user can provide an experiment configuration file with only the parameters that need to be customized. The MAGI configuration validation process would update the user provided configuration to fill in the missing configuration. The experiment wide configuration file consists of three sections. MesDL : Messaging overlay configuration DBDL : Data layer configuration ExpDL : Other experiment information","title":"Experiment Configuration"},{"location":"orchestrator/orchestrator-config/#mesdl-configure-the-magi-control-overlay","text":"The MAGI bootstrap process establishes a networking overlay over which it communicates with the experiment nodes. The control overlay provides a control plane across heterogeneous resources, such as containers, federation, and specialized hardware, on the DETERLab testbed. The control overlay provides a network to reliably propagate discrete control events across the experiment. Hence, it is critical to establish a robust and scalable overlay to ensure the experiments are orchestrated correctly. MAGI provides multi-point to multi-point group communication. A \u201cgroup\u201d has a set of nodes as members. Each group has a logical name. Any member of the group can send a message to any other member of the group. By default, one MAGI overlay is established for the experiment and all the control messages are passed over it. However, it may be necessary to establish two or more overlays based on the experiment topology embedding, control and data management requirements. The required overlay structure can be explicitly specified during the experiment bootstrap process. The overlay configuration, Messaging Description Language (MesDL), is a configuration of the required overlays and bridges in the experiment. The MesDL section of experiment.conf defines all the overlays and bridges for the experiment. The MAGI \u201coverlay\u201d is built on top of the control network on the DETER testbed. Nodes in the overlay can be thought of as being connected by virtual point-to-point links. The overlay provides a way to communicate with all the MAGI-enabled nodes across the testbed boundaries and even over the internet. \u201cbridges\u201d act as meeting places for all the members of the overlay it serves. For example: # Establish two bridges on node3. # Typically one is inward facing towards the experiment and one # is outward facing towards the experimenter to use with magi tools # magi tools can now connect to the experiment overlay through node3, # port 18808 bridges: - { TCPServer: node3, port: 18808 } - { TCPServer: node3, port: 28808 } # Establish two overlays for the experiment. overlay: # Members of this overlay rendezvous at node3. All experiment nodes # are part of this overlay. - { type: TCPTransport, members: [ '__ALL__' ], server: node3, port: 28808 } # Members of this overlay rendezvous at node2. # node4 and node7 are members of this overlay. - { type: TCPTransport, members: [ 'node4', 'node7' ], server: node2, port: 48808 } In the absence of the MesDL, MAGI creates a single overlay with all the experiment nodes as members. MAGI establishes a bridge node based on the following. Note MAGI establishes one of the nodes in the experiment as a bridge node. MAGI selects the bridge node based on the following rules: If there is a node named \u201ccontrol\u201d, it uses that node as the bridge node. If not, it establishes the node with the lowest alphanumeric node name as the bridge node. MAGI expects the selected bridge node to be MAGI-enabled. However, if that is not the case, the user must provide a custom MesDL. For example, here is the MesDL section of experiment.conf for the Client-Server tutorial mesdl: bridges: - {server: clientnode.clientserver.montage, type: TCPServer, port: 18808} - {server: clientnode.clientserver.montage, type: TCPServer, port: 28808} overlay: - members: [__ALL__] port: 28808 server: clientnode.clientserver.montage type: TCPTransport","title":"MesDL: Configure the MAGI Control Overlay"},{"location":"orchestrator/orchestrator-config/#dbdl-configure-the-magi-data-management-layer","text":"The database configuration contains the following parameters: dbdl: #whether the database management layer is enabled or not isDBEnabled: true #whether the database is sharded or not isDBSharded: false # mapping of sensor/data producer nodes to data collector nodes # __DEFAULT__ maps to the default collector node # if the user does not provide one, the first node in the alpha-numerically # sorted list of collectors is selected as the default collector # In case some of the sensor nodes are not mapped to a collector node, # MAGI would map them to the default collector node sensorToCollectorMap: {node1: node1, node2: node1, __DEFAULT__: node3} # the port at which the collectors listen collectorPort: 27018 #if sharded, where does the global server run globalServerHost: node-1 globalServerPort: 27017 By default, MAGI setups an unsharded setup with a centralized collector. All the sensors collect data at the same collector. However, an experimenter can choose to configure the database whichever way. All the collector nodes need to be MAGI-enabled. The data manager configuration needs to have a collector mapping for each of the MAGI-enabled experiment nodes. If one is not provided, any sensors on a node that does not have a mapped collector, would end up collecting at the default collector. The Data Management section has more information about MAGI\u2019s data management layer. Note The ports are not configurable, they are only for informational purpose.","title":"DBDL: Configure the MAGI Data Management Layer"},{"location":"orchestrator/orchestrator-config/#expdl-other-common-experiment-wide-configuration","text":"The ExpDL part of the experiment configuration file contains common experiment-wide configuration that is used by the various MAGI tools. Most of the configuration in this section is automatically generated. A user can override the default AAL file location and the default directories for config files, log files, database files and temporary files. expdl: aal: /proj/montage/exp/clientserver/procedure.aal nodePaths: {config: /var/log/magi, db: /var/lib/mongodb, logs: /var/log/magi, temp: /tmp} distributionPath: /share/magi/dev experimentName: clientserver projectName: montage nodeList: [clientnode, servernode] testbedPaths: {experimentDir: /proj/montage/exp/clientserver}","title":"ExpDL: Other common experiment wide configuration"},{"location":"orchestrator/orchestrator-config/#node-configuration","text":"The MAGI daemon process runs using a node-specific configuration ( node.conf ). The experiment wide configuration ( experiment.conf ) is converted into a node specific configuration as part of the bootstrap process. The node configuration file contains some additional configuration apart from all the node specific configuration from the experiment wide configuration. A customized node.conf can be provided as an input to the MAGI bootstrap script, or to the MAGI daemon startup script, directly. Similar to the experiment.conf , a user can provide an incomplete configuration file, containing only customized parameters. The MAGI configuration validation process would fill in the missing configuration. transports: - {address: 0.0.0.0, class: TCPServer, port: 18808} - {address: 0.0.0.0, class: TCPServer, port: 28808} database: isDBEnabled: true configHost: clientnode sensorToCollectorMap: {clientnode: clientnode, servernode: servernode} localInfo: configDir: /var/log/magi logDir: /var/log/magi tempDir: /tmp dbDir: /var/lib/mongodb architecture: 32bit controlif: eth0 controlip: 172.16.111.95 distribution: Ubuntu 10.04 (lucid) nodename: clientnode hostname: clientnode.clientserver.montage.isi.deterlab.net interfaceInfo: eth1: expif: eth1 expip: 10.0.0.1 expmac: 00:00:00:00:00:01 linkname: link peernodes: [servernode] software: - {dir: /share/magi/dev/Linux-Ubuntu10.04-i686, type: rpmfile} - {dir: /share/magi/dev/Linux-Ubuntu10.04-i686, type: archive} - {type: apt} - {dir: /share/magi/dev/source, type: source} - {dir: /tmp/src, type: source}","title":"Node Configuration"},{"location":"orchestrator/orchestrator-config/#bootstrap-process","text":"The magi_bootstrap.py is used to configure the overlay and start MAGI on the experiment nodes. The magi_bootstrap.py tool is typically called when the node starts as follows: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\" The magi bootstrap script can be used to install, configure, and start MAGI. The various command line options are as follows. Usage: magi_bootstrap.py [options] Bootstrap script that can be used to install, configure, and start MAGI Options: -h, --help show this help message and exit -p RPATH, --path=RPATH Location of the distribution -U, --noupdate Do not update the system before installing Magi -N, --noinstall Do not install magi and the supporting libraries -v, --verbose Include debugging information -e EXPCONF, --expconf=EXPCONF Path to the experiment wide configuration file -c NODECONF, --nodeconf=NODECONF Path to the node specific configuration file. Cannot use along with -f (see below) -n NODEDIR, --nodedir=NODEDIR Directory to put MAGI daemon specific files -f, --force Recreate node configuration file, even if present. Cannot use along with -c (see above) -D, --nodataman Do not install and setup data manager -o LOGFILE, --logfile=LOGFILE Log file. Default: /tmp/magi_bootstrap.log","title":"Bootstrap Process"},{"location":"orchestrator/orchestrator-guide/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Orchestrator Guide \uf0c1 In this tutorial we walk you through setting up a basic orchestrated experiment. This page also includes common advanced topics. Detailed descriptions of the commands and configuration files are available in the reference section . Basic MAGI Tutorial \uf0c1 In this tutorial, we demonstrate how to set up client and server traffic generators with only one server and one client. (For more complex examples, see the Case Studies .) The basic steps in creating an orchestrated experiment are: Write the AAL file that describes the experiment's workflows. Include a special start command in your topology. Create or use a physical experiment in DETERLab. Run the Orchestrator tool on a physical experiment on users.deterlab.net . The following sections describe each step in detail. Step 1. Write the AAL file \uf0c1 Describe the experiment procedure (ie, workflow) in an AAL (.aal) file. First we'll cover the parts of an AAL file and then we'll walk through writing the AAL file for this tutorial (we also provide the AAL file itself). AAL File Overview \uf0c1 Agent Activation Language (AAL) is a YAML-based descriptive language that describes an experiment\u2019s workflow. It identifies groups (nodes with similar behaviors), agents (a set of behaviors that may be invoked as events) and events/triggers (the different things you want agents to do and the things that trigger them). An AAL specification has mainly three parts: groups, agents and event streams. Groups \uf0c1 Groups define sets of one or more nodes with the same behavior and enable a coupling between the experiment procedure and the experiment nodes. Example: groups: clients: [ node1, node2, node7 ] defender: [ router3 ] Agents \uf0c1 Agents map a functional behavior onto a set of experiment nodes. An agent provides a set of behaviors that may be invoked with events. The agent directive requires the following keys: agent Name for the set of nodes that represent the behavior group A set of experiment nodes the will functional as the agent path The path to the agent implementation code code Directory name of the agent implementation. The code directive is used in the absence of a path directive. MAGI requires the agent implementation to be part of the python distribution if the path directive is not specified. execargs Zero or more arguments that will be passed to the agent during initialization. Example: agents: smallwebclient: group: smallclients path: /share/magi/modules/http_client/http_client.tar.gz execargs: { servers: [ servernode ], interval: '2', sizes: 'minmax(300,500)'} Event Streams \uf0c1 Event Streams are lists of events and triggers that are parsed and executed by the Orchestrator tool. A procedure typically contains multiple event streams. Different event streams execute concurrently and are synchronized with each other using triggers. The set of event streams listed using the streamstarts directive are invoked at the start of procedure. However, note that the Orchestrator will perform several setup actions, such as create groups, load agents, get status, before the event streams start. Events \uf0c1 Events invoke a procedure implemented in an agent. An event is sent to a group. An event directive requires the following keys: agent :: The agent to send the event. method :: The method to be invoked. args :: Zero or more arguments required by the method. Additionally, it may also contain the trigger key to flag the return status from the method. The return status may be either True or False . Example: - type: event agent: server_agent method: startServer trigger: serverStartedSuccessfully args: {} Triggers \uf0c1 Triggers are used as a synchronization mechanism, guard points, or rendezvous points in an experiment procedure. There are two types of triggers that may be combined in several different ways: Event-based triggers are received from agents after a method. The serverStartedSuccessfully is an example of an event-based trigger. The Orchestrator keeps track of outstanding triggers to follow the experiment execution state space. When the server_agent returns True after the method startServer , the Orchestrator tags it as a received trigger. Example: - type: trigger triggers: [ {event: ClientStopped} ] Time-based triggers wait for a specified amount time to elapse at the Orchestrator before proceeding. Example: - type: trigger triggers: { [ timeout: 60000 ] } # wait for 60 seconds You may find several specific examples of declaring groups, agents, events, and triggers in the Case Studies . For this basic tutorial, save this code to a file named procedure.aal and save it to the experiment folder. Our AAL Example \uf0c1 Now we'll write an AAL that demonstrates three aspects of MAGI: specifying multiple event streams, synchronizing with triggers, and a special target called exit to unload agents. Event Streams \uf0c1 This example has three events streams; the server stream, the client stream, and the cleanup stream. The coordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers (such as after the server has started) or time-based triggers (such as wait for 30 seconds). The triggers are indicated as wait states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above. Server Stream \uf0c1 The server event stream consists of three states. The start state generates a trigger, called serverStarted , once the server agent is activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, when the server is deactivated or terminated. Here is the relevant AAL description: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ {event: ClientStopped} ] - type: event agent: server_agent method: stopServer trigger: ServerStopped args: {} Client Stream \uf0c1 The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger, the client agent is activated in the start state. Next, the client stream waits for a period of time and then terminates the client agents in the stop state. On termination, the client agents send a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated. Here is the relevant AAL description: clientstream: - type: trigger triggers: [ {event: ServerStarted} ] - type: event agent: client_agent method: startClient args: {} - type: trigger triggers: [ {timeout: 60000} ] - type: event agent: client_agent method: stopClient trigger: clientStopped args: {} Cleanup Stream \uf0c1 The last event stream, the cleanup stream consists of two states. First, it waits for all the servers to stop. Then it enters the exit state. The exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator. It causes the Orchestrator to send agent unload and disband group messages to all of the experiment nodes and then it exits the Orchestrator. Here is the relevant AAL code: cleanup: - type: trigger triggers: [ {event: ServerStopped, target: exit} ] You can see all of the code together in this file: casestudy_clientserver.aal . Step 2: Swap in the the physical experiment using topology with MAGI start command \uf0c1 Swap in the experiment using this network description file: casestudy_clientserver.tcl . This start command installs MAGI and supporting tools on all nodes at startup. The normal syntax is as follows: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\" where $NodeName is the control node. If you look at this file, you'll see the MAGI start command is added as a variable and then used for two nodes: the clientnode and servernode. In this example, we set the start command as a variable: set magi_start \"sudo python /share/magi/current/magi_bootstrap.py\" and then use it Step 2: Set up your environment \uf0c1 Set up environment variables for your environment, replacing the value for myExp with your experiment name and myProj with your project name. PROJ=myExp EXP=myProj AAL=casestudy_clientserver.aal Create/Use an experiment in DETERLab \uf0c1 MAGI needs to be enabled on a new or existing swapped-in DETERLab experiment (via interface or using startexp on the commandline). You will need its Experiment Name and Project Name when you run the Orchestrator in the next step. Make sure you\u2019ve swapped-in resources before the next step. Run the magi_orchestrator.py tool \uf0c1 The MAGI Orchestrator tool, magi_orchestrator.py , is a tool that reads the procedure's AAL file and orchestrates an experiment based on the specified procedures. The Orchestrator does the following in this order: Joins Groups - The Orchestrator iterates over the list of groups and for each group sends a request to all the mapped nodes to join the group. A corresponding reply adds the replier to the group. Messages addressed to a group are sent to all the nodes that are part of the group. Loads Agents - The Orchestrator iterates over the list of agents and for each agent sends an agent load request to the mapped groups. An agent load message tells the Orchestrator to start an instance of the agent implementation and to put it in a listening mode, where the instance waits for further messages. Executes Event Streams - Next, the Orchestrator concurrently executes all the event streams listed as part of streamstarts . The Orchestrator has a predefined event stream called exit . The purpose of this event stream is to unload all the agents and disjoin groups. All workflows should end with executing this stream for a clean exit. From your home directory on users.deterlab.net , run the following command: /share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal where: clientnode equals the node you want to start with myExp is the Experiment Name myProj is the Project Name procedural.aal is the name of the AAL file. The various command line options are as follows Usage: magi_orchestrator.py [options] Options: -h, --help show this help message and exit -c CONTROL, --control=CONTROL The control node to connect to (i.e. control.exp.proj) -f EVENTS, --events=EVENTS The events.aal file(s) to use. Can be specified multiple times for multiple AAL files -l LOGLEVEL, --loglevel=LOGLEVEL The level at which to log. Must be one of none, debug, info, warning, error, or critical. Default is info. -o LOGFILE, --logfile=LOGFILE If given, log to the file instead of the console (stdout). -e EXITONFAILURE, --exitOnFailure=EXITONFAILURE If any method call fails (returns False), then exit all streams, unload all agents, and exit the orchestrator. Default value is True -g GROUPBUILDTIMEOUT, --groupBuildTimeout=GROUPBUILDTIMEOUT When building the initial groups for agents in the given AAL, use the timeout given (in milliseconds) when waiting for group formation to complete. --nocolor If given, do not use color in output. -v, --verbose Tell orchestrator to print info about what its doing -n, --tunnel Tell orchestrator to tunnel data through Deter Ops (users.deterlab.net).","title":"Orchestrator guide"},{"location":"orchestrator/orchestrator-guide/#orchestrator-guide","text":"In this tutorial we walk you through setting up a basic orchestrated experiment. This page also includes common advanced topics. Detailed descriptions of the commands and configuration files are available in the reference section .","title":"Orchestrator Guide"},{"location":"orchestrator/orchestrator-guide/#basic-magi-tutorial","text":"In this tutorial, we demonstrate how to set up client and server traffic generators with only one server and one client. (For more complex examples, see the Case Studies .) The basic steps in creating an orchestrated experiment are: Write the AAL file that describes the experiment's workflows. Include a special start command in your topology. Create or use a physical experiment in DETERLab. Run the Orchestrator tool on a physical experiment on users.deterlab.net . The following sections describe each step in detail.","title":"Basic MAGI Tutorial"},{"location":"orchestrator/orchestrator-guide/#step-1-write-the-aal-file","text":"Describe the experiment procedure (ie, workflow) in an AAL (.aal) file. First we'll cover the parts of an AAL file and then we'll walk through writing the AAL file for this tutorial (we also provide the AAL file itself).","title":"Step 1. Write the AAL file"},{"location":"orchestrator/orchestrator-guide/#aal-file-overview","text":"Agent Activation Language (AAL) is a YAML-based descriptive language that describes an experiment\u2019s workflow. It identifies groups (nodes with similar behaviors), agents (a set of behaviors that may be invoked as events) and events/triggers (the different things you want agents to do and the things that trigger them). An AAL specification has mainly three parts: groups, agents and event streams.","title":"AAL File Overview"},{"location":"orchestrator/orchestrator-guide/#groups","text":"Groups define sets of one or more nodes with the same behavior and enable a coupling between the experiment procedure and the experiment nodes. Example: groups: clients: [ node1, node2, node7 ] defender: [ router3 ]","title":"Groups"},{"location":"orchestrator/orchestrator-guide/#agents","text":"Agents map a functional behavior onto a set of experiment nodes. An agent provides a set of behaviors that may be invoked with events. The agent directive requires the following keys: agent Name for the set of nodes that represent the behavior group A set of experiment nodes the will functional as the agent path The path to the agent implementation code code Directory name of the agent implementation. The code directive is used in the absence of a path directive. MAGI requires the agent implementation to be part of the python distribution if the path directive is not specified. execargs Zero or more arguments that will be passed to the agent during initialization. Example: agents: smallwebclient: group: smallclients path: /share/magi/modules/http_client/http_client.tar.gz execargs: { servers: [ servernode ], interval: '2', sizes: 'minmax(300,500)'}","title":"Agents"},{"location":"orchestrator/orchestrator-guide/#event-streams","text":"Event Streams are lists of events and triggers that are parsed and executed by the Orchestrator tool. A procedure typically contains multiple event streams. Different event streams execute concurrently and are synchronized with each other using triggers. The set of event streams listed using the streamstarts directive are invoked at the start of procedure. However, note that the Orchestrator will perform several setup actions, such as create groups, load agents, get status, before the event streams start.","title":"Event Streams"},{"location":"orchestrator/orchestrator-guide/#events","text":"Events invoke a procedure implemented in an agent. An event is sent to a group. An event directive requires the following keys: agent :: The agent to send the event. method :: The method to be invoked. args :: Zero or more arguments required by the method. Additionally, it may also contain the trigger key to flag the return status from the method. The return status may be either True or False . Example: - type: event agent: server_agent method: startServer trigger: serverStartedSuccessfully args: {}","title":"Events"},{"location":"orchestrator/orchestrator-guide/#triggers","text":"Triggers are used as a synchronization mechanism, guard points, or rendezvous points in an experiment procedure. There are two types of triggers that may be combined in several different ways: Event-based triggers are received from agents after a method. The serverStartedSuccessfully is an example of an event-based trigger. The Orchestrator keeps track of outstanding triggers to follow the experiment execution state space. When the server_agent returns True after the method startServer , the Orchestrator tags it as a received trigger. Example: - type: trigger triggers: [ {event: ClientStopped} ] Time-based triggers wait for a specified amount time to elapse at the Orchestrator before proceeding. Example: - type: trigger triggers: { [ timeout: 60000 ] } # wait for 60 seconds You may find several specific examples of declaring groups, agents, events, and triggers in the Case Studies . For this basic tutorial, save this code to a file named procedure.aal and save it to the experiment folder.","title":"Triggers"},{"location":"orchestrator/orchestrator-guide/#our-aal-example","text":"Now we'll write an AAL that demonstrates three aspects of MAGI: specifying multiple event streams, synchronizing with triggers, and a special target called exit to unload agents.","title":"Our AAL Example"},{"location":"orchestrator/orchestrator-guide/#event-streams_1","text":"This example has three events streams; the server stream, the client stream, and the cleanup stream. The coordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers (such as after the server has started) or time-based triggers (such as wait for 30 seconds). The triggers are indicated as wait states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above.","title":"Event Streams"},{"location":"orchestrator/orchestrator-guide/#server-stream","text":"The server event stream consists of three states. The start state generates a trigger, called serverStarted , once the server agent is activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, when the server is deactivated or terminated. Here is the relevant AAL description: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ {event: ClientStopped} ] - type: event agent: server_agent method: stopServer trigger: ServerStopped args: {}","title":"Server Stream"},{"location":"orchestrator/orchestrator-guide/#client-stream","text":"The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger, the client agent is activated in the start state. Next, the client stream waits for a period of time and then terminates the client agents in the stop state. On termination, the client agents send a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated. Here is the relevant AAL description: clientstream: - type: trigger triggers: [ {event: ServerStarted} ] - type: event agent: client_agent method: startClient args: {} - type: trigger triggers: [ {timeout: 60000} ] - type: event agent: client_agent method: stopClient trigger: clientStopped args: {}","title":"Client Stream"},{"location":"orchestrator/orchestrator-guide/#cleanup-stream","text":"The last event stream, the cleanup stream consists of two states. First, it waits for all the servers to stop. Then it enters the exit state. The exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator. It causes the Orchestrator to send agent unload and disband group messages to all of the experiment nodes and then it exits the Orchestrator. Here is the relevant AAL code: cleanup: - type: trigger triggers: [ {event: ServerStopped, target: exit} ] You can see all of the code together in this file: casestudy_clientserver.aal .","title":"Cleanup Stream"},{"location":"orchestrator/orchestrator-guide/#step-2-swap-in-the-the-physical-experiment-using-topology-with-magi-start-command","text":"Swap in the experiment using this network description file: casestudy_clientserver.tcl . This start command installs MAGI and supporting tools on all nodes at startup. The normal syntax is as follows: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\" where $NodeName is the control node. If you look at this file, you'll see the MAGI start command is added as a variable and then used for two nodes: the clientnode and servernode. In this example, we set the start command as a variable: set magi_start \"sudo python /share/magi/current/magi_bootstrap.py\" and then use it","title":"Step 2: Swap in the the physical experiment using topology with MAGI start command"},{"location":"orchestrator/orchestrator-guide/#step-2-set-up-your-environment","text":"Set up environment variables for your environment, replacing the value for myExp with your experiment name and myProj with your project name. PROJ=myExp EXP=myProj AAL=casestudy_clientserver.aal","title":"Step 2: Set up your environment"},{"location":"orchestrator/orchestrator-guide/#createuse-an-experiment-in-deterlab","text":"MAGI needs to be enabled on a new or existing swapped-in DETERLab experiment (via interface or using startexp on the commandline). You will need its Experiment Name and Project Name when you run the Orchestrator in the next step. Make sure you\u2019ve swapped-in resources before the next step.","title":"Create/Use an experiment in DETERLab"},{"location":"orchestrator/orchestrator-guide/#run-the-magi_orchestratorpy-tool","text":"The MAGI Orchestrator tool, magi_orchestrator.py , is a tool that reads the procedure's AAL file and orchestrates an experiment based on the specified procedures. The Orchestrator does the following in this order: Joins Groups - The Orchestrator iterates over the list of groups and for each group sends a request to all the mapped nodes to join the group. A corresponding reply adds the replier to the group. Messages addressed to a group are sent to all the nodes that are part of the group. Loads Agents - The Orchestrator iterates over the list of agents and for each agent sends an agent load request to the mapped groups. An agent load message tells the Orchestrator to start an instance of the agent implementation and to put it in a listening mode, where the instance waits for further messages. Executes Event Streams - Next, the Orchestrator concurrently executes all the event streams listed as part of streamstarts . The Orchestrator has a predefined event stream called exit . The purpose of this event stream is to unload all the agents and disjoin groups. All workflows should end with executing this stream for a clean exit. From your home directory on users.deterlab.net , run the following command: /share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal where: clientnode equals the node you want to start with myExp is the Experiment Name myProj is the Project Name procedural.aal is the name of the AAL file. The various command line options are as follows Usage: magi_orchestrator.py [options] Options: -h, --help show this help message and exit -c CONTROL, --control=CONTROL The control node to connect to (i.e. control.exp.proj) -f EVENTS, --events=EVENTS The events.aal file(s) to use. Can be specified multiple times for multiple AAL files -l LOGLEVEL, --loglevel=LOGLEVEL The level at which to log. Must be one of none, debug, info, warning, error, or critical. Default is info. -o LOGFILE, --logfile=LOGFILE If given, log to the file instead of the console (stdout). -e EXITONFAILURE, --exitOnFailure=EXITONFAILURE If any method call fails (returns False), then exit all streams, unload all agents, and exit the orchestrator. Default value is True -g GROUPBUILDTIMEOUT, --groupBuildTimeout=GROUPBUILDTIMEOUT When building the initial groups for agents in the given AAL, use the timeout given (in milliseconds) when waiting for group formation to complete. --nocolor If given, do not use color in output. -v, --verbose Tell orchestrator to print info about what its doing -n, --tunnel Tell orchestrator to tunnel data through Deter Ops (users.deterlab.net).","title":"Run the magi_orchestrator.py tool"},{"location":"orchestrator/orchestrator-quickstart/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Orchestrator Quickstart \uf0c1 This page describes basic information about the MAGI Orchestrator and provides a high-level overview of how to use it. More details are available in the Orchestrator Guide . What is the MAGI Orchestrator? \uf0c1 MAGI allows you to automate and manage the procedures of a DETERLab experiment which is very useful for highly complex experiments. It's essentially a workflow management system for DETERLab that provides deterministic control and orchestration over event streams, repeatable enactment of procedures and control and data management for experiments. MAGI replaces the SEER experimentation framework and is part of the DETER experiment lifecycle management tools. How does it work? \uf0c1 The procedure for a MAGI experiment is expressed in a YAML-based Agent Activation Language (AAL) file. The MAGI Orchestrator tool parses the procedure AAL file and maintains experiment-wide state to execute the procedure. The Orchestrator then sends and receives events from agents on the experiment nodes and enforces synchronization points -- called as triggers \u2014 to deterministically execute the procedure. How do I use the Orchestrator? \uf0c1 1. Include a special start command in your topology \uf0c1 Add a special start command in your topology to install MAGI and supporting tools on all nodes at startup. The command will be similar to the following: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\" 2. Write the AAL file that describes the experiment's workflows \uf0c1 Describe the experiment procedure (ie, workflow) in a YAML-based AAL (.aal) file that describes: groups - one or more nodes of with similar behavior. agents - sets of behaviors event streams - list of events and triggers that make up a procedure. events - invoke a procedure implemented in an agent triggers - synchronization mechanism based on events or time. The following is an example of an event in the AAL file: - type: event agent: server_agent method: startServer trigger: serverStartedSuccessfully args: {} You'll find more detailed information about writing the AAL file in the Orchestrator Guide . 3. Run the magi_orchestrator.py tool on a physical experiment in DETERLab \uf0c1 Similar to Containers , you run the Orchestrator tool in conjunction with a swapped-in physical experiment in DETERLab. magi_orchestrator.py reads the procedure's AAL file and orchestrates an experiment based on the specified procedures. You would run a command similar to the following on users : /share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal 4. View results by accessing nodes, modify the experiment as needed. \uf0c1 In an orchestrated experiment, you can access the virtual nodes with the same directories mounted as in a Core DETERLab experiment. You can load and run software and conduct experiments as you would in a Core experiment. 5. Save your work and swap out your experiment (release the resources) \uf0c1 As with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users. More Information \uf0c1 For more detailed information about the Orchestrator, read the following: Orchestrator Guide - This guide walks you through a basic example of using the Orchestrator and includes some advanced topics. Orchestrator Case Studies - Includes details of real-world examples of using Orchestrator. Orchestrator Reference - This reference includes commands, configuration details and logs.","title":"Orchestrator quickstart"},{"location":"orchestrator/orchestrator-quickstart/#orchestrator-quickstart","text":"This page describes basic information about the MAGI Orchestrator and provides a high-level overview of how to use it. More details are available in the Orchestrator Guide .","title":"Orchestrator Quickstart"},{"location":"orchestrator/orchestrator-quickstart/#what-is-the-magi-orchestrator","text":"MAGI allows you to automate and manage the procedures of a DETERLab experiment which is very useful for highly complex experiments. It's essentially a workflow management system for DETERLab that provides deterministic control and orchestration over event streams, repeatable enactment of procedures and control and data management for experiments. MAGI replaces the SEER experimentation framework and is part of the DETER experiment lifecycle management tools.","title":"What is the MAGI Orchestrator?"},{"location":"orchestrator/orchestrator-quickstart/#how-does-it-work","text":"The procedure for a MAGI experiment is expressed in a YAML-based Agent Activation Language (AAL) file. The MAGI Orchestrator tool parses the procedure AAL file and maintains experiment-wide state to execute the procedure. The Orchestrator then sends and receives events from agents on the experiment nodes and enforces synchronization points -- called as triggers \u2014 to deterministically execute the procedure.","title":"How does it work?"},{"location":"orchestrator/orchestrator-quickstart/#how-do-i-use-the-orchestrator","text":"","title":"How do I use the Orchestrator?"},{"location":"orchestrator/orchestrator-quickstart/#1-include-a-special-start-command-in-your-topology","text":"Add a special start command in your topology to install MAGI and supporting tools on all nodes at startup. The command will be similar to the following: tb-set-node-startcmd $NodeName \"sudo python /share/magi/current/magi_bootstrap.py\"","title":"1. Include a special start command in your topology"},{"location":"orchestrator/orchestrator-quickstart/#2-write-the-aal-file-that-describes-the-experiments-workflows","text":"Describe the experiment procedure (ie, workflow) in a YAML-based AAL (.aal) file that describes: groups - one or more nodes of with similar behavior. agents - sets of behaviors event streams - list of events and triggers that make up a procedure. events - invoke a procedure implemented in an agent triggers - synchronization mechanism based on events or time. The following is an example of an event in the AAL file: - type: event agent: server_agent method: startServer trigger: serverStartedSuccessfully args: {} You'll find more detailed information about writing the AAL file in the Orchestrator Guide .","title":"2. Write the AAL file that describes the experiment's workflows"},{"location":"orchestrator/orchestrator-quickstart/#3-run-the-magi_orchestratorpy-tool-on-a-physical-experiment-in-deterlab","text":"Similar to Containers , you run the Orchestrator tool in conjunction with a swapped-in physical experiment in DETERLab. magi_orchestrator.py reads the procedure's AAL file and orchestrates an experiment based on the specified procedures. You would run a command similar to the following on users : /share/magi/current/magi_orchestrator.py --control clientnode.myExp.myProj --events procedure.aal","title":"3. Run the magi_orchestrator.py tool on a physical experiment in DETERLab"},{"location":"orchestrator/orchestrator-quickstart/#4-view-results-by-accessing-nodes-modify-the-experiment-as-needed","text":"In an orchestrated experiment, you can access the virtual nodes with the same directories mounted as in a Core DETERLab experiment. You can load and run software and conduct experiments as you would in a Core experiment.","title":"4. View results by accessing nodes, modify the experiment as needed."},{"location":"orchestrator/orchestrator-quickstart/#5-save-your-work-and-swap-out-your-experiment-release-the-resources","text":"As with all DETERLab experiment, when you are ready to stop working on an experiment but know you want to work on it again, save your files in specific protected directories and swap-out (via web interface or commandline) to release resources back to the testbed. This helps ensure there are enough resources for all DETERLab users.","title":"5. Save your work and swap out your experiment (release the resources)"},{"location":"orchestrator/orchestrator-quickstart/#more-information","text":"For more detailed information about the Orchestrator, read the following: Orchestrator Guide - This guide walks you through a basic example of using the Orchestrator and includes some advanced topics. Orchestrator Case Studies - Includes details of real-world examples of using Orchestrator. Orchestrator Reference - This reference includes commands, configuration details and logs.","title":"More Information"},{"location":"orchestrator/orchestrator-reference/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Orchestrator Reference \uf0c1 The following sections contain reference information for the MAGI orchestrator system components: MAGI System Files MAGI Agent Library MAGI Tools MAGI Desktop MAGI Developer Codebase","title":"Orchestrator reference"},{"location":"orchestrator/orchestrator-reference/#orchestrator-reference","text":"The following sections contain reference information for the MAGI orchestrator system components: MAGI System Files MAGI Agent Library MAGI Tools MAGI Desktop MAGI Developer Codebase","title":"Orchestrator Reference"},{"location":"orchestrator/scaled-client-server/","text":"Scaled Client Server Case Study \uf0c1 In this example we demonstrate how to set up client server traffic generators in a larger topology. This case study is identical to Simple Client Case Study except the topology is significantly larger. Event Streams \uf0c1 As in the simpler case , this example has three events streams; the server stream, the client stream, and the cleanup stream. Mapping to the Topology \uf0c1 The groups directive in the AAL file allows mapping an agent behavior to one or more nodes. groups: client_group: [clientnode-1, clientnode-2, clientnode-3, clientnode-4, clientnode-5, clientnode-6, clientnode-7, clientnode-8, clientnode-9, clientnode-10, clientnode-11, clientnode-12, clientnode-13, clientnode-14, clientnode-15, clientnode-16, clientnode-17, clientnode-18, clientnode-19, clientnode-20, clientnode-21, clientnode-22, clientnode-23, clientnode-24, clientnode-25, clientnode-26, clientnode-27, clientnode-28, clientnode-29, clientnode-30, clientnode-31, clientnode-32, clientnode-33, clientnode-34, clientnode-35, clientnode-36, clientnode-37, clientnode-38, clientnode-39, clientnode-40, clientnode-41, clientnode-42, clientnode-43, clientnode-44, clientnode-45, clientnode-46, clientnode-47, clientnode-48, clientnode-49, clientnode-50 ] server_group: &slist [ servernode-1, servernode-2, servernode-3, servernode-4, servernode-5 ] In this example, we observe that there are two groups: client_group which consists of all 50 clientnodes, and server_group which consists of 5 servernodes. Additionally, we use YAML pointers to annotate the server_group as slist . The slist annotation is used to refer to the list of servers for configuring the client_agent in the section below. Configuring the Agents \uf0c1 There are two types of agents, a client_agent and a server_agent. Each agent description consists of at least three directives; group, path and execargs. group : indicates the set of nodes that the client_agent should be deployed on. path : indicates the path to the agent code (also called an agent module). execargs : can be used to parameterize the agents at load time. The agents may be reconfigured later in the AAL also using the setConfiguration method. agents: client_agent: group: client_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '5', sizes: 'minmax(1000,10000)'} server_agent: group: server_group path: /share/magi/modules/apache/apache.tar.gz execargs: [] Server Stream \uf0c1 The server event stream consists of three states. The start state which generates a trigger, called serverStarted , once all the server agents are activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, where the server is deactivated or terminated. The AAL description is the same as the one used in the Simple Client case study . Client Stream \uf0c1 The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all 50 nodes. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger the client agent is activated in the start state. Each client_agent fetches web pages for one of the listed servers. Next, the client stream waits for a period (\\Delta) t and then terminates the client agents in the stop state. On termination, the client agents sends a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated. Running the Experiment \uf0c1 Step 1: Swap in the experiment \uf0c1 Swap in the experiment using this network description file: cs55_topology.tcl Step 2: Set up your environment \uf0c1 Set up your environment. Assuming your experiment is named myExp , your DETER project is myProj , and the AAL file is called procedure.aal . PROJ=myExp EXP=myProj AAL=procedure.aal Step 3: Run the Orchestrator \uf0c1 Once the experiment is swapped in, run the orchestrator, giving it the AAL above and the experiment and project name. > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseClientServer\u201d: The Orchestrator enacts an internally defined stream called initilization that is responsible for establishing the server_group and the client_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , The initialization stream is complete. Now the serverstream, clientstream and the cleanup stream start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The clienstream on receiving the trigger serverStarted from the server_group, sends the startClient event to the client_group. One minute later, the clientstream sends the event stopClient to the client_group and terminates the clientstream. All members of the client_group, terminate the client_agent and generate a clientStopped trigger which is sent back to the orchestrator. Once the serverstream receives the clientStopped trigger from the client_group, it sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream. On receiving the serverStopped trigger, the cleanupstream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment. Procedure: casestudy_clientserver55.aal Topology: casestudy_clientserver55.tcl Archive Logs: casestudy_clientserver55.tar.gz Orchestration: casestudy_clientserver55.orch.log Visualizing Experiment Results \uf0c1 Note This process is the same as for the Simple Client case - we are reproducing here for your convenience. In order to visualize the traffic on the network, modify the above mentioned procedure to add another stream called \u201cmonitorstream\u201d. This stream deploys a packet sensor agent on the router node to measure the aggregated traffic between the server nodes and the client nodes. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [router] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: clientStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the below mentioned tools to create a traffic plot. In order to populate the traffic data, re-run the experiment using the updated procedure. The updated procedure file and the corresponding logs are attached below. The aggregated traffic can then be plotted in two ways: Offline: A plot of the traffic flowing through the router node connecting the clients and the servers can be generated using the MAGI Graph Creation Tool. > GRAPHCONF=cs55_magi_graph.conf > /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject The procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below. Procedure: casestudy_clientserver55_monitor.aal Topology: cs55_topology.tcl Archived Logs: casestudy_clientserver55_monitor.tar.gz Graph Config: casestudy_clientserver55_magi_graph.conf","title":"Scaled Client Server Case Study"},{"location":"orchestrator/scaled-client-server/#scaled-client-server-case-study","text":"In this example we demonstrate how to set up client server traffic generators in a larger topology. This case study is identical to Simple Client Case Study except the topology is significantly larger.","title":"Scaled Client Server Case Study"},{"location":"orchestrator/scaled-client-server/#event-streams","text":"As in the simpler case , this example has three events streams; the server stream, the client stream, and the cleanup stream.","title":"Event Streams"},{"location":"orchestrator/scaled-client-server/#mapping-to-the-topology","text":"The groups directive in the AAL file allows mapping an agent behavior to one or more nodes. groups: client_group: [clientnode-1, clientnode-2, clientnode-3, clientnode-4, clientnode-5, clientnode-6, clientnode-7, clientnode-8, clientnode-9, clientnode-10, clientnode-11, clientnode-12, clientnode-13, clientnode-14, clientnode-15, clientnode-16, clientnode-17, clientnode-18, clientnode-19, clientnode-20, clientnode-21, clientnode-22, clientnode-23, clientnode-24, clientnode-25, clientnode-26, clientnode-27, clientnode-28, clientnode-29, clientnode-30, clientnode-31, clientnode-32, clientnode-33, clientnode-34, clientnode-35, clientnode-36, clientnode-37, clientnode-38, clientnode-39, clientnode-40, clientnode-41, clientnode-42, clientnode-43, clientnode-44, clientnode-45, clientnode-46, clientnode-47, clientnode-48, clientnode-49, clientnode-50 ] server_group: &slist [ servernode-1, servernode-2, servernode-3, servernode-4, servernode-5 ] In this example, we observe that there are two groups: client_group which consists of all 50 clientnodes, and server_group which consists of 5 servernodes. Additionally, we use YAML pointers to annotate the server_group as slist . The slist annotation is used to refer to the list of servers for configuring the client_agent in the section below.","title":"Mapping to the Topology"},{"location":"orchestrator/scaled-client-server/#configuring-the-agents","text":"There are two types of agents, a client_agent and a server_agent. Each agent description consists of at least three directives; group, path and execargs. group : indicates the set of nodes that the client_agent should be deployed on. path : indicates the path to the agent code (also called an agent module). execargs : can be used to parameterize the agents at load time. The agents may be reconfigured later in the AAL also using the setConfiguration method. agents: client_agent: group: client_group path: /share/magi/modules/http_client/http_client.tar.gz execargs: {servers: *slist, interval: '5', sizes: 'minmax(1000,10000)'} server_agent: group: server_group path: /share/magi/modules/apache/apache.tar.gz execargs: []","title":"Configuring the Agents"},{"location":"orchestrator/scaled-client-server/#server-stream","text":"The server event stream consists of three states. The start state which generates a trigger, called serverStarted , once all the server agents are activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, where the server is deactivated or terminated. The AAL description is the same as the one used in the Simple Client case study .","title":"Server Stream"},{"location":"orchestrator/scaled-client-server/#client-stream","text":"The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process on all 50 nodes. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger the client agent is activated in the start state. Each client_agent fetches web pages for one of the listed servers. Next, the client stream waits for a period (\\Delta) t and then terminates the client agents in the stop state. On termination, the client agents sends a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated.","title":"Client Stream"},{"location":"orchestrator/scaled-client-server/#running-the-experiment","text":"","title":"Running the Experiment"},{"location":"orchestrator/scaled-client-server/#step-1-swap-in-the-experiment","text":"Swap in the experiment using this network description file: cs55_topology.tcl","title":"Step 1: Swap in the experiment"},{"location":"orchestrator/scaled-client-server/#step-2-set-up-your-environment","text":"Set up your environment. Assuming your experiment is named myExp , your DETER project is myProj , and the AAL file is called procedure.aal . PROJ=myExp EXP=myProj AAL=procedure.aal","title":"Step 2: Set up your environment"},{"location":"orchestrator/scaled-client-server/#step-3-run-the-orchestrator","text":"Once the experiment is swapped in, run the orchestrator, giving it the AAL above and the experiment and project name. > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project \u201cmontage\u201d with experiment \u201ccaseClientServer\u201d: The Orchestrator enacts an internally defined stream called initilization that is responsible for establishing the server_group and the client_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , The initialization stream is complete. Now the serverstream, clientstream and the cleanup stream start concurrently. The serverstream sends the startServer event to the server_group. All members of the server_group start the server and fire a trigger serverStarted. The clienstream on receiving the trigger serverStarted from the server_group, sends the startClient event to the client_group. One minute later, the clientstream sends the event stopClient to the client_group and terminates the clientstream. All members of the client_group, terminate the client_agent and generate a clientStopped trigger which is sent back to the orchestrator. Once the serverstream receives the clientStopped trigger from the client_group, it sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream. On receiving the serverStopped trigger, the cleanupstream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the casestudy are attached below. Additionally, we have attached a detailed orchestration log that lists triggers from the clientnodes and the servernodes in the experiment. Procedure: casestudy_clientserver55.aal Topology: casestudy_clientserver55.tcl Archive Logs: casestudy_clientserver55.tar.gz Orchestration: casestudy_clientserver55.orch.log","title":"Step 3: Run the Orchestrator"},{"location":"orchestrator/scaled-client-server/#visualizing-experiment-results","text":"Note This process is the same as for the Simple Client case - we are reproducing here for your convenience. In order to visualize the traffic on the network, modify the above mentioned procedure to add another stream called \u201cmonitorstream\u201d. This stream deploys a packet sensor agent on the router node to measure the aggregated traffic between the server nodes and the client nodes. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [router] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: clientStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the below mentioned tools to create a traffic plot. In order to populate the traffic data, re-run the experiment using the updated procedure. The updated procedure file and the corresponding logs are attached below. The aggregated traffic can then be plotted in two ways: Offline: A plot of the traffic flowing through the router node connecting the clients and the servers can be generated using the MAGI Graph Creation Tool. > GRAPHCONF=cs55_magi_graph.conf > /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject The procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below. Procedure: casestudy_clientserver55_monitor.aal Topology: cs55_topology.tcl Archived Logs: casestudy_clientserver55_monitor.tar.gz Graph Config: casestudy_clientserver55_magi_graph.conf","title":"Visualizing Experiment Results"},{"location":"orchestrator/simple-client-server/","text":"Simple Client Server Case Study \uf0c1 In this example, we demonstrate how to set up client and server traffic generators with only one server and one client. (In the Scaled Server Case Study , we will show how the same procedure can be used for a significantly larger topology.) We demonstrate three aspects of MAGI: specifying multiple event streams, synchronizing with triggers, and a special target called exit to unload agents. Event Streams \uf0c1 This example has three events streams; the server stream, the client stream, and the cleanup stream. The coordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers (such as after the server has started) or time-based triggers (such as wait for 30 seconds). The triggers are indicated as wait states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above. Server Stream \uf0c1 The server event stream consists of three states. The start state generates a trigger, called serverStarted , once the server agent is activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, when the server is deactivated or terminated. Here is the relevant AAL description: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ {event: ClientStopped} ] - type: event agent: server_agent method: stopServer trigger: ServerStopped args: {} Client Stream \uf0c1 The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger, the client agent is activated in the start state. Next, the client stream waits for a period of time and then terminates the client agents in the stop state. On termination, the client agents send a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated. Here is the relevant AAL description: clientstream: - type: trigger triggers: [ {event: ServerStarted} ] - type: event agent: client_agent method: startClient args: {} - type: trigger triggers: [ {timeout: 60000} ] - type: event agent: client_agent method: stopClient trigger: clientStopped args: {} Cleanup Stream \uf0c1 The last event stream, the cleanup stream consists of two states. First, it waits for all the servers to stop. Then it enters the exit state. The exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator. It causes the Orchestrator to send agent unload and disband group messages to all of the experiment nodes and then it exits the Orchestrator. Here is the relevant AAL code: cleanup: - type: trigger triggers: [ {event: ServerStopped, target: exit} ] Running the Experiment \uf0c1 Step 1: Swap in the experiment \uf0c1 Swap in the experiment using this network description file: casestudy_clientserver.tcl . Step 2: Set up your environment \uf0c1 Assuming your experiment is named myExp , your project is named myProj , and the AAL file is called casestudy_clientserver.aal , include this in your environment: PROJ=myExp EXP=myProj AAL=casestudy_clientserver.aal Step 3: Run the Orchestrator \uf0c1 Once the experiment is swapped in, run the Orchestrator using this AAL file: casestudy_clientserver.aal . > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project montage with experiment caseClientServer . The output will be as follows: The Orchestrator runs an internally defined stream called initilization that is responsible for establishing the server_group and the client_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , the initialization stream is complete. Now the serverstream , clientstream and the cleanup stream start concurrently. The serverstream sends the startServer event to the server_group . All members of the server_group start the server and fire a trigger serverStarted . The clientstream then sends the startClient event to the client_group. One minute later, the clientstream sends the event stopClient to the client_group and terminates the clientstream . All members of the client_group terminate the client_agent and generate a clientStopped trigger which is sent back to the Orchestrator. Once the serverstream receives the clientStopped trigger from the client_group, it sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream . Upon receiving the serverStopped trigger, the cleanupstream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the case study are attached below. Procedure: casestudy_clientserver.aal Topology: casestudy_clientserver.tcl Archived Logs: casestudy_clientserver.tar.gz Visualizing Experiment Results \uf0c1 In order to visualize the traffic on the network, modify the above-mentioned procedure to add another stream called monitorstream . This stream deploys a packet sensor agent on the server node to measure the traffic on the link in the experiment. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [servernode] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: clientStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the [below mentioned] tools to create a traffic plot. In order to populate the traffic data, re-run the experiment using the updated procedure file: casestudy_clientserver_monitor.aal . The corresponding logs are also available here . The traffic may then be plotted in two ways: Offline Plotting \uf0c1 A plot of the traffic on the link connecting the client and the server can be generated by the MAGI Graph Creation Tool. > GRAPHCONF=cs_magi_graph.conf > /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png Real-time Plotting \uf0c1 You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf The database config node is as follows: dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject The procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below. Procedure: casestudy_clientserver_monitor.aal Archived Logs: casestudy_clientserver_monitor.tar.gz Graph Config: cs_magi_graph.conf Scaling the Experiment \uf0c1 Now suppose you wanted to generate web traffic for a larger topology. We discuss how the above AAL can be applied to a topology of 55 nodes in the next tutorial .","title":"Simple Client Server Case Study"},{"location":"orchestrator/simple-client-server/#simple-client-server-case-study","text":"In this example, we demonstrate how to set up client and server traffic generators with only one server and one client. (In the Scaled Server Case Study , we will show how the same procedure can be used for a significantly larger topology.) We demonstrate three aspects of MAGI: specifying multiple event streams, synchronizing with triggers, and a special target called exit to unload agents.","title":"Simple Client Server Case Study"},{"location":"orchestrator/simple-client-server/#event-streams","text":"This example has three events streams; the server stream, the client stream, and the cleanup stream. The coordination between the events can be illustrated as follows: Event streams can be synchronized using event-based triggers (such as after the server has started) or time-based triggers (such as wait for 30 seconds). The triggers are indicated as wait states (in gray). Forming the groups and loading the agents, which are also automated by the orchestrator tool, are not illustrated above.","title":"Event Streams"},{"location":"orchestrator/simple-client-server/#server-stream","text":"The server event stream consists of three states. The start state generates a trigger, called serverStarted , once the server agent is activated on the experiment nodes. It then enters the wait state where it waits for a trigger from the client event stream. Once the trigger is received, it enters the stop state, when the server is deactivated or terminated. Here is the relevant AAL description: serverstream: - type: event agent: server_agent method: startServer trigger: serverStarted args: {} - type: trigger triggers: [ {event: ClientStopped} ] - type: event agent: server_agent method: stopServer trigger: ServerStopped args: {}","title":"Server Stream"},{"location":"orchestrator/simple-client-server/#client-stream","text":"The client event stream consists of five states. First, the client agent implementation is parameterized by the configuration state. This occurs as part of the agent loading process. The client stream then synchronizes with the server stream by waiting for the serverStarted trigger from the server nodes. Once it receives the trigger, the client agent is activated in the start state. Next, the client stream waits for a period of time and then terminates the client agents in the stop state. On termination, the client agents send a clientStopped trigger that allows the server stream to synchronize and terminate the servers only after all the client have terminated. Here is the relevant AAL description: clientstream: - type: trigger triggers: [ {event: ServerStarted} ] - type: event agent: client_agent method: startClient args: {} - type: trigger triggers: [ {timeout: 60000} ] - type: event agent: client_agent method: stopClient trigger: clientStopped args: {}","title":"Client Stream"},{"location":"orchestrator/simple-client-server/#cleanup-stream","text":"The last event stream, the cleanup stream consists of two states. First, it waits for all the servers to stop. Then it enters the exit state. The exit state unloads and tears down all the communication mechanisms between the agents. The exit state is entered by the key target and is used to transfer control to a reserved state internal to the Orchestrator. It causes the Orchestrator to send agent unload and disband group messages to all of the experiment nodes and then it exits the Orchestrator. Here is the relevant AAL code: cleanup: - type: trigger triggers: [ {event: ServerStopped, target: exit} ]","title":"Cleanup Stream"},{"location":"orchestrator/simple-client-server/#running-the-experiment","text":"","title":"Running the Experiment"},{"location":"orchestrator/simple-client-server/#step-1-swap-in-the-experiment","text":"Swap in the experiment using this network description file: casestudy_clientserver.tcl .","title":"Step 1: Swap in the experiment"},{"location":"orchestrator/simple-client-server/#step-2-set-up-your-environment","text":"Assuming your experiment is named myExp , your project is named myProj , and the AAL file is called casestudy_clientserver.aal , include this in your environment: PROJ=myExp EXP=myProj AAL=casestudy_clientserver.aal","title":"Step 2: Set up your environment"},{"location":"orchestrator/simple-client-server/#step-3-run-the-orchestrator","text":"Once the experiment is swapped in, run the Orchestrator using this AAL file: casestudy_clientserver.aal . > /share/magi/current/magi_orchestrator.py --experiment $EXP --project $PROJ --events $AAL Once run, you will see the orchestrator step through the events in the AAL file. The example output below uses the project montage with experiment caseClientServer . The output will be as follows: The Orchestrator runs an internally defined stream called initilization that is responsible for establishing the server_group and the client_group and loading the agents. Once the agents are loaded, as indicated by the received trigger AgentLoadDone , the initialization stream is complete. Now the serverstream , clientstream and the cleanup stream start concurrently. The serverstream sends the startServer event to the server_group . All members of the server_group start the server and fire a trigger serverStarted . The clientstream then sends the startClient event to the client_group. One minute later, the clientstream sends the event stopClient to the client_group and terminates the clientstream . All members of the client_group terminate the client_agent and generate a clientStopped trigger which is sent back to the Orchestrator. Once the serverstream receives the clientStopped trigger from the client_group, it sends out the stopServer event on the server_group. Once all the servers are stopped, the members of the server_group respond with a serverStopped trigger, which is forwarded to the cleanupstream . Upon receiving the serverStopped trigger, the cleanupstream enacts an internally define stream called exit that is responsible for unloading agents and tearing down the groups. The experiment artifacts, the procedure and topology file that were used for the case study are attached below. Procedure: casestudy_clientserver.aal Topology: casestudy_clientserver.tcl Archived Logs: casestudy_clientserver.tar.gz","title":"Step 3: Run the Orchestrator"},{"location":"orchestrator/simple-client-server/#visualizing-experiment-results","text":"In order to visualize the traffic on the network, modify the above-mentioned procedure to add another stream called monitorstream . This stream deploys a packet sensor agent on the server node to measure the traffic on the link in the experiment. The packet sensor agent records the traffic data using MAGI\u2019s data management layer. monitor_group: [servernode] monitor_agent: group: monitor_group path: /share/magi/modules/pktcounters/pktCountersAgent.tar.gz execargs: {} monitorstream: - type: trigger triggers: [ { event: serverStarted } ] - type: event agent: monitor_agent method: startCollection trigger: collectionServer args: {} - type: trigger triggers: [ { event: clientStopped } ] - type: event agent: monitor_agent method: stopCollection args: {} The recorded data is then pulled out by the [below mentioned] tools to create a traffic plot. In order to populate the traffic data, re-run the experiment using the updated procedure file: casestudy_clientserver_monitor.aal . The corresponding logs are also available here . The traffic may then be plotted in two ways:","title":"Visualizing Experiment Results"},{"location":"orchestrator/simple-client-server/#offline-plotting","text":"A plot of the traffic on the link connecting the client and the server can be generated by the MAGI Graph Creation Tool. > GRAPHCONF=cs_magi_graph.conf > /share/magi/current/magi_graph.py -e $EXP -p $PROJ -c $GRAPHCONF -o cs_traffic_plot.png","title":"Offline Plotting"},{"location":"orchestrator/simple-client-server/#real-time-plotting","text":"You can find the database config node for your experiment by reading your experiment\u2019s configuration file, similar to the following. > cat /proj/myProject/exp/myExperiment/experiment.conf The database config node is as follows: dbdl: configHost: node-1 expdl: experimentName: myExperiment projectName: myProject Then edit the simulated traffic plot URL, passing it the hostname. host=node-1.myExperiment.myProject http://<web-host>/traffic.html?host=node-1.myExperiment.myProject The procedure, graph configuration, and archived log files that were used for the visualization of this case study are attached below. Procedure: casestudy_clientserver_monitor.aal Archived Logs: casestudy_clientserver_monitor.tar.gz Graph Config: cs_magi_graph.conf","title":"Real-time Plotting"},{"location":"orchestrator/simple-client-server/#scaling-the-experiment","text":"Now suppose you wanted to generate web traffic for a larger topology. We discuss how the above AAL can be applied to a topology of 55 nodes in the next tutorial .","title":"Scaling the Experiment"},{"location":"orchestrator/system-files/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. MAGI System Organization \uf0c1 The MAGI system is divided into magicore and magimodules . magicore \uf0c1 The magicore consists of the group communication, control, and data management infrastructure. The latest available version for use on DETERLab may be found on users at: /share/magi/current/ The MAGI codebase is also available at a publically accessible repository. https://github.com/deter-project/magi.git magimodules \uf0c1 The magimodules are agent function implementations that enable a particular behavior on the experiment nodes. The agents along with supporting documentation and file are located on users at: /share/magi/modules The MAGi modules are also available at a publically accessible repository. https://github.com/deter-project/magi-modules.git Logs \uf0c1 You can find helpful logs in these locations in users LOG_DIR Check experiment configuration file. Default: /var/log/magi MAGI Bootstrap Log /tmp/magi_bootstrap.log MAGI Daemon Log $LOG_DIR/daemon.log MongoDB Log $LOG_DIR/mongo.log Configuration Files \uf0c1 The following configuration files are available in users : Agent Name for the set of nodes that represent the behavior Experiment Configuration /proj/<project_name>/exp/<experiment_name>/experiment.conf CONF_DIR Check experiment configuration file. Default: /var/log/magi Node Configuration $CONF_DIR/node.conf","title":"System files"},{"location":"orchestrator/system-files/#magi-system-organization","text":"The MAGI system is divided into magicore and magimodules .","title":"MAGI System Organization"},{"location":"orchestrator/system-files/#magicore","text":"The magicore consists of the group communication, control, and data management infrastructure. The latest available version for use on DETERLab may be found on users at: /share/magi/current/ The MAGI codebase is also available at a publically accessible repository. https://github.com/deter-project/magi.git","title":"magicore"},{"location":"orchestrator/system-files/#magimodules","text":"The magimodules are agent function implementations that enable a particular behavior on the experiment nodes. The agents along with supporting documentation and file are located on users at: /share/magi/modules The MAGi modules are also available at a publically accessible repository. https://github.com/deter-project/magi-modules.git","title":"magimodules"},{"location":"orchestrator/system-files/#logs","text":"You can find helpful logs in these locations in users LOG_DIR Check experiment configuration file. Default: /var/log/magi MAGI Bootstrap Log /tmp/magi_bootstrap.log MAGI Daemon Log $LOG_DIR/daemon.log MongoDB Log $LOG_DIR/mongo.log","title":"Logs"},{"location":"orchestrator/system-files/#configuration-files","text":"The following configuration files are available in users : Agent Name for the set of nodes that represent the behavior Experiment Configuration /proj/<project_name>/exp/<experiment_name>/experiment.conf CONF_DIR Check experiment configuration file. Default: /var/log/magi Node Configuration $CONF_DIR/node.conf","title":"Configuration Files"},{"location":"orchestrator/writing-agents/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Specialized User \uf0c1 This page gives you a brief introduction on writing your own Magi Agent. It is designed to give you sample code, briefly explain it, then show you the pieces needed to run it. After reading this page you should be able to write and run a basic MAGI Agent. Further details and more advanced agent information may be found in the Magi Agent Library document (link). Basic Agent Information \uf0c1 An Agent runs in two modes: a threaded mode and a process mode. Threaded mode: The MAGI Daemon loads python codes directly, and runs the Agent in a thread in its own process space. Process mode: The MAGI Daemon runs the agent in a process space separate from itself and communicates with the Agent via a pipe or a socket. DispatchAgent class \uf0c1 In most cases you will want to use the Orchestrator (link) and an AAL file (link) to run and coordinate your Agent actions. In order to get the basic Agent control (via remote procedure calls), you'll need to derive your agent from the DispatchAgent base class. The DispatchAgent implements the simple remote procedure call (RPC) mechanism used by the Orchestrator (and available through the MAGI python API). This allows any method in a class derived from DispatchAgent to be invoked in an AAL (or by a MagiMessage if using the MAGI python interface directly). The DispatchAgent code simply loops, parsing incoming messages, looking for an event message. When it finds one, it attempts to call the method specified in the message with the arguments given. You needn't worry about message handling or parsing when you derive from DispatchAgent , you can simply write your code and call that code from the AAL. Basic Elements of Writing a Client \uf0c1 To write and execute your agent you need the following three things: The Agent code to implement the Agent. Also, every Agent must implement a method named getAgent which returns an instance of the Agent to run. The MAGI Daemon uses this method to get an instance of the Agent to run in a local thread and communicate with the Agent instance. An Interface Description Language (IDL) file to describe the Agent function and specify things the MAGI Daemon needs to know to load and execute the Agent code (among these is the location of the Agent code and the execution mode). An AAL file (as described here ) Deploying and Executing a Sample Agent \uf0c1 Step 1: Create a local directory named \"FileCreator\" \uf0c1 MAGI Agents are usually contained in a single directory. Step 2: Create the Agent implementation code file \uf0c1 Copy the following Agent implementation code to the file \"FileCreator/FileCreator.py\". This example Agent code has the following characteristics: It creates a simple Agent which creates a single file on a host. The agent is called FileCreator. It has a single method, createFile , which creates the file /tmp/newfile when called For this agent, we will always run in threaded-mode. from magi.util.agent import DispatchAgent, agentmethod from magi.util.processAgent import initializeProcessAgent # The FileCreator agent implementation, derived from DispatchAgent. class FileCreator(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) self.filename = '/tmp/newfile' # A single method which creates the file named by self.filename. # (The @agentmethod() decorator is not required, but is encouraged. # it does nothing of substance now, but may in the future.) @agentmethod() def createFile(self, msg): '''Create a file on the host.''' # open and immediately close the file to create it. open(self.filename, 'w').close() # the getAgent() method must be defined somewhere for all agents. # The Magi daemon invokes this mehod to get a reference to an # agent. It uses this reference to run and interact with an agent # instance. def getAgent(): agent = FileCreator() return agent # In case the agent is run as a separate process, we need to # create an instance of the agent, initialize the required # parameters based on the received arguments, and then call the # run method defined in DispatchAgent. if __name__ == \"__main__\": agent = FileCreator() initializeProcessAgent(agent, sys.argv) agent.run() Step 3: Create the IDL file \uf0c1 Copy the IDL below to a file named 'FileCreator/FileCreator.idl'. Note The file and directory may be named anything, but if you deviate from the naming scheme given, make sure the mainfile setting in the IDL and the code setting in your AAL (below) matches your naming scheme. The following example IDL file has the following characteristics: The execution mode is \"thread\" and the inheritance is specified as \"DispatchAgent\". When you run this, you must specify the name of your implementation file (i.e., the Agent code from the previous step). This example assumes the file is in the local directory and is named \"FileCreator.py\". It lists methods and internal variables that the author wants exposed to external configuration. In our case, we expose the variable filename , but currently only use the default setting. Later we will describe how to set this outside of the Agent implementation. name: FileCreator display: File Creator Agent description: This agent creates a file on the test node. execute: thread mainfile: FileCreator.py inherits: - DispatchAgent methods: - name: createFile help: Create the file args: [] variables: - name: filename help: the full path of the file to create type: string Step 4: Create the AAL file \uf0c1 Copy the sample AAL code below to a file named \"FileCreator/myEvents.aal\". Make sure to: Replace PATH with the full path to your \"FileCreator\" directory. Note: the PATH you use must include the NFS-mounted location on the test nodes. Replace NODES with the comma-separated list of nodes on your testbed on which you want to run the Agent. streamstarts: [main] groups: myFileCreatorGroup [NODES] agents: myFileCreators: group: myFileCreatorGroup # (note: the \"code\" argument is the Agent directory. The # directory must contain an IDL and Agent implementation.) code: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: createFile args: { } Note The AAL is in YAML format; therefore, it cannot have tabs. If you cut and paste the above code, make sure to remove any possible inserted tabs. Because your Agent code is on an NFS-mounted filesystem, all MAGI Daemons may read the code directly. Step 5: Run Orchestrator \uf0c1 Run the MAGI Orchestrator to run the event streams in your AAL file - and thus your agent code: magi_orchestrator --control $control --events myEvents.aal -o run.log -v Where $control is the fully qualified domain of your DETERLab node, i.e. myNode.myGroup.myProject } This command runs the Orchestrator, which connects to the $control node, runs the events in the myEvents.aal file and writes verbose output to run.log . In this example, the method createFile will be called on all test nodes associated with myAgentGroup in the AAL file. On standard out, you should see Orchestrator output similar to the following: stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. This output shows the two execution streams the orchestrator runs. The first, initialization , is internal to the Orchestrator and sets up group communication and loads the agents. The second, main , is the event stream specified in your AAL file. If you do not see the \"correct\" output above, refer to the [#troubleshooting Troubleshooting] section below. To confirm the Agent ran and executed the createFile method, run the following (from users ): ssh myNode.myExperiment.myGroup ls -l /tmp/newfile Where myNode.myExperiment.myGroup is the domain name of a node on which you executed the Agent. You may download the sample code as a tar file here: [FileCreator.tbz] (attach). Runtime Agent Configuration \uf0c1 The sample Agent FileCreator always creates the same file, each time it is run. What if you wanted to create a different file? Or a series of files? It is possible to specify Agent configuration in an AAL file - configuration that can modify the internal variables of your Agent at run time. See MAGI Agent Configuration for details. Troubleshooting \uf0c1 Look at the Magi Daemon log file at /var/log/magi/daemon.log on your control and test nodes looking for errors. If there are syntax errors in Agent execution, they may show up here. Error: You see a 'No such file' error \uf0c1 You may see an error indicating there is no such file as follows: Run-time exception in agent <Daemon(daemon, started 140555403872000)> on node(s) control in method __init__, line 71, in file threadInterface.py. Error: [Errno 2] No such file or directory Solution: You probably did not specify the correct \"mainfile\" in the IDL file. It must not be pathed out and must match the name of the main Agent implementation file, for example, \"myAgent.py\". Error: You see a \"no attribute 'getAgent'\" message \uf0c1 Solution: The Magi Daemon needs the well-known method getAgent to exist in the Agent module. Add it to your Agent code. Error: 'module' object has no attribute 'getAgent' \uf0c1 Solution: Make sure your agent defines (and exports or make available) a getAgent method and that it returns an instance of your agent.","title":"Writing agents"},{"location":"orchestrator/writing-agents/#specialized-user","text":"This page gives you a brief introduction on writing your own Magi Agent. It is designed to give you sample code, briefly explain it, then show you the pieces needed to run it. After reading this page you should be able to write and run a basic MAGI Agent. Further details and more advanced agent information may be found in the Magi Agent Library document (link).","title":"Specialized User"},{"location":"orchestrator/writing-agents/#basic-agent-information","text":"An Agent runs in two modes: a threaded mode and a process mode. Threaded mode: The MAGI Daemon loads python codes directly, and runs the Agent in a thread in its own process space. Process mode: The MAGI Daemon runs the agent in a process space separate from itself and communicates with the Agent via a pipe or a socket.","title":"Basic Agent Information"},{"location":"orchestrator/writing-agents/#dispatchagent-class","text":"In most cases you will want to use the Orchestrator (link) and an AAL file (link) to run and coordinate your Agent actions. In order to get the basic Agent control (via remote procedure calls), you'll need to derive your agent from the DispatchAgent base class. The DispatchAgent implements the simple remote procedure call (RPC) mechanism used by the Orchestrator (and available through the MAGI python API). This allows any method in a class derived from DispatchAgent to be invoked in an AAL (or by a MagiMessage if using the MAGI python interface directly). The DispatchAgent code simply loops, parsing incoming messages, looking for an event message. When it finds one, it attempts to call the method specified in the message with the arguments given. You needn't worry about message handling or parsing when you derive from DispatchAgent , you can simply write your code and call that code from the AAL.","title":"DispatchAgent class"},{"location":"orchestrator/writing-agents/#basic-elements-of-writing-a-client","text":"To write and execute your agent you need the following three things: The Agent code to implement the Agent. Also, every Agent must implement a method named getAgent which returns an instance of the Agent to run. The MAGI Daemon uses this method to get an instance of the Agent to run in a local thread and communicate with the Agent instance. An Interface Description Language (IDL) file to describe the Agent function and specify things the MAGI Daemon needs to know to load and execute the Agent code (among these is the location of the Agent code and the execution mode). An AAL file (as described here )","title":"Basic Elements of Writing a Client"},{"location":"orchestrator/writing-agents/#deploying-and-executing-a-sample-agent","text":"","title":"Deploying and Executing a Sample Agent"},{"location":"orchestrator/writing-agents/#step-1-create-a-local-directory-named-filecreator","text":"MAGI Agents are usually contained in a single directory.","title":"Step 1: Create a local directory named \"FileCreator\""},{"location":"orchestrator/writing-agents/#step-2-create-the-agent-implementation-code-file","text":"Copy the following Agent implementation code to the file \"FileCreator/FileCreator.py\". This example Agent code has the following characteristics: It creates a simple Agent which creates a single file on a host. The agent is called FileCreator. It has a single method, createFile , which creates the file /tmp/newfile when called For this agent, we will always run in threaded-mode. from magi.util.agent import DispatchAgent, agentmethod from magi.util.processAgent import initializeProcessAgent # The FileCreator agent implementation, derived from DispatchAgent. class FileCreator(DispatchAgent): def __init__(self): DispatchAgent.__init__(self) self.filename = '/tmp/newfile' # A single method which creates the file named by self.filename. # (The @agentmethod() decorator is not required, but is encouraged. # it does nothing of substance now, but may in the future.) @agentmethod() def createFile(self, msg): '''Create a file on the host.''' # open and immediately close the file to create it. open(self.filename, 'w').close() # the getAgent() method must be defined somewhere for all agents. # The Magi daemon invokes this mehod to get a reference to an # agent. It uses this reference to run and interact with an agent # instance. def getAgent(): agent = FileCreator() return agent # In case the agent is run as a separate process, we need to # create an instance of the agent, initialize the required # parameters based on the received arguments, and then call the # run method defined in DispatchAgent. if __name__ == \"__main__\": agent = FileCreator() initializeProcessAgent(agent, sys.argv) agent.run()","title":"Step 2: Create the Agent implementation code file"},{"location":"orchestrator/writing-agents/#step-3-create-the-idl-file","text":"Copy the IDL below to a file named 'FileCreator/FileCreator.idl'. Note The file and directory may be named anything, but if you deviate from the naming scheme given, make sure the mainfile setting in the IDL and the code setting in your AAL (below) matches your naming scheme. The following example IDL file has the following characteristics: The execution mode is \"thread\" and the inheritance is specified as \"DispatchAgent\". When you run this, you must specify the name of your implementation file (i.e., the Agent code from the previous step). This example assumes the file is in the local directory and is named \"FileCreator.py\". It lists methods and internal variables that the author wants exposed to external configuration. In our case, we expose the variable filename , but currently only use the default setting. Later we will describe how to set this outside of the Agent implementation. name: FileCreator display: File Creator Agent description: This agent creates a file on the test node. execute: thread mainfile: FileCreator.py inherits: - DispatchAgent methods: - name: createFile help: Create the file args: [] variables: - name: filename help: the full path of the file to create type: string","title":"Step 3: Create the IDL file"},{"location":"orchestrator/writing-agents/#step-4-create-the-aal-file","text":"Copy the sample AAL code below to a file named \"FileCreator/myEvents.aal\". Make sure to: Replace PATH with the full path to your \"FileCreator\" directory. Note: the PATH you use must include the NFS-mounted location on the test nodes. Replace NODES with the comma-separated list of nodes on your testbed on which you want to run the Agent. streamstarts: [main] groups: myFileCreatorGroup [NODES] agents: myFileCreators: group: myFileCreatorGroup # (note: the \"code\" argument is the Agent directory. The # directory must contain an IDL and Agent implementation.) code: PATH/FileCreator execargs: [] eventstreams: main: - type: event agent: myFileCreators method: createFile args: { } Note The AAL is in YAML format; therefore, it cannot have tabs. If you cut and paste the above code, make sure to remove any possible inserted tabs. Because your Agent code is on an NFS-mounted filesystem, all MAGI Daemons may read the code directly.","title":"Step 4: Create the AAL file"},{"location":"orchestrator/writing-agents/#step-5-run-orchestrator","text":"Run the MAGI Orchestrator to run the event streams in your AAL file - and thus your agent code: magi_orchestrator --control $control --events myEvents.aal -o run.log -v Where $control is the fully qualified domain of your DETERLab node, i.e. myNode.myGroup.myProject } This command runs the Orchestrator, which connects to the $control node, runs the events in the myEvents.aal file and writes verbose output to run.log . In this example, the method createFile will be called on all test nodes associated with myAgentGroup in the AAL file. On standard out, you should see Orchestrator output similar to the following: stream initialization : sent : joinGroup myFileCreatorGroup --> __ALL__ stream initialization : done : trigger GroupBuildDone myFileCreatorGroup complete. stream initialization : sent : loadAgent myFileCreators --> myFileCreatorGroup stream initialization : done : trigger AgentLoadDone myFileCreators complete. stream initialization : DONE : complete. stream main : sent : createFile(None) --> myFileCreatorGroup stream main : DONE : complete. This output shows the two execution streams the orchestrator runs. The first, initialization , is internal to the Orchestrator and sets up group communication and loads the agents. The second, main , is the event stream specified in your AAL file. If you do not see the \"correct\" output above, refer to the [#troubleshooting Troubleshooting] section below. To confirm the Agent ran and executed the createFile method, run the following (from users ): ssh myNode.myExperiment.myGroup ls -l /tmp/newfile Where myNode.myExperiment.myGroup is the domain name of a node on which you executed the Agent. You may download the sample code as a tar file here: [FileCreator.tbz] (attach).","title":"Step 5: Run Orchestrator"},{"location":"orchestrator/writing-agents/#runtime-agent-configuration","text":"The sample Agent FileCreator always creates the same file, each time it is run. What if you wanted to create a different file? Or a series of files? It is possible to specify Agent configuration in an AAL file - configuration that can modify the internal variables of your Agent at run time. See MAGI Agent Configuration for details.","title":"Runtime Agent Configuration"},{"location":"orchestrator/writing-agents/#troubleshooting","text":"Look at the Magi Daemon log file at /var/log/magi/daemon.log on your control and test nodes looking for errors. If there are syntax errors in Agent execution, they may show up here.","title":"Troubleshooting"},{"location":"orchestrator/writing-agents/#error-you-see-a-no-such-file-error","text":"You may see an error indicating there is no such file as follows: Run-time exception in agent <Daemon(daemon, started 140555403872000)> on node(s) control in method __init__, line 71, in file threadInterface.py. Error: [Errno 2] No such file or directory Solution: You probably did not specify the correct \"mainfile\" in the IDL file. It must not be pathed out and must match the name of the main Agent implementation file, for example, \"myAgent.py\".","title":"Error: You see a 'No such file' error"},{"location":"orchestrator/writing-agents/#error-you-see-a-no-attribute-getagent-message","text":"Solution: The Magi Daemon needs the well-known method getAgent to exist in the Agent module. Add it to your Agent code.","title":"Error: You see a \"no attribute 'getAgent'\" message"},{"location":"orchestrator/writing-agents/#error-module-object-has-no-attribute-getagent","text":"Solution: Make sure your agent defines (and exports or make available) a getAgent method and that it returns an instance of your agent.","title":"Error: 'module' object has no attribute 'getAgent'"},{"location":"support/class/","text":"Note This page is updated to show the workflow with our new platform . Opening an Account for Class \uf0c1 DETERLab accounts for class use can be created by instructors at any institution. Students need to take no action to create an account. Their instructor creates an account for them. Note For more information on using DETERLab for education, please refer to the DETERLab in Class . Instructors: When creating a new account, please select your password wisely. See our password guidelines . Requesting a New Project for Class - Instructors only \uf0c1 Note If you already have a DETERLab account, please login first. This will help streamline the process. If you are an instructor who wants to request a new project on DETERLab, fill out the \u200b New Project Application Form . Upon submission, your application must be approved by the DETER Executive Committee ; this generally takes a few days. They may contact you and ask for clarification. You will receive an email notification upon approval and your user account will be active. You may then log in with the username and password you entered on the form. If you are curious about the progress on your application, you may \u200bcontact us . Creating Accounts for Your TA and Students \uf0c1 DO NOT ask students or TAs to open accounts themselves. You will create accounts for them. They will be able to access class materials on our old platform and to perform experiments on our new platform. Note If this is a repeat offering of the class, make sure to recycle all accounts first (see how). Materials will remain in the class so you can reuse them in the current offering if you like. Remember to change the visibility of materials manually (see how in Manage materials ). To create accounts: Copy and paste your students' (or TA's) emails, one per line. Account creation takes up to a minute per student. When accounts are created, the system will automatically email the students/TAs, so make sure to alert them to the fact that you are signing them up for a DETERLab account. Students and TAs will receive their username and password in email. Passwords cannot be changed by TAs/students. Note If you or your TAs/students later desire to use DETERLab in research please use our new platform to apply for an account. Class accounts are only for class use. Getting Help \uf0c1 If you run into trouble, please \u200b contact us and we will be happy to assist.","title":"Class users"},{"location":"support/class/#opening-an-account-for-class","text":"DETERLab accounts for class use can be created by instructors at any institution. Students need to take no action to create an account. Their instructor creates an account for them. Note For more information on using DETERLab for education, please refer to the DETERLab in Class . Instructors: When creating a new account, please select your password wisely. See our password guidelines .","title":"Opening an Account for Class"},{"location":"support/class/#requesting-a-new-project-for-class-instructors-only","text":"Note If you already have a DETERLab account, please login first. This will help streamline the process. If you are an instructor who wants to request a new project on DETERLab, fill out the \u200b New Project Application Form . Upon submission, your application must be approved by the DETER Executive Committee ; this generally takes a few days. They may contact you and ask for clarification. You will receive an email notification upon approval and your user account will be active. You may then log in with the username and password you entered on the form. If you are curious about the progress on your application, you may \u200bcontact us .","title":"Requesting a New Project for Class - Instructors only"},{"location":"support/class/#creating-accounts-for-your-ta-and-students","text":"DO NOT ask students or TAs to open accounts themselves. You will create accounts for them. They will be able to access class materials on our old platform and to perform experiments on our new platform. Note If this is a repeat offering of the class, make sure to recycle all accounts first (see how). Materials will remain in the class so you can reuse them in the current offering if you like. Remember to change the visibility of materials manually (see how in Manage materials ). To create accounts: Copy and paste your students' (or TA's) emails, one per line. Account creation takes up to a minute per student. When accounts are created, the system will automatically email the students/TAs, so make sure to alert them to the fact that you are signing them up for a DETERLab account. Students and TAs will receive their username and password in email. Passwords cannot be changed by TAs/students. Note If you or your TAs/students later desire to use DETERLab in research please use our new platform to apply for an account. Class accounts are only for class use.","title":"Creating Accounts for Your TA and Students"},{"location":"support/class/#getting-help","text":"If you run into trouble, please \u200b contact us and we will be happy to assist.","title":"Getting Help"},{"location":"support/elab-elab-ssh-proxy/","text":"Setting Up FoxyProxy \uf0c1 This only needs to be done once. Install the FoxyProxy extension for Firefox. Restart your browser to complete the installation. Follow these steps to set up the proxy. Text you should input is '''bold'''. Click the \"Add New Proxy\" button Proxy Details tab: Use \"Manual Proxy Configuration\". Host: localhost Port: 1080 Check the box for SOCKS 5 proxy URL Patterns tab: Click \"Add New Pattern\" Pattern Name: '''elab in elab''' (or anything you want) URL pattern: '''https?://myboss.. deterlab.net/''' (for MacOS, append '''. ''') Change \"Pattern Contains\" to Regular expression Click OK General tab: Give it a good name, like '''Elab in Elab tunnel''' Click OK Use the \"Move Up\" button to move the new pattern to the top of the list Change the Mode at the top to Use Proxies based on their pre-defined patterns and priorities . Connecting to Inner Boss \uf0c1 Once you've set up FoxyProxy, whenever you want to connect to inner boss you must start an SSH SOCKS proxy. To do this using the standard OpenSSH command line client on linux, BSD, or OsX: ssh -D 1080 username@users.isi.deterlab.net You can also do this under windows using the PuTTY client: In the PuTTY configuration window expand \"Connection\", then \"SSH\", and click on tunnels. Hit the radio button for dynamic, fill in the port 1080, and click ADD. Don't specify the host here - do it after clicking on the main Session Category. If you save the parameters you won't have to do this again. You may then access inner boss using the url: http://myboss.${experiment_name}.${project_name}.isi.deterlab.net/ Fill in ${experiment_name} and ${project_name} with your experiment and project. For instance, the experiment test123 in the example project would use this URL: http://myboss.test123.example.isi.deterlab.net/ Once you are done, you may close the SSH tunnel connection. You may have to use Ctrl-C if it does not disconnect after you log out.","title":"Setting Up FoxyProxy"},{"location":"support/elab-elab-ssh-proxy/#setting-up-foxyproxy","text":"This only needs to be done once. Install the FoxyProxy extension for Firefox. Restart your browser to complete the installation. Follow these steps to set up the proxy. Text you should input is '''bold'''. Click the \"Add New Proxy\" button Proxy Details tab: Use \"Manual Proxy Configuration\". Host: localhost Port: 1080 Check the box for SOCKS 5 proxy URL Patterns tab: Click \"Add New Pattern\" Pattern Name: '''elab in elab''' (or anything you want) URL pattern: '''https?://myboss.. deterlab.net/''' (for MacOS, append '''. ''') Change \"Pattern Contains\" to Regular expression Click OK General tab: Give it a good name, like '''Elab in Elab tunnel''' Click OK Use the \"Move Up\" button to move the new pattern to the top of the list Change the Mode at the top to Use Proxies based on their pre-defined patterns and priorities .","title":"Setting Up FoxyProxy"},{"location":"support/elab-elab-ssh-proxy/#connecting-to-inner-boss","text":"Once you've set up FoxyProxy, whenever you want to connect to inner boss you must start an SSH SOCKS proxy. To do this using the standard OpenSSH command line client on linux, BSD, or OsX: ssh -D 1080 username@users.isi.deterlab.net You can also do this under windows using the PuTTY client: In the PuTTY configuration window expand \"Connection\", then \"SSH\", and click on tunnels. Hit the radio button for dynamic, fill in the port 1080, and click ADD. Don't specify the host here - do it after clicking on the main Session Category. If you save the parameters you won't have to do this again. You may then access inner boss using the url: http://myboss.${experiment_name}.${project_name}.isi.deterlab.net/ Fill in ${experiment_name} and ${project_name} with your experiment and project. For instance, the experiment test123 in the example project would use this URL: http://myboss.test123.example.isi.deterlab.net/ Once you are done, you may close the SSH tunnel connection. You may have to use Ctrl-C if it does not disconnect after you log out.","title":"Connecting to Inner Boss"},{"location":"support/faqs/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Frequently Asked questions \uf0c1 Why can't I log in? \uf0c1 Too many failed attempts to log into the web interface will result in your account being locked. You will get a message saying that your account has been frozen if you trigger it. If you are a student, please contact your TA. Otherwise, please \u200b contact us . Also: - You must use your actual account name , not an email address, to log into users.deterlab.net . - Too many failed attempts to log into users.deterlab.net will result in an IP address ban . We automatically whitelist all IP addresses that have successfully logged into the \u200bweb interface and this list is synchronized every 15 minutes. So if you find yourself banned from connecting to users.deterlab.net please log into the \u200bweb interface and then wait 15 minutes. How do I copy files from my workstation to a node in an experiment? \uf0c1 Your home directory from users is available on the nodes in your experiment. Copy your files to users.deterlab.net using scp or sftp to make them available on your nodes. How can I copy files from a node in the testbed to my workstation? \uf0c1 The reverse of the previous question: copy the files you want to your home directory, then download them from users.deterlab.net using scp or sftp . How can I install software on my nodes? \uf0c1 The currently supported operating system images (see the \u200b Recommended list for currently supported OS images ) have access to full package repositories on a local mirror. Depending on your OS you may use yum , apt-get , or pkg_add to install software that has been pre-packaged for each OS. If there is no package for the software you wish to install, you may install from source. Copy the source tarball to the testbed (see our guide to file coping or use wget or curl on users ), then follow the package's installation instructions. While we will do everything we can to assist any issues you face, we do not have the resources to help individual users install software. I try to swap in and get the error: Admission Control: $project/$experiment has too many nodes allocated! \uf0c1 If you are a class user: the maximum number of nodes that can be allocated for a class is limited (see Class Resource Limits for details). Wait for some of your classmates to free up resources before trying to swap in again. You are less likely to encounter this during non-peak hours (late night and early morning) and when deadlines are distant. If you are NOT a class user: please file a ticket , because something is broken. I try to swap out and get the error: /usr/testbed/bin/nfree: Please cleanup the previous errors. \uf0c1 This may not be so frequent an error, but may arise when the experiment deliberately brings an interface down without bringing it back up prior to swap out. Try scheduling the link back up before the end of experiment. Your site claims that my new password is in the dictionary. I checked the dictionary and 'qwerty1234' is not in it. \uf0c1 Please see our Passwords page .","title":"FAQs"},{"location":"support/faqs/#frequently-asked-questions","text":"","title":"Frequently Asked questions"},{"location":"support/faqs/#why-cant-i-log-in","text":"Too many failed attempts to log into the web interface will result in your account being locked. You will get a message saying that your account has been frozen if you trigger it. If you are a student, please contact your TA. Otherwise, please \u200b contact us . Also: - You must use your actual account name , not an email address, to log into users.deterlab.net . - Too many failed attempts to log into users.deterlab.net will result in an IP address ban . We automatically whitelist all IP addresses that have successfully logged into the \u200bweb interface and this list is synchronized every 15 minutes. So if you find yourself banned from connecting to users.deterlab.net please log into the \u200bweb interface and then wait 15 minutes.","title":"Why can't I log in?"},{"location":"support/faqs/#how-do-i-copy-files-from-my-workstation-to-a-node-in-an-experiment","text":"Your home directory from users is available on the nodes in your experiment. Copy your files to users.deterlab.net using scp or sftp to make them available on your nodes.","title":"How do I copy files from my workstation to a node in an experiment?"},{"location":"support/faqs/#how-can-i-copy-files-from-a-node-in-the-testbed-to-my-workstation","text":"The reverse of the previous question: copy the files you want to your home directory, then download them from users.deterlab.net using scp or sftp .","title":"How can I copy files from a node in the testbed to my workstation?"},{"location":"support/faqs/#how-can-i-install-software-on-my-nodes","text":"The currently supported operating system images (see the \u200b Recommended list for currently supported OS images ) have access to full package repositories on a local mirror. Depending on your OS you may use yum , apt-get , or pkg_add to install software that has been pre-packaged for each OS. If there is no package for the software you wish to install, you may install from source. Copy the source tarball to the testbed (see our guide to file coping or use wget or curl on users ), then follow the package's installation instructions. While we will do everything we can to assist any issues you face, we do not have the resources to help individual users install software.","title":"How can I install software on my nodes?"},{"location":"support/faqs/#i-try-to-swap-in-and-get-the-error-admission-control-projectexperiment-has-too-many-nodes-allocated","text":"If you are a class user: the maximum number of nodes that can be allocated for a class is limited (see Class Resource Limits for details). Wait for some of your classmates to free up resources before trying to swap in again. You are less likely to encounter this during non-peak hours (late night and early morning) and when deadlines are distant. If you are NOT a class user: please file a ticket , because something is broken.","title":"I try to swap in and get the error: Admission Control: $project/$experiment has too many nodes allocated!"},{"location":"support/faqs/#i-try-to-swap-out-and-get-the-error-usrtestbedbinnfree-please-cleanup-the-previous-errors","text":"This may not be so frequent an error, but may arise when the experiment deliberately brings an interface down without bringing it back up prior to swap out. Try scheduling the link back up before the end of experiment.","title":"I try to swap out and get the error: /usr/testbed/bin/nfree: Please cleanup the previous errors."},{"location":"support/faqs/#your-site-claims-that-my-new-password-is-in-the-dictionary-i-checked-the-dictionary-and-qwerty1234-is-not-in-it","text":"Please see our Passwords page .","title":"Your site claims that my new password is in the dictionary. I checked the dictionary and 'qwerty1234' is not in it."},{"location":"support/getting-help/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Getting Help with DETER \uf0c1 Summary: take all of the details about your problem (project/experiment IDs, logs, error output, etc) and file a ticket in our Trac issue-tracking system. If you are using DETERLab for your class do not file a ticket. Email your instructor or TA instead. Note You must log into Trac using your DETERLab username and password. What to do first if you are having trouble? \uf0c1 Read the FAQ page to see if your question has already been answered. You can also search the documentation (see the search box at the top of the left sidebar) and trouble tickets on the Trac Search page . If you still need help, create a Trac support ticket \uf0c1 Step 1: Collect the information we need to help you \uf0c1 Please include the following information in your ticket for best results: Your DETERLab username Your project name Your experiment (if applicable) Include a link in this format: [experiment:pid:eid] where pid is the project name and eid is the experiment name. A description of the problem. Be as detailed as you can. The following are all helpful: What you typed or clicked, what you expected to happen as a result, and what did happen Any error messages or logs (verbatim errors are best) Which nodes in your experiment are having a problem (if applicable) If your problem is with an experiment, please leave a copy swapped in for us (disable idle swap) Step 2: Create a new support ticket \uf0c1 Go to the new ticket page and log in again with your DETERLab username and password. Note: you will receive an Error:Forbidden message when you first click the link. This is expected (the ticket system currently requires a separate login). Fill in all the details you gathered in the previous step as well as the following: Summary : Use a brief description of the issue Reporter : Use your DETERLab username (this should be pre-filled) Message : This is where you include the project name and all of the other details you collected in the previous step. Attachments : You may include screenshots and any other files (logs, etc) that could be useful to us in understanding your issue. We are notified when new tickets are opened and respond to them quickly, often as soon as we receive them (during business hours, US Pacific time). You may use the View Tickets button in the navigation bar to find any tickets you have created. Tips for the fastest response \uf0c1 If you are reporting a support problem, do not email us. Instead, follow the instructions above for filing a support ticket. If for some reason you must email us, DETERLab testbed operations may be reached at testbed-ops at deterlab.net . Response to this email is typically slower than response to a ticket. Always send email to the testbed-ops address and not just an individual who responded to you. Then, if the individual is not available, someone will still be aware of your email.","title":"Getting Help"},{"location":"support/getting-help/#getting-help-with-deter","text":"Summary: take all of the details about your problem (project/experiment IDs, logs, error output, etc) and file a ticket in our Trac issue-tracking system. If you are using DETERLab for your class do not file a ticket. Email your instructor or TA instead. Note You must log into Trac using your DETERLab username and password.","title":"Getting Help with DETER"},{"location":"support/getting-help/#what-to-do-first-if-you-are-having-trouble","text":"Read the FAQ page to see if your question has already been answered. You can also search the documentation (see the search box at the top of the left sidebar) and trouble tickets on the Trac Search page .","title":"What to do first if you are having trouble?"},{"location":"support/getting-help/#if-you-still-need-help-create-a-trac-support-ticket","text":"","title":"If you still need help, create a Trac support ticket"},{"location":"support/getting-help/#step-1-collect-the-information-we-need-to-help-you","text":"Please include the following information in your ticket for best results: Your DETERLab username Your project name Your experiment (if applicable) Include a link in this format: [experiment:pid:eid] where pid is the project name and eid is the experiment name. A description of the problem. Be as detailed as you can. The following are all helpful: What you typed or clicked, what you expected to happen as a result, and what did happen Any error messages or logs (verbatim errors are best) Which nodes in your experiment are having a problem (if applicable) If your problem is with an experiment, please leave a copy swapped in for us (disable idle swap)","title":"Step 1: Collect the information we need to help you"},{"location":"support/getting-help/#step-2-create-a-new-support-ticket","text":"Go to the new ticket page and log in again with your DETERLab username and password. Note: you will receive an Error:Forbidden message when you first click the link. This is expected (the ticket system currently requires a separate login). Fill in all the details you gathered in the previous step as well as the following: Summary : Use a brief description of the issue Reporter : Use your DETERLab username (this should be pre-filled) Message : This is where you include the project name and all of the other details you collected in the previous step. Attachments : You may include screenshots and any other files (logs, etc) that could be useful to us in understanding your issue. We are notified when new tickets are opened and respond to them quickly, often as soon as we receive them (during business hours, US Pacific time). You may use the View Tickets button in the navigation bar to find any tickets you have created.","title":"Step 2: Create a new support ticket"},{"location":"support/getting-help/#tips-for-the-fastest-response","text":"If you are reporting a support problem, do not email us. Instead, follow the instructions above for filing a support ticket. If for some reason you must email us, DETERLab testbed operations may be reached at testbed-ops at deterlab.net . Response to this email is typically slower than response to a ticket. Always send email to the testbed-ops address and not just an individual who responded to you. Then, if the individual is not available, someone will still be aware of your email.","title":"Tips for the fastest response"},{"location":"support/migration/","text":"Note This page is updated to show the workflow with our new platform . Migration Instructions \uf0c1 At this time all projects must migrate to our new platform . Class projects can continue to use this user interface for class management purposes. All experimentation will occur on our new platform and old machines will be decomissioned. Instructions below show how to migrate existing projects to our new platform. Migrating Class Projects \uf0c1 Edit your User Profile to set up Merge password (it is currently set at random, please reset it to a string of your choosing if you wish). See illustrations below. Access your profile: Edit profile: Set up Merge password: 2. Click on Teaching tab, then choose your class and click on Migrate Class from the left menu. To migrate your existing class materials please see our guidelines here . After you have migrated your class you can continue using this interface to manage your materials and students. Students will use our new platform to perform experiments. Your, TAs' and students' accounts will be synchronized between the two platforms (same usernames and passwords). You can read more about the new platform and how students use it here . Migrating Research Projects \uf0c1 Carefully read Merge documentation Apply for account on the new DeterLab here . Please select a password that is strong but easy for you to remember and type. For example, passwords that contain 5+ words significant to you should work. Wait for approval, which should arrive via email Log into users.deterlab.net and edit .ssh/config file by following instructions here . Set up an XDC (experiment development container), which will serve as your gateway into the new DeterLab, by following the instructions here . If everything is set up correctly, you should be able to SSH into your first XDC by typing on users.deterlab.net the following command ssh <xdcname>-<yourusername> . There is no need to create new experiments at this step, although you are welcome to try it. Migrate your projects, experiments and data by following instructions below this list Put in a DeterLab ticket to let us know you have migrated your items. From this point on please do not use old DeterLab anymore. To migrate a project X using your new username U, type on users.deterlab.net : migrate U X (you will be prompted for password) Experiments in your project will be automatically converted and migrated to a new project on our new infrastructure. Project and experiment naming conventions have changed, so you may notice some differences. New project and experiment names must be all lower-case and alphanumeric only. In addition to this, current OS images on the new infrastructure differ from the existing OS images, and there is currently no support for startup commands. Thus, during experiment conversion OS, hardware and startup directives are skipped. You will also need to migrate any data in your personal user directory, which you want to keep. To do so, create folder newdeter in your home directory, and move into it all files and folders that you want to keep. Then run: migrate U (you will be prompted for password) Note We can only support migration of up to 10 GB per user or per project. Please delete any large files that you will not need going forward. If you need more space, please contact us at testbed-ops@isi.deterlab.net Migration FAQ \uf0c1 Who should migrate? \uf0c1 All research projects and users (project leaders and research students) should migrate to our new infrastructure by May 1st, 2023. Class users and projects can remain on the old infrastructure until summer 2023. They should plan to migrate over the summer. If you are project leader on DeterLab and you have both research and class projects, you should migrate your account and your research projects now. What are the advantages of the new infrastructure? \uf0c1 There are multiple advantages to users from migrating to the new infrastructure: You will be able to request virtual machines, and thus run larger scale experiments. You should experience less resource contention. It will be easier to obtain resources. Experiments will be set up quickly - within seconds instead of minutes Experiment specification errors will be caught early, before resources are committed All experiments will have external access for software installation What are the differences between the old and the new infrastructure? \uf0c1 The new infrastructure is running a new software, called Merge, which is more stable, robust and modular than the old software. The new infrastructure has all new hardware - end servers and switches - and will be more stable. The new infrastructure has a new UI for experiment management (creation, swap in/out, termination) The new infrastructure has a new access process: you still SSH twice but the SSH process should be smoother The new infrastructure has a new topology definition language Where can I learn more? \uf0c1 Please read Merge documentation","title":"Migration Instructions"},{"location":"support/migration/#migration-instructions","text":"At this time all projects must migrate to our new platform . Class projects can continue to use this user interface for class management purposes. All experimentation will occur on our new platform and old machines will be decomissioned. Instructions below show how to migrate existing projects to our new platform.","title":"Migration Instructions"},{"location":"support/migration/#migrating-class-projects","text":"Edit your User Profile to set up Merge password (it is currently set at random, please reset it to a string of your choosing if you wish). See illustrations below. Access your profile: Edit profile: Set up Merge password: 2. Click on Teaching tab, then choose your class and click on Migrate Class from the left menu. To migrate your existing class materials please see our guidelines here . After you have migrated your class you can continue using this interface to manage your materials and students. Students will use our new platform to perform experiments. Your, TAs' and students' accounts will be synchronized between the two platforms (same usernames and passwords). You can read more about the new platform and how students use it here .","title":"Migrating Class Projects"},{"location":"support/migration/#migrating-research-projects","text":"Carefully read Merge documentation Apply for account on the new DeterLab here . Please select a password that is strong but easy for you to remember and type. For example, passwords that contain 5+ words significant to you should work. Wait for approval, which should arrive via email Log into users.deterlab.net and edit .ssh/config file by following instructions here . Set up an XDC (experiment development container), which will serve as your gateway into the new DeterLab, by following the instructions here . If everything is set up correctly, you should be able to SSH into your first XDC by typing on users.deterlab.net the following command ssh <xdcname>-<yourusername> . There is no need to create new experiments at this step, although you are welcome to try it. Migrate your projects, experiments and data by following instructions below this list Put in a DeterLab ticket to let us know you have migrated your items. From this point on please do not use old DeterLab anymore. To migrate a project X using your new username U, type on users.deterlab.net : migrate U X (you will be prompted for password) Experiments in your project will be automatically converted and migrated to a new project on our new infrastructure. Project and experiment naming conventions have changed, so you may notice some differences. New project and experiment names must be all lower-case and alphanumeric only. In addition to this, current OS images on the new infrastructure differ from the existing OS images, and there is currently no support for startup commands. Thus, during experiment conversion OS, hardware and startup directives are skipped. You will also need to migrate any data in your personal user directory, which you want to keep. To do so, create folder newdeter in your home directory, and move into it all files and folders that you want to keep. Then run: migrate U (you will be prompted for password) Note We can only support migration of up to 10 GB per user or per project. Please delete any large files that you will not need going forward. If you need more space, please contact us at testbed-ops@isi.deterlab.net","title":"Migrating Research Projects"},{"location":"support/migration/#migration-faq","text":"","title":"Migration FAQ"},{"location":"support/migration/#who-should-migrate","text":"All research projects and users (project leaders and research students) should migrate to our new infrastructure by May 1st, 2023. Class users and projects can remain on the old infrastructure until summer 2023. They should plan to migrate over the summer. If you are project leader on DeterLab and you have both research and class projects, you should migrate your account and your research projects now.","title":"Who should migrate?"},{"location":"support/migration/#what-are-the-advantages-of-the-new-infrastructure","text":"There are multiple advantages to users from migrating to the new infrastructure: You will be able to request virtual machines, and thus run larger scale experiments. You should experience less resource contention. It will be easier to obtain resources. Experiments will be set up quickly - within seconds instead of minutes Experiment specification errors will be caught early, before resources are committed All experiments will have external access for software installation","title":"What are the advantages of the new infrastructure?"},{"location":"support/migration/#what-are-the-differences-between-the-old-and-the-new-infrastructure","text":"The new infrastructure is running a new software, called Merge, which is more stable, robust and modular than the old software. The new infrastructure has all new hardware - end servers and switches - and will be more stable. The new infrastructure has a new UI for experiment management (creation, swap in/out, termination) The new infrastructure has a new access process: you still SSH twice but the SSH process should be smoother The new infrastructure has a new topology definition language","title":"What are the differences between the old and the new infrastructure?"},{"location":"support/migration/#where-can-i-learn-more","text":"Please read Merge documentation","title":"Where can I learn more?"},{"location":"support/passwords/","text":"Password Guidelines \uf0c1 We are a computer security testbed, so please use a strong password . You may be reading this because you were told that your new password, 'qwerty1234', is in the dictionary. We do not mean the Oxford English Dictionary here. What we use is a large list of dictionary words combined with actual passwords that have been found in the wild. For example, the \u200b RockYou hack ended up revealing the unencrypted passwords of 32 million people (and about 14 million unique passwords). Since this list is one of the go-to lists for the bad guys, we use it too. This means that many passwords that seem clever or obscure fail our test because someone else thought up the same thing. Password tips: The longer your password, the less likely it is to be in the dictionary. Try combining multiple words mixed with numbers and symbols. If you are interested in learning more about password security and cracking, this arstechnica article is a pretty good introduction: \u200bAnatomy of a hack: How crackers ransack passwords like \u201cqeadzcwrsfxv1331\u201d .","title":"Password Guidelines"},{"location":"support/passwords/#password-guidelines","text":"We are a computer security testbed, so please use a strong password . You may be reading this because you were told that your new password, 'qwerty1234', is in the dictionary. We do not mean the Oxford English Dictionary here. What we use is a large list of dictionary words combined with actual passwords that have been found in the wild. For example, the \u200b RockYou hack ended up revealing the unencrypted passwords of 32 million people (and about 14 million unique passwords). Since this list is one of the go-to lists for the bad guys, we use it too. This means that many passwords that seem clever or obscure fail our test because someone else thought up the same thing. Password tips: The longer your password, the less likely it is to be in the dictionary. Try combining multiple words mixed with numbers and symbols. If you are interested in learning more about password security and cracking, this arstechnica article is a pretty good introduction: \u200bAnatomy of a hack: How crackers ransack passwords like \u201cqeadzcwrsfxv1331\u201d .","title":"Password Guidelines"},{"location":"support/registering/","text":"Registering to use DETERLab \uf0c1 Who may apply for a DETER project or request a user account? \uf0c1 Researchers from academia, government, and industry -- as well as educators from academic institutions -- may apply for a DETERLab project account. A student must have their professor or appropriate faculty member apply for an account, and once it is activated the student can then apply for membership to that project. Note For more information on using DETERLab for education, please refer to the \u200bDETERLab Education Site . To Register \uf0c1 DETER accounts are grouped by projects, therefore the project leader or PI must first request a project, then other users apply for membership to that project. Note If you already have a DETERLab account, please login first. This will help streamline the process. Requesting a New Project \uf0c1 If you are a PI, project leader or instructor who wants to request a new project on DETERLab, fill out the \u200b New Project Application Form . No other users (such as students) should apply for a new project. You will be asked a number of questions about your project and how you intend to use DETER. Please be detailed, especially with respect to any possible risks from your experiment. A DETER staff member may contact you to discuss or clarify any potential issues. The project leader is responsible for ensuring that the project adheres to the Project Plan included in the application form. Instructors should indicate this project is for educational purposes. Once the project is created, you will receive further instructions including how to create accounts for your students. Upon submission, your application must be approved by the DETER Executive Committee ; this generally takes a few days. They may contact you and ask for clarification. You will receive an email notification upon approval and your user account will be active. You may then log in with the username and password you entered on the form. If you are curious about the progress on your application, you may \u200bcontact us . Getting a User Account to Join an Existing Project \uf0c1 If you are a team member who needs to join an existing project , submit the \u200b Apply for Project Membership Form . The project leader will be informed via email of your request and will be required to log in to approve your account. Note to Students \uf0c1 If you are a student who wants to use DETERLab for your own research: Your faculty sponsor must first fill out the \u200b New Project Application Form . A student may not create their own project. Once this has been done, the sponsor will instruct you to \u200bapply for project membership, which will create your DETERLab account. You must obtain the name of the DETERLab project from your sponsor to correctly fill out the form. If you are a student taking a class that uses DETERLab: You do not need to take any action \u2013 your instructor will create accounts and assign them to students. If you need assistance \uf0c1 If you run into trouble, please \u200b contact us and we will be happy to assist.","title":"Registering to use DETERLab"},{"location":"support/registering/#registering-to-use-deterlab","text":"","title":"Registering to use DETERLab"},{"location":"support/registering/#who-may-apply-for-a-deter-project-or-request-a-user-account","text":"Researchers from academia, government, and industry -- as well as educators from academic institutions -- may apply for a DETERLab project account. A student must have their professor or appropriate faculty member apply for an account, and once it is activated the student can then apply for membership to that project. Note For more information on using DETERLab for education, please refer to the \u200bDETERLab Education Site .","title":"Who may apply for a DETER project or request a user account?"},{"location":"support/registering/#to-register","text":"DETER accounts are grouped by projects, therefore the project leader or PI must first request a project, then other users apply for membership to that project. Note If you already have a DETERLab account, please login first. This will help streamline the process.","title":"To Register"},{"location":"support/registering/#requesting-a-new-project","text":"If you are a PI, project leader or instructor who wants to request a new project on DETERLab, fill out the \u200b New Project Application Form . No other users (such as students) should apply for a new project. You will be asked a number of questions about your project and how you intend to use DETER. Please be detailed, especially with respect to any possible risks from your experiment. A DETER staff member may contact you to discuss or clarify any potential issues. The project leader is responsible for ensuring that the project adheres to the Project Plan included in the application form. Instructors should indicate this project is for educational purposes. Once the project is created, you will receive further instructions including how to create accounts for your students. Upon submission, your application must be approved by the DETER Executive Committee ; this generally takes a few days. They may contact you and ask for clarification. You will receive an email notification upon approval and your user account will be active. You may then log in with the username and password you entered on the form. If you are curious about the progress on your application, you may \u200bcontact us .","title":"Requesting a New Project"},{"location":"support/registering/#getting-a-user-account-to-join-an-existing-project","text":"If you are a team member who needs to join an existing project , submit the \u200b Apply for Project Membership Form . The project leader will be informed via email of your request and will be required to log in to approve your account.","title":"Getting a User Account to Join an Existing Project"},{"location":"support/registering/#note-to-students","text":"If you are a student who wants to use DETERLab for your own research: Your faculty sponsor must first fill out the \u200b New Project Application Form . A student may not create their own project. Once this has been done, the sponsor will instruct you to \u200bapply for project membership, which will create your DETERLab account. You must obtain the name of the DETERLab project from your sponsor to correctly fill out the form. If you are a student taking a class that uses DETERLab: You do not need to take any action \u2013 your instructor will create accounts and assign them to students.","title":"Note to Students"},{"location":"support/registering/#if-you-need-assistance","text":"If you run into trouble, please \u200b contact us and we will be happy to assist.","title":"If you need assistance"},{"location":"support/research/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Opening an Account for Research \uf0c1 Each DETERLab account belongs to a project. Therefore the project leader or PI must first request a project, then other users apply for membership to that project. Note When creating a new account, please select your password wisely. See our password guidelines . Requesting a new DETERLab project \uf0c1 Note If you already have a DETERLab account, please login first, and then request a new project. This will help streamline the process. If you are a PI, project leader or instructor who wants to request a new project on DETERLab, fill out the \u200b New Project Application Form . Students that want to use DETERLab must coordinate with their academic advisor who can apply for a new project. You will be asked a number of questions about your project and how you intend to use DETER. Please be detailed, especially with respect to any possible risks from your experiment. Our staff member may contact you to discuss or clarify any potential issues. The project leader is responsible for ensuring that the project adheres to the Project Plan included in the application form. Upon submission, your application must be approved by the DETER Executive Committee ; this generally takes a few days. They may contact you and ask for clarification. You will receive an email notification upon approval and your user account will be active. You may then log in with the username and password you entered on the form. If you are curious about the progress on your application, you may \u200bcontact us . Requesting an Account to Join an Existing Project \uf0c1 If you are a team member or a student who needs to join an existing project , submit the \u200b Apply for Project Membership Form . You will need to know the name of the project you want to join. The project leader will be informed via email of your request and will be required to log in to approve your account. When the project leader approves you, you will receive an email notification and can start using the testbed. Getting Help \uf0c1 If you run into trouble, please \u200b contact us and we will be happy to assist.","title":"Research users"},{"location":"support/research/#opening-an-account-for-research","text":"Each DETERLab account belongs to a project. Therefore the project leader or PI must first request a project, then other users apply for membership to that project. Note When creating a new account, please select your password wisely. See our password guidelines .","title":"Opening an Account for Research"},{"location":"support/research/#requesting-a-new-deterlab-project","text":"Note If you already have a DETERLab account, please login first, and then request a new project. This will help streamline the process. If you are a PI, project leader or instructor who wants to request a new project on DETERLab, fill out the \u200b New Project Application Form . Students that want to use DETERLab must coordinate with their academic advisor who can apply for a new project. You will be asked a number of questions about your project and how you intend to use DETER. Please be detailed, especially with respect to any possible risks from your experiment. Our staff member may contact you to discuss or clarify any potential issues. The project leader is responsible for ensuring that the project adheres to the Project Plan included in the application form. Upon submission, your application must be approved by the DETER Executive Committee ; this generally takes a few days. They may contact you and ask for clarification. You will receive an email notification upon approval and your user account will be active. You may then log in with the username and password you entered on the form. If you are curious about the progress on your application, you may \u200bcontact us .","title":"Requesting a new DETERLab project"},{"location":"support/research/#requesting-an-account-to-join-an-existing-project","text":"If you are a team member or a student who needs to join an existing project , submit the \u200b Apply for Project Membership Form . You will need to know the name of the project you want to join. The project leader will be informed via email of your request and will be required to log in to approve your account. When the project leader approves you, you will receive an email notification and can start using the testbed.","title":"Requesting an Account to Join an Existing Project"},{"location":"support/research/#getting-help","text":"If you run into trouble, please \u200b contact us and we will be happy to assist.","title":"Getting Help"},{"location":"support/usage-policy/","text":"Important This page is deprecated. Please use our new platform and accompanying documentation. Deterlab Usage Policy \uf0c1 Be a good citizen \uf0c1 DETERLab is a shared resource. We expect users to be good citizens by not abusing or wasting the resources we make available to them for free. We ask that you: Read our documentation on opening account and running experiments (see links on the left sidebar). Let us know if anything is unclear. Do not share accounts. We will close accounts that we suspect to be shared. Use good passwords. We are a computer security testbed, so please use a strong password. For more details, see our Passwords page . Swap out your experiment when it is finished. People forget to do this all the time. We do have an idle detection mechanism, but it is not perfect and may keep thinking your experiment is active long since you have collected your results, published your paper, and achieved tenure. Please log back in and free up your nodes so that other researchers can use them . Do not abuse the no idle-swap feature. Only turn off idle swap if there is a true need. Take the time to script the setup process for your experiment or create custom disk images. Make good use of disk space. Our goal at DETERLab is to assist you in running experiments. We are happy to provide you with all the storage necessary to accomplish your experimental goals. Also, we maintain nightly backups going back a few weeks. However, we are not a substitute for personal or institutional storage. We are not a backup service. Please keep your important files backed up offsite at your institution. Our main machine room is technically subject to earthquakes, tsunamis, and liquefaction. Take a look at rsync . We are not an archive. Please clean up or move offsite any large log files and traces after your experiment is done and paper is published. This makes sure that storage is available to researchers who are actively using the testbed. Keep things tidy. Remove unused custom operating system images. Experts tell us that 22% of custom operating system images are never used even once. Why keep that around on disk so that it is backed up day after day? It slows down our backups and crash recovery disk checks, and takes resources away from other researchers. If you do need more space, just contact us. Quota limits and housekeeping requests allows us to have extra space available to allocate to you when you need it. Let us know if you are done with your project. - We'll clean it up for you! Talk to us if you need something - we are here to help you. We often can provide useful suggestions about running experiments on the testbed. First come, first served. Sort of\u2026 \uf0c1 We don't have an advanced scheduler. Unfortunately, our software does not support advance reservations. But we are always willing to work with users to help them acquire nodes they need on time, and then help them hold those nodes as long as they need them. Sometimes this involves blocking the nodes out for a period of time. Other times this involves swapping the experiment in the night before the demo, when most nodes are free. Please submit a ticket if you need a reservation and we will work with you. We may swap out idle experiments. We audit the testbed for idle experiments. When the testbed is close to full we will ping the users with long-running, idle experiments and ask if they can be swapped out. If no response is received within 8 hours, we will swap out the experiment. Note that you may prevent this by promptly replying to our email and telling us you need the nodes. Very rarely, we may restrict use to a part of the testbed. There are times when a large-scale demonstration requires us to block off a part of our testbed. At these times, you will see reduced node availability but your swapped in experiments will not be disturbed. We will inform you about these planned events at least two weeks ahead, via news items on DETERLab's web page. Watch out for downtimes. We have regular weekly downtimes where we reserve the right to perform service-interrupting work on the testbed (most weeks we sail right through without any noticeable interruption of service) and sometimes special downtimes are required. We usually give a few days notice before the special downtimes. Keep an eye on the DETERLab News Page for notices. Privacy \uf0c1 DETERLab is a resource shared by users around the world. While we do our best to keep experiments separated from each other, we can not provide any guarantee of privacy between projects. If you are concerned about privacy, please make sure you understand how to use UNIX permissions and encrypt your files when they are stored on our main file server. Feel free to contact us if you have any questions. Usage is also governed by the University of Southern California\u2019s \u200bPrivacy Policy .","title":"Usage Policy"},{"location":"support/usage-policy/#deterlab-usage-policy","text":"","title":"Deterlab Usage Policy"},{"location":"support/usage-policy/#be-a-good-citizen","text":"DETERLab is a shared resource. We expect users to be good citizens by not abusing or wasting the resources we make available to them for free. We ask that you: Read our documentation on opening account and running experiments (see links on the left sidebar). Let us know if anything is unclear. Do not share accounts. We will close accounts that we suspect to be shared. Use good passwords. We are a computer security testbed, so please use a strong password. For more details, see our Passwords page . Swap out your experiment when it is finished. People forget to do this all the time. We do have an idle detection mechanism, but it is not perfect and may keep thinking your experiment is active long since you have collected your results, published your paper, and achieved tenure. Please log back in and free up your nodes so that other researchers can use them . Do not abuse the no idle-swap feature. Only turn off idle swap if there is a true need. Take the time to script the setup process for your experiment or create custom disk images. Make good use of disk space. Our goal at DETERLab is to assist you in running experiments. We are happy to provide you with all the storage necessary to accomplish your experimental goals. Also, we maintain nightly backups going back a few weeks. However, we are not a substitute for personal or institutional storage. We are not a backup service. Please keep your important files backed up offsite at your institution. Our main machine room is technically subject to earthquakes, tsunamis, and liquefaction. Take a look at rsync . We are not an archive. Please clean up or move offsite any large log files and traces after your experiment is done and paper is published. This makes sure that storage is available to researchers who are actively using the testbed. Keep things tidy. Remove unused custom operating system images. Experts tell us that 22% of custom operating system images are never used even once. Why keep that around on disk so that it is backed up day after day? It slows down our backups and crash recovery disk checks, and takes resources away from other researchers. If you do need more space, just contact us. Quota limits and housekeeping requests allows us to have extra space available to allocate to you when you need it. Let us know if you are done with your project. - We'll clean it up for you! Talk to us if you need something - we are here to help you. We often can provide useful suggestions about running experiments on the testbed.","title":"Be a good citizen"},{"location":"support/usage-policy/#first-come-first-served-sort-of","text":"We don't have an advanced scheduler. Unfortunately, our software does not support advance reservations. But we are always willing to work with users to help them acquire nodes they need on time, and then help them hold those nodes as long as they need them. Sometimes this involves blocking the nodes out for a period of time. Other times this involves swapping the experiment in the night before the demo, when most nodes are free. Please submit a ticket if you need a reservation and we will work with you. We may swap out idle experiments. We audit the testbed for idle experiments. When the testbed is close to full we will ping the users with long-running, idle experiments and ask if they can be swapped out. If no response is received within 8 hours, we will swap out the experiment. Note that you may prevent this by promptly replying to our email and telling us you need the nodes. Very rarely, we may restrict use to a part of the testbed. There are times when a large-scale demonstration requires us to block off a part of our testbed. At these times, you will see reduced node availability but your swapped in experiments will not be disturbed. We will inform you about these planned events at least two weeks ahead, via news items on DETERLab's web page. Watch out for downtimes. We have regular weekly downtimes where we reserve the right to perform service-interrupting work on the testbed (most weeks we sail right through without any noticeable interruption of service) and sometimes special downtimes are required. We usually give a few days notice before the special downtimes. Keep an eye on the DETERLab News Page for notices.","title":"First come, first served. Sort of\u2026"},{"location":"support/usage-policy/#privacy","text":"DETERLab is a resource shared by users around the world. While we do our best to keep experiments separated from each other, we can not provide any guarantee of privacy between projects. If you are concerned about privacy, please make sure you understand how to use UNIX permissions and encrypt your files when they are stored on our main file server. Feel free to contact us if you have any questions. Usage is also governed by the University of Southern California\u2019s \u200bPrivacy Policy .","title":"Privacy"},{"location":"tools/","text":"DETERLab Tools \uf0c1 This page links to information on useful tools available for DETERLab: Internet Atlas Tool","title":"DETERLab Tools"},{"location":"tools/#deterlab-tools","text":"This page links to information on useful tools available for DETERLab: Internet Atlas Tool","title":"DETERLab Tools"},{"location":"tools/internet-atlas/","text":"Internet Atlas to NS2 \uf0c1 Internet Atlas to NS2 is a continuation of Internet Atlas , which provides a geographically anchored representation of the physical Internet including (i) nodes (e.g., hosting facilities and data centers), (ii) conduits/links that connect these nodes, and (iii) relevant meta data (e.g., source provenance). This tool adds the feature of extrapolating the generated map to a Network Simulator 2 topology and allows you to customize it by filtering the topology based on geo-coordinates. The customized map of network topology can be converted into a Network Simulator script that can then be run on an NS engine. Download the Code \uf0c1 To use this tool, (obtain the source code from /deterlab/library/geocoder) and copy it to your local workstation. Then following these instructions: Prerequisites \uf0c1 Python above version 2.7. Note: If you are using Python 3, you will need to convert the syntax of the geocoding.py script and troubleshoot any bugs the conversion may introduce. pip (to install the required package) Install the geocoder Python package via pip: sudo pip install geocoder How To \uf0c1 Run the python script \uf0c1 Run the geocoding python script. You only need to run this once - if you see geocoded.json in the source directory, you may skip this step. python geocoding.py Using the form \uf0c1 Run python's small server locally to host the Internet Atlas form, fill out the form to generate the desired map and then download the .NS file of your new topology: Run the following from the montage directory: python -m SimpleHTTPServer & Launch a browser (preferably a recent version of Google Chrome, Firefox, or Safari). In a browser window, go to: http://localhost:8000/home.html Specify any of the parameters of your choice in the web page. Remember to include the minus sign (-) if applicable. For example, to target nodes North and East of Tucson, AZ, you would enter 32.2217 in the North field and -110.9265 in the East field. Click Generate Map to get a preview of the topology. The above parameters may generate a map such as this: Once you are satisfied with the topology, click Generate NS File to download the corresponding .NS file. Further details \uf0c1 Internet Atlas is a visualization and analysis portal for diverse Internet measurement data. Initial ventures include but are not limited to: R. Durairajan, S. Ghosh, X. Tang, P. Barford, and B. Eriksson. Internet Atlas: A Geographic Database of the Internet. In Proceedings of ACM HotPlanet, 2013. R. Durairajan, P. Barford, J.Sommers and W. Willinger. InterTubes: A Study of the US Long-haul Fiber-optic Infrastructure. In Proceedings of ACM SIGCOMM, 2015.","title":"Internet Atlas to NS2"},{"location":"tools/internet-atlas/#internet-atlas-to-ns2","text":"Internet Atlas to NS2 is a continuation of Internet Atlas , which provides a geographically anchored representation of the physical Internet including (i) nodes (e.g., hosting facilities and data centers), (ii) conduits/links that connect these nodes, and (iii) relevant meta data (e.g., source provenance). This tool adds the feature of extrapolating the generated map to a Network Simulator 2 topology and allows you to customize it by filtering the topology based on geo-coordinates. The customized map of network topology can be converted into a Network Simulator script that can then be run on an NS engine.","title":"Internet Atlas to NS2"},{"location":"tools/internet-atlas/#download-the-code","text":"To use this tool, (obtain the source code from /deterlab/library/geocoder) and copy it to your local workstation. Then following these instructions:","title":"Download the Code"},{"location":"tools/internet-atlas/#prerequisites","text":"Python above version 2.7. Note: If you are using Python 3, you will need to convert the syntax of the geocoding.py script and troubleshoot any bugs the conversion may introduce. pip (to install the required package) Install the geocoder Python package via pip: sudo pip install geocoder","title":"Prerequisites"},{"location":"tools/internet-atlas/#how-to","text":"","title":"How To"},{"location":"tools/internet-atlas/#run-the-python-script","text":"Run the geocoding python script. You only need to run this once - if you see geocoded.json in the source directory, you may skip this step. python geocoding.py","title":"Run the python script"},{"location":"tools/internet-atlas/#using-the-form","text":"Run python's small server locally to host the Internet Atlas form, fill out the form to generate the desired map and then download the .NS file of your new topology: Run the following from the montage directory: python -m SimpleHTTPServer & Launch a browser (preferably a recent version of Google Chrome, Firefox, or Safari). In a browser window, go to: http://localhost:8000/home.html Specify any of the parameters of your choice in the web page. Remember to include the minus sign (-) if applicable. For example, to target nodes North and East of Tucson, AZ, you would enter 32.2217 in the North field and -110.9265 in the East field. Click Generate Map to get a preview of the topology. The above parameters may generate a map such as this: Once you are satisfied with the topology, click Generate NS File to download the corresponding .NS file.","title":"Using the form"},{"location":"tools/internet-atlas/#further-details","text":"Internet Atlas is a visualization and analysis portal for diverse Internet measurement data. Initial ventures include but are not limited to: R. Durairajan, S. Ghosh, X. Tang, P. Barford, and B. Eriksson. Internet Atlas: A Geographic Database of the Internet. In Proceedings of ACM HotPlanet, 2013. R. Durairajan, P. Barford, J.Sommers and W. Willinger. InterTubes: A Study of the US Long-haul Fiber-optic Infrastructure. In Proceedings of ACM SIGCOMM, 2015.","title":"Further details"},{"location":"traffic/manual/","text":"Manual traffic generation \uf0c1 There are many tools that can be used to generate traffic. Below we give an overview of some frequently used tools. This list is not exhaustive. If you discover a useful tool that is not on the list, please let us know. Ping traffic \uf0c1 To just generate any traffic between two nodes, A and B, you can type on node A: ping B This will generate one packet per second from A to B, and B will then send one-packet reply to A. If you want higher-intensity traffic try sudo ping -f B This will generate a flood of packets between A and B. Steady, simple TCP or UDP traffic \uf0c1 To generate steady, simple traffic between A and B, where A just sends lots of data to B you can use iperf program. You may need to install this program on your nodes by typing sudo apt install iperf . On node B type iperf -s and on node A type iperf -c B There are many different parameters you can set for this program, such as length of running, how many parallel flows to create, how much bandwidth to use per flow, which transport protocol to use and if the traffic is unidirectional or bidirectional. Type man iperf to learn about these options. Complex, TCP traffic \uf0c1 You can use Mimic tool to generate complex TCP traffic, which is congestion-responsive. For example you can use this tool to generate flows that are not filling up the whole bandwidth, but rather pausing between sending some traffic chunks to emulate server and user think time. You can use Mimic tool to generate made-up traffic (similar to iperf use) or you can use it to extract data from a pcap trace and replay it in your experiment. While this tool is more complex than iperf it can also generate more complex and realistic traffic scenarios. Clone the tool from https://github.com/STEELISI/mimic into your home directory on users.deterlab.net , then go on to your experimental nodes to compile and use it. Web traffic \uf0c1 You can generate Web traffic between A and B, by running a web server on B and requesting files from A. Here are the necessary steps: On B: sudo apt install apache2 Then, manually create some files in /var/www/html on B. On A: wget B/one-of-files-you-created The above will just generate one request for one file. If you want to continuously request files from B you can do the following 1. store file names you created into a file, e.g., list.txt 2. on A create `run.sh` with the following code: while : do for file in `cat list.txt` ; do wget B/$file & sleep 1 done done Then run the code on A with bash run.sh This script will request one file per second in a forever loop. If you want to request files more aggressively you can use httpperf program on A, instead of writing your own scripts. You will likely have to install it by typing sudo apt install httpperf . Then type man httpperf to learn how to use it. DNS traffic \uf0c1 You can install a DNS server on node B by typing sudo apt install bind9 . Then you can follow steps outlined here to set it up as authoritative server for some domain, e.g., www.example.com . You can use dnsperf tool on A to generate client DNS traffic. Information on how to install and use dnsperf is given here . DDoS traffic \uf0c1 You can use flooder tool to generate DDoS traffic between your nodes. To install it type on your experimental node sudo /share/education/TCPSYNFlood_USC_ISI/install-flooder . After that, type floder --help to see the attack options. Some examples on how to use flooder are here . Scanning traffic \uf0c1 You can use nmap tool to scan a machine on Deterlab. You will likely need to install it by typing sudo apt install nmap . To scan B from A you would type on A nmap B . Type man nmap to learn about many different options that nmap has.","title":"Manual traffic generation"},{"location":"traffic/manual/#manual-traffic-generation","text":"There are many tools that can be used to generate traffic. Below we give an overview of some frequently used tools. This list is not exhaustive. If you discover a useful tool that is not on the list, please let us know.","title":"Manual traffic generation"},{"location":"traffic/manual/#ping-traffic","text":"To just generate any traffic between two nodes, A and B, you can type on node A: ping B This will generate one packet per second from A to B, and B will then send one-packet reply to A. If you want higher-intensity traffic try sudo ping -f B This will generate a flood of packets between A and B.","title":"Ping traffic"},{"location":"traffic/manual/#steady-simple-tcp-or-udp-traffic","text":"To generate steady, simple traffic between A and B, where A just sends lots of data to B you can use iperf program. You may need to install this program on your nodes by typing sudo apt install iperf . On node B type iperf -s and on node A type iperf -c B There are many different parameters you can set for this program, such as length of running, how many parallel flows to create, how much bandwidth to use per flow, which transport protocol to use and if the traffic is unidirectional or bidirectional. Type man iperf to learn about these options.","title":"Steady, simple TCP or UDP traffic"},{"location":"traffic/manual/#complex-tcp-traffic","text":"You can use Mimic tool to generate complex TCP traffic, which is congestion-responsive. For example you can use this tool to generate flows that are not filling up the whole bandwidth, but rather pausing between sending some traffic chunks to emulate server and user think time. You can use Mimic tool to generate made-up traffic (similar to iperf use) or you can use it to extract data from a pcap trace and replay it in your experiment. While this tool is more complex than iperf it can also generate more complex and realistic traffic scenarios. Clone the tool from https://github.com/STEELISI/mimic into your home directory on users.deterlab.net , then go on to your experimental nodes to compile and use it.","title":"Complex, TCP traffic"},{"location":"traffic/manual/#web-traffic","text":"You can generate Web traffic between A and B, by running a web server on B and requesting files from A. Here are the necessary steps: On B: sudo apt install apache2 Then, manually create some files in /var/www/html on B. On A: wget B/one-of-files-you-created The above will just generate one request for one file. If you want to continuously request files from B you can do the following 1. store file names you created into a file, e.g., list.txt 2. on A create `run.sh` with the following code: while : do for file in `cat list.txt` ; do wget B/$file & sleep 1 done done Then run the code on A with bash run.sh This script will request one file per second in a forever loop. If you want to request files more aggressively you can use httpperf program on A, instead of writing your own scripts. You will likely have to install it by typing sudo apt install httpperf . Then type man httpperf to learn how to use it.","title":"Web traffic"},{"location":"traffic/manual/#dns-traffic","text":"You can install a DNS server on node B by typing sudo apt install bind9 . Then you can follow steps outlined here to set it up as authoritative server for some domain, e.g., www.example.com . You can use dnsperf tool on A to generate client DNS traffic. Information on how to install and use dnsperf is given here .","title":"DNS traffic"},{"location":"traffic/manual/#ddos-traffic","text":"You can use flooder tool to generate DDoS traffic between your nodes. To install it type on your experimental node sudo /share/education/TCPSYNFlood_USC_ISI/install-flooder . After that, type floder --help to see the attack options. Some examples on how to use flooder are here .","title":"DDoS traffic"},{"location":"traffic/manual/#scanning-traffic","text":"You can use nmap tool to scan a machine on Deterlab. You will likely need to install it by typing sudo apt install nmap . To scan B from A you would type on A nmap B . Type man nmap to learn about many different options that nmap has.","title":"Scanning traffic"}]}